[{"date": "2014-04-01T01:13:06.297911+00:00", "nick": "HowardwLo", "message": "what happens to images if connection times out too many times?", "links": [], "channel": "scrapy"},
{"date": "2014-04-01T01:13:15.320317+00:00", "nick": "HowardwLo", "message": "does it queue up again towards the end? or just quits ?", "links": [], "channel": "scrapy"},
{"date": "2014-04-01T09:39:56.039779+00:00", "nick": "HowardwLo", "message": "Hello! anyone here?", "links": [], "channel": "scrapy"},
{"date": "2014-04-01T09:40:07.937680+00:00", "nick": "HowardwLo", "message": "I'm using parse with a callback", "links": [], "channel": "scrapy"},
{"date": "2014-04-01T09:41:24.523952+00:00", "nick": "HowardwLo", "message": "Primary parse scrapes a page iwht links, each link is requested and the callback is secondary parse. Secondary parse appends something to a list through request.meta, the list is then yielded by primary parse", "links": [], "channel": "scrapy"},
{"date": "2014-04-01T09:41:35.920719+00:00", "nick": "HowardwLo", "message": "trouble is, the list doesn't yield properly by primary parse :(", "links": [], "channel": "scrapy"},
{"date": "2014-04-01T09:42:32.985818+00:00", "nick": "HowardwLo", "message": "basically, how do i pass something from secondary parse to primary parse?", "links": [], "channel": "scrapy"},
{"date": "2014-04-01T11:14:15.625362+00:00", "nick": "nyov", "message": "HowardwLo: that doesn't work. your primary parse never sees the returns from your secondary parse", "links": [], "channel": "scrapy"},
{"date": "2014-04-01T11:15:37.465027+00:00", "nick": "HowardwLo", "message": "nyov: how would you scrape in that case?", "links": [], "channel": "scrapy"},
{"date": "2014-04-01T11:24:11.596654+00:00", "nick": "nyov", "message": "i thought we've been through that. you'll need to keep state in the spider object somehow, not in the request object", "links": [], "channel": "scrapy"},
{"date": "2014-04-01T11:25:37.915438+00:00", "nick": "nyov", "message": "how I would do that? not sure, never needed to before. probably would look for another solution first, though", "links": [], "channel": "scrapy"},
{"date": "2014-04-01T12:00:31.238145+00:00", "nick": "ananana", "message": "HowardwLo: can't you send a request back to \"primary parse\" from \"secondary parse\" and send what you need through request.meta? I don't know your use case precisely, but...", "links": [], "channel": "scrapy"},
{"date": "2014-04-01T12:08:22.393913+00:00", "nick": "toothrot", "message": "i've used a wrapper that takes a request and returns a deferred that fires when the request is done and use DeferredList to collect the output when needing to collecting output like that", "links": [], "channel": "scrapy"},
{"date": "2014-04-01T12:09:03.324238+00:00", "nick": "toothrot", "message": "meta isn't enough because you don't know when you're done", "links": [], "channel": "scrapy"},
{"date": "2014-04-01T12:09:30.483671+00:00", "nick": "HowardwLo", "message": "i also go through multiple primary parses", "links": [], "channel": "scrapy"},
{"date": "2014-04-01T12:10:16.915698+00:00", "nick": "HowardwLo", "message": "its a category page, that has a list of products. Primary parse looks for the product links. Secondary parse handles the callback and finds the product ID", "links": [], "channel": "scrapy"},
{"date": "2014-04-01T12:10:43.981292+00:00", "nick": "toothrot", "message": "why do you want to collect them in a list?", "links": [], "channel": "scrapy"},
{"date": "2014-04-01T12:10:44.901110+00:00", "nick": "HowardwLo", "message": "I'm trying to make json [{'category_name' : [\"product1\", \"product2\"..]}, {\u2026} ]", "links": [], "channel": "scrapy"},
{"date": "2014-04-01T12:11:14.533939+00:00", "nick": "HowardwLo", "message": "im trying to scrape a m2m relationship :|", "links": [], "channel": "scrapy"},
{"date": "2014-04-01T12:11:46.590610+00:00", "nick": "toothrot", "message": "you could use a database, yield items from your outermost parse", "links": [], "channel": "scrapy"},
{"date": "2014-04-01T12:13:12.484678+00:00", "nick": "HowardwLo", "message": "you mean save the list in the database", "links": [], "channel": "scrapy"},
{"date": "2014-04-01T12:13:13.953072+00:00", "nick": "toothrot", "message": "or just build up a dict really", "links": [], "channel": "scrapy"},
{"date": "2014-04-01T12:13:26.677320+00:00", "nick": "HowardwLo", "message": "thats actually whatim trying to do...", "links": [], "channel": "scrapy"},
{"date": "2014-04-01T12:13:29.809055+00:00", "nick": "toothrot", "message": "doesn't sound like you don't need to make decisions based on these eitems", "links": [], "channel": "scrapy"},
{"date": "2014-04-01T12:13:55.895267+00:00", "nick": "toothrot", "message": "self.the_dict['categoryname'].append('p1)", "links": [], "channel": "scrapy"},
{"date": "2014-04-01T12:14:00.095062+00:00", "nick": "toothrot", "message": "woops", "links": [], "channel": "scrapy"},
{"date": "2014-04-01T12:14:09.465524+00:00", "nick": "toothrot", "message": "self.the_dict = defaultdict(list)", "links": [], "channel": "scrapy"},
{"date": "2014-04-01T12:14:15.245064+00:00", "nick": "HowardwLo", "message": "have the_dict under the spider itself and outside of the parses?", "links": [], "channel": "scrapy"},
{"date": "2014-04-01T12:14:36.945706+00:00", "nick": "toothrot", "message": "then you could json.dumps it at the end of the crawl", "links": [], "channel": "scrapy"},
{"date": "2014-04-01T12:14:50.463723+00:00", "nick": "toothrot", "message": "(of course if you can't hold it all in memory you'll have to use a database)", "links": [], "channel": "scrapy"},
{"date": "2014-04-01T12:15:03.946943+00:00", "nick": "toothrot", "message": "well, or write them out to disk in some other way", "links": [], "channel": "scrapy"},
{"date": "2014-04-01T12:15:31.556795+00:00", "nick": "HowardwLo", "message": "what would you recommend", "links": [], "channel": "scrapy"},
{"date": "2014-04-01T12:15:43.194367+00:00", "nick": "toothrot", "message": "if you don't want a single dict, you can use meta to pass one down at the beginning of the crawl or at each primary", "links": [], "channel": "scrapy"},
{"date": "2014-04-01T12:15:55.695074+00:00", "nick": "toothrot", "message": "i don't know what your goal is", "links": [], "channel": "scrapy"},
{"date": "2014-04-01T12:21:14.512152+00:00", "nick": "HowardwLo", "message": "you know e commerce sites have those list views?", "links": [], "channel": "scrapy"},
{"date": "2014-04-01T12:21:16.616515+00:00", "nick": "HowardwLo", "message": "with pagination", "links": [], "channel": "scrapy"},
{"date": "2014-04-01T12:21:22.411872+00:00", "nick": "HowardwLo", "message": "you can filter by categories", "links": [], "channel": "scrapy"},
{"date": "2014-04-01T12:21:51.140347+00:00", "nick": "HowardwLo", "message": "i want to generate a filter list, meaning which products belong to which filters", "links": [], "channel": "scrapy"},
{"date": "2014-04-01T12:22:07.052352+00:00", "nick": "HowardwLo", "message": "my idea was to have json list of dicts", "links": [], "channel": "scrapy"},
{"date": "2014-04-01T12:22:26.490612+00:00", "nick": "HowardwLo", "message": "the dicts would be {\"filter name\" : [\"product_id\" \u2026 ] }", "links": [], "channel": "scrapy"},
{"date": "2014-04-01T12:22:38.561861+00:00", "nick": "HowardwLo", "message": "trouble is, product_ids are only on the product pages themselves", "links": [], "channel": "scrapy"},
{"date": "2014-04-01T12:52:09.097492+00:00", "nick": "bahamas", "message": "HowardwLo: have you had any problems so far scraping amazon? I know they have a team dedicated to prevent bots from scraping", "links": [], "channel": "scrapy"},
{"date": "2014-04-01T13:08:17.924220+00:00", "nick": "nyov", "message": "amazon is so bogus...", "links": [], "channel": "scrapy"},
{"date": "2014-04-01T13:09:59.150374+00:00", "nick": "nyov", "message": "now I understand why one wouldn't want bots to waste your site ressources, when you provide a fine api for the job", "links": [], "channel": "scrapy"},
{"date": "2014-04-01T13:10:40.551666+00:00", "nick": "nyov", "message": "but then the ToS for that api are pretty heavy, almost unacceptable if you don't have a lawyer to check them over", "links": [], "channel": "scrapy"},
{"date": "2014-04-01T13:11:23.544628+00:00", "nick": "nyov", "message": "like the requirement to provide your source code to them for 'inspection' upon request...", "links": [], "channel": "scrapy"},
{"date": "2014-04-01T13:21:59.743369+00:00", "nick": "nyov", "message": "HowardwLo: I thought about your issue a bit, and I'd use a 'recursive' parse method, funnel all the responses through the same callback, which has my queue/unqueue state logic", "links": [], "channel": "scrapy"},
{"date": "2014-04-01T13:22:31.463248+00:00", "nick": "gilbo", "message": "bahamas: are there any legal terms they have about crawlers or just try to stop them with technical means like captchas?", "links": [], "channel": "scrapy"},
{"date": "2014-04-01T13:22:38.423932+00:00", "nick": "gilbo", "message": "I'm not sure", "links": [], "channel": "scrapy"},
{"date": "2014-04-01T13:26:12.429972+00:00", "nick": "nyov", "message": "what I wouldn't want is having Requests kept around artificially, by references. so only storing the item in a \"queue\" list, and 'throwing' request/response away/get garbage collected", "links": [], "channel": "scrapy"},
{"date": "2014-04-01T13:27:00.795326+00:00", "nick": "nyov", "message": "and then it'd be important to me to know a 'win' condition, when all the responses for an item have come in, so I can free up the memory ASAP by returning a finished item", "links": [], "channel": "scrapy"},
{"date": "2014-04-01T13:54:03.075450+00:00", "nick": "bahamas", "message": "gilbo: I have no idea. I just know they have a team handling this", "links": [], "channel": "scrapy"},
{"date": "2014-04-03T08:25:33.253771+00:00", "nick": "crawlerbob", "message": "Hi. I'm trying to deploy a project to scrapyd server but am getting a permission denied in /var/lib/scrapyd/eggs/project_name when packing. Anyone know how to solve this?", "links": [], "channel": "scrapy"},
{"date": "2014-04-03T08:28:16.001489+00:00", "nick": "crawlerbob", "message": "I'm sort of a noob so please go easy on me", "links": [], "channel": "scrapy"},
{"date": "2014-04-03T08:56:57.568686+00:00", "nick": "nyov", "message": "the paths below (including) /var/lib/scrapyd/ should belong to scrapyd:nogroup, or any other user you have scrapyd installed to run as", "links": [], "channel": "scrapy"},
{"date": "2014-04-03T08:58:18.511101+00:00", "nick": "nyov", "message": "so you need to change the ownership of the directory to allow scrapyd to write to it", "links": [], "channel": "scrapy"},
{"date": "2014-04-03T09:02:54.125017+00:00", "nick": "crawlerbob", "message": "I have these rights settings: root@precise32:/var/lib/scrapyd/eggs# ll total 8 drwxr-xr-x 2 scrapy nogroup 4096 Aug 27  2013 ./ drwxr-xr-x 5 scrapy nogroup 4096 Feb 27 10:33 ../", "links": [], "channel": "scrapy"},
{"date": "2014-04-03T09:03:50.591996+00:00", "nick": "crawlerbob", "message": "how can i see what user I have scrapyd installed to run as?", "links": [], "channel": "scrapy"},
{"date": "2014-04-03T09:04:10.563662+00:00", "nick": "nyov", "message": "make sure anything else below /var/lib/scrapyd/ also has these, including scrapyd itself", "links": [], "channel": "scrapy"},
{"date": "2014-04-03T09:04:26.968244+00:00", "nick": "nyov", "message": "did you install a debian/ubuntu package?", "links": [], "channel": "scrapy"},
{"date": "2014-04-03T09:06:58.201215+00:00", "nick": "nyov", "message": "and the user/group scrapyd runs as is defined in the upstart file", "links": [], "channel": "scrapy"},
{"date": "2014-04-03T09:07:07.800643+00:00", "nick": "crawlerbob", "message": "Yes, it''s from the scrapy apt repo", "links": [], "channel": "scrapy"},
{"date": "2014-04-03T09:08:20.152361+00:00", "nick": "nyov", "message": "i'm not sure where ubuntu has it's upstart files. presumably in /etc/init.d/ also, though", "links": [], "channel": "scrapy"},
{"date": "2014-04-03T09:12:42.524737+00:00", "nick": "crawlerbob", "message": "Hey, I just found the solution (or a great guy at the office did). Scrapyd was running as vagrant. Now it seems to be working", "links": [], "channel": "scrapy"},
{"date": "2014-04-03T09:12:49.476742+00:00", "nick": "crawlerbob", "message": "thank you guys!", "links": [], "channel": "scrapy"},
{"date": "2014-04-03T09:12:58.992675+00:00", "nick": "crawlerbob", "message": "Have a good day", "links": [], "channel": "scrapy"},
{"date": "2014-04-03T11:12:17.418211+00:00", "nick": "crawlerbob", "message": "Does anyone know if it is possible to run spiders that use a webdriver(PhantomJS) via scrapyd?", "links": [], "channel": "scrapy"},
{"date": "2014-04-03T12:01:17.258702+00:00", "nick": "lawyer", "message": "hey has anyone worked on the training part in scrapely ?", "links": [], "channel": "scrapy"},
{"date": "2014-04-03T14:15:34.793103+00:00", "nick": "crawlerbob", "message": "Does anyone know if it is possible to run spiders that use a webdriver(PhantomJS) via scrapyd?", "links": [], "channel": "scrapy"},
{"date": "2014-04-03T18:41:26.166638+00:00", "nick": "ayoub", "message": "hello", "links": [], "channel": "scrapy"},
{"date": "2014-04-03T18:41:59.762796+00:00", "nick": "ayoub", "message": "can weuse scrapy on java application", "links": [], "channel": "scrapy"},
{"date": "2014-04-03T18:56:44.843655+00:00", "nick": "ayoub", "message": "some here ?", "links": [], "channel": "scrapy"},
{"date": "2014-04-03T19:32:54.222606+00:00", "nick": "ayoub", "message": "well", "links": [], "channel": "scrapy"},
{"date": "2014-04-03T19:32:58.825464+00:00", "nick": "ayoub", "message": "well well", "links": [], "channel": "scrapy"},
{"date": "2014-04-03T19:33:11.797561+00:00", "nick": "ayoub", "message": "i think we can scrap java application :/", "links": [], "channel": "scrapy"},
{"date": "2014-04-04T07:58:03.611046+00:00", "nick": "Crawlerman", "message": "Does anyone know if I can use webdriver(PhantomJS) spiders through scrapyd? And if so, how?", "links": [], "channel": "scrapy"},
{"date": "2014-04-04T08:00:20.696461+00:00", "nick": "Crawlerman", "message": "I can schedule jobs with these spiders without any errors but when I look at the log in the web interface it says resource not found", "links": [], "channel": "scrapy"},
{"date": "2014-04-04T08:04:36.822567+00:00", "nick": "Crawlerman", "message": "or nore specifically it says: No Such Resource  File not found.", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T06:21:15.857045+00:00", "nick": "HowardwLo", "message": "can you declare IMAGES_STORE per spider?", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T06:24:46.157199+00:00", "nick": "nyov", "message": "HowardwLo: you can (possibly) use the deprecated `from scrapy.conf import settings` settings for that", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T06:25:32.286128+00:00", "nick": "nyov", "message": "I use that for doing a `settings.overrides['COOKIES_ENABLED']` in the spider's __init__", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T06:26:09.566620+00:00", "nick": "nyov", "message": "because it needs to be as early as possible afaik or it wont work", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T06:28:16.363628+00:00", "nick": "nyov", "message": "(didn't work with crawler.settings as I recall)", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T06:35:07.798790+00:00", "nick": "HowardwLo", "message": "nyov: so it would be in the __init__ ?", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T06:38:53.929208+00:00", "nick": "nyov", "message": "HowardwLo: yes, spider_opened was also too late as I recall. and the changed setting won't appear in the spider startup output", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T06:39:05.915408+00:00", "nick": "nyov", "message": "so I'd say it's a hack", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T06:39:13.761614+00:00", "nick": "HowardwLo", "message": "nyov: blah, if i have to change in __init__ i might as well just change in settings", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T06:40:04.273274+00:00", "nick": "nyov", "message": "well, the spider's __init__() setting. not the global init file", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T06:40:14.805506+00:00", "nick": "nyov", "message": "s/setting/method/", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T07:16:51.257088+00:00", "nick": "HowardwLo", "message": "oh", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T07:16:56.637085+00:00", "nick": "HowardwLo", "message": "ah ok. cool thanks", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T07:23:58.898778+00:00", "nick": "HowardwLo", "message": "nyov: https://dpaste.de/ipj5 ?", "links": ["https://dpaste.de/ipj5"], "channel": "scrapy"},
{"date": "2014-04-05T07:28:21.446315+00:00", "nick": "nyov", "message": "HowardwLo: no", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T07:28:26.386789+00:00", "nick": "nyov", "message": "settings.overrides", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T07:29:38.519726+00:00", "nick": "nyov", "message": "https://dpaste.de/qUYm", "links": ["https://dpaste.de/qUYm"], "channel": "scrapy"},
{"date": "2014-04-05T07:34:04.045705+00:00", "nick": "HowardwLo", "message": "AH ok", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T07:34:12.832226+00:00", "nick": "HowardwLo", "message": "and just stick that at the top before all the parses and whatnot?", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T07:35:16.104070+00:00", "nick": "nyov", "message": "where __init__ is inside your code doesn't matter. it's executed when the class is instantiated", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T07:36:47.772357+00:00", "nick": "nyov", "message": "though since this is still a global override of settings, you should put it in your other spiders as well, to change location \"back\" before they are run", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T07:37:06.302905+00:00", "nick": "nyov", "message": "at least as far as I understand it", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T07:37:25.311681+00:00", "nick": "nyov", "message": "if they never run in parallel it doesn't matter", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T07:38:09.240278+00:00", "nick": "HowardwLo", "message": "ya i only know how to run one spider at a time", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T07:38:30.073347+00:00", "nick": "HowardwLo", "message": "i was just tired of having to change settings every time i run it because i wanted to organize where my images were saved", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T08:04:53.295686+00:00", "nick": "HowardwLo", "message": "can't figure this out, keeps saying 'Category' object does not support item assignment. https://dpaste.de/fKcN", "links": ["https://dpaste.de/fKcN"], "channel": "scrapy"},
{"date": "2014-04-05T08:17:28.166688+00:00", "nick": "nyov", "message": "possibly means you're overriding `item` somewhere in between instancing Category() and trying to assign something", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T08:35:05.117427+00:00", "nick": "HowardwLo", "message": "Ah ya, my damn spider is called Category \u2026.jeeze", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T14:43:05.327288+00:00", "nick": "duschendestroyer", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T14:44:28.967740+00:00", "nick": "duschendestroyer", "message": "since scrapy is evented does the thing block when I block in the item pipeline?", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T14:52:23.488658+00:00", "nick": "nyov", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T14:54:46.347617+00:00", "nick": "nyov", "message": "item pipeline is just another middleware in a way", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T14:58:52.179166+00:00", "nick": "duschendestroyer", "message": "so I can use this for flow control? my pipeline pushes items to a queue. but when the queue fills up because the consumer is too slow the push method blocks and therefore limits the crawler to the speed of the consumer.", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T15:04:22.061586+00:00", "nick": "nyov", "message": "well, I would consider blocking the whole system a bad thing. all active connections, database things and whatever else scrapy does. but maybe it works out", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T15:04:55.058246+00:00", "nick": "nyov", "message": "if it blocks too long, connections may time out or other weirdness may happen. haven't ever tested that", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T15:10:32.443114+00:00", "nick": "nyov", "message": "duschendestroyer: ah i think I read you wrong. having a event waiting/blocked as opposed to the whole system is entirely possible for flow control", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T15:12:38.625626+00:00", "nick": "nyov", "message": "so you may want to defer it to a thread in process_item", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T15:18:55.973769+00:00", "nick": "duschendestroyer", "message": "all I want is ensure that items aren't generated faster than I can deal with them", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T15:19:47.543597+00:00", "nick": "duschendestroyer", "message": "because I have to do very expensive post-processing", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T15:24:12.612387+00:00", "nick": "nyov", "message": "twisted is using a thread pool. if you defer your blocking code to a thread, and the threads are all used up, scrapy will wait until one becomes available before processing the next item", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T15:25:41.274840+00:00", "nick": "nyov", "message": "so I'd do the heavy post-processing and item returning in the same thread then.", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T15:29:42.924340+00:00", "nick": "nyov", "message": "duschendestroyer: here's the most simple example how to do this, that I found this quickly: https://github.com/darkrho/scrapy-redis/blob/ma...", "links": ["https://github.com/darkrho/scrapy-redis/blob/master/scrapy_redis/pipelines.py#L25"], "channel": "scrapy"},
{"date": "2014-04-05T15:32:41.318464+00:00", "nick": "duschendestroyer", "message": "this will do", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T16:12:19.068367+00:00", "nick": "duschendestroyer", "message": "is it possible that process_item in a pipeline is called by multiple threads in parallel?", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T18:26:58.946211+00:00", "nick": "chuxy", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T18:28:07.715936+00:00", "nick": "chuxy", "message": "I'm playing with scrapy but I have some doubts, anybody can help?", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T18:30:18.960688+00:00", "nick": "chuxy", "message": "anyone?", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T18:30:34.395663+00:00", "nick": "chuxy", "message": "why this rule", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T18:30:35.330251+00:00", "nick": "chuxy", "message": "Rule(SgmlLinkExtractor(allow='r\\.htm\\?ot=2\\&op=2014*', deny='&order',), callback='parse_item', follow=False),", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T18:30:56.438580+00:00", "nick": "chuxy", "message": "is matching this url? r.htm?op=2015&ot=2", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T18:32:21.504821+00:00", "nick": "nyov", "message": "maybe the url you get is actually parameter re-ordered, so it would match", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T18:34:13.900745+00:00", "nick": "chuxy", "message": "but i said in allow -> op=2014", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T18:34:20.220060+00:00", "nick": "chuxy", "message": "why is 2015 matching then?", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T18:34:50.530348+00:00", "nick": "nyov", "message": "ah", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T18:36:25.864590+00:00", "nick": "chuxy", "message": "ok, without the ending * works, but i'm not sure why ;)", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T18:37:35.743347+00:00", "nick": "nyov", "message": "http://www.regular-expressions.info/repeat.html > \"The asterisk or star tells the engine to attempt to match the preceding token zero or more times.\"", "links": ["http://www.regular-expressions.info/repeat.html"], "channel": "scrapy"},
{"date": "2014-04-05T18:37:53.218978+00:00", "nick": "chuxy", "message": "so that's why", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T18:37:54.376410+00:00", "nick": "nyov", "message": "so 4 in 2014 _may_ exists according to the regex", "links": [], "channel": "scrapy"},
{"date": "2014-04-05T18:38:02.866938+00:00", "nick": "chuxy", "message": "thanks, now i get it", "links": [], "channel": "scrapy"},
{"date": "2014-04-06T02:53:48.698815+00:00", "nick": "JoeHazzers", "message": "is it possible to pass a \"scope\" for add_xpath on an ItemLoader?", "links": [], "channel": "scrapy"},
{"date": "2014-04-06T07:36:33.121107+00:00", "nick": "HowardwLo", "message": "nyov: hey, you recall the settings.overrides for IMAGE_STORE? I'm getting a permission denied even though the folder has open permissions. any idea how to fix?", "links": [], "channel": "scrapy"},
{"date": "2014-04-06T07:37:32.482660+00:00", "nick": "HowardwLo", "message": "oh wait\u2026.its trying to make the folder in the wrong place\u2026.nvm :D", "links": [], "channel": "scrapy"},
{"date": "2014-04-06T09:50:22.543515+00:00", "nick": "schux", "message": "hi, 9'm playing with scrapy shell and I have a problem", "links": [], "channel": "scrapy"},
{"date": "2014-04-06T09:51:05.175615+00:00", "nick": "schux", "message": "I have this page on the sell, http://rcdb.com/760.htm and when I run len(sel.xpath('//table')) it return 7, and in that page there are only two tables", "links": ["http://rcdb.com/760.htm"], "channel": "scrapy"},
{"date": "2014-04-06T09:52:35.282638+00:00", "nick": "schux", "message": "any idea?", "links": [], "channel": "scrapy"},
{"date": "2014-04-06T09:55:30.106548+00:00", "nick": "nyov", "message": "JoeHazzers: somewhat", "links": [], "channel": "scrapy"},
{"date": "2014-04-06T09:56:11.869101+00:00", "nick": "schux", "message": "about the question i asked before:", "links": [], "channel": "scrapy"},
{"date": "2014-04-06T09:56:18.117909+00:00", "nick": "schux", "message": "I have this page on the sell, http://rcdb.com/760.htm and when I run len(sel.xpath('//table')) it return 7, and in that page there are only two tables", "links": ["http://rcdb.com/760.htm"], "channel": "scrapy"},
{"date": "2014-04-06T09:56:59.304698+00:00", "nick": "schux", "message": "if i run this on the chrome dev tools console $x(\"//table\").length returns me 2, that is what I've expected", "links": [], "channel": "scrapy"},
{"date": "2014-04-06T09:57:41.793202+00:00", "nick": "nyov", "message": "schux: it's not 'fixing' up the html the same in both tools", "links": [], "channel": "scrapy"},
{"date": "2014-04-06T09:58:07.220629+00:00", "nick": "nyov", "message": "e.g. some unclosed tag might get closed by ff/chrome 'autocorrection', but not by lxml", "links": [], "channel": "scrapy"},
{"date": "2014-04-06T09:58:25.636973+00:00", "nick": "nyov", "message": "try your xpath in scrapy shell", "links": [], "channel": "scrapy"},
{"date": "2014-04-06T09:59:08.903719+00:00", "nick": "schux", "message": "i'm in the scrappy shell", "links": [], "channel": "scrapy"},
{"date": "2014-04-06T10:00:05.486837+00:00", "nick": "schux", "message": "and if I run w3c validator against that url there isn't any errors", "links": [], "channel": "scrapy"},
{"date": "2014-04-06T10:00:08.005170+00:00", "nick": "schux", "message": "strange...", "links": [], "channel": "scrapy"},
{"date": "2014-04-06T10:00:16.494310+00:00", "nick": "nyov", "message": "JoeHazzers: see here https://github.com/scrapy/scrapy/issues/568#iss... or alternatively here https://github.com/scrapy/scrapy/pull/378/files", "links": ["https://github.com/scrapy/scrapy/issues/568#issuecomment-34903613", "https://github.com/scrapy/scrapy/pull/378/files"], "channel": "scrapy"},
{"date": "2014-04-06T10:00:48.194799+00:00", "nick": "nyov", "message": "schux: i'll take a look", "links": [], "channel": "scrapy"},
{"date": "2014-04-06T10:01:57.022973+00:00", "nick": "schux", "message": "ok thanks", "links": [], "channel": "scrapy"},
{"date": "2014-04-06T10:04:52.796731+00:00", "nick": "nyov", "message": "schux: looks like scrapy gets a different page. must be the server deciding to Vary by User-Agent", "links": [], "channel": "scrapy"},
{"date": "2014-04-06T10:05:12.169758+00:00", "nick": "nyov", "message": "do a view(response) in the shell and see the page in a browser", "links": [], "channel": "scrapy"},
{"date": "2014-04-06T10:07:02.464653+00:00", "nick": "schux", "message": "thats the fist thing I thought", "links": [], "channel": "scrapy"},
{"date": "2014-04-06T10:07:11.534236+00:00", "nick": "schux", "message": "that page has autolanguage detection", "links": [], "channel": "scrapy"},
{"date": "2014-04-06T10:07:27.358859+00:00", "nick": "schux", "message": "if i d oa view(response) on the sell, i see the same webpage", "links": [], "channel": "scrapy"},
{"date": "2014-04-06T10:07:40.068270+00:00", "nick": "schux", "message": "that's that webpage on english language", "links": [], "channel": "scrapy"},
{"date": "2014-04-06T10:08:37.181059+00:00", "nick": "nyov", "message": "its all english here. but the page from scrapy looks vastly different as well, here", "links": [], "channel": "scrapy"},
{"date": "2014-04-06T10:08:45.568542+00:00", "nick": "nyov", "message": "more tables ;)", "links": [], "channel": "scrapy"},
{"date": "2014-04-06T10:09:08.164341+00:00", "nick": "schux", "message": "mmm", "links": [], "channel": "scrapy"},
{"date": "2014-04-06T10:09:19.555293+00:00", "nick": "schux", "message": "how can i see the response html?", "links": [], "channel": "scrapy"},
{"date": "2014-04-06T10:10:07.821574+00:00", "nick": "nyov", "message": "print(response.body)", "links": [], "channel": "scrapy"},
{"date": "2014-04-06T10:10:32.020063+00:00", "nick": "schux", "message": "mmm thats the older version, you are right ;)", "links": [], "channel": "scrapy"},
{"date": "2014-04-06T10:10:48.942601+00:00", "nick": "schux", "message": "how is that even posible !!!! jeje thanks nyov", "links": [], "channel": "scrapy"},
{"date": "2014-04-06T10:10:59.884021+00:00", "nick": "nyov", "message": "no problem", "links": [], "channel": "scrapy"},
{"date": "2014-04-06T10:11:37.882652+00:00", "nick": "schux", "message": "any idea how can i force the new version on scrapy shell?", "links": [], "channel": "scrapy"},
{"date": "2014-04-06T10:12:01.765291+00:00", "nick": "nyov", "message": "you might have to fake useragent", "links": [], "channel": "scrapy"},
{"date": "2014-04-06T10:22:08.458788+00:00", "nick": "schux", "message": ">>> len(sel.xpath('//table[@id=\"statTable\"]')) now prints 1, so it's perfect", "links": [], "channel": "scrapy"},
{"date": "2014-04-06T10:22:25.062613+00:00", "nick": "schux", "message": "nyov: if you came to spain you have a free bear ;)", "links": [], "channel": "scrapy"},
{"date": "2014-04-06T10:22:43.530197+00:00", "nick": "schux", "message": "whait, beer", "links": [], "channel": "scrapy"},
{"date": "2014-04-06T10:23:55.219186+00:00", "nick": "nyov", "message": "hahaha, maybe it'll happen some time ;)", "links": [], "channel": "scrapy"},
{"date": "2014-04-06T10:25:56.595142+00:00", "nick": "schux", "message": "i'm new to python and i wasted one hour thinking it was a python/scrapy/xpath whatever... xD", "links": [], "channel": "scrapy"},
{"date": "2014-04-06T11:20:28.479979+00:00", "nick": "truedon", "message": "I love scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-04-06T11:24:10.016512+00:00", "nick": "truedon", "message": "I'm writing a scraper that needs to run in two parts, I've scraped the list of urls and now I need to process that list. What's the best way to do that?", "links": [], "channel": "scrapy"},
{"date": "2014-04-06T12:12:56.297578+00:00", "nick": "nyov", "message": "truedon: many options. the best way would depend on your circumstances", "links": [], "channel": "scrapy"},
{"date": "2014-04-06T12:14:52.783156+00:00", "nick": "nyov", "message": "but one option is to yield each request from your url list in start_requests() of your spider", "links": [], "channel": "scrapy"},
{"date": "2014-04-06T12:15:22.592608+00:00", "nick": "nyov", "message": "something like https://gist.github.com/nyov/9266443", "links": ["https://gist.github.com/nyov/9266443"], "channel": "scrapy"},
{"date": "2014-04-06T18:15:16.704987+00:00", "nick": "duschendestroyer", "message": "I do log.msg(\"foo\", level=log.WARNING) in my pipeline but it does not show on stdout. log level is info.", "links": [], "channel": "scrapy"},
{"date": "2014-04-07T07:39:29.389630+00:00", "nick": "har777", "message": "Hi", "links": [], "channel": "scrapy"},
{"date": "2014-04-07T07:39:39.835565+00:00", "nick": "har777", "message": "I have a doubt regarding mongo scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-04-07T07:39:59.366765+00:00", "nick": "har777", "message": "I've managed to scrap data recursively.", "links": [], "channel": "scrapy"},
{"date": "2014-04-07T07:40:12.103108+00:00", "nick": "har777", "message": "But only the first item gets inserted into mongo", "links": [], "channel": "scrapy"},
{"date": "2014-04-07T07:40:58.428698+00:00", "nick": "har777", "message": "def __init__(self):", "links": [], "channel": "scrapy"},
{"date": "2014-04-07T07:41:12.042421+00:00", "nick": "har777", "message": "client = MongoClient()", "links": [], "channel": "scrapy"},
{"date": "2014-04-07T07:41:21.138308+00:00", "nick": "har777", "message": "db = client.scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-04-07T07:41:37.064151+00:00", "nick": "har777", "message": "collection = db.my_items", "links": [], "channel": "scrapy"},
{"date": "2014-04-07T07:41:50.068874+00:00", "nick": "har777", "message": "def process_item(self, item, spider):", "links": [], "channel": "scrapy"},
{"date": "2014-04-07T07:42:00.164420+00:00", "nick": "har777", "message": "for items in item:", "links": [], "channel": "scrapy"},
{"date": "2014-04-07T07:42:12.339864+00:00", "nick": "har777", "message": "self.collection.insert(dict(items))", "links": [], "channel": "scrapy"},
{"date": "2014-04-07T07:42:19.783092+00:00", "nick": "har777", "message": "return item", "links": [], "channel": "scrapy"},
{"date": "2014-04-07T07:42:31.307233+00:00", "nick": "har777", "message": "Sorry very new to python.", "links": [], "channel": "scrapy"},
{"date": "2014-04-07T07:52:35.816596+00:00", "nick": "crawlerman", "message": "Hi! Does anyone know if it is possible to run webdriver(PhantomJS) spiders via scrapyd?", "links": [], "channel": "scrapy"},
{"date": "2014-04-07T08:15:05.233999+00:00", "nick": "_vatsalj_", "message": "har777 process_item takes 1 item as argument...", "links": [], "channel": "scrapy"},
{"date": "2014-04-07T08:15:15.366385+00:00", "nick": "_vatsalj_", "message": "not the complete list of items", "links": [], "channel": "scrapy"},
{"date": "2014-04-07T08:15:49.973280+00:00", "nick": "_vatsalj_", "message": "so, you're iterating over the single object you have and that is getting inserted in the DB...", "links": [], "channel": "scrapy"},
{"date": "2014-04-07T12:58:24.376420+00:00", "nick": "nkuttler", "message": "so i need to use selenium to display all links i want to scrape, but i'm not sure at all how to integrate that into the spider class", "links": [], "channel": "scrapy"},
{"date": "2014-04-07T12:58:56.022215+00:00", "nick": "nkuttler", "message": "it seems like i could generate a fake request object and pass that to scrapy.. anybody ever done this?", "links": [], "channel": "scrapy"},
{"date": "2014-04-07T12:59:35.597415+00:00", "nick": "nkuttler", "message": "or actually, if i could just have a callable to populate start_urls...", "links": [], "channel": "scrapy"},
{"date": "2014-04-07T12:59:43.744003+00:00", "nick": "nkuttler", "message": ".. yeah.. i guess i'll just do that..", "links": [], "channel": "scrapy"},
{"date": "2014-04-07T13:28:33.234515+00:00", "nick": "crawlerman", "message": "I usually load the page with the webdriver (webdriver.get(response.url)) in the function parse_start_url, then extract all the links using webdriver.find_elements_by_xpath(). Then I make a list of Request objects (with callback if I wanna parse items) and return it. Scrapy will take care of the rest", "links": [], "channel": "scrapy"},
{"date": "2014-04-07T13:29:24.036983+00:00", "nick": "crawlerman", "message": "Maybe there is a prettier way but it usually works fine for me", "links": [], "channel": "scrapy"},
{"date": "2014-04-07T13:35:03.123043+00:00", "nick": "nkuttler", "message": "crawlerman: thanks, i'll look into that next time", "links": [], "channel": "scrapy"},
{"date": "2014-04-07T16:40:29.300997+00:00", "nick": "nkuttler", "message": "task accomplished. i hate it when i can't polish code to do the right thing, but w/e.. i have all the data", "links": [], "channel": "scrapy"},
{"date": "2014-04-07T19:54:43.485226+00:00", "nick": "pvt_petey", "message": "hi there", "links": [], "channel": "scrapy"},
{"date": "2014-04-07T19:54:53.148619+00:00", "nick": "pvt_petey", "message": "I want to deploy a scrapy project using scrapyd", "links": [], "channel": "scrapy"},
{"date": "2014-04-07T19:55:08.117960+00:00", "nick": "pvt_petey", "message": "how can I specify paths that are relative to the install directory ?", "links": [], "channel": "scrapy"},
{"date": "2014-04-07T19:55:16.780646+00:00", "nick": "pvt_petey", "message": "in the source code", "links": [], "channel": "scrapy"},
{"date": "2014-04-07T19:55:31.119620+00:00", "nick": "pvt_petey", "message": "so that I save say images in the 'right' directory", "links": [], "channel": "scrapy"},
{"date": "2014-04-07T20:23:44.135571+00:00", "nick": "benjabean1", "message": "Hi!", "links": [], "channel": "scrapy"},
{"date": "2014-04-07T20:23:57.920004+00:00", "nick": "benjabean1", "message": "Can scrapy be used to POST data?", "links": [], "channel": "scrapy"},
{"date": "2014-04-07T20:25:38.179755+00:00", "nick": "benjabean1", "message": "nevermind, i think so.", "links": [], "channel": "scrapy"},
{"date": "2014-04-07T22:52:53.957519+00:00", "nick": "xnotte", "message": "anyone has idea why on a page i can get the xpath; \"//p[@class=\"txtmedioazulb\"]/table\" and on scrappy shell, or spider, i just have result with \"//p[@class=\"txtmedioazulb\"]\" ? Table doesn't exist.", "links": [], "channel": "scrapy"},
{"date": "2014-04-07T23:19:13.776404+00:00", "nick": "xnotte", "message": "can anyone help me test live url ? I'll give you the url, just to see if you can get xpath", "links": [], "channel": "scrapy"},
{"date": "2014-04-08T05:36:12.073330+00:00", "nick": "truedon", "message": "Hey guys, I am trying to scrape a page of links and then subsequentially process those. I am trying to use the Rules SGML Link Extractor, I have set the callback to parse_items but it appears I must have parse defined within the class. The examples do not work, how is this done?", "links": [], "channel": "scrapy"},
{"date": "2014-04-08T05:37:29.637404+00:00", "nick": "truedon", "message": "I just found the solution by asking this hehe - I was not extending the CrawlSpider class just the Spider base. Problem solved :)", "links": [], "channel": "scrapy"},
{"date": "2014-04-08T18:01:55.952242+00:00", "nick": "nyov", "message": "wtf? 16WAAAFQC? whats the occasion?", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T21:50:21.338007+00:00", "nick": "timg", "message": "Hi", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T21:50:44.107386+00:00", "nick": "timg", "message": "I'm trying to pip install Scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T21:51:32.320615+00:00", "nick": "timg", "message": "I can't seem to get past an error: http://pastebin.com/YVbH27Xw", "links": ["http://pastebin.com/YVbH27Xw"], "channel": "scrapy"},
{"date": "2014-04-09T21:51:55.303335+00:00", "nick": "timg", "message": "I've installed python-dev", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T21:52:27.455107+00:00", "nick": "timg", "message": "I've installed python-openssl", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T21:52:44.397952+00:00", "nick": "timg", "message": "I even installed python-scrapy to see if that got some dependency I was missing", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T21:52:46.745576+00:00", "nick": "timg", "message": "same error though", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:04:27.011481+00:00", "nick": "remote", "message": "timg: I think this is the key:", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:04:28.678230+00:00", "nick": "remote", "message": "openssl/aes.h: No such file or directory", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:04:36.285638+00:00", "nick": "remote", "message": "you're missing the openssl devel headers", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:05:24.423183+00:00", "nick": "timg", "message": "remote: yes thank you, I ended up googling that and finding out that I needed to apt-get libssl-dev", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:05:39.900863+00:00", "nick": "pvt_petey", "message": "hello", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:05:49.564301+00:00", "nick": "pvt_petey", "message": "I'm trying to deploy scrapy with scrapyd", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:06:12.561591+00:00", "nick": "pvt_petey", "message": "but also within the context of bundling the scraper within a cocoa app", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:06:42.122026+00:00", "nick": "pvt_petey", "message": "is there any way for the scraper to refer to some global settings on where the top level cocoa app expects things ?", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:06:53.273641+00:00", "nick": "timg", "message": "Is there a reason why these things aren't mentioned in the pre-requisites here: http://doc.scrapy.org/en/latest/intro/install.html ?", "links": ["http://doc.scrapy.org/en/latest/intro/install.html"], "channel": "scrapy"},
{"date": "2014-04-09T22:08:02.575020+00:00", "nick": "remote", "message": "OpenSSL. This comes preinstalled in all operating systems except Windows (see Platform specific installation notes)", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:09:11.361372+00:00", "nick": "timg", "message": "I'm not running Windows", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:09:17.592092+00:00", "nick": "timg", "message": "I wonder why I wouldn't have it", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:09:25.437553+00:00", "nick": "remote", "message": "this is not windows specific", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:09:49.675220+00:00", "nick": "remote", "message": "it doesn't come pre-installed in all operating systems except Windows, but it's still mentionned in the requirements", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:10:30.577230+00:00", "nick": "remote", "message": "i agree it could be made clearer", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:10:35.064618+00:00", "nick": "timg", "message": "oh. the thing you just pasted seems to say that the prerequisite is preinstalled on all operating systems except Windows", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:11:27.363726+00:00", "nick": "timg", "message": "and I've finally got it installed", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:11:35.264378+00:00", "nick": "timg", "message": "thank you", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:23:18.756576+00:00", "nick": "remote", "message": "pvt_pete_: I read a bit of the conversation you had in #python", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:23:48.010987+00:00", "nick": "remote", "message": "I don't know anything about cocoa but what's the purpose of deploying the egg with a cocoa app?", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:24:03.718335+00:00", "nick": "remote", "message": "i mean installing", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:27:24.396923+00:00", "nick": "pvt_pete_", "message": ":)", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:27:26.399062+00:00", "nick": "pvt_pete_", "message": "the app is a computer vision project", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:27:28.398072+00:00", "nick": "pvt_pete_", "message": "it requires the web scraper to generate test data and scripts", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:27:30.398820+00:00", "nick": "pvt_pete_", "message": "and the app obviously needs to have the results of this accessible", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:29:17.009895+00:00", "nick": "remote", "message": "the part that confuses me is why you need the app to install the egg", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:29:54.819871+00:00", "nick": "remote", "message": "can't you just verify if the scraper is installed and scrapyd available during its install instead", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:30:08.371000+00:00", "nick": "pvt_pete_", "message": "good point", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:30:23.757690+00:00", "nick": "pvt_pete_", "message": "that would be simpler", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:30:34.770187+00:00", "nick": "pvt_pete_", "message": "even so", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:31:14.600341+00:00", "nick": "remote", "message": "i'm assuming you need to run the scraper via the app for a reason i ignore but if it wasn't the case you could just query the database where your spider stores data from your cocoa app", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:32:50.304112+00:00", "nick": "pvt_pete_", "message": "the app needs to tell scrapy d to update some folder full of scraped images", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:33:21.274833+00:00", "nick": "pvt_pete_", "message": "as you said", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:33:31.986892+00:00", "nick": "pvt_pete_", "message": "but you're right", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:33:38.534532+00:00", "nick": "pvt_pete_", "message": "maybe deployment separately would be o", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:33:42.816058+00:00", "nick": "pvt_pete_", "message": "ok", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:33:58.212888+00:00", "nick": "pvt_pete_", "message": "maybe I could make an installer somehow", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:35:01.385794+00:00", "nick": "pvt_pete_", "message": "hmm", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:36:29.648774+00:00", "nick": "remote", "message": "if I would deploy a spider automatically it would likely to something like: download spider, setup virtualenv, install dependencies, initialize virtualenv, run spider and/or install a cronjob to run it", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:38:14.538675+00:00", "nick": "pvt_pete_", "message": "ok", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:38:28.859027+00:00", "nick": "remote", "message": "and maybe configure log rotation at system level", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:38:44.249859+00:00", "nick": "remote", "message": "by that I mean like in logrotate, not sure what OSX uses", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:39:40.608759+00:00", "nick": "pvt_pete_", "message": "log rotation ?", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:40:32.690364+00:00", "nick": "remote", "message": "for instance in this spider I wrote a cronjob: https://github.com/ncouture/infodebit-scrapy/bl...", "links": ["https://github.com/ncouture/infodebit-scrapy/blob/master/hydrospider-cronjob.sh"], "channel": "scrapy"},
{"date": "2014-04-09T22:40:44.503038+00:00", "nick": "pvt_pete_", "message": "cronjob ?", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:40:56.326805+00:00", "nick": "remote", "message": "it redirects scrapy's output to a log file and I keep it for an amount of days as configured in https://github.com/ncouture/infodebit-scrapy/bl...", "links": ["https://github.com/ncouture/infodebit-scrapy/blob/master/hydrospider-logrotate.conf"], "channel": "scrapy"},
{"date": "2014-04-09T22:41:13.177979+00:00", "nick": "remote", "message": "do you know what crond is?", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:41:25.134509+00:00", "nick": "pvt_pete_", "message": "never heard of it", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:41:31.631648+00:00", "nick": "remote", "message": "http://en.wikipedia.org/wiki/Cron", "links": ["http://en.wikipedia.org/wiki/Cron"], "channel": "scrapy"},
{"date": "2014-04-09T22:41:59.381573+00:00", "nick": "remote", "message": "it's only to schedule a task to run (command line)", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:42:08.625735+00:00", "nick": "remote", "message": "so you can run your spider for instance, every day at midnight", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:42:14.409557+00:00", "nick": "remote", "message": "automatically", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:43:36.632472+00:00", "nick": "pvt_pete_", "message": "ahh ok", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:43:40.218360+00:00", "nick": "pvt_pete_", "message": "that's cool", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:43:48.353087+00:00", "nick": "pvt_pete_", "message": "i think though", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:44:00.628566+00:00", "nick": "pvt_pete_", "message": "because there are a very large number of images (9gb)", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:44:10.015583+00:00", "nick": "pvt_pete_", "message": "it will have to be on demand", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:44:10.944992+00:00", "nick": "pvt_pete_", "message": "lol", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:44:20.822042+00:00", "nick": "remote", "message": "makes sence", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:44:28.754744+00:00", "nick": "pvt_pete_", "message": "so download spider", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:44:33.730438+00:00", "nick": "pvt_pete_", "message": "locally into scrapyd?", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:44:59.781577+00:00", "nick": "pvt_pete_", "message": "i specified in the setup.py dependencies for scrapy and scrapyd", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:45:57.862995+00:00", "nick": "remote", "message": "i guess that should do it then just test if scrapy and your spider is available from your cocoa app and bind a button to execute them or whatever", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:46:06.696771+00:00", "nick": "remote", "message": "sorry I couldn't be of more help, gtg", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:46:07.721332+00:00", "nick": "remote", "message": ";-)", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:46:09.889249+00:00", "nick": "remote", "message": "good luck", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:46:11.972761+00:00", "nick": "pvt_pete_", "message": "laters", "links": [], "channel": "scrapy"},
{"date": "2014-04-09T22:46:13.569997+00:00", "nick": "pvt_pete_", "message": "and thx!", "links": [], "channel": "scrapy"},
{"date": "2014-04-10T12:53:25.426609+00:00", "nick": "stranac", "message": "hey guys, does scrapy have a way to extract the tail text of an element?", "links": [], "channel": "scrapy"},
{"date": "2014-04-10T12:53:43.400676+00:00", "nick": "stranac", "message": "i know i can do ._root.tail, but not sure if that's the right way", "links": [], "channel": "scrapy"},
{"date": "2014-04-10T13:28:22.426491+00:00", "nick": "nyov", "message": "stranac: what do you mean by tail? is there no xpath for that?", "links": [], "channel": "scrapy"},
{"date": "2014-04-10T13:29:31.035299+00:00", "nick": "nyov", "message": "maybe text()[last()] is what you mean?", "links": [], "channel": "scrapy"},
{"date": "2014-04-10T13:34:15.681670+00:00", "nick": "stranac", "message": "nyov: i mean something like \"<some tag>This is text</some tag>This is tail\"", "links": [], "channel": "scrapy"},
{"date": "2014-04-10T13:34:50.218392+00:00", "nick": "stranac", "message": "nyov: but i found an xpath that works: \"./following-sibling::text()[1]\"", "links": [], "channel": "scrapy"},
{"date": "2014-04-10T13:45:49.490611+00:00", "nick": "nyov", "message": "okay then", "links": [], "channel": "scrapy"},
{"date": "2014-04-10T15:10:57.898232+00:00", "nick": "Pharaoh", "message": "Hello everyone. I'm looking to scrape a website. I'm brand new to programming and don't know if I should start trying to learn scrapy or try to learn pyquery in order to do it.", "links": [], "channel": "scrapy"},
{"date": "2014-04-10T15:11:04.295877+00:00", "nick": "Pharaoh", "message": "I've been told both, so I don't know what to do.", "links": [], "channel": "scrapy"},
{"date": "2014-04-10T20:16:55.348351+00:00", "nick": "Nanashi_", "message": "Hello, is asking for help re: Scrapy allowed in this channel?", "links": [], "channel": "scrapy"},
{"date": "2014-04-10T20:26:57.900689+00:00", "nick": "nyov", "message": "Nanashi_: yes", "links": [], "channel": "scrapy"},
{"date": "2014-04-10T20:28:34.351462+00:00", "nick": "Nanashi_", "message": "Thanks for replying. I'm just beginning with Scrapy.", "links": [], "channel": "scrapy"},
{"date": "2014-04-10T20:29:04.435849+00:00", "nick": "Nanashi_", "message": "But I've already used BS and LXML extensively before. Got to admit that it's a bit difficult to make the jump from HTML parsing to crawling.", "links": [], "channel": "scrapy"},
{"date": "2014-04-10T20:29:47.954917+00:00", "nick": "Nanashi_", "message": "Just wondering though. How would one call feeding Scrapy a site, then having Scrapy get all the links in the page, go into each page, and parse said page's content?", "links": [], "channel": "scrapy"},
{"date": "2014-04-10T20:29:55.694121+00:00", "nick": "Nanashi_", "message": "Is this what they call recursive crawling?", "links": [], "channel": "scrapy"},
{"date": "2014-04-10T20:44:34.385657+00:00", "nick": "Nanashi_", "message": "Hello, anyone here available for a few simple questions? :)", "links": [], "channel": "scrapy"},
{"date": "2014-04-10T20:44:39.194556+00:00", "nick": "Nanashi_", "message": "Re: Scrapy?", "links": [], "channel": "scrapy"},
{"date": "2014-04-10T20:45:11.564467+00:00", "nick": "stranac", "message": "Nanashi_: go ahead and ask, someone will probably answer if they can", "links": [], "channel": "scrapy"},
{"date": "2014-04-10T20:47:45.653388+00:00", "nick": "Nanashi_", "message": "Oh, okay. Thanks. Nevermind. :)", "links": [], "channel": "scrapy"},
{"date": "2014-04-10T20:52:26.937370+00:00", "nick": "stranac", "message": "Nanashi_: if it's still about your previous question, then yes, some people call it that", "links": [], "channel": "scrapy"},
{"date": "2014-04-11T03:13:24.199517+00:00", "nick": "jno33", "message": "Can anybody offer any help with this error:  exceptions.OverflowError: integer 2147486719 does not fit '32-bit int' ?", "links": [], "channel": "scrapy"},
{"date": "2014-04-11T03:19:29.276277+00:00", "nick": "pvt_petey", "message": "man scrapy really hates relative imports with a passion", "links": [], "channel": "scrapy"},
{"date": "2014-04-11T04:18:34.305457+00:00", "nick": "pvt_petey", "message": "question", "links": [], "channel": "scrapy"},
{"date": "2014-04-11T04:18:44.206126+00:00", "nick": "pvt_petey", "message": "how can I get access to my crawlers signals api ?", "links": [], "channel": "scrapy"},
{"date": "2014-04-11T04:57:34.484790+00:00", "nick": "pvt_petey", "message": "ahh k", "links": [], "channel": "scrapy"},
{"date": "2014-04-11T16:20:58.111208+00:00", "nick": "pvt_petey", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2014-04-11T16:21:00.121332+00:00", "nick": "pvt_petey", "message": "how exactly does cls work in the context of extensions ?", "links": [], "channel": "scrapy"},
{"date": "2014-04-11T18:26:26.993604+00:00", "nick": "rage_", "message": "Hi. I have a problem with scrapy inserting only the first crawled item into mongodb", "links": [], "channel": "scrapy"},
{"date": "2014-04-11T18:26:51.165672+00:00", "nick": "rage_", "message": "https://github.com/har777/bloomberg_scraping", "links": ["https://github.com/har777/bloomberg_scraping"], "channel": "scrapy"},
{"date": "2014-04-11T18:26:56.908645+00:00", "nick": "rage_", "message": "Could someone please help ?", "links": [], "channel": "scrapy"},
{"date": "2014-04-11T18:37:21.404064+00:00", "nick": "rage_", "message": "While scraping if i clear the collection and do find again, the immediate item scraped is visible. Others are not inserted", "links": [], "channel": "scrapy"},
{"date": "2014-04-11T19:07:51.602449+00:00", "nick": "rage_", "message": "Anyone any ideas ?", "links": [], "channel": "scrapy"},
{"date": "2014-04-11T19:35:55.777967+00:00", "nick": "pvt_petey", "message": "how can I write my own signals ?", "links": [], "channel": "scrapy"},
{"date": "2014-04-11T20:54:28.862759+00:00", "nick": "pvt_petey", "message": "hello", "links": [], "channel": "scrapy"},
{"date": "2014-04-11T20:54:38.022790+00:00", "nick": "pvt_petey", "message": "how can I get a spiders signal manager ?", "links": [], "channel": "scrapy"},
{"date": "2014-04-11T20:54:43.859420+00:00", "nick": "pvt_petey", "message": "anyone plz :)", "links": [], "channel": "scrapy"},
{"date": "2014-04-11T21:43:00.722005+00:00", "nick": "pvt_petey", "message": "also", "links": [], "channel": "scrapy"},
{"date": "2014-04-11T21:43:05.029221+00:00", "nick": "pvt_petey", "message": "I've created an extension", "links": [], "channel": "scrapy"},
{"date": "2014-04-11T21:43:25.492555+00:00", "nick": "pvt_petey", "message": "apparently it's never loaded, even though the path is correct", "links": [], "channel": "scrapy"},
{"date": "2014-04-11T21:57:07.193384+00:00", "nick": "pvt_petey", "message": "how exactly are extensions loaded ?", "links": [], "channel": "scrapy"},
{"date": "2014-04-11T21:57:10.305466+00:00", "nick": "pvt_petey", "message": "staticly ?", "links": [], "channel": "scrapy"},
{"date": "2014-04-12T15:30:00.651968+00:00", "nick": "toothrot", "message": "pvt_petey, i've no idea what 'staticly' would mean in python", "links": [], "channel": "scrapy"},
{"date": "2014-04-12T15:37:22.774642+00:00", "nick": "gregor22", "message": "hello, I'm parsing an item which contains a redirect link which I want to extract and store within the Item, how would I do that  (https://gist.github.com/anonymous/10541814)", "links": ["https://gist.github.com/anonymous/10541814"], "channel": "scrapy"},
{"date": "2014-04-12T15:43:45.275765+00:00", "nick": "toothrot", "message": "gregor22, i've done this. you have to set... (from docs)", "links": [], "channel": "scrapy"},
{"date": "2014-04-12T15:43:48.255617+00:00", "nick": "toothrot", "message": "`The handle_httpstatus_list key of Request.meta can also be used to specify which response codes to allow on a per-request basis.`", "links": [], "channel": "scrapy"},
{"date": "2014-04-12T15:44:08.514109+00:00", "nick": "toothrot", "message": "Keep in mind, however, that it\u2019s usually a bad idea to handle non-200 responses, unless you really know what you\u2019re doing.", "links": [], "channel": "scrapy"},
{"date": "2014-04-12T15:44:28.974760+00:00", "nick": "toothrot", "message": "there's also a class attributee you can set that applies to all requests, but that probably isn't what you want", "links": [], "channel": "scrapy"},
{"date": "2014-04-12T15:46:32.788423+00:00", "nick": "toothrot", "message": "so if you just want to collect the URL, your meta could be something like: meta = {'dont_redirect': True, 'handle_httpstatus_list': [301, 302, 303, 307]}", "links": [], "channel": "scrapy"},
{"date": "2014-04-12T15:46:48.437490+00:00", "nick": "toothrot", "message": "and then you would use resp.headers['Location'] in your callback", "links": [], "channel": "scrapy"},
{"date": "2014-04-12T15:47:18.592367+00:00", "nick": "toothrot", "message": "(if there are multiple redirects you'll have to handle that differently of course)", "links": [], "channel": "scrapy"},
{"date": "2014-04-12T15:49:09.914212+00:00", "nick": "gregor22", "message": "thank you, sounds like a good idea, I will try it", "links": [], "channel": "scrapy"},
{"date": "2014-04-12T15:50:09.133716+00:00", "nick": "toothrot", "message": "(I used something similar for collecting download links in one crawler, but i always knew there would only be one redirect in that case)", "links": [], "channel": "scrapy"},
{"date": "2014-04-12T15:53:35.997725+00:00", "nick": "toothrot", "message": "but you could always continue following it, until the url matches some criteria", "links": [], "channel": "scrapy"},
{"date": "2014-04-12T16:02:20.296565+00:00", "nick": "gregor22", "message": "@toothrot it works! great, thank you, took me some time because I only skimmed the docs", "links": [], "channel": "scrapy"},
{"date": "2014-04-12T16:43:35.407585+00:00", "nick": "pvt_petey", "message": "hello", "links": [], "channel": "scrapy"},
{"date": "2014-04-13T22:52:52.068323+00:00", "nick": "HowardwLo", "message": "how do you write to postgresql directly?", "links": [], "channel": "scrapy"},
{"date": "2014-04-13T22:53:06.592144+00:00", "nick": "HowardwLo", "message": "or do you write to a pg_dump ?", "links": [], "channel": "scrapy"},
{"date": "2014-04-13T23:16:25.267622+00:00", "nick": "nyov", "message": "thats a strange question", "links": [], "channel": "scrapy"},
{"date": "2014-04-14T00:06:46.057915+00:00", "nick": "HowardwLo", "message": "nyov: is it? someone told me it would be faster to write to database instead", "links": [], "channel": "scrapy"},
{"date": "2014-04-14T00:06:57.187396+00:00", "nick": "HowardwLo", "message": "i wasn't sure what they meant, but it made sense", "links": [], "channel": "scrapy"},
{"date": "2014-04-14T00:13:15.217135+00:00", "nick": "HowardwLo", "message": "haven't been able to figure out exactly how to do it though\u2026.. :|", "links": [], "channel": "scrapy"},
{"date": "2014-04-14T00:25:25.030741+00:00", "nick": "nyov", "message": "HowardwLo: faster instead of what? hmm i really don't get what you're asking here", "links": [], "channel": "scrapy"},
{"date": "2014-04-14T00:27:13.622487+00:00", "nick": "HowardwLo", "message": "nyov:  I originally was scraping and writing to a JSON. THen reading the json and loading those items into my django project", "links": [], "channel": "scrapy"},
{"date": "2014-04-14T00:27:25.960377+00:00", "nick": "HowardwLo", "message": "someone suggested I write directly to the DB instead of JSON", "links": [], "channel": "scrapy"},
{"date": "2014-04-14T00:27:54.838634+00:00", "nick": "HowardwLo", "message": "they left before I could get more details, but htats the gist. I think they meant creating the row in my DB?", "links": [], "channel": "scrapy"},
{"date": "2014-04-14T00:47:51.661815+00:00", "nick": "nyov", "message": "HowardwLo: for django, you might want to use django's orm and django-item instead", "links": [], "channel": "scrapy"},
{"date": "2014-04-14T00:48:12.238033+00:00", "nick": "nyov", "message": "but sure you can write to the database yourself, if you know the schema", "links": [], "channel": "scrapy"},
{"date": "2014-04-14T21:22:38.971046+00:00", "nick": "dangra", "message": "welcome pgayane", "links": [], "channel": "scrapy"},
{"date": "2014-04-14T21:23:05.919242+00:00", "nick": "pgayane", "message": ":) so you think this will work ? section.xpath('.a/@href')", "links": [], "channel": "scrapy"},
{"date": "2014-04-14T21:23:17.999343+00:00", "nick": "pgayane", "message": "if I want to get the value of href attribute", "links": [], "channel": "scrapy"},
{"date": "2014-04-14T21:23:28.825172+00:00", "nick": "dangra", "message": "I think it should be: ./a/@href", "links": [], "channel": "scrapy"},
{"date": "2014-04-14T21:24:30.475729+00:00", "nick": "dangra", "message": "it is lot more easy if you use \"scrapy shell\" to debug your xpath queries live", "links": [], "channel": "scrapy"},
{"date": "2014-04-14T21:24:46.743558+00:00", "nick": "dangra", "message": "it's a shame it is error'ing for you", "links": [], "channel": "scrapy"},
{"date": "2014-04-14T21:24:58.463090+00:00", "nick": "dangra", "message": "btw, I never got the  \"scrapy shell error\" email from you, if you have time can you resend it to dangra@gmail.com please", "links": ["mailto:dangra@gmail.com"], "channel": "scrapy"},
{"date": "2014-04-14T21:25:34.697515+00:00", "nick": "pgayane", "message": "ok just a sec", "links": [], "channel": "scrapy"},
{"date": "2014-04-14T22:13:57.077188+00:00", "nick": "yhager", "message": "Hi! How do you prevent a spider from crawling into https:// URLs?", "links": [], "channel": "scrapy"},
{"date": "2014-04-14T22:14:47.799084+00:00", "nick": "dangra", "message": "you can block them in a downloader middleware or disable https download handler", "links": [], "channel": "scrapy"},
{"date": "2014-04-14T22:27:13.706527+00:00", "nick": "dangra", "message": "yhager: did you have any luck?", "links": [], "channel": "scrapy"},
{"date": "2014-04-14T22:27:56.377746+00:00", "nick": "yhager", "message": "dangra: oh, sorry, didn't see your original response. I guess that would work. Is there a way to do that per-spider?", "links": [], "channel": "scrapy"},
{"date": "2014-04-14T22:28:12.243602+00:00", "nick": "dangra", "message": "yes, with a middleware in that case", "links": [], "channel": "scrapy"},
{"date": "2014-04-14T22:34:38.713410+00:00", "nick": "yhager", "message": "but a middleware is not per-spider, is it? I'm somewhat new to scrapy, so I might have missed that part in the architecture", "links": [], "channel": "scrapy"},
{"date": "2014-04-14T22:36:01.208738+00:00", "nick": "yhager", "message": "also, how do I disable the https download handler without overriding the *_BASE setting?", "links": [], "channel": "scrapy"},
{"date": "2014-04-14T22:42:43.371497+00:00", "nick": "toothrot", "message": "yhager, set the priority to None in SPIDER_MIDDLEWARES", "links": [], "channel": "scrapy"},
{"date": "2014-04-14T22:43:06.681195+00:00", "nick": "toothrot", "message": "http://doc.scrapy.org/en/latest/topics/spider-m...", "links": ["http://doc.scrapy.org/en/latest/topics/spider-middleware.html#activating-a-spider-middleware"], "channel": "scrapy"},
{"date": "2014-04-14T22:54:52.163909+00:00", "nick": "yhager", "message": "toothrot: okay, cool. But there is no https download middleware..I'll just write one, but it won't be per spider, it would be per-project.", "links": [], "channel": "scrapy"},
{"date": "2014-04-14T22:55:28.100860+00:00", "nick": "yhager", "message": "just some context: in one of the sites I'm scanning, every http://X url has the same content as https://X, so I don't want to bother with https, and it's not technically a duplicate.", "links": ["http://X", "https://X"], "channel": "scrapy"},
{"date": "2014-04-14T22:58:46.956085+00:00", "nick": "nyov", "message": "if it's duplicate content (http urls == https urls), you could just rewrite https requests into http urls and let the dupefilter handle not fetching it", "links": [], "channel": "scrapy"},
{"date": "2014-04-14T23:00:05.017094+00:00", "nick": "yhager", "message": "nyov: okay, that's a good idea. Where do I do that so that it runs before the dupefilter?", "links": [], "channel": "scrapy"},
{"date": "2014-04-14T23:00:06.061204+00:00", "nick": "toothrot", "message": "sorry, it's not a middleware", "links": [], "channel": "scrapy"},
{"date": "2014-04-14T23:01:25.873530+00:00", "nick": "toothrot", "message": "rather, it's not a spider middleware", "links": [], "channel": "scrapy"},
{"date": "2014-04-14T23:02:51.568084+00:00", "nick": "toothrot", "message": "set that in DOWNLOAD_HANDLERS", "links": [], "channel": "scrapy"},
{"date": "2014-04-14T23:03:01.944010+00:00", "nick": "toothrot", "message": "http://doc.scrapy.org/en/latest/topics/settings...", "links": ["http://doc.scrapy.org/en/latest/topics/settings.html?highlight=handlers#std:setting-DOWNLOAD_HANDLERS"], "channel": "scrapy"},
{"date": "2014-04-14T23:03:38.499667+00:00", "nick": "toothrot", "message": "i think changing the requests is a good idea though, because then you've got it covered if it only links https", "links": [], "channel": "scrapy"},
{"date": "2014-04-14T23:04:37.382156+00:00", "nick": "toothrot", "message": "(if there's way to automatically do it that is)", "links": [], "channel": "scrapy"},
{"date": "2014-04-14T23:06:21.260500+00:00", "nick": "yhager", "message": "yes, I agree, only I'm not sure how to change it before dupefilter sees it.", "links": [], "channel": "scrapy"},
{"date": "2014-04-14T23:06:28.312755+00:00", "nick": "nyov", "message": "yhager: probably best to handle in a downloader middleware, somewhere after RetryMiddleware (500) and before RedirectMiddleware (600) I think", "links": [], "channel": "scrapy"},
{"date": "2014-04-14T23:07:28.601865+00:00", "nick": "yhager", "message": "nyov: okay, I'll do it this way. thanks", "links": [], "channel": "scrapy"},
{"date": "2014-04-14T23:07:56.451284+00:00", "nick": "nyov", "message": "i haven't done it though, not sure if I'm right, here", "links": [], "channel": "scrapy"},
{"date": "2014-04-14T23:09:54.179046+00:00", "nick": "nyov", "message": "you'd rewrite the outgoing requests in process_request; though the same can be possible with a spider middleware", "links": [], "channel": "scrapy"},
{"date": "2014-04-14T23:11:09.197385+00:00", "nick": "nyov", "message": "but then it wouldn't see requests not originating from the spider (redirects or such)", "links": [], "channel": "scrapy"},
{"date": "2014-04-14T23:25:40.687687+00:00", "nick": "yhager", "message": "oh, I wasn't aware of spider middlewares until now. thx", "links": [], "channel": "scrapy"},
{"date": "2014-04-14T23:29:03.960748+00:00", "nick": "yhager", "message": "nyov: isn't it better *after* the redirect? in case we get redirected to https to filter it out?", "links": [], "channel": "scrapy"},
{"date": "2014-04-14T23:38:51.514362+00:00", "nick": "nyov", "message": "possibly. that would mean you'd still follow redirect(s) to a http ressource, and then rewrite the final destination, if it passed", "links": [], "channel": "scrapy"},
{"date": "2014-04-14T23:39:12.041881+00:00", "nick": "nyov", "message": "to a *https* ressource, sorry", "links": [], "channel": "scrapy"},
{"date": "2014-04-14T23:39:31.985803+00:00", "nick": "yhager", "message": "ok, thx", "links": [], "channel": "scrapy"},
{"date": "2014-04-14T23:40:54.660557+00:00", "nick": "nyov", "message": "i think both ways could result in redirect loops, if a http url tries to force an https upgrade, but doing it after the redirects may be smarter", "links": [], "channel": "scrapy"},
{"date": "2014-04-14T23:41:40.136732+00:00", "nick": "nyov", "message": "though doing that would mean more possibly duplicate https urls fetched (and cached)", "links": [], "channel": "scrapy"},
{"date": "2014-04-15T03:56:43.879073+00:00", "nick": "remote", "message": "are you guys sprinting at pycon?", "links": [], "channel": "scrapy"},
{"date": "2014-04-15T04:00:47.322848+00:00", "nick": "shane42", "message": "yes, we were there today", "links": [], "channel": "scrapy"},
{"date": "2014-04-15T04:01:15.176927+00:00", "nick": "shane42", "message": "this is a project we worked on: https://github.com/scrapinghub/pycon-speakers", "links": ["https://github.com/scrapinghub/pycon-speakers"], "channel": "scrapy"},
{"date": "2014-04-15T04:01:19.720565+00:00", "nick": "shane42", "message": "contributions welcome :)", "links": [], "channel": "scrapy"},
{"date": "2014-04-15T13:47:30.639382+00:00", "nick": "h4k1m", "message": "hi guys", "links": [], "channel": "scrapy"},
{"date": "2014-04-15T13:48:19.417805+00:00", "nick": "h4k1m", "message": "I have a little issue with reponse object inside a request callback when the page returns a 404", "links": [], "channel": "scrapy"},
{"date": "2014-04-15T13:48:32.778719+00:00", "nick": "h4k1m", "message": "if the page exists (200) response is of type htmlresponse", "links": [], "channel": "scrapy"},
{"date": "2014-04-15T13:49:00.004276+00:00", "nick": "h4k1m", "message": "if the page returns 404, response is of type instance which contain some error messages", "links": [], "channel": "scrapy"},
{"date": "2014-04-15T13:49:19.370942+00:00", "nick": "h4k1m", "message": "and this latter case, status isnt inside response object", "links": [], "channel": "scrapy"},
{"date": "2014-04-15T13:49:44.883989+00:00", "nick": "h4k1m", "message": "so I cant know if the response status is 404 only if I verify response class", "links": [], "channel": "scrapy"},
{"date": "2014-04-15T13:55:01.107353+00:00", "nick": "h4k1m", "message": "how do we know that a page returns  404 if response.status isnt available", "links": [], "channel": "scrapy"},
{"date": "2014-04-15T15:11:12.821740+00:00", "nick": "h4k1m", "message": "hi guys", "links": [], "channel": "scrapy"},
{"date": "2014-04-15T15:21:39.177483+00:00", "nick": "rage_", "message": "Hi. Is this a valid xpath ? res.xpath('/span[@id=\"app_rtg\"]/tex...", "links": ["mailto:res.xpath('/span[@id=\"app_rtg\"]/text()').extract()"], "channel": "scrapy"},
{"date": "2014-04-15T15:21:53.595393+00:00", "nick": "rage_", "message": "Sorry I mean res.xpath('//span[@id=\"app_rtg\"]/te...", "links": ["mailto:res.xpath('//span[@id=\"app_rtg\"]/text()').extract()"], "channel": "scrapy"},
{"date": "2014-04-15T15:27:45.622251+00:00", "nick": "rage_", "message": "The thing is i have a span class whose title(also inside the span) I want to extract.", "links": [], "channel": "scrapy"},
{"date": "2014-04-15T15:29:23.807263+00:00", "nick": "rage_", "message": "<span id=\"app_rtg\" rel=\"v:rating\" class=\"srating\" title=\"rating: 4.5 / 5 stars\"> </span>", "links": [], "channel": "scrapy"},
{"date": "2014-04-15T15:29:31.951138+00:00", "nick": "rage_", "message": "How can I extract the title ?", "links": [], "channel": "scrapy"},
{"date": "2014-04-15T21:45:48.045218+00:00", "nick": "gregor22", "message": "Hi, I'm getting an error when running \" scrapy bench \" on Win7 and I don't understand what causes it ( https://gist.github.com/anonymous/10779160 )", "links": ["https://gist.github.com/anonymous/10779160"], "channel": "scrapy"},
{"date": "2014-04-15T21:46:00.866794+00:00", "nick": "gregor22", "message": "maybe someone had the same error and got a solution for it", "links": [], "channel": "scrapy"},
{"date": "2014-04-15T21:51:32.219531+00:00", "nick": "gregor22", "message": "I'm using scrapy version 0.22.2", "links": [], "channel": "scrapy"},
{"date": "2014-04-15T22:09:17.006831+00:00", "nick": "rage_", "message": "I'm really stuck here. I have a couple of names in a list. I want to do a search of the name in the website first and then do recursive searching on the output page for data. Now I can easily do it for one item. But how would I restart the spider for each item in the list after changing the starting link to match its search name ?", "links": [], "channel": "scrapy"},
{"date": "2014-04-15T22:12:18.413750+00:00", "nick": "nyov", "message": "gregor22: your local system doesnt run the necessary server on port 8998", "links": [], "channel": "scrapy"},
{"date": "2014-04-15T22:12:54.713874+00:00", "nick": "gregor22", "message": "what server do I need to start?", "links": [], "channel": "scrapy"},
{"date": "2014-04-15T22:13:02.848201+00:00", "nick": "nyov", "message": "no idea", "links": [], "channel": "scrapy"},
{"date": "2014-04-15T22:13:10.061877+00:00", "nick": "gregor22", "message": "xD me neither", "links": [], "channel": "scrapy"},
{"date": "2014-04-15T22:13:18.954148+00:00", "nick": "gregor22", "message": "but thanks anyways", "links": [], "channel": "scrapy"},
{"date": "2014-04-15T23:05:07.425951+00:00", "nick": "toothrot", "message": "gregor22 i get the same error", "links": [], "channel": "scrapy"},
{"date": "2014-04-15T23:37:35.483627+00:00", "nick": "yhager|away", "message": "rage_: just return a Request for the new URL to search for the next name from your parse function", "links": [], "channel": "scrapy"},
{"date": "2014-04-15T23:55:36.466562+00:00", "nick": "rage_", "message": "Can I return boith my item and a request ?", "links": [], "channel": "scrapy"},
{"date": "2014-04-16T00:06:12.070388+00:00", "nick": "yhager", "message": "rage_: yes", "links": [], "channel": "scrapy"},
{"date": "2014-04-16T00:06:33.607935+00:00", "nick": "rage_", "message": "Thanks a lot !!", "links": [], "channel": "scrapy"},
{"date": "2014-04-16T00:06:42.368553+00:00", "nick": "rage_", "message": "Found a stack overflow answer too", "links": [], "channel": "scrapy"},
{"date": "2014-04-16T00:06:42.920376+00:00", "nick": "rage_", "message": "http://stackoverflow.com/questions/12736257/why...", "links": ["http://stackoverflow.com/questions/12736257/why-dont-my-scrapy-crawlspider-rules-work"], "channel": "scrapy"},
{"date": "2014-04-16T00:16:24.909901+00:00", "nick": "yhager", "message": "rage_: sometimes, the fine manual is quite enough: http://doc.scrapy.org/en/latest/topics/spiders....", "links": ["http://doc.scrapy.org/en/latest/topics/spiders.html#scrapy.spider.Spider.parse"], "channel": "scrapy"},
{"date": "2014-04-16T00:17:18.444164+00:00", "nick": "yhager", "message": "scrapy is one of the rare cases where the docs are actually better than the content in stackoverflow..", "links": [], "channel": "scrapy"},
{"date": "2014-04-16T13:44:38.493289+00:00", "nick": "gregor22", "message": "Hi, I'm getting an error when running \"scrapy bench\" on Win7 and I don't get why ( https://gist.github.com/anonymous/10877547 )", "links": ["https://gist.github.com/anonymous/10877547"], "channel": "scrapy"},
{"date": "2014-04-16T13:44:50.358908+00:00", "nick": "gregor22", "message": "maybe someone had the same problem and got a solution for it", "links": [], "channel": "scrapy"},
{"date": "2014-04-16T13:45:07.586132+00:00", "nick": "gregor22", "message": "I'm running scrapy 0.22.2 on Win7", "links": [], "channel": "scrapy"},
{"date": "2014-04-16T14:24:13.596002+00:00", "nick": "zrth", "message": "Hi! I am testing the example at http://stackoverflow.com/a/5857202 together with the fix posted one reply below, the login works, but then i cannot get scrapy to get the start_urls[] . Any idea what could be needed?", "links": ["http://stackoverflow.com/a/5857202"], "channel": "scrapy"},
{"date": "2014-04-16T18:30:28.203691+00:00", "nick": "panaggio", "message": "what are scrapy debian package build dependencies?", "links": [], "channel": "scrapy"},
{"date": "2014-04-16T18:30:38.642775+00:00", "nick": "panaggio", "message": "can't find that info anywhere", "links": [], "channel": "scrapy"},
{"date": "2014-04-16T21:03:01.466332+00:00", "nick": "nyov", "message": "panaggio: apt-get build-dep <package>", "links": [], "channel": "scrapy"},
{"date": "2014-04-16T21:19:56.870627+00:00", "nick": "panaggio", "message": "apt-get build-dep <package> you help me if scrapy was installable on my host, but it isn't. I want to build the .deb package from sourse here", "links": [], "channel": "scrapy"},
{"date": "2014-04-16T21:20:02.405883+00:00", "nick": "panaggio", "message": "nyov ^", "links": [], "channel": "scrapy"},
{"date": "2014-04-16T21:25:22.175046+00:00", "nick": "nyov", "message": "what do you mean. you're downloading the source package, right? apt-get source <package> ?", "links": [], "channel": "scrapy"},
{"date": "2014-04-16T21:29:41.245311+00:00", "nick": "nyov", "message": "panaggio: oh wait, this is about scrapy from scrapinghub? they might not have a source package i guess", "links": [], "channel": "scrapy"},
{"date": "2014-04-16T21:33:01.018765+00:00", "nick": "nyov", "message": "so whats your source?", "links": [], "channel": "scrapy"},
{"date": "2014-04-16T21:34:21.758923+00:00", "nick": "panaggio", "message": "nope, I'm getting source code from scrapy's github and trying to build a deb package from a branch", "links": [], "channel": "scrapy"},
{"date": "2014-04-16T21:35:35.504636+00:00", "nick": "nyov", "message": "https://github.com/scrapy/scrapy/blob/master/de...", "links": ["https://github.com/scrapy/scrapy/blob/master/debian/control#L5"], "channel": "scrapy"},
{"date": "2014-04-16T21:37:18.299677+00:00", "nick": "panaggio", "message": "ok, so this is written in the most obvious place, I'm the one who didn't know whre to find it. thanks nyov!", "links": [], "channel": "scrapy"},
{"date": "2014-04-16T21:37:53.058710+00:00", "nick": "nyov", "message": "no problem", "links": [], "channel": "scrapy"},
{"date": "2014-04-17T02:42:26.130003+00:00", "nick": "DuffyKM", "message": "Howdy!", "links": [], "channel": "scrapy"},
{"date": "2014-04-17T02:44:48.524638+00:00", "nick": "DuffyKM", "message": "Does anyone here have experience installing Scrapy on to an Ubuntu box?", "links": [], "channel": "scrapy"},
{"date": "2014-04-17T02:47:55.704153+00:00", "nick": "DuffyKM", "message": "I have a pip.log file showing an error during installation and was hoping someone knew where to look or what to change to complete the installation.", "links": [], "channel": "scrapy"},
{"date": "2014-04-17T02:58:09.621289+00:00", "nick": "DuffyKM", "message": "Can anyone tell me of a free forum then maybe to ask my questions since we're all a bit quiet here? :)", "links": [], "channel": "scrapy"},
{"date": "2014-04-17T03:10:14.047467+00:00", "nick": "DuffyKM", "message": "Hello?", "links": [], "channel": "scrapy"},
{"date": "2014-04-17T22:45:21.663848+00:00", "nick": "HowardwLo", "message": "Hi, i'm getting an error when I try to scrape directly to django postgres DB. https://dpaste.de/W6aN", "links": ["https://dpaste.de/W6aN"], "channel": "scrapy"},
{"date": "2014-04-17T22:45:33.430660+00:00", "nick": "HowardwLo", "message": "I'm not sure what it means", "links": [], "channel": "scrapy"},
{"date": "2014-04-18T00:12:59.222876+00:00", "nick": "nyov", "message": "HowardwLo: it means your transaction is done for and was likely rolled back because of errors", "links": [], "channel": "scrapy"},
{"date": "2014-04-18T00:13:19.029776+00:00", "nick": "nyov", "message": "and you'll have to start a new one before issueing another sql command", "links": [], "channel": "scrapy"},
{"date": "2014-04-18T02:07:18.748294+00:00", "nick": "HowardwLo", "message": "nyov: how do i go about fixing it?", "links": [], "channel": "scrapy"},
{"date": "2014-04-18T02:10:00.875281+00:00", "nick": "HowardwLo", "message": "should i fix the errors or is this something that happens every so often and I should account for it?", "links": [], "channel": "scrapy"},
{"date": "2014-04-18T03:05:46.900802+00:00", "nick": "Desparate", "message": "I need some with crawling a site - anyone op for a challange?", "links": [], "channel": "scrapy"},
{"date": "2014-04-19T20:43:23.766383+00:00", "nick": "mib_mib", "message": "hey - are there any good web ui's for Scrapyd?", "links": [], "channel": "scrapy"},
{"date": "2014-04-19T20:43:46.951220+00:00", "nick": "mib_mib", "message": "like bootstrap plugins or what not that are more of a 'dashboard' that let you start / stop workers, etc?", "links": [], "channel": "scrapy"},
{"date": "2014-04-19T20:59:30.758734+00:00", "nick": "mib_mib", "message": "or does it not yet exist?", "links": [], "channel": "scrapy"},
{"date": "2014-04-20T00:58:22.517691+00:00", "nick": "pvt_petey", "message": "hi I'm having some problems implementing an extension for batch crawling", "links": [], "channel": "scrapy"},
{"date": "2014-04-20T00:58:29.913821+00:00", "nick": "pvt_petey", "message": "can anyone help ?", "links": [], "channel": "scrapy"},
{"date": "2014-04-20T08:37:19.254419+00:00", "nick": "muodov", "message": "hi. Can anybody tell me the right way to make a database query in downloader middleware?", "links": [], "channel": "scrapy"},
{"date": "2014-04-20T08:40:10.754036+00:00", "nick": "muodov", "message": "MiddlewareManager doesn't take Deferred as return values like in Pipelines, I tried to call twisted's adbapi pool.connect() in the __init__ of the middleware to get a db connection, but still not sure if this is ok to do like this", "links": [], "channel": "scrapy"},
{"date": "2014-04-20T17:03:52.186333+00:00", "nick": "muodov", "message": "hi. Can anybody tell me the right way to make an async database query in downloader middleware? preferably, twisted adbapi", "links": [], "channel": "scrapy"},
{"date": "2014-04-20T19:15:33.079971+00:00", "nick": "Randomaniac", "message": "!hurray", "links": [], "channel": "scrapy"},
{"date": "2014-04-20T19:15:44.345433+00:00", "nick": "Randomaniac", "message": "*cough*", "links": [], "channel": "scrapy"},
{"date": "2014-04-20T21:51:29.382209+00:00", "nick": "D5", "message": "If I have to deal with 2 kinds of pages is changeing the callback the best parctice? for exmaple like this: requests.append(Request(url=URL_BASE + userIDs[i] + FILTER_SETTINGS, callback=self.parseReviewer))", "links": [], "channel": "scrapy"},
{"date": "2014-04-21T04:55:14.351380+00:00", "nick": "coder46", "message": "Big day today ... GSOC !!! :)", "links": [], "channel": "scrapy"},
{"date": "2014-04-21T09:48:43.233853+00:00", "nick": "Randomaniac", "message": "Doomsday is upon us", "links": [], "channel": "scrapy"},
{"date": "2014-04-21T09:48:52.380166+00:00", "nick": "Randomaniac", "message": "Best of luck with GSoC, Scrapy!", "links": [], "channel": "scrapy"},
{"date": "2014-04-21T11:00:36.375892+00:00", "nick": "h4k1m", "message": "hi guys", "links": [], "channel": "scrapy"},
{"date": "2014-04-21T11:00:59.612982+00:00", "nick": "h4k1m", "message": "I have an issue with scrapy deploy when trying to load a yaml file from inside my spider", "links": [], "channel": "scrapy"},
{"date": "2014-04-21T15:34:45.437179+00:00", "nick": "emollient", "message": "hey, can anyone help me with one of my spiders?", "links": [], "channel": "scrapy"},
{"date": "2014-04-21T15:34:51.462539+00:00", "nick": "emollient", "message": "stackoverflow has been less than helpful :(", "links": [], "channel": "scrapy"},
{"date": "2014-04-21T15:54:48.488615+00:00", "nick": "Randomaniac", "message": "emollient, try me", "links": [], "channel": "scrapy"},
{"date": "2014-04-21T15:58:17.922640+00:00", "nick": "emollient", "message": "http://pastebin.com/11GjZNVV", "links": ["http://pastebin.com/11GjZNVV"], "channel": "scrapy"},
{"date": "2014-04-21T15:58:38.589847+00:00", "nick": "emollient", "message": "so sites should return an array of all the stuff in the product_box divs, correct?", "links": [], "channel": "scrapy"},
{"date": "2014-04-21T15:58:57.926365+00:00", "nick": "emollient", "message": "I have it parse through there and pull out the stuff I want", "links": [], "channel": "scrapy"},
{"date": "2014-04-21T15:59:13.333965+00:00", "nick": "emollient", "message": "however, the json it returns is 8 items with all the product boxes contained in the json", "links": [], "channel": "scrapy"},
{"date": "2014-04-21T15:59:25.443529+00:00", "nick": "emollient", "message": "instead of just one product_box per json object", "links": [], "channel": "scrapy"},
{"date": "2014-04-21T16:00:42.696175+00:00", "nick": "emollient", "message": "for instance: this is the output", "links": [], "channel": "scrapy"},
{"date": "2014-04-21T16:00:44.009941+00:00", "nick": "emollient", "message": "http://pastebin.com/aeAMUKzw", "links": ["http://pastebin.com/aeAMUKzw"], "channel": "scrapy"},
{"date": "2014-04-21T16:00:50.319282+00:00", "nick": "emollient", "message": "I'm not sure what I did wrong :(", "links": [], "channel": "scrapy"},
{"date": "2014-04-21T16:01:48.079662+00:00", "nick": "emollient", "message": "it's returning that output 8 times identically", "links": [], "channel": "scrapy"},
{"date": "2014-04-21T16:36:20.998342+00:00", "nick": "Randomaniac", "message": "well emollient you'll want to change how you extract a single brand name for example", "links": [], "channel": "scrapy"},
{"date": "2014-04-21T16:36:27.485712+00:00", "nick": "Randomaniac", "message": "I tried it out in my shell and it gave my the whole list", "links": [], "channel": "scrapy"},
{"date": "2014-04-21T16:36:36.348713+00:00", "nick": "Randomaniac", "message": "[u'Vancouver', u'Trento', u'Portland', u'Leonardo', u'Stuyvesant', u'Stuyvesant', u'Augusta', u'Retro Eyewear 103', u'Fontana', u'Oxygen', u'Argon', u'Margherita', u'Trento', u'Warhol', u'College', u'Visalia', u'Helium', u'Fulton', u'Cambridge', u'Oxygen', u'Clark', u'Argon', u'Portland', u'Forsyth']", "links": [], "channel": "scrapy"},
{"date": "2014-04-21T16:38:39.882998+00:00", "nick": "Randomaniac", "message": "well never mind then", "links": [], "channel": "scrapy"},
{"date": "2014-04-21T17:39:06.259911+00:00", "nick": "joanfihu", "message": "Hi! I'm new with Scrappy and Python and I have two Python versions: 2.7 and 3.4 I'm trying to run Scrapy but it automatically uses Python 3.4. How can I tell Scrapy to use python 2.7?", "links": [], "channel": "scrapy"},
{"date": "2014-04-21T17:39:08.742943+00:00", "nick": "joanfihu", "message": "Thanks :)", "links": [], "channel": "scrapy"},
{"date": "2014-04-21T17:40:14.438706+00:00", "nick": "Randomaniac", "message": "My maybe-not-so clean approach is this:", "links": [], "channel": "scrapy"},
{"date": "2014-04-21T17:40:25.359273+00:00", "nick": "Randomaniac", "message": "check out where python is with `which python`", "links": [], "channel": "scrapy"},
{"date": "2014-04-21T17:40:32.886709+00:00", "nick": "Randomaniac", "message": "then replace that link with a link to python2.7", "links": [], "channel": "scrapy"},
{"date": "2014-04-21T17:42:18.359716+00:00", "nick": "shane42", "message": "it will use whatever python interpreter is the default - if you modify your PATH to put 2.7 first it will use that", "links": [], "channel": "scrapy"},
{"date": "2014-04-21T17:42:29.385895+00:00", "nick": "shane42", "message": "or you can edit teh scrapy script", "links": [], "channel": "scrapy"},
{"date": "2014-04-21T17:42:48.531382+00:00", "nick": "shane42", "message": "or invoke scrapy wiht the python interpreter you want to use", "links": [], "channel": "scrapy"},
{"date": "2014-04-21T17:43:25.897813+00:00", "nick": "Randomaniac", "message": "I like the first two :)", "links": [], "channel": "scrapy"},
{"date": "2014-04-21T17:47:22.636420+00:00", "nick": "joanfihu", "message": "THanks for the answer, I'm going to try and see if it works :p", "links": [], "channel": "scrapy"},
{"date": "2014-04-21T18:02:19.678905+00:00", "nick": "joanfihu", "message": "Hi Guys! I managed to solve it I added the package path to my bash profile and it worked... Thanks", "links": [], "channel": "scrapy"},
{"date": "2014-04-21T19:05:41.580033+00:00", "nick": "thedarki", "message": "hi, who accepted for gsoc?", "links": [], "channel": "scrapy"},
{"date": "2014-04-21T21:39:31.165452+00:00", "nick": "D5", "message": "is there a way to change the order of the headers in the requests?", "links": [], "channel": "scrapy"},
{"date": "2014-04-22T00:25:59.726401+00:00", "nick": "susan", "message": "hey, I was asking for some help earlier with xpath/spider stuff, but no one was around", "links": [], "channel": "scrapy"},
{"date": "2014-04-22T00:26:09.604763+00:00", "nick": "susan", "message": "can anyone poke some help my way?", "links": [], "channel": "scrapy"},
{"date": "2014-04-22T07:35:03.712011+00:00", "nick": "pvt_petey", "message": "hi there", "links": [], "channel": "scrapy"},
{"date": "2014-04-22T07:35:20.402466+00:00", "nick": "pvt_petey", "message": "I'm having some problems with adding extensions", "links": [], "channel": "scrapy"},
{"date": "2014-04-22T07:35:26.760710+00:00", "nick": "pvt_petey", "message": "i.e. my own custom extensions", "links": [], "channel": "scrapy"},
{"date": "2014-04-22T07:35:30.588352+00:00", "nick": "pvt_petey", "message": "I know that the path is ok", "links": [], "channel": "scrapy"},
{"date": "2014-04-22T07:35:38.320497+00:00", "nick": "pvt_petey", "message": "but it's not being enabled somehow", "links": [], "channel": "scrapy"},
{"date": "2014-04-22T07:46:53.375592+00:00", "nick": "pvt_petey", "message": "i just don't get the documentation at all :/", "links": [], "channel": "scrapy"},
{"date": "2014-04-22T07:47:48.065025+00:00", "nick": "pvt_petey", "message": "\\me", "links": [], "channel": "scrapy"},
{"date": "2014-04-22T07:52:39.623822+00:00", "nick": "D5", "message": "i suspect it sucks on purpose. the fun guys who wrote the project do love money and have consultancy thing going on.", "links": [], "channel": "scrapy"},
{"date": "2014-04-22T07:53:01.199231+00:00", "nick": "D5", "message": "http://scrapinghub.com/", "links": ["http://scrapinghub.com/"], "channel": "scrapy"},
{"date": "2014-04-22T07:53:06.849709+00:00", "nick": "D5", "message": "pay me money get bitchs", "links": [], "channel": "scrapy"},
{"date": "2014-04-22T07:53:31.769694+00:00", "nick": "nyov", "message": "bullshit", "links": [], "channel": "scrapy"},
{"date": "2014-04-22T07:53:55.155850+00:00", "nick": "nyov", "message": "pvt_petey: what don't you get? extensions are enabled in the settings", "links": [], "channel": "scrapy"},
{"date": "2014-04-22T07:54:20.312495+00:00", "nick": "pvt_petey", "message": "I've enabled it in the settings", "links": [], "channel": "scrapy"},
{"date": "2014-04-22T07:54:30.988210+00:00", "nick": "pvt_petey", "message": "and copied exactly the example from the documentation", "links": [], "channel": "scrapy"},
{"date": "2014-04-22T07:54:38.863412+00:00", "nick": "nyov", "message": "D5: the documentation is only as good as the people who write it can make it. you're welcome to improve and send pull requests", "links": [], "channel": "scrapy"},
{"date": "2014-04-22T07:54:40.568380+00:00", "nick": "pvt_petey", "message": "for a custom extension", "links": [], "channel": "scrapy"},
{"date": "2014-04-22T07:55:18.764314+00:00", "nick": "D5", "message": "nyov: fuck no. aint nobody got time for that.", "links": [], "channel": "scrapy"},
{"date": "2014-04-22T07:55:30.426567+00:00", "nick": "pvt_petey", "message": "I'm using the right path", "links": [], "channel": "scrapy"},
{"date": "2014-04-22T07:55:40.567004+00:00", "nick": "pvt_petey", "message": "if I change it to relative it throws a tantrum", "links": [], "channel": "scrapy"},
{"date": "2014-04-22T07:56:07.288936+00:00", "nick": "nyov", "message": "path? the path in EXTENSIONS?", "links": [], "channel": "scrapy"},
{"date": "2014-04-22T07:57:21.847422+00:00", "nick": "pvt_petey", "message": "in the settings yes", "links": [], "channel": "scrapy"},
{"date": "2014-04-22T07:57:44.295358+00:00", "nick": "pvt_petey", "message": "http://ideone.com/RsnVY0", "links": ["http://ideone.com/RsnVY0"], "channel": "scrapy"},
{"date": "2014-04-22T07:59:44.980786+00:00", "nick": "nyov", "message": "that looks correct", "links": [], "channel": "scrapy"},
{"date": "2014-04-22T08:00:40.057074+00:00", "nick": "nyov", "message": "so your classname is SpiderOpenCloseLogging. and it's __init__ does not execute?", "links": [], "channel": "scrapy"},
{"date": "2014-04-22T08:02:28.975949+00:00", "nick": "pvt_petey", "message": "nope", "links": [], "channel": "scrapy"},
{"date": "2014-04-22T08:03:54.475047+00:00", "nick": "nyov", "message": "maybe it's the filename, having problems with underscores? I'd try to rename it. and remember you need an __init__.py file in the extensions/ folder", "links": [], "channel": "scrapy"},
{"date": "2014-04-22T08:07:13.275262+00:00", "nick": "pvt_petey", "message": "there's already one there", "links": [], "channel": "scrapy"},
{"date": "2014-04-22T08:07:59.568866+00:00", "nick": "pvt_petey", "message": "my directory structure is main.extensions.image_path_tool.py (which also has an __init__.py", "links": [], "channel": "scrapy"},
{"date": "2014-04-22T08:12:45.547629+00:00", "nick": "nyov", "message": "one more thing you can try is adding a custom from_crawler classmethod which should run before init.", "links": [], "channel": "scrapy"},
{"date": "2014-04-22T08:13:25.361745+00:00", "nick": "nyov", "message": "otherwise go back and try to rename and use an existing extension, step by step making it your own", "links": [], "channel": "scrapy"},
{"date": "2014-04-22T08:16:57.710423+00:00", "nick": "pvt_petey", "message": "I made a new class", "links": [], "channel": "scrapy"},
{"date": "2014-04-22T08:17:09.531402+00:00", "nick": "pvt_petey", "message": "with the same name as the closeopenextension in the documentation", "links": [], "channel": "scrapy"},
{"date": "2014-04-22T10:13:59.452610+00:00", "nick": "jez", "message": "Anybody here has experience in scraping jscript stuff", "links": [], "channel": "scrapy"},
{"date": "2014-04-22T10:14:00.300420+00:00", "nick": "jez", "message": "?", "links": [], "channel": "scrapy"},
{"date": "2014-04-22T20:36:21.357541+00:00", "nick": "D5", "message": "is there a way to make scrapy not send a Connection: close header?", "links": [], "channel": "scrapy"},
{"date": "2014-04-22T20:58:36.590867+00:00", "nick": "D5", "message": "its a little bug(?) it sends that shit whenever you use a proxy", "links": [], "channel": "scrapy"},
{"date": "2014-04-23T00:21:49.283081+00:00", "nick": "nick1", "message": "$10 via paypal to someone who can help me create a spider for https://banweb.banner.vt.edu/ssb/prod/HZSKVTSC....", "links": ["https://banweb.banner.vt.edu/ssb/prod/HZSKVTSC.P_ProcRequest?CAMPUS=0&amp=&TERMYEAR=201409&amp=&SCHDTYPE=%25&amp=&SUBJ_CODE=&amp=&CRSE_NUMBER=&amp=&open_only=&amp=&CRN=&amp=&INST_NAME=&amp=&CORE_CODE=AR%25&amp=&PRINT_FRIEND=Y&amp=&history=N&amp=&BTN_PRESSED=Printer+Friendly+List"], "channel": "scrapy"},
{"date": "2014-04-23T00:23:14.735541+00:00", "nick": "nick1", "message": "I have a base with most of the coding already done here: http://pastebin.com/CWabf0KF", "links": ["http://pastebin.com/CWabf0KF"], "channel": "scrapy"},
{"date": "2014-04-23T00:26:11.286558+00:00", "nick": "nick1", "message": "If you are interested, or just want to say hi send me a message or highlight me here! Thanks guys.", "links": [], "channel": "scrapy"},
{"date": "2014-04-23T11:22:26.343995+00:00", "nick": "fpghost84", "message": "I use import locale; locale.setlocale(locale.LC_ALL, 'es_ES.utf8'), to select the language as spanish for one of my spider so I can process Spanish dates. However this locale seems to be now in effect throughout all my spiders....Is there a way I can specialise different locales for different spiders?", "links": [], "channel": "scrapy"},
{"date": "2014-04-23T21:41:51.440685+00:00", "nick": "Eypton", "message": "Hi all.", "links": [], "channel": "scrapy"},
{"date": "2014-04-24T01:30:52.510058+00:00", "nick": "cesurasean1", "message": "anyone here?", "links": [], "channel": "scrapy"},
{"date": "2014-04-24T02:29:56.258917+00:00", "nick": "Guest12778", "message": "hi everybody, how I can work in  scrapy with cookies?", "links": [], "channel": "scrapy"},
{"date": "2014-04-24T02:32:50.709447+00:00", "nick": "willl", "message": "hi everybody, how I can work in  scrapy with cookies?", "links": [], "channel": "scrapy"},
{"date": "2014-04-24T02:34:27.874489+00:00", "nick": "dangra_", "message": "hi willl , what do you need to do with cookies? Scrapy handles cookies by default", "links": [], "channel": "scrapy"},
{"date": "2014-04-24T02:37:19.598699+00:00", "nick": "willl", "message": "where should I set the cookies from the website", "links": [], "channel": "scrapy"},
{"date": "2014-04-24T02:38:10.491185+00:00", "nick": "willl", "message": "(vos hablas espa\u00f1ol ?)", "links": [], "channel": "scrapy"},
{"date": "2014-04-24T02:40:55.777217+00:00", "nick": "willl", "message": "no se si me entiendas si no tratare de traducirlo, lo que yo  usando scrapy para obtener los datos de una pagina , pero me di cuenta que un dato que necesito solo lo muestra por  una cookie", "links": [], "channel": "scrapy"},
{"date": "2014-04-24T02:40:56.402890+00:00", "nick": "dangra_", "message": "Cookies are managed automatically, see http://doc.scrapy.org/en/latest/topics/download...", "links": ["http://doc.scrapy.org/en/latest/topics/downloader-middleware.html#module-scrapy.contrib.downloadermiddleware.cookies"], "channel": "scrapy"},
{"date": "2014-04-24T02:42:53.562822+00:00", "nick": "dangra_", "message": "I understand spanish but this is a english only channel and there isn't a #scrapy-en-espa\u00f1ol room yet :)", "links": [], "channel": "scrapy"},
{"date": "2014-04-24T02:43:42.998199+00:00", "nick": "dangra_", "message": "aside of that, there is a simple option like looking for Set-Cookie headers in Responses", "links": [], "channel": "scrapy"},
{"date": "2014-04-24T02:44:42.924364+00:00", "nick": "dangra_", "message": "another option requires more work, you have to access the cookies middleware from the spider and extract the cookies associated with the domain", "links": [], "channel": "scrapy"},
{"date": "2014-04-24T02:49:38.258305+00:00", "nick": "willl", "message": "muchas gracias por tu ayuda mirare el link que me recomendaste, se que esto es en ingles, tengo algunas cosas para decir pero al no dominar el ingles no se si al traducirlas queden bien, Gracias.", "links": [], "channel": "scrapy"},
{"date": "2014-04-24T02:50:42.798420+00:00", "nick": "dangra_", "message": "entiendes las 2 soluciones que te comente en ingles?", "links": [], "channel": "scrapy"},
{"date": "2014-04-24T02:51:04.641933+00:00", "nick": "dangra_", "message": "la primera es la mas sencilla solo que tenes que atrapar justo el response que setea la cookie", "links": [], "channel": "scrapy"},
{"date": "2014-04-24T02:54:06.912913+00:00", "nick": "willl", "message": "voy a probarla, solo que tengo  la duda ya que al poner  un request = Request(ur=\"...\",cookies={'...':'...'}) esto no se puede iterar", "links": [], "channel": "scrapy"},
{"date": "2014-04-24T15:56:09.616151+00:00", "nick": "Eypton", "message": "Anyone have issues installing pip in Mac 10.9", "links": [], "channel": "scrapy"},
{"date": "2014-04-24T16:03:08.047001+00:00", "nick": "vee__", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2014-04-24T16:03:22.910543+00:00", "nick": "vee__", "message": "I need to install scrapy on mac machine", "links": [], "channel": "scrapy"},
{"date": "2014-04-24T16:03:36.574896+00:00", "nick": "vee__", "message": "i was wondering if anyone can help me", "links": [], "channel": "scrapy"},
{"date": "2014-04-24T16:21:19.986501+00:00", "nick": "nyov", "message": "yesss... now, who's the first to monkeypatch a shortcut for xpath to x, extract to e and write code as `parse(self, r); r.x().e()` ? mwahaha", "links": [], "channel": "scrapy"},
{"date": "2014-04-24T19:02:20.056460+00:00", "nick": "Eypton", "message": "Does anyone know how to install LXML in MAc Os 10.9", "links": [], "channel": "scrapy"},
{"date": "2014-04-24T19:02:26.291471+00:00", "nick": "Eypton", "message": "I am on day 2", "links": [], "channel": "scrapy"},
{"date": "2014-04-24T19:03:17.253148+00:00", "nick": "Eypton", "message": "brb", "links": [], "channel": "scrapy"},
{"date": "2014-04-24T22:55:05.767450+00:00", "nick": "D5", "message": "i know you wont ansewr but i'll ask anyway", "links": [], "channel": "scrapy"},
{"date": "2014-04-24T22:55:24.190884+00:00", "nick": "D5", "message": "nm actually", "links": [], "channel": "scrapy"},
{"date": "2014-04-25T00:01:26.455969+00:00", "nick": "pepe__", "message": "someone has worked with scrapy with cookies", "links": [], "channel": "scrapy"},
{"date": "2014-04-25T08:51:39.529421+00:00", "nick": "fpghost84", "message": "Hi, how can I get access to cookies via the response object? I've tried response.meta['cookiejar'] and response.meta['cookies'] but hit KeyErrors...", "links": [], "channel": "scrapy"},
{"date": "2014-04-25T09:10:07.320202+00:00", "nick": "h2o_", "message": "hello", "links": [], "channel": "scrapy"},
{"date": "2014-04-25T15:10:01.951040+00:00", "nick": "JoeLinux", "message": "Hi guys. Is it possible to modify a Response object in-place, before the spiders process it?", "links": [], "channel": "scrapy"},
{"date": "2014-04-25T15:13:29.487579+00:00", "nick": "hzopak", "message": "this is the task of custom middlewares?", "links": [], "channel": "scrapy"},
{"date": "2014-04-25T15:14:03.140647+00:00", "nick": "JoeLinux", "message": "hzopak: Well, I have a custom SpiderMiddleware written, but I am trying to figure out how to actually modify the response that the spider will get.", "links": [], "channel": "scrapy"},
{"date": "2014-04-25T15:14:27.684357+00:00", "nick": "JoeLinux", "message": "hzopak: response.replace(body=new_body) doesn't actually do that. It only returns a new Response object.", "links": [], "channel": "scrapy"},
{"date": "2014-04-25T15:14:45.246096+00:00", "nick": "hzopak", "message": "ahh, hmm", "links": [], "channel": "scrapy"},
{"date": "2014-04-25T15:14:49.635928+00:00", "nick": "JoeLinux", "message": "hzopak: The given response object in the middleware appears to be read-only.", "links": [], "channel": "scrapy"},
{"date": "2014-04-25T15:14:54.152420+00:00", "nick": "hzopak", "message": "unsure", "links": [], "channel": "scrapy"},
{"date": "2014-04-25T15:14:54.937078+00:00", "nick": "nyov", "message": "yes it's possible, read the docs on response.replace", "links": [], "channel": "scrapy"},
{"date": "2014-04-25T15:15:01.069012+00:00", "nick": "nyov", "message": "response.replace(body=body)", "links": [], "channel": "scrapy"},
{"date": "2014-04-25T15:15:28.420769+00:00", "nick": "nyov", "message": "or rather response = response.replace(body=body)", "links": [], "channel": "scrapy"},
{"date": "2014-04-25T15:15:29.917156+00:00", "nick": "JoeLinux", "message": "nyov: That seems to return a new Response with the given arguments, not modify the existing response.", "links": [], "channel": "scrapy"},
{"date": "2014-04-25T15:15:38.633515+00:00", "nick": "nyov", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2014-04-25T15:15:51.476349+00:00", "nick": "JoeLinux", "message": "nyov: Right, but how do I pass that new response back into the stream of middlewares?", "links": [], "channel": "scrapy"},
{"date": "2014-04-25T15:16:04.350545+00:00", "nick": "JoeLinux", "message": "nyov: I want to keep the response flowing through the chain as expected... but modified slightly.", "links": [], "channel": "scrapy"},
{"date": "2014-04-25T15:16:20.076178+00:00", "nick": "nyov", "message": "you have to pass on the new response, depends on the type of middleware i think", "links": [], "channel": "scrapy"},
{"date": "2014-04-25T15:16:52.495549+00:00", "nick": "JoeLinux", "message": "nyov: I'm using a SpiderMiddleware. Can I return a response and it will be used? It seems from the docs that I can either return None or raise an exception only.", "links": [], "channel": "scrapy"},
{"date": "2014-04-25T15:17:21.907726+00:00", "nick": "JoeLinux", "message": "nyov: DownloaderMiddleware won't work, because I'd be working with a Request rather than a finished Response.", "links": [], "channel": "scrapy"},
{"date": "2014-04-25T15:17:55.068847+00:00", "nick": "nyov", "message": "i can't currently search for you, unless you give me the IP of scrapy.readthedocs.org", "links": ["http://scrapy.readthedocs.org"], "channel": "scrapy"},
{"date": "2014-04-25T15:18:34.337060+00:00", "nick": "nyov", "message": "but spider middleware imho returns reponses", "links": [], "channel": "scrapy"},
{"date": "2014-04-25T15:19:31.955025+00:00", "nick": "JoeLinux", "message": "nyov: http://162.209.99.176/en/latest/topics/spider-m...", "links": ["http://162.209.99.176/en/latest/topics/spider-middleware.html#writing-your-own-spider-middleware"], "channel": "scrapy"},
{"date": "2014-04-25T15:19:43.718708+00:00", "nick": "JoeLinux", "message": "nyov: That's the section I've been reading up on.", "links": [], "channel": "scrapy"},
{"date": "2014-04-25T15:20:46.072124+00:00", "nick": "JoeLinux", "message": "nyov: Eh, that link didn't work. That's the IP for scrapy.readthedocs.org, but it looks like maybe the domain is handled by the web server and not directly via the IP.", "links": ["http://scrapy.readthedocs.org"], "channel": "scrapy"},
{"date": "2014-04-25T15:20:54.923257+00:00", "nick": "JoeLinux", "message": "nyov: VirtualHost maybe", "links": [], "channel": "scrapy"},
{"date": "2014-04-25T15:21:17.405628+00:00", "nick": "nyov", "message": "works, though a bit slow without media.rtd lol", "links": [], "channel": "scrapy"},
{"date": "2014-04-25T15:21:44.644402+00:00", "nick": "JoeLinux", "message": "nyov: Ha", "links": [], "channel": "scrapy"},
{"date": "2014-04-25T15:22:46.493707+00:00", "nick": "nyov", "message": "okay, so process_spider_output() works on spider output", "links": [], "channel": "scrapy"},
{"date": "2014-04-25T15:23:02.582144+00:00", "nick": "JoeLinux", "message": "nyov: Right. But that's too late in the chain.", "links": [], "channel": "scrapy"},
{"date": "2014-04-25T15:23:07.290147+00:00", "nick": "nyov", "message": "what do you actually try to do? in the spider middleware, the response is already done", "links": [], "channel": "scrapy"},
{"date": "2014-04-25T15:23:30.986282+00:00", "nick": "JoeLinux", "message": "nyov: I'm trying to run an XSLT transformation on the response.body before the spider gets to it.", "links": [], "channel": "scrapy"},
{"date": "2014-04-25T15:24:02.801216+00:00", "nick": "JoeLinux", "message": "nyov: Sorry, to be more specific, I want this done before parse() is called in the spider.", "links": [], "channel": "scrapy"},
{"date": "2014-04-25T15:25:15.180014+00:00", "nick": "nyov", "message": "hm, can't you do it in the spider code? if you need it in multiple spiders, you should fix it in a downloader middleware I think", "links": [], "channel": "scrapy"},
{"date": "2014-04-25T15:26:12.683499+00:00", "nick": "nyov", "message": "there you only have to take into account process_response() which returns a response", "links": [], "channel": "scrapy"},
{"date": "2014-04-25T15:26:24.028888+00:00", "nick": "nyov", "message": "and you can just do return response.replace(body=body)", "links": [], "channel": "scrapy"},
{"date": "2014-04-25T15:31:16.140574+00:00", "nick": "JoeLinux", "message": "nyov: process_response() <-- genius", "links": [], "channel": "scrapy"},
{"date": "2014-04-25T15:31:28.946012+00:00", "nick": "JoeLinux", "message": "nyov: I was looking in the wrong place. Thanks a bunch!", "links": [], "channel": "scrapy"},
{"date": "2014-04-25T15:31:40.778638+00:00", "nick": "nyov", "message": "no problem", "links": [], "channel": "scrapy"},
{"date": "2014-04-25T15:36:00.084382+00:00", "nick": "JoeLinux", "message": "nyov: Yep that worked. The response seen by the parse() method is now the new XSLT transformed HTML. Perfect, you're the man.", "links": [], "channel": "scrapy"},
{"date": "2014-04-25T15:36:13.488231+00:00", "nick": "JoeLinux", "message": "nyov: Or woman.", "links": [], "channel": "scrapy"},
{"date": "2014-04-25T19:47:46.806910+00:00", "nick": "sdf10809", "message": "hi i have a scrapy project that's pretty much complete and i can scrape a single url with scrapy crawl spider -a url={URL} but how do i start scrapy the many links automatically going off my starturl and the sitemap links, which is how it is designed", "links": [], "channel": "scrapy"},
{"date": "2014-04-25T23:36:41.233740+00:00", "nick": "cesurasean1", "message": "using the craigslist tutorial/example, how would i go about scraping emails from craigslist into a csv?", "links": [], "channel": "scrapy"},
{"date": "2014-04-26T03:24:58.538717+00:00", "nick": "cesurasean1", "message": "anyone?", "links": [], "channel": "scrapy"},
{"date": "2014-04-26T14:56:30.463028+00:00", "nick": "fpghost84", "message": "Hi, anyone know how I can get access to the stats when using the basic Spider? Instead of getting the stats printed into the log at the end of a scrape, I'd like to email them somewhere. I know the Crawler has the spider_stats method, but I can't see anything for the Spider. How to do I also do this upon \"scrape finished\"?", "links": [], "channel": "scrapy"},
{"date": "2014-04-26T15:08:22.060836+00:00", "nick": "Digenis_", "message": "I would be interested in this too", "links": [], "channel": "scrapy"},
{"date": "2014-04-26T15:10:35.990076+00:00", "nick": "Digenis", "message": "fpghost84: https://scrapy.readthedocs.org/en/latest/topics...", "links": ["https://scrapy.readthedocs.org/en/latest/topics/signals.html#engine-stopped"], "channel": "scrapy"},
{"date": "2014-04-26T15:11:21.054524+00:00", "nick": "fpghost84", "message": "Digenis: I found I get access to the stats at least with self.crawler.stats.get_stats() in my spider class", "links": [], "channel": "scrapy"},
{"date": "2014-04-26T15:11:22.239031+00:00", "nick": "Digenis", "message": "I guess you could hook an extension on that signal", "links": [], "channel": "scrapy"},
{"date": "2014-04-26T15:11:51.084044+00:00", "nick": "fpghost84", "message": "There is also apparently the method \"handle_spider_closed\" to override?", "links": [], "channel": "scrapy"},
{"date": "2014-04-26T15:11:51.178477+00:00", "nick": "Digenis", "message": "maybe you could do this in the spider", "links": [], "channel": "scrapy"},
{"date": "2014-04-26T15:12:11.172796+00:00", "nick": "Digenis", "message": "I thought you wanted something for a whole project", "links": [], "channel": "scrapy"},
{"date": "2014-04-26T15:12:28.958521+00:00", "nick": "fpghost84", "message": "well, yes I guess that might be better..", "links": [], "channel": "scrapy"},
{"date": "2014-04-26T15:12:52.932970+00:00", "nick": "fpghost84", "message": "So how to catch this signal exactly?", "links": [], "channel": "scrapy"},
{"date": "2014-04-26T15:12:57.801169+00:00", "nick": "Digenis", "message": "So, in you project dir", "links": [], "channel": "scrapy"},
{"date": "2014-04-26T15:13:22.117006+00:00", "nick": "Digenis", "message": "write python file that defines an extention class", "links": [], "channel": "scrapy"},
{"date": "2014-04-26T15:14:01.856451+00:00", "nick": "Digenis", "message": "which hooks on that signal", "links": [], "channel": "scrapy"},
{"date": "2014-04-26T15:14:44.473440+00:00", "nick": "fpghost84", "message": "ok, then add it to the settings.py EXTENSIONS dictionary...", "links": [], "channel": "scrapy"},
{"date": "2014-04-26T15:14:52.527005+00:00", "nick": "Digenis", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2014-04-26T15:15:13.134241+00:00", "nick": "Digenis", "message": "see the link above, this one https://scrapy.readthedocs.org/en/latest/topics...", "links": ["https://scrapy.readthedocs.org/en/latest/topics/api.html#topics-api-signals"], "channel": "scrapy"},
{"date": "2014-04-26T15:15:37.227667+00:00", "nick": "Digenis", "message": "and the documentation on the crawler stats", "links": [], "channel": "scrapy"},
{"date": "2014-04-26T15:15:51.615054+00:00", "nick": "fpghost84", "message": "ok thanks, I need to have a good read", "links": [], "channel": "scrapy"},
{"date": "2014-04-26T15:17:30.818385+00:00", "nick": "Digenis", "message": "this part of the doc on the stat collector https://scrapy.readthedocs.org/en/latest/topics...", "links": ["https://scrapy.readthedocs.org/en/latest/topics/api.html#scrapy.statscol.StatsCollector.get_stats"], "channel": "scrapy"},
{"date": "2014-04-26T15:29:15.091190+00:00", "nick": "fpghost84", "message": "Digenis: just read in the extensions docs there is already a StatsMailer extension that does exactly this scrapy.contrib.statsmailer.StatsMailer....!", "links": [], "channel": "scrapy"},
{"date": "2014-04-26T15:33:20.560970+00:00", "nick": "fpghost84", "message": "Nevertheless, custom one could be nice to only email stats if errors etc...", "links": [], "channel": "scrapy"},
{"date": "2014-04-26T15:43:29.583181+00:00", "nick": "Digenis", "message": "lol, didn't notice", "links": [], "channel": "scrapy"},
{"date": "2014-04-27T09:15:57.488158+00:00", "nick": "gautamgupta", "message": "Hi, I'm trying to install scrapy since some hours. But I'm stuck at this error: ImportError: No module named lxml.html", "links": [], "channel": "scrapy"},
{"date": "2014-04-27T09:16:04.024066+00:00", "nick": "gautamgupta", "message": "when I write scrapy in the terminal.", "links": [], "channel": "scrapy"},
{"date": "2014-04-27T09:16:43.457378+00:00", "nick": "gautamgupta", "message": "Googled, checked out some questions on SO, but can't seem to get it fixed.", "links": [], "channel": "scrapy"},
{"date": "2014-04-27T09:19:46.803742+00:00", "nick": "hzopak", "message": "you'll have to install lxml.html", "links": [], "channel": "scrapy"},
{"date": "2014-04-27T09:20:29.028508+00:00", "nick": "hzopak", "message": "well, more explicitly lxml", "links": [], "channel": "scrapy"},
{"date": "2014-04-27T09:20:36.032696+00:00", "nick": "gautamgupta", "message": "$ STATIC_DEPS=true pip install lxml Requirement already satisfied (use --upgrade to upgrade): lxml in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages", "links": [], "channel": "scrapy"},
{"date": "2014-04-27T09:20:46.144874+00:00", "nick": "hzopak", "message": "http://lxml.de/installation.html", "links": ["http://lxml.de/installation.html"], "channel": "scrapy"},
{"date": "2014-04-27T09:20:54.111532+00:00", "nick": "hzopak", "message": "try and install that first indepentantly", "links": [], "channel": "scrapy"},
{"date": "2014-04-27T09:21:09.895092+00:00", "nick": "hzopak", "message": "hmm, ok", "links": [], "channel": "scrapy"},
{"date": "2014-04-27T09:21:10.817856+00:00", "nick": "gautamgupta", "message": "When I do --upgrade --force-reinstall, it still doesn't work", "links": [], "channel": "scrapy"},
{"date": "2014-04-27T09:21:17.111332+00:00", "nick": "hzopak", "message": "well, just open up a terminal and type", "links": [], "channel": "scrapy"},
{"date": "2014-04-27T09:21:19.160895+00:00", "nick": "hzopak", "message": "import lxml.html", "links": [], "channel": "scrapy"},
{"date": "2014-04-27T09:21:26.552718+00:00", "nick": "hzopak", "message": "python terminal*", "links": [], "channel": "scrapy"},
{"date": "2014-04-27T09:23:32.585895+00:00", "nick": "gautamgupta", "message": "executes with no output.", "links": [], "channel": "scrapy"},
{"date": "2014-04-27T09:23:35.988926+00:00", "nick": "gautamgupta", "message": "(ofc)", "links": [], "channel": "scrapy"},
{"date": "2014-04-27T09:24:14.141889+00:00", "nick": "hzopak", "message": "well yes, that is quite strange indeed", "links": [], "channel": "scrapy"},
{"date": "2014-04-27T09:24:36.171075+00:00", "nick": "gautamgupta", "message": "http://i.imgur.com/UuRdbhg.png", "links": ["http://i.imgur.com/UuRdbhg.png"], "channel": "scrapy"},
{"date": "2014-04-27T09:26:23.552159+00:00", "nick": "hzopak", "message": "on that same window, try: import scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-04-27T09:27:35.956537+00:00", "nick": "gautamgupta", "message": "Runs. :(", "links": [], "channel": "scrapy"},
{"date": "2014-04-27T09:29:13.880753+00:00", "nick": "hzopak", "message": "in your normal terminal", "links": [], "channel": "scrapy"},
{"date": "2014-04-27T09:29:15.924927+00:00", "nick": "hzopak", "message": "type which scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-04-27T09:29:33.675054+00:00", "nick": "hzopak", "message": "make sure it's executing from a place you'd expect", "links": [], "channel": "scrapy"},
{"date": "2014-04-27T09:30:35.501039+00:00", "nick": "gautamgupta", "message": "/Library/Frameworks/Python.framework/Versions/2.7/bin/scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-04-27T09:30:59.923554+00:00", "nick": "gautamgupta", "message": "$ which lxml howevery doesn't return anything", "links": [], "channel": "scrapy"},
{"date": "2014-04-27T09:40:51.966199+00:00", "nick": "hzopak", "message": "no, it shouldn't anything either", "links": [], "channel": "scrapy"},
{"date": "2014-04-27T09:40:58.632512+00:00", "nick": "hzopak", "message": "od", "links": [], "channel": "scrapy"},
{"date": "2014-04-27T09:41:05.486407+00:00", "nick": "hzopak", "message": "i'm unsure, sorry", "links": [], "channel": "scrapy"},
{"date": "2014-04-27T09:47:17.056569+00:00", "nick": "gautamgupta", "message": "hmm :(", "links": [], "channel": "scrapy"},
{"date": "2014-04-27T10:26:35.217454+00:00", "nick": "jsjc", "message": "has anybody used scrapy-redis?", "links": [], "channel": "scrapy"},
{"date": "2014-04-27T10:26:55.789695+00:00", "nick": "jsjc", "message": "Is there a way of speeding up and making more concurrent requests? instead of going one by one?", "links": [], "channel": "scrapy"},
{"date": "2014-04-27T10:35:04.727096+00:00", "nick": "gautamgupta", "message": "hzopak: So this was the error which was not letting lxml build -- http://kaspermunck.github.io/2014/03/fixing-cla...", "links": ["http://kaspermunck.github.io/2014/03/fixing-clang-error/"], "channel": "scrapy"},
{"date": "2014-04-27T10:35:23.338671+00:00", "nick": "gautamgupta", "message": "Side-effects of upgrading to the latest software.", "links": [], "channel": "scrapy"},
{"date": "2014-04-27T11:24:08.790844+00:00", "nick": "hzopak", "message": "ahh ok, well it's good that you fixed it", "links": [], "channel": "scrapy"},
{"date": "2014-04-27T15:46:42.499150+00:00", "nick": "AndroidLoverInSF", "message": "how to workaround amazon capchta, sometimes i get a captcha with message \"Sorry, we just need to make sure you're not a robot. For best results, please make sure your browser is accepting cookies\"", "links": [], "channel": "scrapy"},
{"date": "2014-04-27T15:47:52.067347+00:00", "nick": "AndroidLoverInSF", "message": "and scrappy has nothing to scrape.  how are they detecting scrappy? when i go to firefox manually amazon page shows up fine. even when i turn off cookies in my browser, i can still see the amazon page fine, but scrappy can't, especially during off hours", "links": [], "channel": "scrapy"},
{"date": "2014-04-27T15:51:40.167127+00:00", "nick": "nyov", "message": ":D", "links": [], "channel": "scrapy"},
{"date": "2014-04-27T15:56:29.148451+00:00", "nick": "nyov", "message": "there's a hundred ways to differentiate between a bot and a legit browser based on their behaviour", "links": [], "channel": "scrapy"},
{"date": "2014-04-27T20:37:57.196714+00:00", "nick": "seni", "message": "Hi!", "links": [], "channel": "scrapy"},
{"date": "2014-04-27T20:38:58.345003+00:00", "nick": "seni", "message": "Is it possible to set a spider to download only first n bytes from a webpage?", "links": [], "channel": "scrapy"},
{"date": "2014-04-28T03:15:22.187835+00:00", "nick": "seni_", "message": "Hi!", "links": [], "channel": "scrapy"},
{"date": "2014-04-28T03:16:19.903526+00:00", "nick": "seni_", "message": "Is it possible to set a spider to download only first n bytes from a webpage? I have a \"big-sized\" webpage and I only need the head tags.", "links": [], "channel": "scrapy"},
{"date": "2014-04-28T03:34:22.403082+00:00", "nick": "Digenis", "message": "http defines a request header for this", "links": [], "channel": "scrapy"},
{"date": "2014-04-28T04:07:48.520427+00:00", "nick": "seni_", "message": "I tried setting a middleware with request.headers['Content-length']= 100 in process_request, but the spider just hangs there", "links": [], "channel": "scrapy"},
{"date": "2014-04-28T04:30:17.759368+00:00", "nick": "Digenis", "message": "hangs?", "links": [], "channel": "scrapy"},
{"date": "2014-04-28T04:34:23.765228+00:00", "nick": "Digenis", "message": "I think it shouldn't", "links": [], "channel": "scrapy"},
{"date": "2014-04-28T04:34:54.984026+00:00", "nick": "Digenis", "message": "but as I said, http merely defines", "links": [], "channel": "scrapy"},
{"date": "2014-04-28T04:35:23.137250+00:00", "nick": "Digenis", "message": "the http server should implement this feature", "links": [], "channel": "scrapy"},
{"date": "2014-04-28T04:35:39.244023+00:00", "nick": "Digenis", "message": "and maybe not all of them do it properly or at all", "links": [], "channel": "scrapy"},
{"date": "2014-04-28T16:22:48.386423+00:00", "nick": "seni", "message": "<Digenis> Thanks for your help. I found that the website is using chunked transfers, so 'content-length' is not needed.", "links": [], "channel": "scrapy"},
{"date": "2014-04-28T16:23:00.517277+00:00", "nick": "seni", "message": "Any idea how to work this around?", "links": [], "channel": "scrapy"},
{"date": "2014-04-28T19:04:00.520509+00:00", "nick": "hello12345", "message": "pablohof: Hi", "links": [], "channel": "scrapy"},
{"date": "2014-04-28T23:52:36.901208+00:00", "nick": "nick1", "message": "Hey guys I could really use some help with my scrapy project", "links": [], "channel": "scrapy"},
{"date": "2014-04-28T23:53:07.432305+00:00", "nick": "nick1", "message": "I have a spider here: http://pastebin.com/CWabf0KF but I need to rework it slightly to work with a new URL", "links": ["http://pastebin.com/CWabf0KF"], "channel": "scrapy"},
{"date": "2014-04-28T23:53:11.771226+00:00", "nick": "nick1", "message": "could anybody help me with this?", "links": [], "channel": "scrapy"},
{"date": "2014-04-29T00:01:38.810541+00:00", "nick": "remote", "message": "nick1: help you as-in offering advice?", "links": [], "channel": "scrapy"},
{"date": "2014-04-29T00:03:41.799463+00:00", "nick": "nick1", "message": "remote, I have very little programming experience so any advice would be appreciated!", "links": [], "channel": "scrapy"},
{"date": "2014-04-29T00:04:44.605904+00:00", "nick": "nick1", "message": "This worked for a different URL but i changed the start_urls", "links": [], "channel": "scrapy"},
{"date": "2014-04-29T00:04:55.502090+00:00", "nick": "nick1", "message": "If you go to https://banweb.banner.vt.edu/ssb/prod/HZSKVTSC....", "links": ["https://banweb.banner.vt.edu/ssb/prod/HZSKVTSC.P_ProcRequest?CAMPUS=0&amp=&TERMYEAR=201409&amp=&SCHDTYPE=%25&amp=&SUBJ_CODE=&amp=&CRSE_NUMBER=&amp=&open_only=&amp=&CRN=&amp=&INST_NAME=&amp=&CORE_CODE=AR%25&amp=&PRINT_FRIEND=Y&amp=&history=N&amp=&BTN_PRESSED=Printer+Friendly+List"], "channel": "scrapy"},
{"date": "2014-04-29T00:05:06.308049+00:00", "nick": "nick1", "message": "You can see I want to parse out each course for it's own object in the database", "links": [], "channel": "scrapy"},
{"date": "2014-04-29T00:05:16.443770+00:00", "nick": "nick1", "message": "do you think this is something you could help with remote?", "links": [], "channel": "scrapy"},
{"date": "2014-04-29T00:07:11.206250+00:00", "nick": "remote", "message": "it's hard to help without a precise problem", "links": [], "channel": "scrapy"},
{"date": "2014-04-29T00:07:37.666815+00:00", "nick": "remote", "message": "what's the problem and what have you tried to do to fix it?", "links": [], "channel": "scrapy"},
{"date": "2014-04-29T00:08:06.434222+00:00", "nick": "nick1", "message": "Sorry haha, back in a minute with the exact problem. Thank you for taking the time to respond.", "links": [], "channel": "scrapy"},
{"date": "2014-04-29T00:13:37.826312+00:00", "nick": "nick1", "message": "remote, when I run http://dpaste.com/1802440/ it doesn't find any courses. I think the selector on line 68 is incorrect but I'm not sure how to fix it.", "links": ["http://dpaste.com/1802440/"], "channel": "scrapy"},
{"date": "2014-04-29T00:30:10.236951+00:00", "nick": "nick1", "message": "remote are you still there?", "links": [], "channel": "scrapy"},
{"date": "2014-04-29T00:30:18.241273+00:00", "nick": "nick1", "message": "I think I'm missing a call to parseCourses,", "links": [], "channel": "scrapy"},
{"date": "2014-04-29T16:21:44.349676+00:00", "nick": "adam__", "message": "Hi!  Is scrapyd the canonical way to distribute scrapy scrapes across multiple machines?  Or are there other solutions I should look into?  https://github.com/scrapy/scrapyd", "links": ["https://github.com/scrapy/scrapyd"], "channel": "scrapy"},
{"date": "2014-04-29T16:35:23.185236+00:00", "nick": "adam__", "message": "Or, I've seen this mentioned favorably, too?  https://github.com/darkrho/scrapy-redis", "links": ["https://github.com/darkrho/scrapy-redis"], "channel": "scrapy"},
{"date": "2014-04-30T11:00:45.532759+00:00", "nick": "h4k1m", "message": "hi guys", "links": [], "channel": "scrapy"},
{"date": "2014-04-30T11:01:07.438038+00:00", "nick": "h4k1m", "message": "anyone has tried to install scrapyd on a non-ubuntu os?", "links": [], "channel": "scrapy"},
{"date": "2014-04-30T11:27:52.484184+00:00", "nick": "Roux_taff", "message": "which os do you need?", "links": [], "channel": "scrapy"},
{"date": "2014-04-30T11:28:18.589052+00:00", "nick": "Roux_taff", "message": "i made a rpm package for centos that should work also on fedora", "links": [], "channel": "scrapy"},
{"date": "2014-04-30T11:29:27.966439+00:00", "nick": "Roux_taff", "message": "see here https://github.com/medialab/scrapyd/tree/medial...", "links": ["https://github.com/medialab/scrapyd/tree/medialab/rpms"], "channel": "scrapy"},
{"date": "2014-04-30T14:08:05.083977+00:00", "nick": "regenpijp", "message": "morning", "links": [], "channel": "scrapy"},
{"date": "2014-04-30T16:21:19.735020+00:00", "nick": "h4k1m", "message": "hi guys", "links": [], "channel": "scrapy"},
{"date": "2014-04-30T16:21:39.427624+00:00", "nick": "h4k1m", "message": "anyone has tried to install scrapyd service on a non-ubuntu machine?", "links": [], "channel": "scrapy"},
{"date": "2014-04-30T17:47:15.449295+00:00", "nick": "gautamgupta", "message": "If I want to scrape links from DoctorProfile.aspx?Key=0 to DoctorProfile.aspx?Key=100, how should I put my start_urls?", "links": [], "channel": "scrapy"},
{"date": "2014-04-30T18:37:30.671552+00:00", "nick": "gautamgupta", "message": "Is anyone there who can help me with a spider?", "links": [], "channel": "scrapy"},
{"date": "2014-04-30T18:41:33.196902+00:00", "nick": "gautamgupta", "message": "I've this exact same query: http://stackoverflow.com/questions/15880644/scr...", "links": ["http://stackoverflow.com/questions/15880644/scrapy-getting-response-url-inside-crawlspider-rule"], "channel": "scrapy"},
{"date": "2014-04-30T18:43:01.902524+00:00", "nick": "gautamgupta", "message": "I've many links (#1i), from which I get doctor links (#2i) (which calls parse_doctor). But I want to access #1i from inside parse_doctor. How can I access response.url there?", "links": [], "channel": "scrapy"},
{"date": "2014-04-30T19:15:21.893981+00:00", "nick": "nyov", "message": "< gautamgupta> If I want to scrape links from DoctorProfile.aspx?Key=0 to DoctorProfile.aspx?Key=100, how should I put my start_urls?", "links": [], "channel": "scrapy"},
{"date": "2014-04-30T19:15:24.897223+00:00", "nick": "nyov", "message": "gautamgupta: https://gist.github.com/nyov/9266443", "links": ["https://gist.github.com/nyov/9266443"], "channel": "scrapy"},
{"date": "2014-04-30T19:16:09.866208+00:00", "nick": "gautamgupta", "message": "nyov: Thanks! And do you've an answer to my second query?", "links": [], "channel": "scrapy"},
{"date": "2014-04-30T19:17:15.727187+00:00", "nick": "nyov", "message": "i don't understand that question, sorry", "links": [], "channel": "scrapy"},
{"date": "2014-04-30T19:18:11.856126+00:00", "nick": "gautamgupta", "message": "nyov: Check http://stackoverflow.com/questions/15880644/scr... want to use", "links": ["http://stackoverflow.com/questions/15880644/scrapy-getting-response-url-inside-crawlspider-ruleI"], "channel": "scrapy"},
{"date": "2014-04-30T19:18:22.440379+00:00", "nick": "gautamgupta", "message": "http://stackoverflow.com/questions/15880644/scr...", "links": ["http://stackoverflow.com/questions/15880644/scrapy-getting-response-url-inside-crawlspider-rule"], "channel": "scrapy"},
{"date": "2014-04-30T19:18:39.464159+00:00", "nick": "gautamgupta", "message": "I want to use cb_kwargs to pass response.url as the third argument to parse function", "links": [], "channel": "scrapy"},
{"date": "2014-04-30T19:18:46.191578+00:00", "nick": "gautamgupta", "message": "Is that possible?", "links": [], "channel": "scrapy"},
{"date": "2014-04-30T19:19:27.896271+00:00", "nick": "nyov", "message": "no, response.url isn't known at that time.", "links": [], "channel": "scrapy"},
{"date": "2014-04-30T19:20:24.950424+00:00", "nick": "gautamgupta", "message": "nyov: and inside the parse function, there is no way to access the 'referer' ?", "links": [], "channel": "scrapy"},
{"date": "2014-04-30T19:20:48.908491+00:00", "nick": "nyov", "message": "hm, you want to do parse_response1() -> parse_response2()?", "links": [], "channel": "scrapy"},
{"date": "2014-04-30T19:22:27.415836+00:00", "nick": "nyov", "message": "the referer is in request.headers.get('Referer') I think", "links": [], "channel": "scrapy"},
{"date": "2014-04-30T19:23:46.169173+00:00", "nick": "gautamgupta", "message": "nyov: check the highlighted line in http://pastebin.com/KGG70Fj6", "links": ["http://pastebin.com/KGG70Fj6"], "channel": "scrapy"},
{"date": "2014-04-30T19:24:04.483994+00:00", "nick": "nyov", "message": "gautamgupta: sorry, in the parse function that would be response.request.headers.get('Referer')", "links": [], "channel": "scrapy"},
{"date": "2014-04-30T19:24:58.594577+00:00", "nick": "gautamgupta", "message": "nyov: Thanks a lot man! I'll try that :)", "links": [], "channel": "scrapy"},
{"date": "2014-04-30T19:27:13.892290+00:00", "nick": "gautamgupta", "message": "nyov: Works like a charm!", "links": [], "channel": "scrapy"},
{"date": "2014-04-30T19:27:18.417655+00:00", "nick": "nyov", "message": "nice", "links": [], "channel": "scrapy"},
{"date": "2014-04-30T19:30:37.826441+00:00", "nick": "gautamgupta", "message": "nyov: And in the same code, can I implement strip() on doctor['name'] to remove all spaces, \\r's, \\n's or would I have to define Item Loaders? This is my first time with Scrapy and Python, sorry about the naive question.", "links": [], "channel": "scrapy"},
{"date": "2014-04-30T19:31:44.870187+00:00", "nick": "gautamgupta", "message": "input_processor=MapCompose(remove_entities, lambda v: v.strip()), in items.py doesn't seem to work", "links": [], "channel": "scrapy"},
{"date": "2014-04-30T21:10:42.332856+00:00", "nick": "jab3z", "message": "mkoistinen: cool stuff littleipsum. Until now I was using sublime text lorem command.", "links": [], "channel": "scrapy"},
{"date": "2014-04-30T21:11:13.813920+00:00", "nick": "jab3z", "message": "sorry guys, wrong channel :(", "links": [], "channel": "scrapy"},
{"date": "2014-04-30T23:55:29.269121+00:00", "nick": "mbailey__", "message": "hello everyone", "links": [], "channel": "scrapy"},
{"date": "2014-04-30T23:55:47.869037+00:00", "nick": "mbailey__", "message": "Can anyone point me to an example of using the crawling spider with an initial login", "links": [], "channel": "scrapy"},
{"date": "2014-04-30T23:55:48.842790+00:00", "nick": "mbailey__", "message": "?", "links": [], "channel": "scrapy"},
{"date": "2014-05-01T04:12:38.425674+00:00", "nick": "muxdemux", "message": "hey", "links": [], "channel": "scrapy"},
{"date": "2014-05-01T04:12:54.478405+00:00", "nick": "muxdemux", "message": "how do i tie a request to it\u2019s corresponding response?", "links": [], "channel": "scrapy"},
{"date": "2014-05-01T04:45:31.551222+00:00", "nick": "nyov", "message": "muxdemux: that's already done", "links": [], "channel": "scrapy"},
{"date": "2014-05-01T04:46:00.116427+00:00", "nick": "nyov", "message": "response.request is the request that resulted in the response", "links": [], "channel": "scrapy"},
{"date": "2014-05-01T04:46:01.192335+00:00", "nick": "muxdemux", "message": "via callbacks?", "links": [], "channel": "scrapy"},
{"date": "2014-05-01T04:46:07.188132+00:00", "nick": "muxdemux", "message": "AHHHHHH", "links": [], "channel": "scrapy"},
{"date": "2014-05-01T04:46:09.347876+00:00", "nick": "muxdemux", "message": "hoooray", "links": [], "channel": "scrapy"},
{"date": "2014-05-01T04:46:16.517852+00:00", "nick": "muxdemux", "message": "nyov: YTMND", "links": [], "channel": "scrapy"},
{"date": "2014-05-01T04:47:03.215466+00:00", "nick": "nyov", "message": "i'll have to google that? :D", "links": [], "channel": "scrapy"},
{"date": "2014-05-01T04:47:19.953571+00:00", "nick": "muxdemux", "message": "you the man now, dog", "links": [], "channel": "scrapy"},
{"date": "2014-05-01T04:47:25.441080+00:00", "nick": "nyov", "message": "lol", "links": [], "channel": "scrapy"},
{"date": "2014-05-01T04:49:15.732026+00:00", "nick": "muxdemux", "message": "so where can i get at the response?", "links": [], "channel": "scrapy"},
{"date": "2014-05-01T04:49:19.770569+00:00", "nick": "muxdemux", "message": "in some of the signals?", "links": [], "channel": "scrapy"},
{"date": "2014-05-01T04:49:25.578437+00:00", "nick": "muxdemux", "message": "like signals.item_passed?", "links": [], "channel": "scrapy"},
{"date": "2014-05-01T04:49:56.596817+00:00", "nick": "muxdemux", "message": "hmmm", "links": [], "channel": "scrapy"},
{"date": "2014-05-01T04:49:58.993425+00:00", "nick": "nyov", "message": "i don't think", "links": [], "channel": "scrapy"},
{"date": "2014-05-01T04:50:06.377335+00:00", "nick": "muxdemux", "message": "looks like only item_scraped, but not item_passed", "links": [], "channel": "scrapy"},
{"date": "2014-05-01T04:52:02.284991+00:00", "nick": "nyov", "message": "you have the response passed in the spider's parse() method. not enough?", "links": [], "channel": "scrapy"},
{"date": "2014-05-01T04:52:45.554243+00:00", "nick": "muxdemux", "message": "well maybe but I\u2019m already using signals", "links": [], "channel": "scrapy"},
{"date": "2014-05-01T04:53:19.472102+00:00", "nick": "muxdemux", "message": "AH hooray", "links": [], "channel": "scrapy"},
{"date": "2014-05-01T04:53:25.502532+00:00", "nick": "muxdemux", "message": "this just completed my work for the night!", "links": [], "channel": "scrapy"},
{"date": "2014-05-01T04:53:33.756784+00:00", "nick": "muxdemux", "message": "I\u2019m a super noob to scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-05-01T04:53:41.224346+00:00", "nick": "muxdemux", "message": "but I\u2019ve been diving into it over the last couple days", "links": [], "channel": "scrapy"},
{"date": "2014-05-01T04:53:48.922847+00:00", "nick": "muxdemux", "message": "finally sorta got a handle on it", "links": [], "channel": "scrapy"},
{"date": "2014-05-01T04:53:56.591640+00:00", "nick": "nyov", "message": "nice", "links": [], "channel": "scrapy"},
{"date": "2014-05-01T04:54:00.591083+00:00", "nick": "muxdemux", "message": "or at least I\u2019ve bent it to my will", "links": [], "channel": "scrapy"},
{"date": "2014-05-01T04:54:09.722961+00:00", "nick": "muxdemux", "message": "but likely I\u2019m not using it as intended", "links": [], "channel": "scrapy"},
{"date": "2014-05-01T04:55:04.818473+00:00", "nick": "nyov", "message": "well, it's pretty open in how to use it ;)", "links": [], "channel": "scrapy"},
{"date": "2014-05-01T04:55:31.032121+00:00", "nick": "muxdemux", "message": "yea i enjoyed reading through the source a bit today", "links": [], "channel": "scrapy"},
{"date": "2014-05-01T04:56:06.840631+00:00", "nick": "muxdemux", "message": "idk why but between readthedocs and having the source on github it felt \u2026 some how more accessible that other things", "links": [], "channel": "scrapy"},
{"date": "2014-05-01T04:57:56.112228+00:00", "nick": "nyov", "message": "yeah, I usually grep in my git clone first when looking for something. thats pretty nice", "links": [], "channel": "scrapy"},
{"date": "2014-05-01T08:45:21.264770+00:00", "nick": "gautamgupta", "message": "Is there a data sanitization 101 tutorial somewhere?", "links": [], "channel": "scrapy"},
{"date": "2014-05-01T10:48:20.741080+00:00", "nick": "gautamgupta", "message": "nyov: Trying to use the ImagePipeline, I've installed Pillow. Do I need to install PIL? It says ImportError: No module named PIL.", "links": [], "channel": "scrapy"},
{"date": "2014-05-01T14:33:21.388746+00:00", "nick": "JoeLinux", "message": "Hey guys, does anyone know if custom middleware ALWAYS runs after built-in middleware, or if they're all in the same \"priority\" chain?", "links": [], "channel": "scrapy"},
{"date": "2014-05-01T16:19:00.211939+00:00", "nick": "JoeLinux", "message": "Does anyone have experience using scrapy-inline-requests, or just in general making \"side\" requests that should return before continuing processing?", "links": [], "channel": "scrapy"},
{"date": "2014-05-01T17:59:07.664702+00:00", "nick": "nyov", "message": "JoeLinux: middlewares run in the order of their defined priority", "links": [], "channel": "scrapy"},
{"date": "2014-05-01T17:59:30.137154+00:00", "nick": "JoeLinux", "message": "nyov: Right, but is that separate priorities for built-in and custom? Or can they intertwine with each other?", "links": [], "channel": "scrapy"},
{"date": "2014-05-01T18:00:10.091697+00:00", "nick": "JoeLinux", "message": "nyov: For instance, RetryMiddleware has a priority of 600. If I make a custom middleware with 650, will I get my middleware between RetryMiddleware and the next built-in?", "links": [], "channel": "scrapy"},
{"date": "2014-05-01T18:00:33.846387+00:00", "nick": "JoeLinux", "message": "nyov: Or does custom middleware not even run until all built-in middleware has completed?", "links": [], "channel": "scrapy"},
{"date": "2014-05-01T18:01:31.446471+00:00", "nick": "nyov", "message": "you can mix them however you like.  \"builtin\" doesn't get distinguished by the system", "links": [], "channel": "scrapy"},
{"date": "2014-05-01T18:01:46.515172+00:00", "nick": "nyov", "message": "its just another import", "links": [], "channel": "scrapy"},
{"date": "2014-05-01T18:01:49.355829+00:00", "nick": "JoeLinux", "message": "nyov: Ah, ok", "links": [], "channel": "scrapy"},
{"date": "2014-05-01T18:02:14.725001+00:00", "nick": "JoeLinux", "message": "nyov: I noticed in the source that they're combining both lists (built-in and custom), but I wasn't sure whether there was something separating them later in the code.", "links": [], "channel": "scrapy"},
{"date": "2014-05-01T18:02:22.375137+00:00", "nick": "JoeLinux", "message": "nyov: Thanks for clearing that up", "links": [], "channel": "scrapy"},
{"date": "2014-05-01T18:05:52.638613+00:00", "nick": "nyov", "message": "gautamgupta: if the image pipeline wants PIL, i guess you need PIL.", "links": [], "channel": "scrapy"},
{"date": "2014-05-01T18:06:35.695748+00:00", "nick": "gautamgupta", "message": "nyov: No it says that one can make it work using Pillow. Though I figured out that bit. Thanks :)", "links": [], "channel": "scrapy"},
{"date": "2014-05-01T18:07:16.132200+00:00", "nick": "nyov", "message": "ok. thought so, after all the time between your question and my answer", "links": [], "channel": "scrapy"},
{"date": "2014-05-01T18:08:40.943607+00:00", "nick": "JoeLinux", "message": "nyov: One more question. The priority number, is it processed ascending or descending? I actually thought originally it was ascending, but now I suspect it's the opposite. Can't find this in the docs.", "links": [], "channel": "scrapy"},
{"date": "2014-05-01T18:09:13.525960+00:00", "nick": "JoeLinux", "message": "nyov: I only notice now that two middlewares depend on this order. Otherwise, these things go unnoticed ;)", "links": [], "channel": "scrapy"},
{"date": "2014-05-01T18:13:43.230736+00:00", "nick": "gautamgupta", "message": "ha", "links": [], "channel": "scrapy"},
{"date": "2014-05-01T21:32:05.683763+00:00", "nick": "jsjc", "message": "Hi!", "links": [], "channel": "scrapy"},
{"date": "2014-05-01T21:32:40.664140+00:00", "nick": "jsjc", "message": "I wonder if Dupefilter fingerprinting does take into consideration request.meta details to get its fingerprint???", "links": [], "channel": "scrapy"},
{"date": "2014-05-01T23:53:56.987111+00:00", "nick": "susan", "message": "hi, have there been any instances of scrapy stopping at a tag, even though there are tags deeper in the tree", "links": [], "channel": "scrapy"},
{"date": "2014-05-02T09:01:34.037423+00:00", "nick": "Styx__", "message": "hi all, just come across this software, looks good! Being a total newb at scraping before I jump in and learn how this all works, can someone advise whether or not scrapy can be used to scrape data from a particular site?", "links": [], "channel": "scrapy"},
{"date": "2014-05-02T09:01:57.153016+00:00", "nick": "Styx__", "message": "http://www.allhomes.com.au/", "links": ["http://www.allhomes.com.au/"], "channel": "scrapy"},
{"date": "2014-05-02T09:05:00.373877+00:00", "nick": "styx___", "message": "hi all", "links": [], "channel": "scrapy"},
{"date": "2014-05-02T09:05:31.282655+00:00", "nick": "styx___", "message": "i typed a msg, but network dropped, did it go through?", "links": [], "channel": "scrapy"},
{"date": "2014-05-02T09:06:29.715041+00:00", "nick": "styx___", "message": "test", "links": [], "channel": "scrapy"},
{"date": "2014-05-02T09:17:53.971328+00:00", "nick": "styx___", "message": "anyhow in case it didnt go through and someone comes on, I am hoping to scrape data from a particular website www.allhomes.com.au", "links": ["http://www.allhomes.com.au"], "channel": "scrapy"},
{"date": "2014-05-02T09:18:23.186913+00:00", "nick": "styx___", "message": "can scrapy handle this? and can someone with no programming pick this up easy enough?", "links": [], "channel": "scrapy"},
{"date": "2014-05-02T09:18:24.742029+00:00", "nick": "styx___", "message": "thanks", "links": [], "channel": "scrapy"},
{"date": "2014-05-02T11:21:59.898387+00:00", "nick": "styx___", "message": "hello?", "links": [], "channel": "scrapy"},
{"date": "2014-05-02T11:59:08.509530+00:00", "nick": "jsjc", "message": "how can I check defalt middleware orders?", "links": [], "channel": "scrapy"},
{"date": "2014-05-02T12:03:23.191913+00:00", "nick": "rafallo", "message": "Hi guys! I' just trying to create persistent crawler which waits for RMQ messages. I achieve this by raising DontCloseSpider exception on spider_idle event. But, why in ExecutionEngine is 5 seconds delay between requests when spider is paused? What about putting this 5 seconds in settings.py?", "links": [], "channel": "scrapy"},
{"date": "2014-05-02T12:36:26.101574+00:00", "nick": "JoeLinux", "message": "jsjc: http://doc.scrapy.org/en/latest/topics/settings...", "links": ["http://doc.scrapy.org/en/latest/topics/settings.html#std:setting-DOWNLOADER_MIDDLEWARES_BASE"], "channel": "scrapy"},
{"date": "2014-05-02T12:42:07.762333+00:00", "nick": "jsjc", "message": "Thanks JoeLinux", "links": [], "channel": "scrapy"},
{"date": "2014-05-02T12:43:32.542182+00:00", "nick": "JoeLinux", "message": "jsjc: http://doc.scrapy.org/en/latest/topics/settings...", "links": ["http://doc.scrapy.org/en/latest/topics/settings.html#std:setting-SPIDER_MIDDLEWARES_BASE"], "channel": "scrapy"},
{"date": "2014-05-02T12:43:48.879344+00:00", "nick": "JoeLinux", "message": "jsjc: First one was for DownloaderMiddleware, second is for SpiderMiddleware", "links": [], "channel": "scrapy"},
{"date": "2014-05-02T21:57:21.727790+00:00", "nick": "D5", "message": "any idea whats the name of this anticrawling thing http://pastebin.com/wubm2Kci ?", "links": ["http://pastebin.com/wubm2Kci"], "channel": "scrapy"},
{"date": "2014-05-02T22:01:23.100257+00:00", "nick": "nyov", "message": "anticrawling thing? what is it", "links": [], "channel": "scrapy"},
{"date": "2014-05-02T22:14:04.222585+00:00", "nick": "D5", "message": "its some system also has a captcha feature", "links": [], "channel": "scrapy"},
{"date": "2014-05-02T22:14:15.949799+00:00", "nick": "D5", "message": "im trying to identify the vendor", "links": [], "channel": "scrapy"},
{"date": "2014-05-02T22:14:18.665102+00:00", "nick": "D5", "message": "and the product", "links": [], "channel": "scrapy"},
{"date": "2014-05-02T22:21:19.134740+00:00", "nick": "nyov", "message": "sorry, no idea. but what does it do, or is it just an obfuscated error message", "links": [], "channel": "scrapy"},
{"date": "2014-05-02T22:33:38.769704+00:00", "nick": "D5", "message": "i'll get around it", "links": [], "channel": "scrapy"},
{"date": "2014-05-02T22:33:59.481069+00:00", "nick": "D5", "message": "i would just enjoy it more if i knew what it was", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T10:52:57.273611+00:00", "nick": "Jaspreet", "message": "Hello", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T10:53:05.696771+00:00", "nick": "Jaspreet", "message": "Hi", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T10:54:24.074671+00:00", "nick": "Jaspreet", "message": "I am new to Scrapy. Need some help", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T10:55:04.442004+00:00", "nick": "Jaspreet", "message": "can anyone please help me?", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T12:15:51.219598+00:00", "nick": "nyov", "message": "Jaspreet: that depends on what you need help with", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T12:16:51.425767+00:00", "nick": "Jaspreet", "message": "I am trying to scrape news articles on the pages with a url pattern", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T12:18:45.233253+00:00", "nick": "nyov", "message": "okay. sorry, but if you don't have a specific question, we can't try to answer it", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T12:20:25.862750+00:00", "nick": "nyov", "message": "on IRC, just ask. it's not rude and saves everyone some time", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T12:21:03.690626+00:00", "nick": "Jaspreet", "message": "fine.", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T12:21:11.515816+00:00", "nick": "Jaspreet", "message": "here is my code", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T12:21:13.143947+00:00", "nick": "Jaspreet", "message": "http://pastebin.com/Hn3vsW7Q", "links": ["http://pastebin.com/Hn3vsW7Q"], "channel": "scrapy"},
{"date": "2014-05-04T12:22:05.064039+00:00", "nick": "nyov", "message": "okay, but what's the problem?", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T12:22:06.170915+00:00", "nick": "Jaspreet", "message": "I have a list of articles to crawl. the list of urls is fetched fomr this page http://indianexpress.com/section/business/", "links": ["http://indianexpress.com/section/business/"], "channel": "scrapy"},
{"date": "2014-05-04T12:22:54.009981+00:00", "nick": "Jaspreet", "message": "for each url the page needs to be crawled for items like date, heading, article description", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T12:23:26.978610+00:00", "nick": "Jaspreet", "message": "in my code the parse_news method is not getting invoded, even when i have specified it in the Rules", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T12:24:14.912824+00:00", "nick": "Jaspreet", "message": "a typical news article url is like this link http://indianexpress.com/article/business/busin...", "links": ["http://indianexpress.com/article/business/business-others/both-samsung-apple-infringed-patents-us-jury/"], "channel": "scrapy"},
{"date": "2014-05-04T12:25:12.863367+00:00", "nick": "nyov", "message": "do you want the articles, or just the data from the index page?", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T12:25:43.954994+00:00", "nick": "Jaspreet", "message": "the articles.. from indeex page i m just fetching the urls", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T12:26:32.231959+00:00", "nick": "Jaspreet", "message": "i used this tutorial http://doc.scrapy.org/en/latest/intro/overview.... as it was similar to this scenario", "links": ["http://doc.scrapy.org/en/latest/intro/overview.html"], "channel": "scrapy"},
{"date": "2014-05-04T12:26:41.289657+00:00", "nick": "nyov", "message": "okay, I'd hazard a guess that your regex isn't matching the full absolute url there", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T12:28:45.348394+00:00", "nick": "Jaspreet", "message": "it is matching as I can see one of the url being fetched in the log when i run the \"scrapy crawl website\" command", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T12:28:51.832599+00:00", "nick": "nyov", "message": "actually, change the string array [] to a ()", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T12:29:39.119575+00:00", "nick": "nyov", "message": "allow=(r'/article/business/[a-z-]+/[a-z-]+/'))", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T12:30:50.112304+00:00", "nick": "Jaspreet", "message": "u mean prefix with letter r?", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T12:30:51.014728+00:00", "nick": "nyov", "message": "hm, if it's matching I don't know", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T12:32:53.887750+00:00", "nick": "Jaspreet", "message": "ok. here's the log if it may help http://pastebin.com/Yw9XH7wv", "links": ["http://pastebin.com/Yw9XH7wv"], "channel": "scrapy"},
{"date": "2014-05-04T12:33:31.577109+00:00", "nick": "nyov", "message": "DEBUG: Filtered offsite request to 'indianexpress.com': <GET http://indianexpress.com/article/business/busin...;", "links": ["http://indianexpress.com/article/business/business-others/launspad-more-power/&amp;gt"], "channel": "scrapy"},
{"date": "2014-05-04T12:33:37.084204+00:00", "nick": "nyov", "message": "there you go", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T12:34:31.648132+00:00", "nick": "nyov", "message": "allowed domains is not a URI, only a domain name", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T12:34:39.895086+00:00", "nick": "nyov", "message": "allowed_domains = [\"indianexpress.com\"]", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T12:35:22.682271+00:00", "nick": "nyov", "message": "frankly thats confusing and I think it should be changed in scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T12:35:58.092067+00:00", "nick": "nyov", "message": "happens to a lot of people", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T12:36:40.699144+00:00", "nick": "Jaspreet", "message": "I have used allowed_domains = x.com by mistake when i was actually scraping content from y.com, and it works", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T12:37:08.251715+00:00", "nick": "Jaspreet", "message": "so i dont think that should create any problem", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T12:37:46.661000+00:00", "nick": "nyov", "message": "then either it was your start url, or in a list of urls you provided directly.", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T12:38:05.857517+00:00", "nick": "nyov", "message": "it plainly is your problem though, as it's filtered out as an offsite request in your log", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T12:39:05.757202+00:00", "nick": "Jaspreet", "message": "i m not sure why is it being marked as offsite", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T12:40:08.459028+00:00", "nick": "nyov", "message": "because, as I said, allowed_domains doesn't understand a URI, so your spider has no valid domains to crawl", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T12:40:34.439959+00:00", "nick": "Jaspreet", "message": "ohh got it.", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T12:44:22.934527+00:00", "nick": "Jaspreet", "message": "its working now nvoy. Thanks", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T12:44:49.891490+00:00", "nick": "nyov", "message": "you're welcome", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T12:45:03.316492+00:00", "nick": "Jaspreet", "message": "there is one minor issue I m facing...", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T12:45:28.237629+00:00", "nick": "Jaspreet", "message": "The URLs are fetched correctly", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T12:45:51.227620+00:00", "nick": "Jaspreet", "message": "however some of the URLs have artciles which run into multiple pages", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T12:46:01.020820+00:00", "nick": "Jaspreet", "message": "for example http://indianexpress.com/article/business/busin...", "links": ["http://indianexpress.com/article/business/business-others/both-samsung-apple-infringed-patents-us-jury/"], "channel": "scrapy"},
{"date": "2014-05-04T12:46:42.219600+00:00", "nick": "Jaspreet", "message": "there is an option to view these articles in a single page. this will help me in single web page crawl for each article.", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T12:47:25.865813+00:00", "nick": "Jaspreet", "message": "the single page artciles have the same link suffixed by 99 in the url so for above link it will be http://indianexpress.com/article/business/busin...", "links": ["http://indianexpress.com/article/business/business-others/both-samsung-apple-infringed-patents-us-jury/99/"], "channel": "scrapy"},
{"date": "2014-05-04T12:48:05.359424+00:00", "nick": "Jaspreet", "message": "even if the article is single paged and we suffix it with 99 it remains the same...", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T12:49:15.718326+00:00", "nick": "Jaspreet", "message": "is there any way can i specify the matching URL's for the index page without suffix 99 but want the actual pages to be crawled with the suffix 99", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T12:51:59.818784+00:00", "nick": "nyov", "message": "you could use process_value in http://scrapy.readthedocs.org/en/latest/topics/...", "links": ["http://scrapy.readthedocs.org/en/latest/topics/link-extractors.html#sgmllinkextractor"], "channel": "scrapy"},
{"date": "2014-05-04T12:52:04.888498+00:00", "nick": "nyov", "message": "to rewrite the url", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T12:52:49.674058+00:00", "nick": "nyov", "message": "or wait, i think that's for attributes.. not sure", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T12:54:36.573670+00:00", "nick": "nyov", "message": "though it is probably easier to write another parse method for the index page itself, and rewrite the URLs in there before yielding them", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T12:58:47.480544+00:00", "nick": "nyov", "message": "ok, on second glance, process_value should work. so it's your choice between changing your Rule(), or moving the Rule's logic into another parse*() method and rewriting URLs in there, with a callback to parse_news", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T13:00:19.259179+00:00", "nick": "nyov", "message": "and I have to leave now. cheers, and good luck", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T13:00:35.819499+00:00", "nick": "Jaspreet", "message": "i guess changing the rule might be straight forward.", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T13:00:58.630882+00:00", "nick": "nyov", "message": "yeah, just write a process_value callback function", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T13:01:31.703223+00:00", "nick": "Jaspreet", "message": "i need to extend the function process_value, how to change the new rule value in that?", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T13:01:31.864649+00:00", "nick": "nyov", "message": "def process_value(value): return value + '99/'", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T13:02:22.636619+00:00", "nick": "nyov", "message": "sorry? i didn't understand that", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T13:02:32.355598+00:00", "nick": "Jaspreet", "message": "ok. let me try this. Thanks", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T13:05:00.486054+00:00", "nick": "Jaspreet", "message": "what should be the value to be passed for the process_value parameter", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T13:06:08.043597+00:00", "nick": "Jaspreet", "message": "and process_value function should be in the spider class itself?", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T13:06:39.278706+00:00", "nick": "nyov", "message": "self.process_value", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T13:06:58.644922+00:00", "nick": "nyov", "message": "from what I see in the source. and then it would be in the spider class itself, yes", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T13:07:20.009281+00:00", "nick": "nyov", "message": "or just process_value, if it's a module scope function", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T13:08:51.390198+00:00", "nick": "nyov", "message": "okay, really have to go now. you'll make it work i'm sure", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T13:09:22.368200+00:00", "nick": "Jaspreet", "message": "let me frame it differently. what should be the vale of the parameter to be passed when invoking SgmlLinkExtractor", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T13:09:58.192551+00:00", "nick": "Jaspreet", "message": "okay fine, Thanks a lot for your help.", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T19:13:04.107630+00:00", "nick": "asd", "message": "why do i keep getting this error message when i type in terminal: sudo pip install Scrapy? http://paste.ubuntu.com/7394554/", "links": ["http://paste.ubuntu.com/7394554/"], "channel": "scrapy"},
{"date": "2014-05-04T20:52:53.236057+00:00", "nick": "JoeLinux", "message": "Hi, I'm having a problem yielding Request objects within the parse() method. The URLs are valid, I see log messages on the preceding line, but the Request is never created (and middleware isn't called). Am I missing something basic?", "links": [], "channel": "scrapy"},
{"date": "2014-05-04T20:53:10.860179+00:00", "nick": "JoeLinux", "message": "This is for a Spider instance, btw.", "links": [], "channel": "scrapy"},
{"date": "2014-05-05T08:42:14.685799+00:00", "nick": "expert", "message": "hi guys", "links": [], "channel": "scrapy"},
{"date": "2014-05-07T01:30:12.782265+00:00", "nick": "thedaniel", "message": "hi gaiz", "links": [], "channel": "scrapy"},
{"date": "2014-05-07T01:31:09.713209+00:00", "nick": "thedaniel", "message": "I have a super noob question", "links": [], "channel": "scrapy"},
{"date": "2014-05-07T01:32:23.380887+00:00", "nick": "thedaniel", "message": "Just to start, I'm not even close to a programmer. Having said that, I'm trying to install Portia on top of Scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-05-07T01:34:06.939168+00:00", "nick": "thedaniel", "message": "trying to install Scrapely as well, and getting the error message \"error: unable to find vcvarsall.bat\"", "links": [], "channel": "scrapy"},
{"date": "2014-05-07T01:35:21.100279+00:00", "nick": "thedaniel", "message": "I found others who have gotten this error message installing other packages, but none for Scrapely", "links": [], "channel": "scrapy"},
{"date": "2014-05-07T01:37:31.902577+00:00", "nick": "thedaniel", "message": "any thoughts on if this might be a Visual Studio issue or a missing environmental variable in Path?", "links": [], "channel": "scrapy"},
{"date": "2014-05-07T01:37:37.294691+00:00", "nick": "thedaniel", "message": "or other", "links": [], "channel": "scrapy"},
{"date": "2014-05-07T07:10:44.327388+00:00", "nick": "skylined101", "message": "Hi anyone here ? :) i just finished dmoz spider tutorial . i want to ask . what is the easy way to publish the scraped data to Django-cms 3  ? pls", "links": [], "channel": "scrapy"},
{"date": "2014-05-07T08:51:27.994741+00:00", "nick": "nyov", "message": "pfft", "links": [], "channel": "scrapy"},
{"date": "2014-05-07T11:59:09.271953+00:00", "nick": "h4k1m", "message": "hi guys", "links": [], "channel": "scrapy"},
{"date": "2014-05-07T11:59:32.136617+00:00", "nick": "h4k1m", "message": "is there a way to automate scrapy crawlers deployement with a simple twisted server?", "links": [], "channel": "scrapy"},
{"date": "2014-05-07T12:39:58.998782+00:00", "nick": "nyov", "message": "scrapyd? it's basically a twistd server", "links": [], "channel": "scrapy"},
{"date": "2014-05-07T15:07:36.446504+00:00", "nick": "anth0ny_", "message": "Hello all", "links": [], "channel": "scrapy"},
{"date": "2014-05-07T15:08:14.712815+00:00", "nick": "anth0ny_", "message": "Using scrapy, how do I combine fields in a loader?  Ie, I want to get an \u2018address\u2019 field based on the scraped values for \u2018street-address\u2019, \u2018city\u2019, \u2018state\u2019, \u2018zipcode\u2019", "links": [], "channel": "scrapy"},
{"date": "2014-05-07T15:13:38.479383+00:00", "nick": "nyov", "message": "anth0ny_: hmm, namirezuy posted an example for that at some point in time. I'd have to go dig, though", "links": [], "channel": "scrapy"},
{"date": "2014-05-07T15:55:47.815914+00:00", "nick": "h4k1m", "message": "nyov, a twisted on top of another one that should work right?", "links": [], "channel": "scrapy"},
{"date": "2014-05-07T16:08:29.944791+00:00", "nick": "nyov", "message": "h4k1m: no", "links": [], "channel": "scrapy"},
{"date": "2014-05-07T16:09:28.299712+00:00", "nick": "nyov", "message": "i mean, it won't", "links": [], "channel": "scrapy"},
{"date": "2014-05-07T17:40:04.857467+00:00", "nick": "h4k1m", "message": "nyov, actually it worked :p", "links": [], "channel": "scrapy"},
{"date": "2014-05-07T18:07:28.031581+00:00", "nick": "h4k1m", "message": "what does it mean when scrapyd doesnt return logs (No Such Resource  File not found)?", "links": [], "channel": "scrapy"},
{"date": "2014-05-08T00:39:35.590762+00:00", "nick": "anth0ny_", "message": "Is there anything special to keep in mind when creating regex statements for css/xpath selectors?  Mine don\u2019t seem to be operating as expected", "links": [], "channel": "scrapy"},
{"date": "2014-05-08T03:06:01.308164+00:00", "nick": "tim", "message": "Is anyone here good with downloadmiddleware?", "links": [], "channel": "scrapy"},
{"date": "2014-05-08T03:06:11.484376+00:00", "nick": "tim", "message": "I'm having an issue with scrapy not loading one in a project", "links": [], "channel": "scrapy"},
{"date": "2014-05-08T16:13:32.125056+00:00", "nick": "Robin_", "message": "Need some help on storing scraped data to mysql db", "links": [], "channel": "scrapy"},
{"date": "2014-05-08T16:15:06.245023+00:00", "nick": "Robin_", "message": "My items field are weblink, webpage_text, date", "links": [], "channel": "scrapy"},
{"date": "2014-05-08T16:15:59.561441+00:00", "nick": "Robin_", "message": "i need a primary key in the table to avoid duplicates", "links": [], "channel": "scrapy"},
{"date": "2014-05-08T16:17:04.698670+00:00", "nick": "Robin_", "message": "i wanted to keep the weblink as primary key", "links": [], "channel": "scrapy"},
{"date": "2014-05-08T16:19:52.575248+00:00", "nick": "Robin_", "message": "size of my weblink primary key is varchar(1000) in mysql, however max key length is 767 bytes", "links": [], "channel": "scrapy"},
{"date": "2014-05-08T16:21:35.332358+00:00", "nick": "Robin_", "message": "any suggestions?", "links": [], "channel": "scrapy"},
{"date": "2014-05-08T16:46:41.803604+00:00", "nick": "Robin_", "message": "is it safe to use hash value of the weblink(url) as the primary key in the db", "links": [], "channel": "scrapy"},
{"date": "2014-05-08T17:49:59.210808+00:00", "nick": "nyov", "message": "Robin_: you might be better asking that in an sql-related channel", "links": [], "channel": "scrapy"},
{"date": "2014-05-08T17:50:14.280628+00:00", "nick": "nyov", "message": "that said, if your hash values are unique, it should work", "links": [], "channel": "scrapy"},
{"date": "2014-05-08T17:51:01.065419+00:00", "nick": "nyov", "message": "though for better speed, a pk should be a sequential integer", "links": [], "channel": "scrapy"},
{"date": "2014-05-08T17:52:19.694791+00:00", "nick": "nyov", "message": "this is somewhat related in that regard (pk performance), http://www.mysqlperformanceblog.com/2007/03/13/...", "links": ["http://www.mysqlperformanceblog.com/2007/03/13/to-uuid-or-not-to-uuid/"], "channel": "scrapy"},
{"date": "2014-05-08T17:53:00.051651+00:00", "nick": "samtc", "message": "Robin_: and in some case if the webpage is rendered from an http post, the url could be the same for different webpages", "links": [], "channel": "scrapy"},
{"date": "2014-05-08T17:54:17.123212+00:00", "nick": "Robin_", "message": "i am scarping news articles which are most likely to have same url", "links": [], "channel": "scrapy"},
{"date": "2014-05-08T17:54:25.488005+00:00", "nick": "nyov", "message": "actually it shouldn't. in which cases? Vary: * ?", "links": [], "channel": "scrapy"},
{"date": "2014-05-08T17:56:23.229790+00:00", "nick": "nyov", "message": "samtc, oh you mean using POST data. that may be true", "links": [], "channel": "scrapy"},
{"date": "2014-05-08T17:59:03.412235+00:00", "nick": "Robin_", "message": "the reason i wanted url as primary key because in my case the urls are unique and I plan to use \"INSERT IGNORE\" for filtering the duplicates", "links": [], "channel": "scrapy"},
{"date": "2014-05-08T18:28:28.760293+00:00", "nick": "nyov", "message": "Robin_: you can do that with a UNIQUE constraint as well, doesn't have to be the PK", "links": [], "channel": "scrapy"},
{"date": "2014-05-08T18:30:08.935635+00:00", "nick": "Robin_", "message": "max key length is 767 bytes (both for primary and unique keys)", "links": [], "channel": "scrapy"},
{"date": "2014-05-08T18:31:16.825235+00:00", "nick": "samtc", "message": "Robin_: as you said, you could hash it, I don't see a problem here", "links": [], "channel": "scrapy"},
{"date": "2014-05-08T18:32:44.528315+00:00", "nick": "Robin_", "message": "ok. i haven't used hash before. can u suggest me an algorithm with a very high probability of getting unique hash values?", "links": [], "channel": "scrapy"},
{"date": "2014-05-08T18:33:17.866500+00:00", "nick": "samtc", "message": "md5, sha1", "links": [], "channel": "scrapy"},
{"date": "2014-05-08T18:33:19.362315+00:00", "nick": "Robin_", "message": "of-course on the url data", "links": [], "channel": "scrapy"},
{"date": "2014-05-08T18:33:34.153568+00:00", "nick": "Robin_", "message": "ok. Thanks", "links": [], "channel": "scrapy"},
{"date": "2014-05-08T18:33:52.192563+00:00", "nick": "samtc", "message": "md5 is safe enough for an url", "links": [], "channel": "scrapy"},
{"date": "2014-05-08T18:36:21.032512+00:00", "nick": "Robin_", "message": "fine. Thanks", "links": [], "channel": "scrapy"},
{"date": "2014-05-08T23:08:52.225384+00:00", "nick": "mac_", "message": "hi all togehter!", "links": [], "channel": "scrapy"},
{"date": "2014-05-08T23:10:36.784406+00:00", "nick": "mac_", "message": "i have a problem with installing scrapy on wndows 7. the problem is going on if i try to use the the crawler. example: scrapy crawl dmoz", "links": [], "channel": "scrapy"},
{"date": "2014-05-08T23:11:20.941507+00:00", "nick": "mac_", "message": "ImportError: Error loading object 'scrapy.core.downloader.handlers.s3.S3DownloadHandler': DLL load failed: Das angegeben", "links": [], "channel": "scrapy"},
{"date": "2014-05-08T23:11:21.086497+00:00", "nick": "mac_", "message": "e Modul wurde nicht gefunden.", "links": [], "channel": "scrapy"},
{"date": "2014-05-08T23:12:03.122608+00:00", "nick": "mac_", "message": "specific module not found", "links": [], "channel": "scrapy"},
{"date": "2014-05-09T00:06:31.763649+00:00", "nick": "nyov", "message": "weird. sorry, no idea. it's windows. are python modules recognized as DLL files in windows? ...", "links": [], "channel": "scrapy"},
{"date": "2014-05-09T04:00:46.454484+00:00", "nick": "mukesh_", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2014-05-09T04:01:21.673701+00:00", "nick": "mukesh_", "message": "is anybodyt", "links": [], "channel": "scrapy"},
{"date": "2014-05-09T04:01:24.247792+00:00", "nick": "mukesh_", "message": "there", "links": [], "channel": "scrapy"},
{"date": "2014-05-09T04:01:31.344039+00:00", "nick": "mukesh_", "message": "ffor help", "links": [], "channel": "scrapy"},
{"date": "2014-05-09T17:08:11.045452+00:00", "nick": "fpghost84", "message": "Hi, I know I can change the fixed download delay per spider by adding say 'download_delay = 10' to my spider class, but if I still wanted a random download delay, only with a different min and max range is this possible? e.g. random_download_delay =[2,8]?", "links": [], "channel": "scrapy"},
{"date": "2014-05-09T20:02:01.856592+00:00", "nick": "samtc", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2014-05-09T20:02:24.949895+00:00", "nick": "samtc", "message": "can you disable a middleware for just 1 request?", "links": [], "channel": "scrapy"},
{"date": "2014-05-09T20:02:42.867771+00:00", "nick": "samtc", "message": "I would like to disable crawlera/proxyhub middleware for a single request", "links": [], "channel": "scrapy"},
{"date": "2014-05-09T20:05:10.049768+00:00", "nick": "nyov", "message": "samtc: in general, I think not. if the middleware supports it through request headers, possibly. since crawlera is a \"proxy\", it just might", "links": [], "channel": "scrapy"},
{"date": "2014-05-09T20:06:20.225688+00:00", "nick": "nyov", "message": "dangra, pablohof1 - can you disable/circumvent crawlera for a single request?", "links": [], "channel": "scrapy"},
{"date": "2014-05-09T20:08:35.319917+00:00", "nick": "samtc", "message": "it look like you can't modify settings while your spider is running (self.settings)", "links": [], "channel": "scrapy"},
{"date": "2014-05-09T20:11:31.194030+00:00", "nick": "nyov", "message": "is there a real reason you need this? or is it an academic question", "links": [], "channel": "scrapy"},
{"date": "2014-05-09T20:11:54.263649+00:00", "nick": "csalazar", "message": "https://github.com/scrapinghub/scrapylib/blob/m...", "links": ["https://github.com/scrapinghub/scrapylib/blob/master/scrapylib/crawlera.py#L76"], "channel": "scrapy"},
{"date": "2014-05-09T20:12:40.496118+00:00", "nick": "nyov", "message": "ah, good catch, thanks", "links": [], "channel": "scrapy"},
{"date": "2014-05-09T20:13:40.065638+00:00", "nick": "samtc", "message": "csalazar: ahhhh thank you", "links": [], "channel": "scrapy"},
{"date": "2014-05-09T20:13:52.326459+00:00", "nick": "csalazar", "message": "you're welcome", "links": [], "channel": "scrapy"},
{"date": "2014-05-09T23:04:21.004294+00:00", "nick": "yhager", "message": "I'm trying to tackle https://github.com/scrapy/scrapy/issues/547 which is supposed to be easy. But I can't find how to pass the overwrite flag to FileFeedStorage. Any ideas?", "links": ["https://github.com/scrapy/scrapy/issues/547"], "channel": "scrapy"},
{"date": "2014-05-10T14:55:54.010705+00:00", "nick": "peter_scrapy", "message": "hey, i'm looping through a variable number of tr's, i was wondering if anyone knew how to just grab the particular tr element that matches with the current iteration of the loop. currently it grabs all the tr elements for every loop iteration.", "links": [], "channel": "scrapy"},
{"date": "2014-05-10T14:56:26.255304+00:00", "nick": "peter_scrapy", "message": "sites = hxs.select('//html/body/section/div/div[2]/table/tbody/tr')", "links": [], "channel": "scrapy"},
{"date": "2014-05-10T14:56:36.825576+00:00", "nick": "peter_scrapy", "message": "for site in sites:", "links": [], "channel": "scrapy"},
{"date": "2014-05-10T14:56:44.645700+00:00", "nick": "peter_scrapy", "message": "opp1 = ''.join(sites.select('td[2]/p[2]/a/text()').extract()).strip()", "links": [], "channel": "scrapy"},
{"date": "2014-05-10T14:59:11.332958+00:00", "nick": "peter_scrapy", "message": "so if there were 22 tr's, opp1 would be a concatenation of all 22. what i'd like is it to be tr[1] for the first iteration, tr[2] for the second iteration, etc.", "links": [], "channel": "scrapy"},
{"date": "2014-05-10T15:01:05.095747+00:00", "nick": "peter_scrapy", "message": "ive searched around but can't find anything, yet i feel like there's a simple solution", "links": [], "channel": "scrapy"},
{"date": "2014-05-10T16:21:12.117654+00:00", "nick": "peter_scrapy", "message": "anyone know of a way to count the total number of trs in an xpath?", "links": [], "channel": "scrapy"},
{"date": "2014-05-10T17:44:17.776318+00:00", "nick": "tumbolia", "message": "peter_scrapy: I wouldn't be surprised if xpath can not do this", "links": [], "channel": "scrapy"},
{"date": "2014-05-10T17:44:32.234753+00:00", "nick": "tumbolia", "message": "don't bother", "links": [], "channel": "scrapy"},
{"date": "2014-05-10T17:44:53.485713+00:00", "nick": "peter_scrapy", "message": "i found a way if you do extract()[count] and iterate count", "links": [], "channel": "scrapy"},
{"date": "2014-05-10T17:45:03.389668+00:00", "nick": "peter_scrapy", "message": "the problem is i get some errors", "links": [], "channel": "scrapy"},
{"date": "2014-05-10T17:45:07.101561+00:00", "nick": "tumbolia", "message": "do what?", "links": [], "channel": "scrapy"},
{"date": "2014-05-10T17:45:14.541608+00:00", "nick": "tumbolia", "message": "[count] ?", "links": [], "channel": "scrapy"},
{"date": "2014-05-10T17:45:25.854655+00:00", "nick": "peter_scrapy", "message": "opp1 = ''.join(sites.select('td[2]/p[2]/a/text()').extract()[count]).strip(", "links": [], "channel": "scrapy"},
{"date": "2014-05-10T17:45:43.233982+00:00", "nick": "peter_scrapy", "message": "have count iterate from 0, 1, 2, 3 etc with the loop", "links": [], "channel": "scrapy"},
{"date": "2014-05-10T17:45:48.961679+00:00", "nick": "tumbolia", "message": "where do you get count?", "links": [], "channel": "scrapy"},
{"date": "2014-05-10T17:45:59.000895+00:00", "nick": "peter_scrapy", "message": "its something i had to define", "links": [], "channel": "scrapy"},
{"date": "2014-05-10T17:46:01.990979+00:00", "nick": "peter_scrapy", "message": "count = 0", "links": [], "channel": "scrapy"},
{"date": "2014-05-10T17:46:54.248204+00:00", "nick": "peter_scrapy", "message": "the thing was, it was concatenated all the tr elements together, and they were being indexed", "links": [], "channel": "scrapy"},
{"date": "2014-05-10T17:46:55.129723+00:00", "nick": "tumbolia", "message": "ok, your problem caught my attention", "links": [], "channel": "scrapy"},
{"date": "2014-05-10T17:47:33.378685+00:00", "nick": "tumbolia", "message": "but I have to leave", "links": [], "channel": "scrapy"},
{"date": "2014-05-10T17:48:00.136826+00:00", "nick": "peter_scrapy", "message": "oh ok", "links": [], "channel": "scrapy"},
{"date": "2014-05-10T17:48:04.133295+00:00", "nick": "peter_scrapy", "message": "thanks", "links": [], "channel": "scrapy"},
{"date": "2014-05-10T17:48:15.418950+00:00", "nick": "tumbolia", "message": "I see I forgot to join this channel from home", "links": [], "channel": "scrapy"},
{"date": "2014-05-10T17:49:06.713823+00:00", "nick": "tumbolia", "message": "I 'll join a bit later as \"digenis\" and give you a ping", "links": [], "channel": "scrapy"},
{"date": "2014-05-10T17:49:28.066186+00:00", "nick": "tumbolia", "message": "think of recursive functions by then", "links": [], "channel": "scrapy"},
{"date": "2014-05-10T17:49:33.105649+00:00", "nick": "tumbolia", "message": "chao", "links": [], "channel": "scrapy"},
{"date": "2014-05-10T20:16:26.165959+00:00", "nick": "Digenis", "message": "peter_scrapy: ?", "links": [], "channel": "scrapy"},
{"date": "2014-05-10T20:22:11.148249+00:00", "nick": "Digenis", "message": "I remember you said something about count", "links": [], "channel": "scrapy"},
{"date": "2014-05-10T20:23:15.289497+00:00", "nick": "Digenis", "message": "do you have predicates for the topmost <tr> and the lowest one", "links": [], "channel": "scrapy"},
{"date": "2014-05-10T20:26:54.648958+00:00", "nick": "Digenis", "message": "if you can just reproduce them in an xpath like count(ancestor::tr[ancestor::tr[$predicates]]) starting from the lowest part then I guess you don't need any help", "links": [], "channel": "scrapy"},
{"date": "2014-05-10T20:27:28.807450+00:00", "nick": "Digenis", "message": "(help by functions I mean, not me)", "links": [], "channel": "scrapy"},
{"date": "2014-05-10T20:27:54.165111+00:00", "nick": "Digenis", "message": "I had something else in mind at first", "links": [], "channel": "scrapy"},
{"date": "2014-05-10T21:20:47.451512+00:00", "nick": "nyov", "message": "< peter_scrapy> so if there were 22 tr's, opp1 would be a concatenation of all 22. what i'd like is it to be tr[1] for the first iteration, tr[2] for the second iteration, etc.", "links": [], "channel": "scrapy"},
{"date": "2014-05-10T21:20:57.706667+00:00", "nick": "nyov", "message": "opp1 = sites.select('td[2]/p[2]/a/text()')", "links": [], "channel": "scrapy"},
{"date": "2014-05-10T21:21:32.372527+00:00", "nick": "nyov", "message": "for i,site in enumerate(sites):", "links": [], "channel": "scrapy"},
{"date": "2014-05-10T21:21:45.202610+00:00", "nick": "nyov", "message": "i+1 is your count", "links": [], "channel": "scrapy"},
{"date": "2014-05-10T21:22:35.293676+00:00", "nick": "nyov", "message": "''.join(site.extract()[i+1]).strip()", "links": [], "channel": "scrapy"},
{"date": "2014-05-10T21:22:39.018250+00:00", "nick": "nyov", "message": "or something like that", "links": [], "channel": "scrapy"},
{"date": "2014-05-10T21:23:02.410497+00:00", "nick": "nyov", "message": "err, opp1.extract()[i+1]", "links": [], "channel": "scrapy"},
{"date": "2014-05-10T21:24:27.453378+00:00", "nick": "nyov", "message": "< peter_scrapy> anyone know of a way to count the total number of trs in an xpath?", "links": [], "channel": "scrapy"},
{"date": "2014-05-10T21:24:40.758734+00:00", "nick": "nyov", "message": "xpat.select('count(//body/div)')", "links": [], "channel": "scrapy"},
{"date": "2014-05-10T21:24:49.065719+00:00", "nick": "nyov", "message": "xpath, even", "links": [], "channel": "scrapy"},
{"date": "2014-05-10T23:53:54.326366+00:00", "nick": "samtc__", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2014-05-10T23:59:57.151660+00:00", "nick": "samtc__", "message": "what's the best way to have a global variable accessible between the different instance of a spider?", "links": [], "channel": "scrapy"},
{"date": "2014-05-11T00:00:18.386110+00:00", "nick": "samtc__", "message": "for example, if I have N 404 reply I want to stop the crawl", "links": [], "channel": "scrapy"},
{"date": "2014-05-11T00:48:48.537347+00:00", "nick": "peter_scrapy", "message": "thank you both", "links": [], "channel": "scrapy"},
{"date": "2014-05-11T00:49:44.041514+00:00", "nick": "peter_scrapy", "message": "nyov: for i,site in enumerate(sites): works, but for some reason i had to alter the index to i-1 to avoid \"list index out of range\" errors", "links": [], "channel": "scrapy"},
{"date": "2014-05-11T00:50:18.086459+00:00", "nick": "peter_scrapy", "message": "but it seems like itll work nicely", "links": [], "channel": "scrapy"},
{"date": "2014-05-11T03:56:37.782245+00:00", "nick": "kevc", "message": "does anyone know if it's possible to get a scrapy Selectors to return some sort of XML Node object which is comparable?", "links": [], "channel": "scrapy"},
{"date": "2014-05-11T03:57:06.072477+00:00", "nick": "kevc", "message": "I've got a query I'm trying to do where it's getting just too complex for xpath, and I'd rather just iterate in python", "links": [], "channel": "scrapy"},
{"date": "2014-05-11T04:23:17.671467+00:00", "nick": "kevc", "message": "alternatively, if anyone knew how I could us | to combine elements but preserve their relative order in the document", "links": [], "channel": "scrapy"},
{"date": "2014-05-11T04:23:21.133309+00:00", "nick": "kevc", "message": "I don't think that's possible though", "links": [], "channel": "scrapy"},
{"date": "2014-05-11T05:16:20.940715+00:00", "nick": "kevc", "message": "is there a reason I cannot do a predicate on self?", "links": [], "channel": "scrapy"},
{"date": "2014-05-11T05:16:47.523880+00:00", "nick": "kevc", "message": "res.xpath('.[starts-with(@id,\"q_\")]')", "links": [], "channel": "scrapy"},
{"date": "2014-05-11T05:19:42.786903+00:00", "nick": "kevc", "message": "oh, seems I can use descendant-or-self::a[starts-with(@id,\"q_\")]", "links": [], "channel": "scrapy"},
{"date": "2014-05-11T05:44:41.163415+00:00", "nick": "kevc", "message": "is there even a way to determine the node type for an xpath() result?", "links": [], "channel": "scrapy"},
{"date": "2014-05-11T05:44:54.848278+00:00", "nick": "kevc", "message": "res = sel.xpath(...)[0];  print res.type", "links": [], "channel": "scrapy"},
{"date": "2014-05-11T12:28:37.502725+00:00", "nick": "nyov", "message": "kevc: for node in xpath.xelect('//body/div'): x = node.xpath('./my/things')", "links": [], "channel": "scrapy"},
{"date": "2014-05-11T12:28:53.174198+00:00", "nick": "nyov", "message": "bah. i mix up syntax", "links": [], "channel": "scrapy"},
{"date": "2014-05-11T12:29:18.222199+00:00", "nick": "nyov", "message": "for node in sel.xpath", "links": [], "channel": "scrapy"},
{"date": "2014-05-11T12:29:42.765717+00:00", "nick": "nyov", "message": "anyway, that's how you can iterate through a dom tree here", "links": [], "channel": "scrapy"},
{"date": "2014-05-11T12:30:03.400187+00:00", "nick": "nyov", "message": "or xml tree, i dont think it's dom here. heh", "links": [], "channel": "scrapy"},
{"date": "2014-05-11T14:01:18.401357+00:00", "nick": "kevc", "message": "nyov: yes, it was just that . doesn't work with a predicate, but descendant-or-self does", "links": [], "channel": "scrapy"},
{"date": "2014-05-11T14:01:21.531088+00:00", "nick": "kevc", "message": "which is weird", "links": [], "channel": "scrapy"},
{"date": "2014-05-11T14:18:30.806950+00:00", "nick": "Digenis", "message": "kevc I think you need to write it with self::node()[predicate]", "links": [], "channel": "scrapy"},
{"date": "2014-05-11T14:18:44.950359+00:00", "nick": "Digenis", "message": "what do you write after descendant-or-self:: ?", "links": [], "channel": "scrapy"},
{"date": "2014-05-11T14:18:55.746673+00:00", "nick": "Digenis", "message": "replace node() with that", "links": [], "channel": "scrapy"},
{"date": "2014-05-11T14:20:02.810087+00:00", "nick": "Digenis", "message": "or not at all, you already weeded out undesired nodes when assigning res", "links": [], "channel": "scrapy"},
{"date": "2014-05-11T14:20:14.989312+00:00", "nick": "Digenis", "message": "so node() should do too", "links": [], "channel": "scrapy"},
{"date": "2014-05-11T14:25:19.414958+00:00", "nick": "kevc", "message": "Digenis: I got it solved, it just seemed inconsistent that . didn't behave like the full term", "links": [], "channel": "scrapy"},
{"date": "2014-05-11T14:29:51.202290+00:00", "nick": "Digenis", "message": "it seemed inconsistent to me too, even after carefully reading the w3c doc", "links": [], "channel": "scrapy"},
{"date": "2014-05-11T14:30:22.724778+00:00", "nick": "Digenis", "message": "I remember having to use the vervose one quite often too", "links": [], "channel": "scrapy"},
{"date": "2014-05-11T14:30:45.670711+00:00", "nick": "Digenis", "message": "did you solve it exactly that way?", "links": [], "channel": "scrapy"},
{"date": "2014-05-11T14:43:19.379384+00:00", "nick": "nyov", "message": "kevc: possibly because '.' isn't a node type. try using", "links": [], "channel": "scrapy"},
{"date": "2014-05-11T14:43:23.742785+00:00", "nick": "nyov", "message": "res.xpath('*[starts-with(@id,\"q_\")]')", "links": [], "channel": "scrapy"},
{"date": "2014-05-11T14:43:38.109295+00:00", "nick": "nyov", "message": "or res.xpath('./*[starts-with(@id,\"q_\")]')", "links": [], "channel": "scrapy"},
{"date": "2014-05-11T14:51:51.777661+00:00", "nick": "Digenis", "message": "no", "links": [], "channel": "scrapy"},
{"date": "2014-05-11T14:52:19.466135+00:00", "nick": "Digenis", "message": "oh, yes, since you use predicates upon attributes yes", "links": [], "channel": "scrapy"},
{"date": "2014-05-11T14:52:28.129337+00:00", "nick": "Digenis", "message": "but for predicates on the content", "links": [], "channel": "scrapy"},
{"date": "2014-05-11T14:52:44.931715+00:00", "nick": "Digenis", "message": "eg contains(., \"stuff\")", "links": [], "channel": "scrapy"},
{"date": "2014-05-11T14:53:07.697853+00:00", "nick": "Digenis", "message": "wouldn't work as expected", "links": [], "channel": "scrapy"},
{"date": "2014-05-11T14:53:20.563036+00:00", "nick": "Digenis", "message": "I mean for other kinds of nodes", "links": [], "channel": "scrapy"},
{"date": "2014-05-11T14:53:36.566017+00:00", "nick": "Digenis", "message": "comment() or text()", "links": [], "channel": "scrapy"},
{"date": "2014-05-11T14:57:14.588064+00:00", "nick": "Digenis", "message": "also, without self:: you do not select among the selected but among their children", "links": [], "channel": "scrapy"},
{"date": "2014-05-12T18:38:58.576906+00:00", "nick": "mac__", "message": "hi everyone", "links": [], "channel": "scrapy"},
{"date": "2014-05-12T18:40:00.112056+00:00", "nick": "mac__", "message": "\"scrapy shell http://google.com\"; returns error on windows7: twisted.internet.error.DNSLookupError: DNS lookup failed: address 'google.com' not found: [Errno 11001] getaddrinfo failed.", "links": ["http://google.com&#34"], "channel": "scrapy"},
{"date": "2014-05-12T19:03:11.745313+00:00", "nick": "yhager", "message": "mac__: this comes from the underlying name resolution library in python. Make sure your name server is defined correctly and working", "links": [], "channel": "scrapy"},
{"date": "2014-05-12T19:03:22.656383+00:00", "nick": "nyov", "message": "you might want to ask in #twisted", "links": [], "channel": "scrapy"},
{"date": "2014-05-12T19:06:12.785428+00:00", "nick": "mac__", "message": "okay thx for that info, will do one more research", "links": [], "channel": "scrapy"},
{"date": "2014-05-12T19:12:38.636626+00:00", "nick": "mac__", "message": "yhager: how/where can i check if my namserver is defiend?", "links": [], "channel": "scrapy"},
{"date": "2014-05-12T19:17:18.861646+00:00", "nick": "yhager", "message": "mac__: I have no ideas on windows :). try to run python2 -c 'import socket;print socket.gethostbyname(\"google.com\")'", "links": [], "channel": "scrapy"},
{"date": "2014-05-12T19:27:06.513946+00:00", "nick": "mac__", "message": "yhager: NameError: name 'google' is not defined", "links": [], "channel": "scrapy"},
{"date": "2014-05-12T19:27:13.683735+00:00", "nick": "mac__", "message": "sry", "links": [], "channel": "scrapy"},
{"date": "2014-05-12T19:28:07.087506+00:00", "nick": "mac__", "message": "yhager: NameError: name 'google' is not defined", "links": [], "channel": "scrapy"},
{"date": "2014-05-12T19:29:35.058045+00:00", "nick": "yhager", "message": "'google' or 'google.com'? Try 'ping google.com'", "links": [], "channel": "scrapy"},
{"date": "2014-05-12T19:30:47.852036+00:00", "nick": "mac__", "message": "yhager: i ve tried the command you write. and the output was google, not google.com. ping google.com works", "links": ["http://google.com", "http://google.com"], "channel": "scrapy"},
{"date": "2014-05-12T19:34:49.770044+00:00", "nick": "Digenis", "message": "NameError ?", "links": [], "channel": "scrapy"},
{"date": "2014-05-12T19:35:23.964721+00:00", "nick": "Digenis", "message": "the builtin python exception", "links": [], "channel": "scrapy"},
{"date": "2014-05-12T19:35:28.311420+00:00", "nick": "mac__", "message": "\"scrapy shell http://google.com\"; returns error on windows7: twisted.internet.error.DNSLookupError: DNS lookup failed: address 'google.com' not found: [Errno 11001] getaddrinfo failed.", "links": ["http://google.com&#34"], "channel": "scrapy"},
{"date": "2014-05-12T19:35:46.269410+00:00", "nick": "mac__", "message": "@Digenis", "links": [], "channel": "scrapy"},
{"date": "2014-05-12T19:35:53.253526+00:00", "nick": "Digenis", "message": "no, I was refering to the previous line", "links": [], "channel": "scrapy"},
{"date": "2014-05-12T19:36:05.439878+00:00", "nick": "Digenis", "message": "< mac__> yhager: NameError: name 'google' is not defined", "links": [], "channel": "scrapy"},
{"date": "2014-05-12T19:36:43.180931+00:00", "nick": "Digenis", "message": "this exception would be raised by python if you trying to use a variable called google", "links": [], "channel": "scrapy"},
{"date": "2014-05-12T19:37:03.729833+00:00", "nick": "mac__", "message": "Digenis: yes when i run python -c 'import socket;print socket.gethostbyname(\"google.com\")'", "links": [], "channel": "scrapy"},
{"date": "2014-05-12T19:37:08.980327+00:00", "nick": "Digenis", "message": "you were* trying", "links": [], "channel": "scrapy"},
{"date": "2014-05-12T19:38:15.941288+00:00", "nick": "mac__", "message": "in python shell it works", "links": [], "channel": "scrapy"},
{"date": "2014-05-12T19:40:22.130352+00:00", "nick": "mac__", "message": "you mean with wildcard??", "links": [], "channel": "scrapy"},
{"date": "2014-05-12T19:41:20.518540+00:00", "nick": "Digenis", "message": "what wildcard?", "links": [], "channel": "scrapy"},
{"date": "2014-05-12T19:41:26.414135+00:00", "nick": "Digenis", "message": "nyov: yes, I mean shortcuts for most signals, as done in pipelines (and middlewares?)", "links": [], "channel": "scrapy"},
{"date": "2014-05-12T19:41:49.627103+00:00", "nick": "nyov", "message": "sorry, what?", "links": [], "channel": "scrapy"},
{"date": "2014-05-12T19:42:05.004817+00:00", "nick": "Digenis", "message": "sorry, I forgot, nikolaosk here", "links": [], "channel": "scrapy"},
{"date": "2014-05-12T19:42:19.555995+00:00", "nick": "nyov", "message": "ah, okay", "links": [], "channel": "scrapy"},
{"date": "2014-05-12T19:42:44.604599+00:00", "nick": "Digenis", "message": "although naming seems inconsistent, closed and spider_closed", "links": [], "channel": "scrapy"},
{"date": "2014-05-12T19:49:16.070038+00:00", "nick": "nyov", "message": "mac__: did you get help from any twisted guys?", "links": [], "channel": "scrapy"},
{"date": "2014-05-12T19:49:41.992924+00:00", "nick": "mac__", "message": "nyov: no they say ask in #scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-05-12T19:49:52.741072+00:00", "nick": "nyov", "message": "whats your twisted version?", "links": [], "channel": "scrapy"},
{"date": "2014-05-12T19:50:11.341255+00:00", "nick": "Digenis", "message": "I opened #719 if you feel like commenting", "links": [], "channel": "scrapy"},
{"date": "2014-05-12T19:50:13.048196+00:00", "nick": "mac__", "message": "i run 14.0", "links": [], "channel": "scrapy"},
{"date": "2014-05-12T19:50:31.315527+00:00", "nick": "mac__", "message": "tried also with 13.2 cause i have a running scrapy version on my laptop", "links": [], "channel": "scrapy"},
{"date": "2014-05-12T19:50:40.001745+00:00", "nick": "mac__", "message": "now i fixed it", "links": [], "channel": "scrapy"},
{"date": "2014-05-12T19:50:55.609193+00:00", "nick": "mac__", "message": "use easy_install -U to install Scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-05-12T19:51:09.802200+00:00", "nick": "mac__", "message": "and now works, but dont know what was the problem", "links": [], "channel": "scrapy"},
{"date": "2014-05-12T19:54:20.791559+00:00", "nick": "mac__", "message": "thx guys for helping :D", "links": [], "channel": "scrapy"},
{"date": "2014-05-12T19:56:56.828158+00:00", "nick": "nyov", "message": "Digenis: i think close_spider exists, because it's also used in engine and middlewares. I don't see the point in adding stubs for all the signals there, though.", "links": [], "channel": "scrapy"},
{"date": "2014-05-12T19:58:28.190175+00:00", "nick": "Digenis", "message": "\"spider_closed\", this imperative got confusing earlier although it shouldn't", "links": [], "channel": "scrapy"},
{"date": "2014-05-12T19:59:04.950352+00:00", "nick": "Digenis", "message": "not stubs, I red throught the other signals and the only relevant one remaining is spider_idle", "links": [], "channel": "scrapy"},
{"date": "2014-05-12T19:59:16.090268+00:00", "nick": "Digenis", "message": "through*", "links": [], "channel": "scrapy"},
{"date": "2014-05-12T20:02:37.100925+00:00", "nick": "nyov", "message": "right. well, I don't have the full grasp on the internal architecture, so I'll stay out of that pro and cons discussion :)", "links": [], "channel": "scrapy"},
{"date": "2014-05-12T20:08:08.112047+00:00", "nick": "yhager", "message": "I can't figure out why the unit tests failed. These pass when I try locally. Moreover, the line that fails exists verbatim a dozen lines below (from scrapy.conf import settings). https://travis-ci.org/scrapy/scrapy/jobs/24950737", "links": ["https://travis-ci.org/scrapy/scrapy/jobs/24950737"], "channel": "scrapy"},
{"date": "2014-05-12T20:32:59.632205+00:00", "nick": "nyov", "message": "yhager: i don't know, but 've asked for feedback in #547, since more people are subscribed to that", "links": [], "channel": "scrapy"},
{"date": "2014-05-12T20:42:21.341758+00:00", "nick": "yhager", "message": "nyov: thanks. I like your suggestion, I'll work on that. I'll also try to run it like travis, maybe I'll figure it out.", "links": [], "channel": "scrapy"},
{"date": "2014-05-12T20:43:32.890456+00:00", "nick": "nyov", "message": "maybe don't implement it yet. well, it's your choice, but the suggestion still might get shot down by others", "links": [], "channel": "scrapy"},
{"date": "2014-05-12T20:53:48.010685+00:00", "nick": "testerr", "message": "Hello, all!", "links": [], "channel": "scrapy"},
{"date": "2014-05-12T20:55:03.958175+00:00", "nick": "testerr", "message": "I'm scraping a website that includes a javascript link. I know that I can use the python selenium plugin to simulate clicking the link, but I'm not sure how to accomplish that. Is it possible to have Scrapy navigate to a specific page, and then pass the page over to Selenium to click, or do I need to be navigating using Selenium from the beginning?", "links": [], "channel": "scrapy"},
{"date": "2014-05-12T21:01:53.609818+00:00", "nick": "yhager", "message": "nyov: there are multiple suggestions in that issue, I just chose one that was easier to implement. Easier to get feedback this way I figured. I can wait a few days to see if anyone responds. Thanks!", "links": [], "channel": "scrapy"},
{"date": "2014-05-12T21:03:37.667208+00:00", "nick": "yhager", "message": "testerr: maybe it's easier just to parse the javascript function call and figure the resulting page from there. See the example here: http://doc.scrapy.org/en/latest/topics/link-ext...", "links": ["http://doc.scrapy.org/en/latest/topics/link-extractors.html#basesgmllinkextractor"], "channel": "scrapy"},
{"date": "2014-05-12T21:06:55.808132+00:00", "nick": "testerr", "message": "yhager: I just looked at the source code for the page with the link, and I think that that will work well! Thanks for that! For future reference, do you happen to know how the scrapy/selenium dynamic is supposed to happen? It seems like I no longer need it for this project, but am still curious.", "links": [], "channel": "scrapy"},
{"date": "2014-05-12T21:07:26.997364+00:00", "nick": "yhager", "message": "testerr: no idea :)", "links": [], "channel": "scrapy"},
{"date": "2014-05-12T21:07:45.700845+00:00", "nick": "testerr", "message": "yhager: np : ) Thanks for your help!", "links": [], "channel": "scrapy"},
{"date": "2014-05-13T09:32:55.161119+00:00", "nick": "HowardwLo", "message": "Hi, trying to pull out text using xpath. https://dpaste.de/LrQf  how do i write the xpath to get \"Imported. Measurements: Length 39 in\" in one string? I've only managed to get \"Imported. Measurements: \"", "links": ["https://dpaste.de/LrQf"], "channel": "scrapy"},
{"date": "2014-05-13T09:35:36.200216+00:00", "nick": "tomwardill", "message": "HowardwLo: have you tried '//li//text()'?", "links": [], "channel": "scrapy"},
{"date": "2014-05-13T09:36:53.133145+00:00", "nick": "HowardwLo", "message": "tomwardill: ah tahts brilliant", "links": [], "channel": "scrapy"},
{"date": "2014-05-13T09:36:57.907850+00:00", "nick": "HowardwLo", "message": "that seems to grab it perfectly :)", "links": [], "channel": "scrapy"},
{"date": "2014-05-13T09:36:59.068908+00:00", "nick": "tomwardill", "message": "that would get you an array of strings, which you can then join", "links": [], "channel": "scrapy"},
{"date": "2014-05-13T09:37:12.634017+00:00", "nick": "HowardwLo", "message": "is it possible to ignore text in <a> tags in that case?", "links": [], "channel": "scrapy"},
{"date": "2014-05-13T09:38:02.443227+00:00", "nick": "tomwardill", "message": "you might be able to do something with the not() operator", "links": [], "channel": "scrapy"},
{"date": "2014-05-13T10:19:05.475036+00:00", "nick": "HowardwLo", "message": "tomwardill: https://dpaste.de/WF6Y  i'm doing /*[not(self::a)]/text()'), but its missing \"length 39 in\"", "links": ["https://dpaste.de/WF6Y"], "channel": "scrapy"},
{"date": "2014-05-13T10:19:19.797480+00:00", "nick": "HowardwLo", "message": "can't seem to get it :/", "links": [], "channel": "scrapy"},
{"date": "2014-05-13T10:22:03.530429+00:00", "nick": "tomwardill", "message": "HowardwLo: try ending in '//text()'", "links": [], "channel": "scrapy"},
{"date": "2014-05-13T10:22:09.858225+00:00", "nick": "tomwardill", "message": "although not sure that will work", "links": [], "channel": "scrapy"},
{"date": "2014-05-13T10:22:37.251065+00:00", "nick": "HowardwLo", "message": "tomwardill: when i do that, it includes Don't want this, and length 39 in", "links": [], "channel": "scrapy"},
{"date": "2014-05-13T10:22:51.174433+00:00", "nick": "tomwardill", "message": "ah, was afraid of that", "links": [], "channel": "scrapy"},
{"date": "2014-05-13T10:23:55.710637+00:00", "nick": "HowardwLo", "message": "can't i tell it to get text from <li> unless it contains a child <a> ?", "links": [], "channel": "scrapy"},
{"date": "2014-05-13T10:24:24.517356+00:00", "nick": "tomwardill", "message": "with that structure, just './/li/text()' should do it", "links": [], "channel": "scrapy"},
{"date": "2014-05-13T10:24:38.833771+00:00", "nick": "tomwardill", "message": "as it'll only get text that is a child of a 'li'", "links": [], "channel": "scrapy"},
{"date": "2014-05-13T10:26:58.228318+00:00", "nick": "HowardwLo", "message": "ah hah!!", "links": [], "channel": "scrapy"},
{"date": "2014-05-13T10:27:21.448605+00:00", "nick": "HowardwLo", "message": "that works", "links": [], "channel": "scrapy"},
{"date": "2014-05-13T13:11:59.821925+00:00", "nick": "samtc", "message": "any workaround for issue #110 ?", "links": [], "channel": "scrapy"},
{"date": "2014-05-13T20:31:29.484052+00:00", "nick": "nyov", "message": "yay. more people are back. even the bot rejoined us.", "links": [], "channel": "scrapy"},
{"date": "2014-05-13T20:31:50.845099+00:00", "nick": "nyov", "message": "-!- mode/#xonotic [-bbbb developers!*@* developers!*@* developers!*@* developers!*@*] by divVerent", "links": [], "channel": "scrapy"},
{"date": "2014-05-13T20:31:54.505882+00:00", "nick": "nyov", "message": "hahaha", "links": [], "channel": "scrapy"},
{"date": "2014-05-13T23:52:05.140461+00:00", "nick": "HowardwLo", "message": "does scrapy keep track of urls for hte duplicate filter?", "links": [], "channel": "scrapy"},
{"date": "2014-05-13T23:53:45.622684+00:00", "nick": "HowardwLo", "message": "i'm trying to get m2m relationships, so I want to turn that off :D", "links": [], "channel": "scrapy"},
{"date": "2014-05-14T00:13:53.650592+00:00", "nick": "HowardwLo", "message": "or is it just within the context of a single parse?", "links": [], "channel": "scrapy"},
{"date": "2014-05-14T00:32:10.622126+00:00", "nick": "nyov", "message": "?", "links": [], "channel": "scrapy"},
{"date": "2014-05-14T00:32:47.336174+00:00", "nick": "nyov", "message": "actually scrapy doesn't keep track of URLs, but dupefilter keeps track of hashed urls", "links": [], "channel": "scrapy"},
{"date": "2014-05-14T00:36:00.089448+00:00", "nick": "nyov", "message": "or rather hashed requests - as fingerprints. for the time of a crawling run, unless you write them to a filebuffer and read it back in on subsequent runs", "links": [], "channel": "scrapy"},
{"date": "2014-05-14T00:39:18.758189+00:00", "nick": "nyov", "message": "which parts of a request the dupefilter considers 'duplicate' is defined in request_fingerprint for RFPDupeFilter - https://github.com/scrapy/scrapy/blob/master/sc...", "links": ["https://github.com/scrapy/scrapy/blob/master/scrapy/utils/request.py#L19"], "channel": "scrapy"},
{"date": "2014-05-14T00:57:43.188619+00:00", "nick": "HowardwLo", "message": "If i'm trying to record m2m relationships, such as which categories a product belongs to", "links": [], "channel": "scrapy"},
{"date": "2014-05-14T00:57:57.644950+00:00", "nick": "HowardwLo", "message": "would i need to turn off the dupfilter? since each product can belong to many categories", "links": [], "channel": "scrapy"},
{"date": "2014-05-14T01:00:57.399860+00:00", "nick": "HowardwLo", "message": "what im doing is, for each category link, i click on each product, and on the product page i write down the sku", "links": [], "channel": "scrapy"},
{"date": "2014-05-14T01:01:04.153073+00:00", "nick": "HowardwLo", "message": "and put that in a json list/dict", "links": [], "channel": "scrapy"},
{"date": "2014-05-14T01:01:34.971818+00:00", "nick": "HowardwLo", "message": "im worried, that once scrapy hits a product, it'll record that as previously visited, and never go there again to write down the sku for another category", "links": [], "channel": "scrapy"},
{"date": "2014-05-14T01:10:29.058024+00:00", "nick": "nyov", "message": "if you visited an url, you shouldn't need to ever visit it again in a run. take out all the relevant data on the first page visit", "links": [], "channel": "scrapy"},
{"date": "2014-05-14T01:11:31.792203+00:00", "nick": "nyov", "message": "anything else will just get you into crawling loops, be nasty on the visited server and generally get you in a world of hurt", "links": [], "channel": "scrapy"},
{"date": "2014-05-14T01:11:37.207988+00:00", "nick": "nyov", "message": "IMHO", "links": [], "channel": "scrapy"},
{"date": "2014-05-14T01:15:05.197579+00:00", "nick": "nyov", "message": "but... it might be possible your use-case really needs an exception, that's why dupefilter is modular and configurable.", "links": [], "channel": "scrapy"},
{"date": "2014-05-14T01:33:22.217405+00:00", "nick": "HowardwLo", "message": "problem is, m2m isn't always present on the page", "links": [], "channel": "scrapy"},
{"date": "2014-05-14T01:33:43.381457+00:00", "nick": "HowardwLo", "message": "i have to hit all hte combinations :/", "links": [], "channel": "scrapy"},
{"date": "2014-05-14T21:25:24.980516+00:00", "nick": "evrian", "message": "hey, when should i be using an extension vs. a middleware?", "links": [], "channel": "scrapy"},
{"date": "2014-05-14T21:27:44.274499+00:00", "nick": "evrian", "message": "does it even matter?", "links": [], "channel": "scrapy"},
{"date": "2014-05-14T21:57:55.557501+00:00", "nick": "nyov", "message": "extensions are middlewares, so no, it doesn't matter :p", "links": [], "channel": "scrapy"},
{"date": "2014-05-14T21:59:39.430450+00:00", "nick": "evrian", "message": "thanks nyov", "links": [], "channel": "scrapy"},
{"date": "2014-05-14T22:23:12.824332+00:00", "nick": "nyov", "message": "oh, too late", "links": [], "channel": "scrapy"},
{"date": "2014-05-14T22:24:23.166484+00:00", "nick": "nyov", "message": "btw. firefox 29 UI sucks", "links": [], "channel": "scrapy"},
{"date": "2014-05-14T23:44:16.450472+00:00", "nick": "HowardwLo", "message": "Hello! Is there a slick way to extract a full url from a href? .extract() pulls text only, and in this case the relative url", "links": [], "channel": "scrapy"},
{"date": "2014-05-14T23:44:26.228407+00:00", "nick": "HowardwLo", "message": "full --> absolute I should say", "links": [], "channel": "scrapy"},
{"date": "2014-05-14T23:49:36.198999+00:00", "nick": "HowardwLo", "message": "i geuss its open ticket : https://github.com/scrapy/scrapy/issues/548", "links": ["https://github.com/scrapy/scrapy/issues/548"], "channel": "scrapy"},
{"date": "2014-05-14T23:50:21.516869+00:00", "nick": "HowardwLo", "message": "A good workaroudn would be to parse the current url and append the relative?", "links": [], "channel": "scrapy"},
{"date": "2014-05-14T23:52:57.892933+00:00", "nick": "nyov", "message": "import urlparse", "links": [], "channel": "scrapy"},
{"date": "2014-05-14T23:53:04.852613+00:00", "nick": "nyov", "message": "absolute_url = urlparse.urljoin(response.url, relative_url)", "links": [], "channel": "scrapy"},
{"date": "2014-05-15T00:29:07.399958+00:00", "nick": "HowardwLo", "message": "thanks nyov", "links": [], "channel": "scrapy"},
{"date": "2014-05-15T00:37:05.131921+00:00", "nick": "nyov", "message": "HowardwLo: you can of course combine this with your extract() line, if you don't have to clean up the url's first", "links": [], "channel": "scrapy"},
{"date": "2014-05-15T00:37:44.529686+00:00", "nick": "nyov", "message": "but it'd be nice indeed if we could have this wrapped in scrapy, and fix #548", "links": [], "channel": "scrapy"},
{"date": "2014-05-15T05:29:29.374844+00:00", "nick": "HowardwLo", "message": "how do you yield multiple items per page with only 1 field difference?", "links": [], "channel": "scrapy"},
{"date": "2014-05-15T05:29:51.104438+00:00", "nick": "HowardwLo", "message": "Normally its one item per page, but sometimes there are more than one, with very smiliar fields", "links": [], "channel": "scrapy"},
{"date": "2014-05-15T05:30:05.546967+00:00", "nick": "HowardwLo", "message": "Can you yield the same Item() multiple times?", "links": [], "channel": "scrapy"},
{"date": "2014-05-15T05:32:18.164477+00:00", "nick": "HowardwLo", "message": "i'd like to do a callback to itself, but the link is exactly the same :/", "links": [], "channel": "scrapy"},
{"date": "2014-05-15T05:54:03.419654+00:00", "nick": "timkofu", "message": "hi guys", "links": [], "channel": "scrapy"},
{"date": "2014-05-15T05:54:25.827869+00:00", "nick": "timkofu", "message": "what is the relation between an Item and a Spider?", "links": [], "channel": "scrapy"},
{"date": "2014-05-15T05:55:01.283330+00:00", "nick": "timkofu", "message": "because, it seems that I can import django model classes and read and save data in the spider with no apparent need for the DjangoItem", "links": [], "channel": "scrapy"},
{"date": "2014-05-15T05:55:07.880403+00:00", "nick": "timkofu", "message": "what I'm I missing?", "links": [], "channel": "scrapy"},
{"date": "2014-05-15T06:25:13.671763+00:00", "nick": "timkofu", "message": "anyone?", "links": [], "channel": "scrapy"},
{"date": "2014-05-15T06:46:54.719418+00:00", "nick": "timkofu", "message": "helloo", "links": [], "channel": "scrapy"},
{"date": "2014-05-15T09:07:02.314321+00:00", "nick": "jsjc", "message": "Hey! Is there a way to change SETTINGS while spider is running?", "links": [], "channel": "scrapy"},
{"date": "2014-05-15T09:07:23.648017+00:00", "nick": "jsjc", "message": "like concurrent requests, timeouts.....", "links": [], "channel": "scrapy"},
{"date": "2014-05-15T09:42:49.361104+00:00", "nick": "nikolaosk", "message": "jsjc if you want to override setting for a specific spider look at sep19", "links": [], "channel": "scrapy"},
{"date": "2014-05-15T09:42:53.240394+00:00", "nick": "nikolaosk", "message": "wait a sec", "links": [], "channel": "scrapy"},
{"date": "2014-05-15T09:42:59.771995+00:00", "nick": "nikolaosk", "message": "https://github.com/scrapy/scrapy/blob/master/se...", "links": ["https://github.com/scrapy/scrapy/blob/master/sep/sep-019.rst"], "channel": "scrapy"},
{"date": "2014-05-15T09:44:29.201046+00:00", "nick": "nikolaosk", "message": "if you want to dynamically change the crawler behavior by tampering with the settings", "links": [], "channel": "scrapy"},
{"date": "2014-05-15T09:45:18.781240+00:00", "nick": "nikolaosk", "message": "I 'd suggest using the core api", "links": [], "channel": "scrapy"},
{"date": "2014-05-15T09:45:42.296008+00:00", "nick": "nikolaosk", "message": "because you just can't tamper with the settings after the crawler has been configured", "links": [], "channel": "scrapy"},
{"date": "2014-05-15T09:48:30.925337+00:00", "nick": "jsjc", "message": "Mhnm will have a look", "links": [], "channel": "scrapy"},
{"date": "2014-05-15T09:48:50.156523+00:00", "nick": "nikolaosk", "message": "at least I think you can't", "links": [], "channel": "scrapy"},
{"date": "2014-05-15T09:49:38.278009+00:00", "nick": "nikolaosk", "message": "I see nothing in the doc nor in the code", "links": [], "channel": "scrapy"},
{"date": "2014-05-15T09:49:54.946783+00:00", "nick": "nikolaosk", "message": "tell me the setting you want to change", "links": [], "channel": "scrapy"},
{"date": "2014-05-15T11:45:21.182165+00:00", "nick": "vipul_", "message": "hello ! I was crawling one website and I got a list of links from 'href' ... can I crawl those links from the same spider ?", "links": [], "channel": "scrapy"},
{"date": "2014-05-15T13:12:56.452973+00:00", "nick": "jca1981", "message": "Hi all. :)", "links": [], "channel": "scrapy"},
{"date": "2014-05-15T16:02:59.355779+00:00", "nick": "evrian", "message": "hey, what would be the best way to activate a middleware for a specific spider in a project?", "links": [], "channel": "scrapy"},
{"date": "2014-05-15T21:20:59.908316+00:00", "nick": "bwe", "message": "Hi, 'from project.items import Item' fails: \"ImportError: No module named items\"", "links": [], "channel": "scrapy"},
{"date": "2014-05-15T21:22:00.363340+00:00", "nick": "bwe", "message": "Despite 'class Item(Item):' is defined in 'project/items.py'. What do I oversee?", "links": [], "channel": "scrapy"},
{"date": "2014-05-15T21:22:18.241600+00:00", "nick": "bwe", "message": "I am using Scrapy 0.22.2", "links": [], "channel": "scrapy"},
{"date": "2014-05-15T21:41:40.059001+00:00", "nick": "mattpjkt", "message": "Hi. How can I run scrapy from a script?", "links": [], "channel": "scrapy"},
{"date": "2014-05-15T22:01:00.368041+00:00", "nick": "HowardwLo", "message": "bwe: did you name your Item() as Item ?", "links": [], "channel": "scrapy"},
{"date": "2014-05-15T22:01:19.417292+00:00", "nick": "HowardwLo", "message": "use another name, ProductItem(Item):", "links": [], "channel": "scrapy"},
{"date": "2014-05-15T22:01:26.801690+00:00", "nick": "HowardwLo", "message": "thats class ProductItem(Item):", "links": [], "channel": "scrapy"},
{"date": "2014-05-15T22:03:51.877339+00:00", "nick": "bwe", "message": "HowardwLo: Okay, I modified dirbot as to my needs, I suspect it was a naming issue.", "links": [], "channel": "scrapy"},
{"date": "2014-05-15T23:00:47.098416+00:00", "nick": "bwe", "message": "How can I access a name received as an argument by a function __init__ from function parse_item?", "links": [], "channel": "scrapy"},
{"date": "2014-05-15T23:01:42.586998+00:00", "nick": "bwe", "message": "I think I need to hand over the local name from __init__ to parse_item. How can I do that?", "links": [], "channel": "scrapy"},
{"date": "2014-05-15T23:12:26.863248+00:00", "nick": "bwe", "message": "Okay, I assign the item['somename'] = name directly in the __init__ function. Solved.", "links": [], "channel": "scrapy"},
{"date": "2014-05-15T23:12:34.031359+00:00", "nick": "bwe", "message": "Thanks, good night.", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T02:18:19.667450+00:00", "nick": "cornjuliox", "message": "got a question: if a web page is like the one here (https://connect.consolenergy.com:8033/sap/bc/we...), it loads a sort of a java applet, then there's no way for scrapy to scrape data right?", "links": ["https://connect.consolenergy.com:8033/sap/bc/webdynpro/sap/hrrcf_a_unreg_job_search?sap-language=EN&amp=&sap-wd-configId=ZUNREGISTERSEARCH"], "channel": "scrapy"},
{"date": "2014-05-16T02:33:45.378756+00:00", "nick": "HowardwLo", "message": "cornjuliox: i use scrapy with bs4", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T02:34:05.408693+00:00", "nick": "HowardwLo", "message": "to get some js stuff, but if its js heavy i use selenium", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T02:38:32.962842+00:00", "nick": "cornjuliox", "message": "ooh guess i gotta learn how to scrape js then", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T02:47:58.367546+00:00", "nick": "leland_", "message": "Hey guys, is anyone available to help out? I'm having trouble installing Scrapy on Windows 8 after I've installed all of its dependencies", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T02:48:23.218086+00:00", "nick": "cornjuliox", "message": "is the page I linked even JS?", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T02:50:02.765919+00:00", "nick": "cornjuliox", "message": "nvm looks like it is", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:27:29.033625+00:00", "nick": "HowardwLo", "message": "https://dpaste.de/vi0n can something like this work?", "links": ["https://dpaste.de/vi0n"], "channel": "scrapy"},
{"date": "2014-05-16T07:28:37.560045+00:00", "nick": "HowardwLo", "message": "instead of making a request, just directly call the next parse method since the page doesnt change..", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:34:58.480167+00:00", "nick": "nikolaosk", "message": "HowardwLo: I think you just want to split the code for the parsing of the response", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:35:15.368003+00:00", "nick": "nikolaosk", "message": "I don't see any return or yield", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:35:33.005230+00:00", "nick": "HowardwLo", "message": "nikolaosk: should i be returning/yielding even though the page doesnt change?", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:35:40.253233+00:00", "nick": "HowardwLo", "message": "I have yield in productparse, lower down", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:35:44.357956+00:00", "nick": "HowardwLo", "message": "but i didnt include it", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:35:46.072955+00:00", "nick": "nikolaosk", "message": "I mean I see them nowhere at all", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:35:49.998653+00:00", "nick": "nikolaosk", "message": "aha", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:36:17.380917+00:00", "nick": "HowardwLo", "message": "but theres a disconnect at line 7", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:36:31.019421+00:00", "nick": "HowardwLo", "message": "self.productparse(response)\u2026doesnt seem to work :D", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:36:33.537420+00:00", "nick": "nikolaosk", "message": "line 7 does not return", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:36:41.655464+00:00", "nick": "nikolaosk", "message": "or yield", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:36:55.798442+00:00", "nick": "nikolaosk", "message": "the code in product parse is evaluated", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:37:01.583070+00:00", "nick": "nikolaosk", "message": "but the result goes nowhere", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:37:12.885605+00:00", "nick": "nikolaosk", "message": "ok", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:37:18.141697+00:00", "nick": "nikolaosk", "message": "first of all", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:37:19.572729+00:00", "nick": "HowardwLo", "message": "i need to return/yield in order to call productparse?", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:37:22.510245+00:00", "nick": "nikolaosk", "message": "decide if productparse", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:37:37.421033+00:00", "nick": "nikolaosk", "message": "should be a generator", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:37:41.277040+00:00", "nick": "nikolaosk", "message": "or a function", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:37:50.148517+00:00", "nick": "HowardwLo", "message": "um....", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:38:02.389232+00:00", "nick": "nikolaosk", "message": "do you know the difference?", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:38:09.187643+00:00", "nick": "HowardwLo", "message": "no", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:38:15.412653+00:00", "nick": "nikolaosk", "message": "ok, then ask yourself", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:38:42.374662+00:00", "nick": "nikolaosk", "message": "do you want the function to return a single item from the parsed response", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:38:44.084875+00:00", "nick": "nikolaosk", "message": "or many", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:38:53.990032+00:00", "nick": "HowardwLo", "message": "many", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:38:58.756205+00:00", "nick": "nikolaosk", "message": "in the first case ask no further, you need a function", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:39:16.252117+00:00", "nick": "nikolaosk", "message": "in the second case", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:39:19.111491+00:00", "nick": "HowardwLo", "message": "heres the mess https://dpaste.de/rZDZ", "links": ["https://dpaste.de/rZDZ"], "channel": "scrapy"},
{"date": "2014-05-16T07:39:41.267884+00:00", "nick": "HowardwLo", "message": "i think my answer is many? :D", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:40:40.394759+00:00", "nick": "nikolaosk", "message": "aha, I see", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:40:55.642559+00:00", "nick": "nikolaosk", "message": "pagination", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:41:00.861081+00:00", "nick": "HowardwLo", "message": "indeed :D", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:41:22.948243+00:00", "nick": "nikolaosk", "message": "when you yield something from a callback", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:41:38.846387+00:00", "nick": "nikolaosk", "message": "this \"something\" may be either an item or a request", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:41:48.129582+00:00", "nick": "nikolaosk", "message": "items are sent to the pipeline", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:41:58.586095+00:00", "nick": "nikolaosk", "message": "requests to the downloader", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:42:26.899454+00:00", "nick": "HowardwLo", "message": "right", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:42:35.553326+00:00", "nick": "nikolaosk", "message": "do the following in a single callback", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:42:43.504863+00:00", "nick": "nikolaosk", "message": "iterate through all products", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:42:47.920761+00:00", "nick": "nikolaosk", "message": "creating items", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:42:52.952560+00:00", "nick": "nikolaosk", "message": "and yielding them", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:42:57.496500+00:00", "nick": "nikolaosk", "message": "one by one", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:43:32.890244+00:00", "nick": "nikolaosk", "message": "if a next page button exists", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:43:41.263992+00:00", "nick": "nikolaosk", "message": "also yield a request for it", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:44:01.818570+00:00", "nick": "HowardwLo", "message": "but i need the items to belong to one Item()", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:44:03.339432+00:00", "nick": "nikolaosk", "message": "preferably before items", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:44:28.535998+00:00", "nick": "nikolaosk", "message": "a single Item to accumulate all products?", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:44:35.178531+00:00", "nick": "HowardwLo", "message": "correct", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:44:41.308478+00:00", "nick": "HowardwLo", "message": "im trying to establish many2many relationships", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:45:12.664779+00:00", "nick": "HowardwLo", "message": "those paginated items will all be added to a list, and the list will be sent to the pipelines as part of the Item()", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:46:18.218393+00:00", "nick": "HowardwLo", "message": "everything is set, its just that line 7", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:46:23.301861+00:00", "nick": "HowardwLo", "message": "if only that works :P", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:46:41.102749+00:00", "nick": "HowardwLo", "message": "i thought it could, cause a response is a response\u2026.", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:46:55.460519+00:00", "nick": "HowardwLo", "message": "its basically a dict of sorts right?", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:50:42.389265+00:00", "nick": "HowardwLo", "message": "nikolaosk: any ideas? :D", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:54:34.047276+00:00", "nick": "nikolaosk", "message": "the pagination does not work?", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:54:50.942716+00:00", "nick": "nikolaosk", "message": "sure, you are selecting relative urls", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:55:38.959490+00:00", "nick": "HowardwLo", "message": "the pagination works", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:55:40.403993+00:00", "nick": "HowardwLo", "message": "everyhting works", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:55:52.261680+00:00", "nick": "HowardwLo", "message": "except line 7 of https://dpaste.de/vi0n", "links": ["https://dpaste.de/vi0n"], "channel": "scrapy"},
{"date": "2014-05-16T07:56:22.365140+00:00", "nick": "HowardwLo", "message": "i dont know how to get it to just call productparse", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:56:47.345863+00:00", "nick": "nikolaosk", "message": "I don't know if \"called\" would be printed at all", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:56:59.469221+00:00", "nick": "nikolaosk", "message": "write self.log(\"called\")", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:57:15.301338+00:00", "nick": "nikolaosk", "message": "also, drop a_list", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:57:27.502138+00:00", "nick": "nikolaosk", "message": "it's correct but you may confuse yourself", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:57:46.923408+00:00", "nick": "nikolaosk", "message": "write item['items'].append(temp) directly", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:58:44.387086+00:00", "nick": "HowardwLo", "message": "called would be printed, i always print to see where things are :). I'll fix the a_list thing", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T07:58:55.554924+00:00", "nick": "HowardwLo", "message": "but should line 7 work?", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T08:00:05.298830+00:00", "nick": "nikolaosk", "message": "it's correct but it is not meant to eval the productparse code", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T08:00:15.154194+00:00", "nick": "nikolaosk", "message": "just create an iterator", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T08:01:10.264986+00:00", "nick": "HowardwLo", "message": "create an iterator?", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T08:01:26.897887+00:00", "nick": "nikolaosk", "message": "not, _it_ just creates an iterator", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T08:01:51.406491+00:00", "nick": "HowardwLo", "message": "sorry, the CS lingo is lost on me. still an amateur", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T08:02:25.762907+00:00", "nick": "nikolaosk", "message": "ok, try this", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T08:02:35.233756+00:00", "nick": "nikolaosk", "message": "do everything in a single callback", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T08:02:52.584610+00:00", "nick": "nikolaosk", "message": "in productparse", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T08:03:09.223929+00:00", "nick": "nikolaosk", "message": "even the initialization of the item", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T08:03:27.920680+00:00", "nick": "nikolaosk", "message": "use this callback instead of typeparse", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T08:03:53.490033+00:00", "nick": "nikolaosk", "message": "in productparse check if an item is in response.meta", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T08:04:02.741366+00:00", "nick": "HowardwLo", "message": "sometimes things go directly from parse to productprase, other times it must pass through typeprase. how do i seperate the two ?", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T08:04:03.989747+00:00", "nick": "nikolaosk", "message": "if not, then initialize it", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T08:07:16.204512+00:00", "nick": "nikolaosk", "message": "HowardwLo: https://dpaste.de/SMYn#L16", "links": ["https://dpaste.de/SMYn#L16"], "channel": "scrapy"},
{"date": "2014-05-16T08:07:47.284315+00:00", "nick": "nikolaosk", "message": "excuse my indentation, dpaste messed up something", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T08:08:47.266812+00:00", "nick": "nikolaosk", "message": "a, one more mistake I didn't noticed", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T08:08:55.843166+00:00", "nick": "nikolaosk", "message": "you don't initialize item at all", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T08:09:07.492176+00:00", "nick": "nikolaosk", "message": "so do this inside the if", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T08:09:17.873760+00:00", "nick": "HowardwLo", "message": "nikolaosk: well\u2026\u2026. not quite https://dpaste.de/8Rah :D", "links": ["https://dpaste.de/8Rah"], "channel": "scrapy"},
{"date": "2014-05-16T08:12:24.714135+00:00", "nick": "HowardwLo", "message": "tehres two ways to get to productparse, from parse(where Item() is nitialized) and typeparse, which is from parse but without the initialization", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T08:13:43.404373+00:00", "nick": "HowardwLo", "message": "huff puff, was really hoping I could simply call productparse from typeparse", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T08:14:32.702822+00:00", "nick": "nikolaosk", "message": "well, you don't just get from one function to another", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T08:15:03.388957+00:00", "nick": "HowardwLo", "message": "what if i grab the url from typeparse and request into productparse? :$", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T08:15:06.628242+00:00", "nick": "nikolaosk", "message": "you have functions, iterators, callbacks (which happen to be iterators here)", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T08:15:21.629403+00:00", "nick": "nikolaosk", "message": "request again?", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T08:15:25.292554+00:00", "nick": "nikolaosk", "message": "the same url?", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T08:15:25.708001+00:00", "nick": "HowardwLo", "message": "ya", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T08:15:40.421084+00:00", "nick": "nikolaosk", "message": "redundant", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T08:15:57.207147+00:00", "nick": "nikolaosk", "message": "the dublicate filter will filter it", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T08:16:05.228764+00:00", "nick": "HowardwLo", "message": "fudge", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T08:16:14.694884+00:00", "nick": "nikolaosk", "message": "why do you want type parse anyway?", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T08:17:28.796149+00:00", "nick": "nikolaosk", "message": "it looks to me like you meant to separate your code", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T08:17:38.566778+00:00", "nick": "HowardwLo", "message": "due to the complicated nature of what im scraping", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T08:17:45.142538+00:00", "nick": "HowardwLo", "message": "it made sense to me to split it up", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T08:19:05.004712+00:00", "nick": "nikolaosk", "message": "don't do it if you don't understand functions, iterators and callbacks", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T08:19:30.826088+00:00", "nick": "HowardwLo", "message": "where can i read about functions iterators and callbacks?", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T08:24:23.418722+00:00", "nick": "nikolaosk", "message": "pydoc I guess", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T08:24:24.647794+00:00", "nick": "nikolaosk", "message": "https://docs.python.org/2/reference/expressions...", "links": ["https://docs.python.org/2/reference/expressions.html#yieldexpr"], "channel": "scrapy"},
{"date": "2014-05-16T08:25:13.157718+00:00", "nick": "nikolaosk", "message": "but not for callbacks, for this just look up theory about event driven programming", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T08:25:38.359888+00:00", "nick": "nikolaosk", "message": "all this will take you much longer than you think", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T08:27:10.041455+00:00", "nick": "nikolaosk", "message": "if you want to solve this now, confine your logic in a single function", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T08:27:26.698071+00:00", "nick": "HowardwLo", "message": "ok", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T08:27:32.121764+00:00", "nick": "HowardwLo", "message": "i'll give it a shot", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T08:27:32.681384+00:00", "nick": "HowardwLo", "message": "thanks", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T08:28:53.385681+00:00", "nick": "nikolaosk", "message": "you will need less styding but more code-monkeying", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T08:29:03.341029+00:00", "nick": "nikolaosk", "message": "sorry, I have to work now", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T08:33:03.646867+00:00", "nick": "HowardwLo", "message": "thanks for your help", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T09:16:51.403456+00:00", "nick": "styx__", "message": "hey all, wondering if someone could lend thier expertise", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T09:17:08.959602+00:00", "nick": "styx__", "message": "i wrote a spider to scrape a local website and it went well pulling all the fields i wanted", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T09:17:25.482587+00:00", "nick": "styx__", "message": "then tried to add it to a crawlspider, and although it seems to be running through the links on the page", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T09:17:31.961796+00:00", "nick": "styx__", "message": "no longer scraping the information", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T09:18:08.130571+00:00", "nick": "styx__", "message": "https://gist.github.com/anonymous/9962ba13c0381...", "links": ["https://gist.github.com/anonymous/9962ba13c0381cf49e92"], "channel": "scrapy"},
{"date": "2014-05-16T09:18:17.897350+00:00", "nick": "styx__", "message": "is the script", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T09:19:55.765020+00:00", "nick": "styx__", "message": "scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T12:07:18.589104+00:00", "nick": "styx__", "message": "no love?", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T16:39:09.085554+00:00", "nick": "nyov", "message": "styx__: better read the warnings. http://doc.scrapy.org/en/latest/topics/spiders....", "links": ["http://doc.scrapy.org/en/latest/topics/spiders.html#crawling-rules"], "channel": "scrapy"},
{"date": "2014-05-16T16:40:01.197960+00:00", "nick": "nyov", "message": "\"if you override the parse method, the crawl spider will no longer work.\"", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T20:48:23.968292+00:00", "nick": "styx__", "message": "nyov: thanks for your reply, I noticed that and changed it, the spider still crawls each url and doesnt scrape anything..", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T20:48:26.649325+00:00", "nick": "styx__", "message": "https://gist.github.com/anonymous/06be29d1be690...", "links": ["https://gist.github.com/anonymous/06be29d1be690b90ea53"], "channel": "scrapy"},
{"date": "2014-05-16T20:56:14.526843+00:00", "nick": "nyov", "message": "styx__: either that's because your first rule also matches everything the second one does, and gets executed instead", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T20:57:18.682586+00:00", "nick": "nyov", "message": "or your start url is the same as the match url, and thus can never match, since it was already parsed and duplicates get filtered", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T20:58:07.625441+00:00", "nick": "evrian", "message": "hey, can anybody help me with specifying different pipelines for different spiders?", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T20:59:04.453832+00:00", "nick": "evrian", "message": "i'm trying to implement the answer to this question", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T20:59:04.925511+00:00", "nick": "evrian", "message": "http://stackoverflow.com/questions/8372703/how-...", "links": ["http://stackoverflow.com/questions/8372703/how-can-i-use-different-pipelines-for-different-spiders-in-a-single-scrapy-proje"], "channel": "scrapy"},
{"date": "2014-05-16T20:59:36.805847+00:00", "nick": "evrian", "message": "when the item moves to processing i get an error with twisted", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T21:00:13.570029+00:00", "nick": "nyov", "message": "evrian, easiest way? tell the pipeline which spider to \"match\" on", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T21:01:06.542592+00:00", "nick": "evrian", "message": "ok. thanks nyov", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T21:02:22.312795+00:00", "nick": "evrian", "message": "is there a way that doesn't seem so hacky?", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T21:02:32.626592+00:00", "nick": "nyov", "message": "is that enough for you? :P", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T21:02:50.543893+00:00", "nick": "evrian", "message": "it is", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T21:02:56.840502+00:00", "nick": "evrian", "message": "i was just asking for the sake of knowledge", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T21:03:23.393159+00:00", "nick": "nyov", "message": "def process_item(self, item, spider): if spider.name not 'MySpider': return item;", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T21:04:32.405747+00:00", "nick": "nyov", "message": "well, pipelines or middlewares weren't meant to be per spider. otherwise you likely want multiple projects anyway.", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T21:05:24.504173+00:00", "nick": "nyov", "message": "but what that guy in your SO link has, looks like a way to do it", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T21:05:36.772553+00:00", "nick": "styx__", "message": "interesting... ill have a look at that", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T21:06:13.731491+00:00", "nick": "nyov", "message": "(meaning, wrap process_item with a decorator)", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T21:06:28.931065+00:00", "nick": "evrian", "message": "when i apply the decorator i start getting errors", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T21:07:12.594210+00:00", "nick": "nyov", "message": "maybe the code is outdated. I haven't tried it", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T21:07:14.901591+00:00", "nick": "evrian", "message": "an error with twisted i think", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T21:07:20.582936+00:00", "nick": "evrian", "message": "yeah. i don't think it works", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T21:08:24.534340+00:00", "nick": "evrian", "message": "i didn't want to send everything through the same pipelines because i need to use different sql statements", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T21:08:27.686366+00:00", "nick": "nyov", "message": "another idea would be to write your own \"master\" pipeline, which does switching based on your logic and include the rest of your code/pipelines", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T21:08:48.938593+00:00", "nick": "evrian", "message": "i thought it would be good to have one pipeline generate the SQL statement and another to send it to the db", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T21:19:03.406050+00:00", "nick": "styx__", "message": "so i removed changed the start_urls, and then removed the allow rule and it went wild, crawling all sorts, and generally not scraping anything", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T21:19:05.203998+00:00", "nick": "styx__", "message": "then suddenly", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T21:19:16.092576+00:00", "nick": "styx__", "message": "randomly an address here or there worked", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T23:13:19.158915+00:00", "nick": "hugo_______", "message": "hey o/ im saving items through JsonLinesItemExporter in my pipeline, items get stored as i want using scrapy crawl spider, but when using scrapyd and having items_dir set to nothing in my cfg, it still exports what ever is returned in process_items, how can i change this?", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T23:15:58.093460+00:00", "nick": "nyov", "message": "disable the exporter in the config?", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T23:16:41.032410+00:00", "nick": "nyov", "message": "at least it sounds to me that you are using a pipeline there", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T23:17:32.914969+00:00", "nick": "hugo_______", "message": "yeah, im processing my item and store in in files. i want to disable the scrapyd exporter and use my own. not sure what exporter i should disable", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T23:18:24.573452+00:00", "nick": "hugo_______", "message": "i get the results i want using scrapy crawl, but an additional file in items/project/spider/*.jl that i dont want, using scrapyd schedule", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T23:21:14.344958+00:00", "nick": "hugo_______", "message": "my understanding is that, updating my cfg with items_dir = should disable that scrapyd stores anything", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T23:21:25.288324+00:00", "nick": "hugo_______", "message": "but this doesnt seem to be the case", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T23:26:20.881248+00:00", "nick": "hugo_______", "message": "heres my pipeline, http://paste.kde.org/pwtnjodnh/tygtmu", "links": ["http://paste.kde.org/pwtnjodnh/tygtmu"], "channel": "scrapy"},
{"date": "2014-05-16T23:38:13.588335+00:00", "nick": "hugo_______", "message": "Process finished:  project='project' spider='myspider' job='deaf9970dd5211e383273c15c2c45ae2' pid=35812 log='logs/project/myspider/deaf9970dd5211e383273c15c2c45ae2.log' items='items/project/myspider/deaf9970dd5211e383273c15c2c45ae2.jl' <- dont get it", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T23:40:09.758057+00:00", "nick": "hugo_______", "message": "Version: 1.0.1", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T23:48:32.663244+00:00", "nick": "styx__", "message": "is there anyway to tell if the spider is looping?", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T23:48:51.262864+00:00", "nick": "styx__", "message": "since earlier it seems to be goign through and scraping at least some addresses", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T23:49:02.734489+00:00", "nick": "styx__", "message": "but been going on 2 hours now", "links": [], "channel": "scrapy"},
{"date": "2014-05-16T23:49:37.693412+00:00", "nick": "styx__", "message": "i have a depth limit of 2 set", "links": [], "channel": "scrapy"},
{"date": "2014-05-17T00:05:57.555144+00:00", "nick": "nyov", "message": "hugo_______: sorry, haven't looked at scrapyd in a while, I don't know", "links": [], "channel": "scrapy"},
{"date": "2014-05-17T00:06:31.923949+00:00", "nick": "hugo_______", "message": "nyov: np, hopefully someone knows :)", "links": [], "channel": "scrapy"},
{"date": "2014-05-17T00:10:50.859472+00:00", "nick": "nyov", "message": "styx__: you will have to write your Rules in a way that it doesn't download unnecessary pages", "links": [], "channel": "scrapy"},
{"date": "2014-05-17T00:11:06.999757+00:00", "nick": "nyov", "message": "a spider doesn't loop, unless you disable the dupefilter", "links": [], "channel": "scrapy"},
{"date": "2014-05-17T00:11:22.135774+00:00", "nick": "nyov", "message": "or your target page has a bot trap", "links": [], "channel": "scrapy"},
{"date": "2014-05-17T00:15:40.469764+00:00", "nick": "styx__", "message": "ok i see", "links": [], "channel": "scrapy"},
{"date": "2014-05-17T00:15:52.345659+00:00", "nick": "styx__", "message": "can i do this using xpaths or do i need to use regex", "links": [], "channel": "scrapy"},
{"date": "2014-05-17T00:17:13.455271+00:00", "nick": "styx__", "message": "apart from the time,its really doing quite well, scraped info on over 3k properties", "links": [], "channel": "scrapy"},
{"date": "2014-05-17T00:17:42.476386+00:00", "nick": "styx__", "message": "quite impressed, i just wish i wasnt so green", "links": [], "channel": "scrapy"},
{"date": "2014-05-17T00:26:59.939538+00:00", "nick": "nyov", "message": "just read up on Rule() and link extractors. regex in url paths, xpath if you want to narrow down url results inside a page", "links": [], "channel": "scrapy"},
{"date": "2014-05-17T00:35:48.770206+00:00", "nick": "styx__", "message": "ok thanks for your assistance, been very helpful", "links": [], "channel": "scrapy"},
{"date": "2014-05-17T00:37:21.269228+00:00", "nick": "nyov", "message": "you're welcome", "links": [], "channel": "scrapy"},
{"date": "2014-05-17T04:55:45.696897+00:00", "nick": "gururr", "message": "hello, is scrapy's learning algorithm faster than normal scrapy's own imteplementation?", "links": [], "channel": "scrapy"},
{"date": "2014-05-17T04:56:04.984118+00:00", "nick": "gururr", "message": "hello, is Scrapely* learning algorithm faster than normal scrapy's own imteplementation?", "links": [], "channel": "scrapy"},
{"date": "2014-05-17T05:03:49.287878+00:00", "nick": "nyov", "message": "scrapy has no learning algorithm", "links": [], "channel": "scrapy"},
{"date": "2014-05-17T05:05:03.185499+00:00", "nick": "gururr", "message": "but you can extract the items using xpath", "links": [], "channel": "scrapy"},
{"date": "2014-05-17T05:05:35.787385+00:00", "nick": "gururr", "message": "nyov: you can extract the items using xpath. only thing is, you have to do it one by one. after you define for 1 product, scraper does all this job for all other pages too.", "links": [], "channel": "scrapy"},
{"date": "2014-05-17T05:05:46.570235+00:00", "nick": "nyov", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2014-05-17T05:07:50.579484+00:00", "nick": "gururr", "message": "but scrapely does have a learning system", "links": [], "channel": "scrapy"},
{"date": "2014-05-17T05:07:57.048780+00:00", "nick": "gururr", "message": "why i would use scrapely ? with scrapy?", "links": [], "channel": "scrapy"},
{"date": "2014-05-17T14:37:21.968914+00:00", "nick": "Digenis", "message": "I 'll be damned", "links": [], "channel": "scrapy"},
{"date": "2014-05-17T14:37:33.532013+00:00", "nick": "Digenis", "message": "wish I had seen #556 on time", "links": [], "channel": "scrapy"},
{"date": "2014-05-17T14:39:45.929617+00:00", "nick": "Digenis", "message": "nyov: did you move away from item loaders?", "links": [], "channel": "scrapy"},
{"date": "2014-05-17T14:41:08.690148+00:00", "nick": "Digenis", "message": "I had issues with python's None being filtered out when I meant to have a NULL for sql", "links": [], "channel": "scrapy"},
{"date": "2014-05-17T14:42:15.899039+00:00", "nick": "Digenis", "message": "I turned items to defaultdict(lambda: None) just before inserting them", "links": [], "channel": "scrapy"},
{"date": "2014-05-17T14:42:37.225731+00:00", "nick": "Digenis", "message": "which still doesn't fix everything, this logic doesn't apply to all fields", "links": [], "channel": "scrapy"},
{"date": "2014-05-17T14:46:59.359110+00:00", "nick": "Digenis", "message": "I am subclassing them overriding many of their methods but this can't go on for ever", "links": [], "channel": "scrapy"},
{"date": "2014-05-17T14:47:28.644705+00:00", "nick": "Digenis", "message": "(methods with leading _ I mean)", "links": [], "channel": "scrapy"},
{"date": "2014-05-17T23:27:40.186139+00:00", "nick": "nyov", "message": "Digenis: no, i use a custom/patched itemloader. https://github.com/nyov/scrapyext/blob/master/s...", "links": ["https://github.com/nyov/scrapyext/blob/master/scrapyext/loader.py#L25"], "channel": "scrapy"},
{"date": "2014-05-18T12:32:21.750142+00:00", "nick": "remote", "message": "howdy", "links": [], "channel": "scrapy"},
{"date": "2014-05-18T12:33:26.557758+00:00", "nick": "remote", "message": "http://doc.scrapy.org/en/0.12/topics/selectors.... - \"we\u2019ll be using the HtmlXPathSelector object which is found, by default, in the hxs shell variable\"", "links": ["http://doc.scrapy.org/en/0.12/topics/selectors.html"], "channel": "scrapy"},
{"date": "2014-05-18T12:34:08.871040+00:00", "nick": "remote", "message": "when I run my scrapy shell and fetch an url this variable isn't set", "links": [], "channel": "scrapy"},
{"date": "2014-05-18T12:47:22.491116+00:00", "nick": "Digenis", "message": "remote: look at the url", "links": [], "channel": "scrapy"},
{"date": "2014-05-18T12:47:43.661158+00:00", "nick": "Digenis", "message": "doc.scrapy.org/en/0.12/", "links": ["http://doc.scrapy.org/en/0.12/"], "channel": "scrapy"},
{"date": "2014-05-18T12:48:28.684508+00:00", "nick": "Digenis", "message": "I don't think you are using version 0.12", "links": [], "channel": "scrapy"},
{"date": "2014-05-18T12:48:40.229670+00:00", "nick": "Digenis", "message": "probably 0.22", "links": [], "channel": "scrapy"},
{"date": "2014-05-18T12:49:12.571796+00:00", "nick": "Digenis", "message": "the variable you are looking for is named 'sel'", "links": [], "channel": "scrapy"},
{"date": "2014-05-18T13:20:03.060691+00:00", "nick": "remote", "message": "ahh", "links": [], "channel": "scrapy"},
{"date": "2014-05-18T13:20:05.003135+00:00", "nick": "remote", "message": "thanks", "links": [], "channel": "scrapy"},
{"date": "2014-05-18T19:47:48.298998+00:00", "nick": "s1991", "message": "Hello anyone??", "links": [], "channel": "scrapy"},
{"date": "2014-05-18T19:48:55.621979+00:00", "nick": "s1991", "message": "?? :P", "links": [], "channel": "scrapy"},
{"date": "2014-05-19T11:27:01.631664+00:00", "nick": "lucax", "message": "Hello scrapers", "links": [], "channel": "scrapy"},
{"date": "2014-05-19T11:27:38.194221+00:00", "nick": "lucax", "message": "I am new to scrapy and I wonder how does it work? can I extract adresses and emails for promotional use with Scrapy?", "links": [], "channel": "scrapy"},
{"date": "2014-05-19T11:30:14.919133+00:00", "nick": "lucax", "message": "anyone here?", "links": [], "channel": "scrapy"},
{"date": "2014-05-19T13:51:58.018760+00:00", "nick": "moresec", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2014-05-19T13:52:46.302788+00:00", "nick": "moresec", "message": "May I ask a question:   how to get the elements in their origin order withing selector.xpath()", "links": [], "channel": "scrapy"},
{"date": "2014-05-19T13:54:15.302086+00:00", "nick": "moresec", "message": "for example, <ul><li>1</li><li>2</li><li>3</li></ul>,   I want [1, 2, 3], but it return [2,3,1]", "links": [], "channel": "scrapy"},
{"date": "2014-05-19T13:54:48.004445+00:00", "nick": "moresec", "message": "I use this xpath experssion: //ul/li/text()", "links": [], "channel": "scrapy"},
{"date": "2014-05-19T14:01:41.892005+00:00", "nick": "csalazar", "message": "I get them in order", "links": [], "channel": "scrapy"},
{"date": "2014-05-19T14:04:50.199333+00:00", "nick": "moresec", "message": "http://shenzhen.8684.cn/x_04f068e2", "links": ["http://shenzhen.8684.cn/x_04f068e2"], "channel": "scrapy"},
{"date": "2014-05-19T14:05:24.575052+00:00", "nick": "moresec", "message": "name_list = sel.xpath('//div[@id=\"lms-station\"]/ul[@class=\"lms-stationLf\"]/li')", "links": [], "channel": "scrapy"},
{"date": "2014-05-19T14:05:45.844959+00:00", "nick": "moresec", "message": "and then , name_list is not original order from the webpage", "links": [], "channel": "scrapy"},
{"date": "2014-05-19T14:06:11.897585+00:00", "nick": "moresec", "message": "scrapy shell http://shenzhen.8684.cn/x_04f068e2", "links": ["http://shenzhen.8684.cn/x_04f068e2"], "channel": "scrapy"},
{"date": "2014-05-19T14:06:19.056791+00:00", "nick": "moresec", "message": "that is my test page", "links": [], "channel": "scrapy"},
{"date": "2014-05-19T14:07:53.328443+00:00", "nick": "moresec", "message": "it is chinese , you could test it for the first or the end element", "links": [], "channel": "scrapy"},
{"date": "2014-05-19T14:30:07.278567+00:00", "nick": "moresec", "message": "any one could help me ?", "links": [], "channel": "scrapy"},
{"date": "2014-05-19T14:31:33.704861+00:00", "nick": "samtc", "message": "moresec: you don't get them in order?", "links": [], "channel": "scrapy"},
{"date": "2014-05-19T14:31:46.706356+00:00", "nick": "moresec", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2014-05-19T14:32:01.599871+00:00", "nick": "moresec", "message": "https://groups.google.com/forum/#!topic/scrapy-...", "links": ["https://groups.google.com/forum/#!topic/scrapy-users/1t6Z1-FJ2U4"], "channel": "scrapy"},
{"date": "2014-05-19T14:32:20.707124+00:00", "nick": "moresec", "message": "i post a topic in the google group, there is more info , help me pls", "links": [], "channel": "scrapy"},
{"date": "2014-05-19T14:32:36.007321+00:00", "nick": "csalazar", "message": "i get them in order too from the webpage", "links": [], "channel": "scrapy"},
{"date": "2014-05-19T14:39:19.209489+00:00", "nick": "samtc", "message": "if I look at the source: <ul class=\"lms-stationLf\">  <li>  <b>1</b><a href=\"/z_3e6ab30a\">", "links": [], "channel": "scrapy"},
{"date": "2014-05-19T14:42:04.977927+00:00", "nick": "moresec", "message": "it should be z_e8a0d1a1", "links": [], "channel": "scrapy"},
{"date": "2014-05-19T14:44:29.760876+00:00", "nick": "moresec", "message": "z_3e6ab30a   is the href of 46th item", "links": [], "channel": "scrapy"},
{"date": "2014-05-19T14:44:56.395359+00:00", "nick": "moresec", "message": "could it be the difference of our pc?", "links": [], "channel": "scrapy"},
{"date": "2014-05-19T14:45:37.073467+00:00", "nick": "moresec", "message": "z_3e6ab30a   is the href of 41th item                46th-->41th  , I type wrong", "links": [], "channel": "scrapy"},
{"date": "2014-05-19T14:46:17.871545+00:00", "nick": "samtc", "message": "In chrome and scrapy I see z_3e6ab30a as 1st and 13th", "links": [], "channel": "scrapy"},
{"date": "2014-05-19T14:47:15.158197+00:00", "nick": "samtc", "message": "z_e8a0d1a1 is 9 and 26", "links": [], "channel": "scrapy"},
{"date": "2014-05-19T14:49:03.657603+00:00", "nick": "moresec", "message": "maybe js change the order?", "links": [], "channel": "scrapy"},
{"date": "2014-05-19T14:53:01.924913+00:00", "nick": "samtc", "message": "On the mobile version ( http://m.8684.cn/shenzhen_x_04f068e2 ) they are in the \"wrong\" order.", "links": ["http://m.8684.cn/shenzhen_x_04f068e2"], "channel": "scrapy"},
{"date": "2014-05-19T14:58:18.893774+00:00", "nick": "moresec", "message": "there must be sth in js change the order", "links": [], "channel": "scrapy"},
{"date": "2014-05-19T15:03:45.693712+00:00", "nick": "csalazar", "message": "scrapy doesn't interpret js, check the headers that you're sending from your browser vs scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-05-19T15:20:48.702984+00:00", "nick": "jakek", "message": "Hey I'm trying to determine if DOWNLOAD_DELAY is enforced per-domain, or per-spider. It seems like it used to be per-spider, but now it's per-domain? I've tried searching for a concrete answer but ran into a lot of conflicting information", "links": [], "channel": "scrapy"},
{"date": "2014-05-19T17:51:41.722171+00:00", "nick": "lucax_", "message": "dear honourable scrapers worldwide, I am new to scraping and as you know, the installation can be tricky for a linux baby, anyone care for taking me for a walk?", "links": [], "channel": "scrapy"},
{"date": "2014-05-19T18:10:33.455427+00:00", "nick": "ernierock", "message": "Hello", "links": [], "channel": "scrapy"},
{"date": "2014-05-19T18:44:48.988327+00:00", "nick": "Beliq", "message": "Hello folks, I am experiencing build fail for clean (new) virtualenv. Is it possible to install with pip a specific git commit, which is working (probably this one ? https://travis-ci.org/scrapy/scrapy/builds/2457... )", "links": ["https://travis-ci.org/scrapy/scrapy/builds/24573079"], "channel": "scrapy"},
{"date": "2014-05-19T18:46:16.528291+00:00", "nick": "samtc", "message": "beliq: the ffi bug?", "links": [], "channel": "scrapy"},
{"date": "2014-05-19T18:47:10.699957+00:00", "nick": "Beliq", "message": "what is the ffi bug samtc ?", "links": [], "channel": "scrapy"},
{"date": "2014-05-19T18:48:04.005986+00:00", "nick": "samtc", "message": "Beliq: http://stackoverflow.com/questions/13685920/ins...", "links": ["http://stackoverflow.com/questions/13685920/install-specific-git-commit-with-pip"], "channel": "scrapy"},
{"date": "2014-05-19T18:48:42.032711+00:00", "nick": "Beliq", "message": "well yeah, that didn't work for me. I tried something like that pip install git+git://github.com:scrapy/scrapy.git@8bdb6e2e3e1e3fe883e9ca9d54f00dd2e5523192", "links": [], "channel": "scrapy"},
{"date": "2014-05-19T18:49:48.869144+00:00", "nick": "csalazar", "message": "pip install -e \"git+...\"", "links": [], "channel": "scrapy"},
{"date": "2014-05-19T18:51:03.823660+00:00", "nick": "Beliq", "message": "csalazar I got an error \".. is not the right format; it must have #egg=Package\"", "links": [], "channel": "scrapy"},
{"date": "2014-05-19T18:51:38.267990+00:00", "nick": "csalazar", "message": "and pip install -e \"git+git://github.com:scrapy/scrapy.git@8bdb6e2e3e1e3fe883e9ca9d54f00dd2e5523192#egg=scrapy\"", "links": [], "channel": "scrapy"},
{"date": "2014-05-19T18:51:39.066590+00:00", "nick": "csalazar", "message": "?", "links": [], "channel": "scrapy"},
{"date": "2014-05-19T18:53:17.875044+00:00", "nick": "Beliq", "message": "Alright, that brings me to the same error as with the current pip package. Thanks csalazar I will try with another commit.", "links": [], "channel": "scrapy"},
{"date": "2014-05-20T16:15:27.181047+00:00", "nick": "s1991", "message": "Hello, I want to know is scrapy well enough to scrap data from more than a 1000 url in a go", "links": [], "channel": "scrapy"},
{"date": "2014-05-20T16:17:42.332678+00:00", "nick": "s1991", "message": "Hello ???", "links": [], "channel": "scrapy"},
{"date": "2014-05-20T17:00:02.018651+00:00", "nick": "samtc", "message": "s1991: scrapy won't limit the # urls you can scrape.", "links": [], "channel": "scrapy"},
{"date": "2014-05-20T17:05:18.506884+00:00", "nick": "s1991", "message": "samtc: but It'll taking too much time", "links": [], "channel": "scrapy"},
{"date": "2014-05-20T17:06:00.289919+00:00", "nick": "s1991", "message": "should I add them all under spider as start_urls, or there's other way?", "links": [], "channel": "scrapy"},
{"date": "2014-05-20T17:07:07.979392+00:00", "nick": "samtc", "message": "can you generate the urls? or you could load them from a file", "links": [], "channel": "scrapy"},
{"date": "2014-05-20T17:07:27.418164+00:00", "nick": "samtc", "message": "start_urls can be replaced with that function: http://doc.scrapy.org/en/latest/topics/spiders....", "links": ["http://doc.scrapy.org/en/latest/topics/spiders.html#scrapy.spider.Spider.start_requests"], "channel": "scrapy"},
{"date": "2014-05-20T17:10:05.955038+00:00", "nick": "s1991", "message": "samtc: thanks", "links": [], "channel": "scrapy"},
{"date": "2014-05-20T17:49:39.384758+00:00", "nick": "s1991", "message": "samtc: are content in response.body can be different from web-page source?", "links": [], "channel": "scrapy"},
{"date": "2014-05-20T18:00:36.935147+00:00", "nick": "samtc", "message": "s1991: yes it could", "links": [], "channel": "scrapy"},
{"date": "2014-05-20T18:01:26.080875+00:00", "nick": "s1991", "message": "samtc: why? is there any way to get the source?", "links": [], "channel": "scrapy"},
{"date": "2014-05-20T18:01:45.049941+00:00", "nick": "s1991", "message": "have web-site restricted me by some means", "links": [], "channel": "scrapy"},
{"date": "2014-05-20T18:01:53.539323+00:00", "nick": "samtc", "message": "I mean body and body_as_unicode is the source", "links": [], "channel": "scrapy"},
{"date": "2014-05-20T18:02:20.324423+00:00", "nick": "s1991", "message": "you mean response.body_as_unicode", "links": [], "channel": "scrapy"},
{"date": "2014-05-20T18:03:51.760313+00:00", "nick": "samtc", "message": "response.body and response.body_as_unicode() is the raw source returned by the http server", "links": [], "channel": "scrapy"},
{"date": "2014-05-20T19:15:18.322564+00:00", "nick": "Robin___", "message": "The code in my pipeline is not executing when I am running \"scrapy crawl myscrapper\". What could be the reason?", "links": [], "channel": "scrapy"},
{"date": "2014-05-20T22:08:00.078465+00:00", "nick": "timgt", "message": "Hi", "links": [], "channel": "scrapy"},
{"date": "2014-05-20T22:08:44.853199+00:00", "nick": "timgt", "message": "Somehow, I am no longer able to from scrapy.spider import Spider", "links": [], "channel": "scrapy"},
{"date": "2014-05-20T22:10:07.657013+00:00", "nick": "timgt", "message": "When I try to scrapy crawl I get ImportError: cannot import name Spider", "links": [], "channel": "scrapy"},
{"date": "2014-05-20T22:11:17.929625+00:00", "nick": "timgt", "message": "I was experimenting with CrawlSpider and some other things. Is there something I need to do to make this work again?", "links": [], "channel": "scrapy"},
{"date": "2014-05-20T22:57:37.799797+00:00", "nick": "scrpaer", "message": "scrapy-users hi", "links": [], "channel": "scrapy"},
{"date": "2014-05-20T22:57:50.622872+00:00", "nick": "scrpaer", "message": "http://stackoverflow.com/questions/23578415/err...", "links": ["http://stackoverflow.com/questions/23578415/error-installing-cffi-in-new-scrapy-version"], "channel": "scrapy"},
{"date": "2014-05-20T22:57:56.082790+00:00", "nick": "scrpaer", "message": "no answers yet", "links": [], "channel": "scrapy"},
{"date": "2014-05-20T23:05:26.159790+00:00", "nick": "scrpaer", "message": "http://stackoverflow.com/questions/23578415/err...", "links": ["http://stackoverflow.com/questions/23578415/error-installing-cffi-in-new-scrapy-version"], "channel": "scrapy"},
{"date": "2014-05-20T23:33:07.680195+00:00", "nick": "scrpaer", "message": "??", "links": [], "channel": "scrapy"},
{"date": "2014-05-21T04:19:28.922615+00:00", "nick": "peregrino", "message": "hi! I'm trying to parse a site that loads extra content by appending html that comes inside a json. how can I query that content?", "links": [], "channel": "scrapy"},
{"date": "2014-05-21T04:21:51.856141+00:00", "nick": "nyov", "message": "make an new Request or FormRequest, use the same query arguments as the site does", "links": [], "channel": "scrapy"},
{"date": "2014-05-21T04:22:25.021193+00:00", "nick": "nyov", "message": "parse the json manually", "links": [], "channel": "scrapy"},
{"date": "2014-05-21T04:23:04.295493+00:00", "nick": "peregrino", "message": "yeah, I got to that point, where I have a long string containing only html", "links": [], "channel": "scrapy"},
{"date": "2014-05-21T04:23:28.944722+00:00", "nick": "peregrino", "message": "I was wondering if there is a way to load that content and query it with xpath", "links": [], "channel": "scrapy"},
{"date": "2014-05-21T04:25:15.058214+00:00", "nick": "peregrino", "message": "seems like the HtmlXPathSelector only takes a response as parameter", "links": [], "channel": "scrapy"},
{"date": "2014-05-21T04:27:12.403236+00:00", "nick": "nyov", "message": "that's possible, let me find some code", "links": [], "channel": "scrapy"},
{"date": "2014-05-21T04:27:22.202518+00:00", "nick": "peregrino", "message": "uhm. seems \"HtmlXPathSelector(text=<html_string>)\" works", "links": [], "channel": "scrapy"},
{"date": "2014-05-21T04:27:25.946378+00:00", "nick": "peregrino", "message": "but is not documented", "links": [], "channel": "scrapy"},
{"date": "2014-05-21T04:33:29.101839+00:00", "nick": "nyov", "message": "https://gist.github.com/nyov/2f8bcdd656b2ee785422", "links": ["https://gist.github.com/nyov/2f8bcdd656b2ee785422"], "channel": "scrapy"},
{"date": "2014-05-21T04:35:44.779019+00:00", "nick": "peregrino", "message": "ah gotcha", "links": [], "channel": "scrapy"},
{"date": "2014-05-21T04:38:37.319890+00:00", "nick": "nyov", "message": "if you're really still using HtmlXPathSelector, replace Selector with that", "links": [], "channel": "scrapy"},
{"date": "2014-05-21T04:48:38.059173+00:00", "nick": "peregrino", "message": "well, I might be a couple of versions behind on scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-05-21T04:53:35.771238+00:00", "nick": "peregrino", "message": "yeah, I'm using an old scrapy :)", "links": [], "channel": "scrapy"},
{"date": "2014-05-21T09:56:50.389210+00:00", "nick": "zetzx", "message": "Newbie scrapy user here. Can anyone suggest tools (besides firebug/chrome devtools) to help making a spider for a site easier/faster?", "links": [], "channel": "scrapy"},
{"date": "2014-05-21T16:52:36.112577+00:00", "nick": "timgt", "message": "could anyone help me with the scrapy tutorial?", "links": [], "channel": "scrapy"},
{"date": "2014-05-21T17:03:28.575017+00:00", "nick": "nyov", "message": "timgt: maybe. what do you need help with?", "links": [], "channel": "scrapy"},
{"date": "2014-05-21T17:06:31.025753+00:00", "nick": "timgt", "message": "actually, I think it may be an issue with pycharm", "links": [], "channel": "scrapy"},
{"date": "2014-05-21T17:06:43.727667+00:00", "nick": "timgt", "message": "it's failing to 'from scrapy.spider import Spider'", "links": [], "channel": "scrapy"},
{"date": "2014-05-21T17:07:54.936575+00:00", "nick": "timgt", "message": "but it's importing fine outside of pycharm in the same virtualenv", "links": [], "channel": "scrapy"},
{"date": "2014-05-21T17:08:45.935727+00:00", "nick": "nyov", "message": "yeah, well that sounds like a problem with your environment and not a scrapy tutorial. sorry, don't know anything about pycharm", "links": [], "channel": "scrapy"},
{"date": "2014-05-21T18:37:16.495321+00:00", "nick": "Dormeus", "message": "Hi!", "links": [], "channel": "scrapy"},
{"date": "2014-05-21T18:37:59.213042+00:00", "nick": "Dormeus", "message": "I intend to scrape a list of articles (each one with a different link) on a website.", "links": [], "channel": "scrapy"},
{"date": "2014-05-21T18:38:54.929936+00:00", "nick": "Dormeus", "message": "However the text in these articles is placed under random div element for each paragraph and is not ordered from top to bottom", "links": [], "channel": "scrapy"},
{"date": "2014-05-21T18:40:21.760849+00:00", "nick": "Dormeus", "message": "the other page elements are also inside separate div elements again in random order", "links": [], "channel": "scrapy"},
{"date": "2014-05-21T18:40:53.823099+00:00", "nick": "Dormeus", "message": "What strategy can be used to scrape them from top to down (the way we read) and should work across different pages?", "links": [], "channel": "scrapy"},
{"date": "2014-05-21T18:45:08.226961+00:00", "nick": "jab3z", "message": "Dormeus: use css selectors instead of html paths.", "links": [], "channel": "scrapy"},
{"date": "2014-05-21T18:46:06.778137+00:00", "nick": "Dormeus", "message": "css selectors will fetch me id of the div elements", "links": [], "channel": "scrapy"},
{"date": "2014-05-21T18:46:15.943583+00:00", "nick": "Dormeus", "message": "those are placed in random order", "links": [], "channel": "scrapy"},
{"date": "2014-05-21T18:49:48.655610+00:00", "nick": "jab3z", "message": "Dormeus: id's should be ordered with a logic, like <div id=\"article-54\">...</div>, <div id=\"article-93\">, etc. In this example the article is the key word, easily to separate from it's number.", "links": [], "channel": "scrapy"},
{"date": "2014-05-21T18:50:14.941066+00:00", "nick": "jab3z", "message": "find the logic behind the css selectors and the issue it's solved.", "links": [], "channel": "scrapy"},
{"date": "2014-05-21T18:55:20.291721+00:00", "nick": "Dormeus", "message": "I have checked on a few pages and have found them random like wb_Text3, wb_Text2, wb_Text5, wb_Text7 from top to bottom and the text that they place on the webpage is also random using position:absolute in the style attribute of the div.", "links": [], "channel": "scrapy"},
{"date": "2014-05-21T18:59:21.417484+00:00", "nick": "jab3z", "message": "Dormeus: then wb_Text seems to be your keyword. which is a string, can be easily split. 'wb_text45'[:7]", "links": [], "channel": "scrapy"},
{"date": "2014-05-21T18:59:53.348030+00:00", "nick": "jab3z", "message": "or use startswith method.", "links": [], "channel": "scrapy"},
{"date": "2014-05-21T19:01:46.903442+00:00", "nick": "Digenis", "message": "css selectors, as used in scrapy, are a subset of xpath", "links": [], "channel": "scrapy"},
{"date": "2014-05-21T19:02:29.171842+00:00", "nick": "Digenis", "message": "it happens to be a tool that can easily solve many problems", "links": [], "channel": "scrapy"},
{"date": "2014-05-21T19:02:34.882375+00:00", "nick": "Digenis", "message": "but not all", "links": [], "channel": "scrapy"},
{"date": "2014-05-21T19:03:16.204456+00:00", "nick": "jab3z", "message": "you can use css selectors too, not just xpath.", "links": [], "channel": "scrapy"},
{"date": "2014-05-21T19:03:17.879558+00:00", "nick": "Dormeus", "message": "i need to scrape from top to bottom of the article. since the placing is random and some of the wb_Text are refering to text on the page outside the article also", "links": [], "channel": "scrapy"},
{"date": "2014-05-21T19:04:13.666706+00:00", "nick": "jab3z", "message": "it cannot be random, there must be a logic.", "links": [], "channel": "scrapy"},
{"date": "2014-05-21T19:04:56.468672+00:00", "nick": "Dormeus", "message": "unfortunately they are!", "links": [], "channel": "scrapy"},
{"date": "2014-05-21T19:05:17.886259+00:00", "nick": "Digenis", "message": "Dormeus: point me to the website if you want", "links": [], "channel": "scrapy"},
{"date": "2014-05-21T19:05:27.294592+00:00", "nick": "Dormeus", "message": "sure", "links": [], "channel": "scrapy"},
{"date": "2014-05-21T19:06:40.942072+00:00", "nick": "jab3z", "message": "use lxml.html.fromstring insted of TextResponse and you'll be able to go with css selectors.", "links": [], "channel": "scrapy"},
{"date": "2014-05-22T01:43:50.217485+00:00", "nick": "cornjuliox", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2014-05-22T19:47:49.280787+00:00", "nick": "metroidsnes", "message": "Hi, I have Python 2.7.6 x86 on Windows 8.1 64 bit. When I try to install Scrapy with pip or easy_install I have an error, 'UnicodeDecodeError'. I searched google for hours and found no solution. Plz. help me :]", "links": [], "channel": "scrapy"},
{"date": "2014-05-22T21:06:59.488545+00:00", "nick": "Ixio", "message": "Hello", "links": [], "channel": "scrapy"},
{"date": "2014-05-23T14:27:50.249627+00:00", "nick": "baazzilhassan", "message": "Hello :)", "links": [], "channel": "scrapy"},
{"date": "2014-05-23T14:28:28.705942+00:00", "nick": "baazzilhassan", "message": "I have many accounts on a website and I want to scrap each account", "links": [], "channel": "scrapy"},
{"date": "2014-05-23T14:28:49.890766+00:00", "nick": "baazzilhassan", "message": "The problem is how to manage login and logout", "links": [], "channel": "scrapy"},
{"date": "2014-05-23T14:28:56.015187+00:00", "nick": "baazzilhassan", "message": "?", "links": [], "channel": "scrapy"},
{"date": "2014-05-23T14:39:28.182154+00:00", "nick": "baazzilhassan", "message": "I need  your help", "links": [], "channel": "scrapy"},
{"date": "2014-05-23T14:42:32.193469+00:00", "nick": "nyov", "message": "baazzilhassan: well, my browser just crashed. but i'll give you a link in a few", "links": [], "channel": "scrapy"},
{"date": "2014-05-23T14:44:18.979797+00:00", "nick": "baazzilhassan", "message": "okey, thank :)", "links": [], "channel": "scrapy"},
{"date": "2014-05-23T14:54:36.000714+00:00", "nick": "nyov", "message": "sorry for the wait, had to finish another tasklet first", "links": [], "channel": "scrapy"},
{"date": "2014-05-23T14:54:56.055406+00:00", "nick": "nyov", "message": "I assume you want to login on spider start and logout on shutdown?", "links": [], "channel": "scrapy"},
{"date": "2014-05-23T14:56:56.495192+00:00", "nick": "baazzilhassan", "message": "I want to login to an account when I finish all links that I want to scrap, I logout and login to another account", "links": [], "channel": "scrapy"},
{"date": "2014-05-23T15:01:20.604904+00:00", "nick": "nyov", "message": "oh, well that looks like \"usual\" crawl logic then.", "links": [], "channel": "scrapy"},
{"date": "2014-05-23T15:02:00.350171+00:00", "nick": "nyov", "message": "depends very much on the website, so I can't give much help there.", "links": [], "channel": "scrapy"},
{"date": "2014-05-23T15:03:26.241943+00:00", "nick": "nyov", "message": "I was working on a loginspider which I haven't finished though. but there is https://github.com/scrapy/loginform", "links": ["https://github.com/scrapy/loginform"], "channel": "scrapy"},
{"date": "2014-05-23T15:04:14.264009+00:00", "nick": "nyov", "message": "that may help. and I have https://github.com/nyov/scrapyext/blob/master/s...", "links": ["https://github.com/nyov/scrapyext/blob/master/scrapyext/logoutmixin.py"], "channel": "scrapy"},
{"date": "2014-05-23T15:04:41.653066+00:00", "nick": "nyov", "message": "which is a helper to do a logout on crawler shutdown", "links": [], "channel": "scrapy"},
{"date": "2014-05-23T15:09:21.603497+00:00", "nick": "baazzilhassan", "message": "dangra: propose this http://doc.scrapy.org/en/latest/topics/download...", "links": ["http://doc.scrapy.org/en/latest/topics/downloader-middleware.html?highlight=cookiejar#multiple-cookie-sessions-per-spider"], "channel": "scrapy"},
{"date": "2014-05-23T16:41:06.300068+00:00", "nick": "Darni", "message": "Hi! I'm trying to figure out an exception I get on a very simple project, using scrapy 0.22.2", "links": [], "channel": "scrapy"},
{"date": "2014-05-23T16:41:37.608527+00:00", "nick": "Darni", "message": "I started an example project and I'm writing a spider; I've reduced the code to reproduce the problem to this: http://pastebin.com/bePtPrhc", "links": ["http://pastebin.com/bePtPrhc"], "channel": "scrapy"},
{"date": "2014-05-23T16:41:57.929816+00:00", "nick": "Darni", "message": "It works, but from time to time, I get the following traceback: http://pastebin.com/XWJdG0MD", "links": ["http://pastebin.com/XWJdG0MD"], "channel": "scrapy"},
{"date": "2014-05-23T16:42:34.955896+00:00", "nick": "Darni", "message": "I wasn't able to reproduce the traceback using the shell, and running the spider many times does not always produce the exception in the same pages (I assume scrapy is also somewhat non-deterministic)", "links": [], "channel": "scrapy"},
{"date": "2014-05-23T16:42:50.574865+00:00", "nick": "Darni", "message": "any idea of what I'm doing wrong? or where to look? thanks!", "links": [], "channel": "scrapy"},
{"date": "2014-05-23T17:10:45.077463+00:00", "nick": "toothrot", "message": "Darni, can't reproduce with your example (which also doesn't run with a normal `startproject` due to your ExampleItem import)", "links": [], "channel": "scrapy"},
{"date": "2014-05-23T17:11:19.649470+00:00", "nick": "Darni", "message": "ouch, sorry. I commented out code that wasn't relevant to the issue and forgot about that", "links": [], "channel": "scrapy"},
{"date": "2014-05-23T17:12:25.037092+00:00", "nick": "toothrot", "message": "no worries, just thought i'd mention it", "links": [], "channel": "scrapy"},
{"date": "2014-05-23T17:12:36.129865+00:00", "nick": "toothrot", "message": "i get that it doesn't happen all the time", "links": [], "channel": "scrapy"},
{"date": "2014-05-23T17:12:47.369197+00:00", "nick": "toothrot", "message": "but i'm not willing to spam a site to try and repro :(", "links": [], "channel": "scrapy"},
{"date": "2014-05-23T17:14:43.405413+00:00", "nick": "toothrot", "message": "i even tried the link shown in the exception directly", "links": [], "channel": "scrapy"},
{"date": "2014-05-23T17:14:47.961393+00:00", "nick": "toothrot", "message": "as the start link", "links": [], "channel": "scrapy"},
{"date": "2014-05-23T17:17:53.378460+00:00", "nick": "Darni", "message": "toothrot: when I tried that I can't reproduce it. so it seems it's intermittent", "links": [], "channel": "scrapy"},
{"date": "2014-05-23T17:18:02.647087+00:00", "nick": "Darni", "message": "but it happens quite frequently on different links", "links": [], "channel": "scrapy"},
{"date": "2014-05-23T19:30:10.646166+00:00", "nick": "Darni", "message": "toothrot: just curious, when you tested my code, which python version were you using?", "links": [], "channel": "scrapy"},
{"date": "2014-05-23T20:24:51.115524+00:00", "nick": "toothrot", "message": "Darni, 2.7.5", "links": [], "channel": "scrapy"},
{"date": "2014-05-23T20:25:04.227670+00:00", "nick": "toothrot", "message": "scrapy doesn't run on 3 i assume, because of twisted", "links": [], "channel": "scrapy"},
{"date": "2014-05-23T22:14:27.944250+00:00", "nick": "HowardwLo", "message": "https://dpaste.de/rWoA  I cloned my HDD recently, now im getting this error", "links": ["https://dpaste.de/rWoA"], "channel": "scrapy"},
{"date": "2014-05-23T23:28:55.635600+00:00", "nick": "HowardwLo", "message": "welp, seems upgrading to mavericks, you need to update brew, update all the libjpeg packages and waht not, then reinstall pillow/pil :)", "links": [], "channel": "scrapy"},
{"date": "2014-05-24T11:21:39.817268+00:00", "nick": "oquidave", "message": "hello", "links": [], "channel": "scrapy"},
{"date": "2014-05-24T11:22:52.678369+00:00", "nick": "oquidave", "message": "i want to follow the url /mpdata/mps.hei?p=f&n=t&number=7, so i've tried to use Rule (SgmlLinkExtractor(allow=(\"\\?p=f&n=t&number=\\d+\",),),callback=\"parse\", follow= True), but so far it has refused", "links": [], "channel": "scrapy"},
{"date": "2014-05-24T11:23:42.476555+00:00", "nick": "oquidave", "message": "what's  the right syntaxt for following that link", "links": [], "channel": "scrapy"},
{"date": "2014-05-24T12:00:06.443627+00:00", "nick": "oquidave", "message": "how do i know if scrapy is following through the links?", "links": [], "channel": "scrapy"},
{"date": "2014-05-24T12:59:54.847211+00:00", "nick": "Digenis", "message": "oquidave: when you examine the logs, in the GET lines you should see a referrer, the origin url where the link was found", "links": [], "channel": "scrapy"},
{"date": "2014-05-24T13:00:49.039565+00:00", "nick": "Digenis", "message": "sorry, \"referer\", correctly mistyped", "links": [], "channel": "scrapy"},
{"date": "2014-05-24T14:51:15.349498+00:00", "nick": "oquidave", "message": "Digenis: sorry i had steped out", "links": [], "channel": "scrapy"},
{"date": "2014-05-24T14:51:45.399078+00:00", "nick": "oquidave", "message": "Digenis: in the output, i can only see the first page, its not following the link in the rules", "links": [], "channel": "scrapy"},
{"date": "2014-05-24T14:56:37.735621+00:00", "nick": "nyov", "message": "oquidave: a regex there must match the _absolute_ url. are you sure it does?", "links": [], "channel": "scrapy"},
{"date": "2014-05-24T14:57:14.584499+00:00", "nick": "Digenis", "message": "joined with the response.url?", "links": [], "channel": "scrapy"},
{"date": "2014-05-24T14:59:11.640894+00:00", "nick": "nyov", "message": "i think it would have to start like .*? or such", "links": [], "channel": "scrapy"},
{"date": "2014-05-24T15:00:25.211858+00:00", "nick": "Digenis", "message": "https://github.com/scrapy/scrapy/blob/master/sc...", "links": ["https://github.com/scrapy/scrapy/blob/master/scrapy/contrib/linkextractors/sgml.py#L91"], "channel": "scrapy"},
{"date": "2014-05-24T15:00:35.047888+00:00", "nick": "Digenis", "message": "it uses search", "links": [], "channel": "scrapy"},
{"date": "2014-05-24T15:01:02.110443+00:00", "nick": "Digenis", "message": "not match", "links": [], "channel": "scrapy"},
{"date": "2014-05-24T15:02:30.506718+00:00", "nick": "Digenis", "message": "ok, oquidave try this", "links": [], "channel": "scrapy"},
{"date": "2014-05-24T15:03:39.628722+00:00", "nick": "Digenis", "message": "open a scrapy shell", "links": [], "channel": "scrapy"},
{"date": "2014-05-24T15:04:00.783672+00:00", "nick": "Digenis", "message": "import the extractors etc", "links": [], "channel": "scrapy"},
{"date": "2014-05-24T15:04:16.242732+00:00", "nick": "Digenis", "message": "fetch() your page", "links": [], "channel": "scrapy"},
{"date": "2014-05-24T15:04:25.125659+00:00", "nick": "Digenis", "message": "and try out your regex", "links": [], "channel": "scrapy"},
{"date": "2014-05-24T15:12:48.731511+00:00", "nick": "oquidave", "message": "Digenis: the regex matches. I just found out that i had to change my parse function from parse to something else for it to work, if am using the CrawlSpider instead of the BaseSpider!!", "links": [], "channel": "scrapy"},
{"date": "2014-05-24T15:15:50.621841+00:00", "nick": "nyov", "message": "hahaha. gets 'em every time :P", "links": [], "channel": "scrapy"},
{"date": "2014-05-24T15:29:51.438617+00:00", "nick": "Digenis", "message": "I think they are discussing it on github", "links": [], "channel": "scrapy"},
{"date": "2014-05-24T15:29:54.695526+00:00", "nick": "Digenis", "message": "renaming it", "links": [], "channel": "scrapy"},
{"date": "2014-05-24T15:30:21.853882+00:00", "nick": "Digenis", "message": "to deal with subclasses", "links": [], "channel": "scrapy"},
{"date": "2014-05-24T15:31:25.768468+00:00", "nick": "Digenis", "message": "since it's not meant to be called/overriden by subclasses", "links": [], "channel": "scrapy"},
{"date": "2014-05-24T15:31:44.465887+00:00", "nick": "Digenis", "message": "why not mangle it in the first place?", "links": [], "channel": "scrapy"},
{"date": "2014-05-24T15:33:10.769716+00:00", "nick": "nyov", "message": "there is a ticket?", "links": [], "channel": "scrapy"},
{"date": "2014-05-24T15:33:51.785145+00:00", "nick": "nyov", "message": "found it :)", "links": [], "channel": "scrapy"},
{"date": "2014-05-24T16:00:19.634975+00:00", "nick": "NicoW", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2014-05-24T22:30:03.690974+00:00", "nick": "blochchain", "message": "is there a Scrapy method for extracting the domain from a URL?", "links": [], "channel": "scrapy"},
{"date": "2014-05-26T07:46:35.944950+00:00", "nick": "baazzilhassan", "message": "Hi everybody :)", "links": [], "channel": "scrapy"},
{"date": "2014-05-26T07:47:12.827668+00:00", "nick": "baazzilhassan", "message": "I need an example of login to a website and using CookieJar", "links": [], "channel": "scrapy"},
{"date": "2014-05-26T07:48:42.998809+00:00", "nick": "baazzilhassan", "message": "I have many account on a website so I want to connect to each account and scrapping data that I need, so to separate sessions I need to use CookieJar", "links": [], "channel": "scrapy"},
{"date": "2014-05-26T07:49:13.526586+00:00", "nick": "baazzilhassan", "message": "any example that implementing CookieJar can be helpful :)", "links": [], "channel": "scrapy"},
{"date": "2014-05-26T08:05:44.994951+00:00", "nick": "baazzilhassan", "message": "any help please ?", "links": [], "channel": "scrapy"},
{"date": "2014-05-26T10:31:55.390925+00:00", "nick": "Aj91", "message": "good afternoon everyon", "links": [], "channel": "scrapy"},
{"date": "2014-05-26T10:32:59.724818+00:00", "nick": "Aj91", "message": "i want to scrape the ebay website to find out the current selling prices of a item like samsung phones.How do i do that?", "links": [], "channel": "scrapy"},
{"date": "2014-05-26T11:14:35.232679+00:00", "nick": "mailjenil", "message": "Hii", "links": [], "channel": "scrapy"},
{"date": "2014-05-26T11:15:38.499765+00:00", "nick": "mailjenil", "message": "I am new to Scrapy and have few queries", "links": [], "channel": "scrapy"},
{"date": "2014-05-26T12:51:37.866462+00:00", "nick": "lucax_", "message": "hello", "links": [], "channel": "scrapy"},
{"date": "2014-05-26T12:51:56.487115+00:00", "nick": "lucax_", "message": "are there any scrapers here?", "links": [], "channel": "scrapy"},
{"date": "2014-05-26T12:52:01.707457+00:00", "nick": "lucax_", "message": ":)", "links": [], "channel": "scrapy"},
{"date": "2014-05-26T12:53:27.297936+00:00", "nick": "lucax_", "message": "I am new to scrapy and might need some help", "links": [], "channel": "scrapy"},
{"date": "2014-05-26T13:20:53.605572+00:00", "nick": "lucax_", "message": "is there anyone here at all, there is no activity", "links": [], "channel": "scrapy"},
{"date": "2014-05-26T13:25:49.879269+00:00", "nick": "baazzilhassan", "message": "I post a question and no one can answer", "links": [], "channel": "scrapy"},
{"date": "2014-05-26T14:34:07.255060+00:00", "nick": "toothrot", "message": "baazzilhassan, it's in the docs: http://doc.scrapy.org/en/latest/topics/download...", "links": ["http://doc.scrapy.org/en/latest/topics/downloader-middleware.html#std:reqmeta-cookiejar"], "channel": "scrapy"},
{"date": "2014-05-26T14:35:43.954793+00:00", "nick": "baazzilhassan", "message": "yeh thank u so mush :), I make a mistake I use this parameter's cookie_jar insteadof cookiejar", "links": [], "channel": "scrapy"},
{"date": "2014-05-26T14:36:07.884894+00:00", "nick": "baazzilhassan", "message": "it's working now, thank u :)", "links": [], "channel": "scrapy"},
{"date": "2014-05-26T14:36:52.859719+00:00", "nick": "toothrot", "message": "are you just using the identifier?", "links": [], "channel": "scrapy"},
{"date": "2014-05-26T14:37:38.961836+00:00", "nick": "baazzilhassan", "message": "yeh and I pass cookiejar to every sub request", "links": [], "channel": "scrapy"},
{"date": "2014-05-26T15:03:46.719978+00:00", "nick": "xenol", "message": "Hi, I am using scrapy to scrape some htmL tables. Everything works fine, util I have <td></td> and this is not caught by xpath", "links": [], "channel": "scrapy"},
{"date": "2014-05-26T15:03:52.991441+00:00", "nick": "xenol", "message": "what should be done?", "links": [], "channel": "scrapy"},
{"date": "2014-05-26T15:04:51.939106+00:00", "nick": "samtc", "message": "not caught?", "links": [], "channel": "scrapy"},
{"date": "2014-05-26T15:06:38.478562+00:00", "nick": "xenol", "message": "err, it's not detected. len(row.xpath(..)) will print different number as expected", "links": [], "channel": "scrapy"},
{"date": "2014-05-26T15:06:51.017387+00:00", "nick": "xenol", "message": "I traced it to <td></td> html tags", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T09:04:31.088431+00:00", "nick": "baazzilhassan", "message": "Hello", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T09:04:51.284400+00:00", "nick": "baazzilhassan", "message": "How can I order fields in the CSV export ?", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T09:07:33.292419+00:00", "nick": "baazzilhassan", "message": "Where to put this: http://doc.scrapy.org/en/latest/topics/exporter...", "links": ["http://doc.scrapy.org/en/latest/topics/exporters.html#scrapy.contrib.exporter.BaseItemExporter.fields_to_export"], "channel": "scrapy"},
{"date": "2014-05-27T09:07:39.899713+00:00", "nick": "baazzilhassan", "message": "in settings.py ?", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T09:45:47.101756+00:00", "nick": "baazzilhassan", "message": "I get it :)", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T09:46:19.741571+00:00", "nick": "baazzilhassan", "message": "many thank's for scrapy community :)", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T11:53:06.547051+00:00", "nick": "koell", "message": "scrapy :3", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T20:06:44.046982+00:00", "nick": "PrincessPea", "message": "Howdy, anyone worked with the SitemapSpider and created rules to skip certain links? It doesn't seem that SgmlLinkExtractor rules work with the SitemapSpider", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T20:08:08.320438+00:00", "nick": "nyov", "message": "yea, that one could use a few improvements", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T20:12:05.026385+00:00", "nick": "PrincessPea", "message": "Yeah, I'm hoping there's something built in I can use, but so far I haven't found anything - I'm wondering if I'll just have to add a flag to skip certain links before adding to items", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T20:17:19.645031+00:00", "nick": "nyov", "message": "well, the only \"rules\" you can currently create to match/ignore url's are regex", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T20:18:12.607659+00:00", "nick": "nyov", "message": "but that's basically what SgmlLinkExtractor does as well? is matching urls with regex not enough?", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T20:25:19.263525+00:00", "nick": "nyov", "message": "I suppose it could be easier to first match a set of URLs and in a second step filter that set again, if it were possible", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T20:26:01.731373+00:00", "nick": "nyov", "message": "but you can use multiple, more stricter, regex rules with the same callback if that helps", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T20:26:02.928335+00:00", "nick": "PrincessPea", "message": "Yeah, that would be ideal", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T20:26:37.642398+00:00", "nick": "PrincessPea", "message": "The ones I want to skip are nested", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T20:28:13.079522+00:00", "nick": "nyov", "message": "sorry? what do you mean by nested?", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T20:29:42.557693+00:00", "nick": "PrincessPea", "message": "so i want to scrape /admissions page but deny /admissions/transfer_credits/someotherpage", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T20:29:58.960904+00:00", "nick": "PrincessPea", "message": "the /admissions is in the sitemap", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T20:33:00.826887+00:00", "nick": "nyov", "message": "ah, okay", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T20:33:35.328992+00:00", "nick": "nyov", "message": "sitemapspider is using re.search to match URLs. https://github.com/scrapy/scrapy/blob/master/sc...", "links": ["https://github.com/scrapy/scrapy/blob/master/scrapy/contrib/spiders/sitemap.py#L47"], "channel": "scrapy"},
{"date": "2014-05-27T20:33:48.016180+00:00", "nick": "nyov", "message": "so you can use all the rules for it, https://docs.python.org/2/library/re.html#re.se...", "links": ["https://docs.python.org/2/library/re.html#re.search"], "channel": "scrapy"},
{"date": "2014-05-27T20:34:12.483460+00:00", "nick": "nyov", "message": "or just test in ipython. A rule for your case could be like this:", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T20:34:46.239292+00:00", "nick": "nyov", "message": ">>> import re", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T20:35:01.061371+00:00", "nick": "nyov", "message": ">>> re.search(r'/admissions/', '/admissions/transfer_credits/someotherpage')", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T20:35:02.402337+00:00", "nick": "nyov", "message": "<_sre.SRE_Match at 0x7fc3f7857920>", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T20:35:07.111291+00:00", "nick": "nyov", "message": "this would match", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T20:35:18.479653+00:00", "nick": "nyov", "message": ">>> re.search(r'/admissions/$', '/admissions/transfer_credits/someotherpage')", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T20:35:23.763491+00:00", "nick": "nyov", "message": "this would not", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T20:36:46.277345+00:00", "nick": "nyov", "message": "maybe better to make the last slash optional. re.search(r'/admissions/?$', '/admissions')", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T20:37:54.061351+00:00", "nick": "nyov", "message": "and then, if you need another special case under admissions, like \"/admissions/logout\", you can add another rule that matches this one explicitly", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T20:38:32.476873+00:00", "nick": "PrincessPea", "message": "Awesome! thanks I haven't ever used re lib very cool", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T20:41:49.885453+00:00", "nick": "nyov", "message": "regex can be quite complex, it probably helps to have a online regex tester at first. test if things work as expected", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T20:44:08.721635+00:00", "nick": "nyov", "message": "this looks good: https://pythex.org/", "links": ["https://pythex.org/"], "channel": "scrapy"},
{"date": "2014-05-27T20:44:17.709266+00:00", "nick": "PrincessPea", "message": "Yes agreed, I only know the basics but usually at work we use libraries for email and password regex validation so I haven't had the opportunity to learn much beyond the basics. Thanks again!", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T20:46:39.665360+00:00", "nick": "nyov", "message": "okay, but that helps :)", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T20:47:23.740501+00:00", "nick": "nyov", "message": "nice page indeed, glad I found that. https://pythex.org/?regex=%2Fadmissions%2F%3F%2...", "links": ["https://pythex.org/?regex=%2Fadmissions%2F%3F%28.%2A%29%24&amp=&test_string=http%3A%2F%2Fdomain.com%2Fadmissions%2Fsomething&amp=&ignorecase=0&amp=&multiline=0&amp=&dotall=0&amp=&verbose=0"], "channel": "scrapy"},
{"date": "2014-05-27T20:50:07.507785+00:00", "nick": "PrincessPea", "message": "Yeah, that is nice most that I've seen just return a boolean", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T20:51:38.640585+00:00", "nick": "nyov", "message": "in this case, it doesn't matter what is returned (for SitemapSpider). if part of the url matches (is green in that pythex site), it will be downloaded", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T20:52:38.361520+00:00", "nick": "nyov", "message": "so capture groups and all the complex things are not necessary", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T20:59:20.416118+00:00", "nick": "nyov", "message": "oh, one thing very good to know in python regex is using raw strings (r''), so you don't need to backslash-escape every special character", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T21:17:58.464589+00:00", "nick": "HowardwLo", "message": "howdy all!", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T21:18:15.839371+00:00", "nick": "HowardwLo", "message": "I\u2019m trying to use xpath to recognize unicode character in element", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T21:18:20.471274+00:00", "nick": "HowardwLo", "message": "xpath('//div[@class=\"pagination\"]/a...", "links": ["mailto:xpath('//div[@class=\"pagination\"]/a[text()=\"\\xbb\"]').extract()"], "channel": "scrapy"},
{"date": "2014-05-27T21:18:52.264147+00:00", "nick": "HowardwLo", "message": "that works in google chrome console, but in python, it tells me :", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T21:18:53.505386+00:00", "nick": "HowardwLo", "message": "exceptions.ValueError: All strings must be XML compatible: Unicode or ASCII, no NULL bytes or control characters", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T21:20:37.515470+00:00", "nick": "HowardwLo", "message": "i\u2019m not usre how to express \\xbb correctly", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T21:20:50.922908+00:00", "nick": "HowardwLo", "message": "i\u2019ve tried u\u201d\\xbb\u201d", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T21:21:04.695557+00:00", "nick": "nyov", "message": "HowardwLo: try using unicode string. xpath(u'//div[@class=\"pagination\"]/...", "links": ["mailto:xpath(u'//div[@class=\"pagination\"]/a[text()=\"\\xbb\"]').extract()"], "channel": "scrapy"},
{"date": "2014-05-27T21:21:07.738409+00:00", "nick": "baazzilhassan3", "message": "use this", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T21:21:28.231551+00:00", "nick": "baazzilhassan3", "message": "xpath(u'//div[@class=\"pagination\"]/a[text()=\"\u00e9\u00e0\u00e0\u00e7\u00e7\u00e7\u00e7\"]').extract()", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T21:21:46.515868+00:00", "nick": "baazzilhassan3", "message": "remove the code Hexa for the caracter", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T21:22:56.830013+00:00", "nick": "baazzilhassan3", "message": "dont forget to add this to the python file", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T21:22:57.206318+00:00", "nick": "baazzilhassan3", "message": "# -*- coding: utf-8 -*-", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T21:23:28.462008+00:00", "nick": "baazzilhassan3", "message": "it's must be at the begenning of the file", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T21:33:40.326778+00:00", "nick": "HowardwLo", "message": "nyov: that worked! thanks", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T21:33:47.528695+00:00", "nick": "HowardwLo", "message": "baazzilhassan3: thanks!", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T21:34:59.636615+00:00", "nick": "baazzilhassan3", "message": "welcome", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T21:50:05.675773+00:00", "nick": "nyov", "message": "gah, if I'd known phantomjs re-packaging could be so much work, I wouldn't have started :P", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T21:50:59.573534+00:00", "nick": "nyov", "message": "oh well. learning something about qtwebkit internals", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T21:51:28.283093+00:00", "nick": "baazzilhassan3", "message": "what do u prefer, phantomjs or scrapy ?", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T21:51:45.463079+00:00", "nick": "pablohof", "message": "scrapy :)", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T21:52:38.106998+00:00", "nick": "nyov", "message": "haha, definitely scrapy. the vision here, though, is adding a DOM for scrapy to look into", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T21:53:50.609986+00:00", "nick": "baazzilhassan3", "message": "Good me too is my favorite framework for scrapping", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T21:54:44.950485+00:00", "nick": "baazzilhassan3", "message": "but in my company I have some collegue mate that scrap with cURL and phantomjs", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T21:56:08.727342+00:00", "nick": "nyov", "message": "well, rendering the whole f**kin webpage just to get some text is usually overkill, and I dislike. also, more ressource wastage per page", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T21:57:51.190178+00:00", "nick": "nyov", "message": "cURL though, I don't see an issue with. if it's enough for the job. but thats basically just the downloader part in scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T22:02:22.419701+00:00", "nick": "baazzilhassan3", "message": "yehhh, u are right, there is no concept of Item and pipelines", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T22:03:32.669499+00:00", "nick": "nyov", "message": "or even document parsing, if I'm not mistaken :p", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T22:06:17.826685+00:00", "nick": "baazzilhassan3", "message": "yeh", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T22:06:37.411576+00:00", "nick": "baazzilhassan3", "message": "what's your last work on scrapping data ?", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T22:21:15.529976+00:00", "nick": "nyov", "message": "i'm not scrapping any data", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T22:21:33.551955+00:00", "nick": "nyov", "message": "i'd rather buy more storage than scrap anything i have", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T22:22:42.821513+00:00", "nick": "nyov", "message": "take what you can, and give nothing back. we are pieeee rats. YARRRR", "links": [], "channel": "scrapy"},
{"date": "2014-05-27T22:23:29.600940+00:00", "nick": "nyov", "message": "whoops", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T12:16:01.566352+00:00", "nick": "alexenko", "message": "anyone around?", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T12:19:31.046158+00:00", "nick": "nikolaosk", "message": "alexenko: shoot your question and have patience", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T12:19:44.068928+00:00", "nick": "nikolaosk", "message": "until someone that can answer sees it", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T12:24:10.843868+00:00", "nick": "alexenko", "message": "cool. Trying to figure out the proper way to structure code in my project. Parsing this json file in scrapy, it has two parts, universities and courses. Got courses parsed and images processed in their respective pipelines. But now i'm trying to parse universities as well. There is an image files that needs to be downloaded for each university as well as some basic info. But they are technically different", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T12:24:12.746509+00:00", "nick": "alexenko", "message": "items and only one spider out of 10 need to do this", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T12:25:39.002341+00:00", "nick": "alexenko", "message": "right now I just dump the university info into a dict and pass that to the pipeline for further processing at which point the items get created, etc, but I don't download images because they should go under a different S3 store. Got a second image pipeline, but now i don't know how to tie it all together", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T12:28:32.608331+00:00", "nick": "alexenko", "message": "I think i should be making a separate item and then check in the pipeline what isinstance() it's from and not process it unless it's UniversityItem, but I'm not sure that's the right way to do this", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T12:30:52.776021+00:00", "nick": "nikolaosk", "message": "is this a datatype issue", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T12:31:06.532439+00:00", "nick": "nikolaosk", "message": "checking the item's __class__", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T12:31:09.659772+00:00", "nick": "nikolaosk", "message": "?", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T12:32:55.524261+00:00", "nick": "alexenko", "message": "how do you skip processing an item in the pipeline unless it's a specific class", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T12:35:17.316358+00:00", "nick": "nikolaosk", "message": "if not isinstance(item, desired_class): return item", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T12:35:30.160150+00:00", "nick": "nikolaosk", "message": "and then you write your processing code", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T12:37:42.557998+00:00", "nick": "alexenko", "message": "how do i do that in the imagepipeline subclass?", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T12:38:20.346696+00:00", "nick": "alexenko", "message": "process_item() returns an item regardless, so it wouldn't actually skip it", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T12:39:04.369003+00:00", "nick": "alexenko", "message": "or do i just use a different item field that isn't used elsewhere and since there's nothing to parse it just skips", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T12:51:29.749098+00:00", "nick": "alexenko", "message": "would this work? https://gist.github.com/alexenko/4562c97d9dd1a4...", "links": ["https://gist.github.com/alexenko/4562c97d9dd1a448f31b"], "channel": "scrapy"},
{"date": "2014-05-28T13:05:16.695146+00:00", "nick": "nikolaosk", "message": "I don't know what process_item() of the superclass does", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T13:05:21.930284+00:00", "nick": "nikolaosk", "message": "but it should return an item", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T13:05:52.014346+00:00", "nick": "nikolaosk", "message": "process_item should always return item", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T13:06:18.541438+00:00", "nick": "nikolaosk", "message": "if you want to skip it raise DropItem", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T13:06:37.483569+00:00", "nick": "nikolaosk", "message": "by skip I mean drop it completely from the rest of the processing chain", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T13:07:54.909764+00:00", "nick": "nikolaosk", "message": "process item receives an item, maybe modifies it here and there, then returns it for processing by the next one", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T13:08:08.124322+00:00", "nick": "nikolaosk", "message": "you can have side effects like database I/O", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T13:08:46.877088+00:00", "nick": "nikolaosk", "message": "but if you want to pass it as it is to the next pipeline class, just return it without modifing it", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T14:34:16.643503+00:00", "nick": "alexenko", "message": "turns out i was overcomplicating it, in the subclassed ImagesPipeline I just skip items without the uni image_url key", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T14:34:35.066077+00:00", "nick": "alexenko", "message": "ty for help nikolaosk", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:25:50.966254+00:00", "nick": "nyov", "message": "woah, found a REALLY great regex tester http://regex101.com/", "links": ["http://regex101.com/"], "channel": "scrapy"},
{"date": "2014-05-28T18:30:14.282597+00:00", "nick": "nikolaosk", "message": "a colleague of mine uses it", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:30:37.707425+00:00", "nick": "nikolaosk", "message": "I like the color highlighing of groups", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:31:41.958209+00:00", "nick": "nikolaosk", "message": "but I think opening another tmux window with an ipython shell there does the trick without a browser", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:32:02.729277+00:00", "nick": "nyov", "message": "much better than what I had yesterday. elso great explanation doing. and not doing server-side lookups, thats nice", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:32:20.797090+00:00", "nick": "nikolaosk", "message": "and you also have perlconsole for perl, not a proper REPL but does the job for regexes", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:32:30.551553+00:00", "nick": "nyov", "message": "well, actually I was looking for a git diff regex, not python", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:34:16.656842+00:00", "nick": "Robin742", "message": "Hi All.", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:34:25.463861+00:00", "nick": "nyov", "message": "git diff -G does not support negative lookahead, but I want to ignore all Copyright changes in a big file set", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:34:35.789862+00:00", "nick": "nyov", "message": "so I needed this fugly ucker:", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:34:40.662915+00:00", "nick": "nyov", "message": "^([^#:*]|[#:*][^#:*]|[#:*]{2}[^ ]|[#:*]{2} [^C]|[#:*]{2} C[^o]|[#:*]{2} Co[^p]|[#:*]{2} Cop[^y]|[#:*]{2} Copy[^r]|[#:*]{2} Copyr[^i]|[#:*]{2} Copyri[^g]|[#:*]{2} Copyrig[^h]|[#:*]{2} Copyrigh[^t]|[#:*]{2} Copyright[^ ][#:*]{2} Copyright [^\\(]|[#:*]{2} Copyright \\([^C]|[#:*]{2} Copyright \\(C[^\\)]|[#:*]{2} Copyright \\(C\\)[^ ])", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:34:45.776544+00:00", "nick": "nyov", "message": "lol", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:35:19.866077+00:00", "nick": "nyov", "message": "the colorizing on the page really helped", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:35:34.877960+00:00", "nick": "Robin742", "message": "In my spider I am using sel.xpath('') to fetch the portions of the page from where I need to scrape the data", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:35:39.048909+00:00", "nick": "nikolaosk", "message": "omg", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:36:46.235320+00:00", "nick": "Robin742", "message": "xpath has been tested in chrome browser and returns a set of div elements. however when i run the Scrapy I get a empty list.", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:36:59.840698+00:00", "nick": "Robin742", "message": "what could be the possible reason?", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:37:33.560765+00:00", "nick": "nikolaosk", "message": "chrome modifing the document", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:37:50.778628+00:00", "nick": "nikolaosk", "message": "because javascript on the page says so", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:38:23.774136+00:00", "nick": "Robin742", "message": "what to do in such scenario", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:38:51.504952+00:00", "nick": "Robin742", "message": "just to elaborate. I am trying to scrape news data from this page :http://money.rediff.com/news?src=all_pg", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:39:06.168006+00:00", "nick": "nikolaosk", "message": "or chrome taking the initiative to *fix* the html to conform with some lousy standard", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:39:29.870944+00:00", "nick": "nikolaosk", "message": "typical case: table/tr/td   ->   table/tbody/tr/td", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:39:33.490169+00:00", "nick": "Robin742", "message": "and I am using this line to extract the div portions", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:39:38.960887+00:00", "nick": "Robin742", "message": "sites = sel.xpath('//div[@class=\"rtnews_right_more\"]')", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:39:51.474702+00:00", "nick": "nikolaosk", "message": "and this returns nothing", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:39:53.022190+00:00", "nick": "nikolaosk", "message": "right?", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:40:06.741269+00:00", "nick": "Robin742", "message": "yes it returns nothing.", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:40:22.484913+00:00", "nick": "Robin742", "message": "however in chrome it returns a valid list of div's", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:40:40.910508+00:00", "nick": "nikolaosk", "message": "probably javascript", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:40:57.957353+00:00", "nick": "nikolaosk", "message": "try this", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:41:06.829451+00:00", "nick": "nikolaosk", "message": "run scrapy shell", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:41:21.589607+00:00", "nick": "nikolaosk", "message": "fetch('your url')", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:41:36.869207+00:00", "nick": "nikolaosk", "message": "and play around with the sel object it will define for you", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:41:46.286125+00:00", "nick": "nyov", "message": "or view the page with javascript disabled", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:41:50.287259+00:00", "nick": "nikolaosk", "message": "until you get it right", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:41:54.486143+00:00", "nick": "nikolaosk", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:42:13.289031+00:00", "nick": "nikolaosk", "message": "maybe view source shows the pure source", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:43:31.557665+00:00", "nick": "Robin742", "message": "I am sorry. I am quite new to this. How do i run scrapy shell?", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:43:53.557409+00:00", "nick": "nikolaosk", "message": "just scrapy shell", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:44:00.012915+00:00", "nick": "nikolaosk", "message": "like scrapy crawl", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:44:13.963154+00:00", "nick": "nyov", "message": "usually from a unix terminal. not sure in windows?", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:44:17.019505+00:00", "nick": "nikolaosk", "message": "if you do it in your project dir you can use your modules", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:44:36.567139+00:00", "nick": "Robin742", "message": "fine i am into shell", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:44:36.690522+00:00", "nick": "nikolaosk", "message": "oh,  did you say windows?", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:44:54.202073+00:00", "nick": "Robin742", "message": "I am on a mac", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:44:58.860249+00:00", "nick": "nikolaosk", "message": "ok", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:45:06.642232+00:00", "nick": "nikolaosk", "message": "at least it has a shell", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:45:08.953995+00:00", "nick": "nikolaosk", "message": "and vim", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:45:14.322593+00:00", "nick": "nikolaosk", "message": "and other stuff", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:45:22.227454+00:00", "nick": "Robin742", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:45:36.718616+00:00", "nick": "nikolaosk", "message": "not type", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:45:45.358881+00:00", "nick": "nikolaosk", "message": "no, call is the right word", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:45:46.513616+00:00", "nick": "nikolaosk", "message": "call", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:45:55.669525+00:00", "nick": "nikolaosk", "message": "fetch('your url')", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:46:04.323805+00:00", "nick": "nikolaosk", "message": "and this will set a global response object", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:46:08.284860+00:00", "nick": "nikolaosk", "message": "and a sel object", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:46:24.282601+00:00", "nick": "nikolaosk", "message": "I mean Selector object", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:48:15.152830+00:00", "nick": "Robin742", "message": "done", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:48:39.233479+00:00", "nick": "Robin742", "message": "it says \"sel   <Selector xpath=None data=u'<html xmlns=\"http://www.w3.org/1999/xhtm'>...;", "links": ["http://xmlns=\"http://www.w3.org/1999/xhtm'>\""], "channel": "scrapy"},
{"date": "2014-05-28T18:49:13.433064+00:00", "nick": "nikolaosk", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:49:26.950861+00:00", "nick": "nikolaosk", "message": "no try your xpath expression in the interactive shell", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:49:39.079961+00:00", "nick": "nikolaosk", "message": "maybe use somehting more generic first", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:49:47.150362+00:00", "nick": "nikolaosk", "message": "until you get what you want", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:49:57.241382+00:00", "nick": "Robin742", "message": "ok", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:50:34.624885+00:00", "nick": "nikolaosk", "message": "wait a sec", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:50:38.365408+00:00", "nick": "nikolaosk", "message": "what is BSE?", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:50:40.207662+00:00", "nick": "nikolaosk", "message": "XBSE?", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:50:48.030449+00:00", "nick": "nikolaosk", "message": "the bucharest stock xchange?", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:51:10.083799+00:00", "nick": "nyov", "message": "?", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:51:26.853424+00:00", "nick": "nikolaosk", "message": "this BSE news it says on your link Robin742", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:52:09.140272+00:00", "nick": "nikolaosk", "message": "no, it's not", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:52:51.898685+00:00", "nick": "Robin742", "message": "no", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:53:16.099779+00:00", "nick": "nikolaosk", "message": "Robin742: try //div[@id=\"marketNews\"]", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:53:40.346714+00:00", "nick": "nikolaosk", "message": "and I think your class name is rtnews_row_more", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:53:52.130729+00:00", "nick": "nikolaosk", "message": "not \"row\" not \"right\"", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:54:26.042331+00:00", "nick": "nikolaosk", "message": "I don't know", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:54:37.162399+00:00", "nick": "nikolaosk", "message": "gotta go, need sleep", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:55:01.366148+00:00", "nick": "Robin742", "message": "row more contains the pic as well which I dont want.", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:55:16.346209+00:00", "nick": "nikolaosk", "message": "ping my other nickname \"Digenis\"", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:55:29.289961+00:00", "nick": "nikolaosk", "message": "although I may not have to to see this even then", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T18:55:36.516462+00:00", "nick": "Robin742", "message": "\"right\" has the only text which i require", "links": [], "channel": "scrapy"},
{"date": "2014-05-28T19:26:59.800908+00:00", "nick": "Robin742", "message": "i disabled the javascript and then took the new xpath, however it still doesn't work", "links": [], "channel": "scrapy"},
{"date": "2014-05-29T08:42:40.936897+00:00", "nick": "Plastio", "message": "Hi !", "links": [], "channel": "scrapy"},
{"date": "2014-05-29T12:18:03.119036+00:00", "nick": "Moki", "message": "Hello,", "links": [], "channel": "scrapy"},
{"date": "2014-05-29T12:19:03.728115+00:00", "nick": "Moki", "message": "could someone explain how to split output xml files based on number of records. for example 1000 per file .", "links": [], "channel": "scrapy"},
{"date": "2014-05-29T12:23:23.725912+00:00", "nick": "Moki", "message": "While crawling whole site, scrapy creates single output file which contains all records. Instead would it be possible to split the files such that each output xml file contains only 1000 records.", "links": [], "channel": "scrapy"},
{"date": "2014-05-29T12:34:14.461842+00:00", "nick": "Moki", "message": "could someone explain how to split output xml files based on number of records. for example 1000 per file .", "links": [], "channel": "scrapy"},
{"date": "2014-05-29T12:34:24.712932+00:00", "nick": "Moki", "message": "While crawling whole site, scrapy creates single output file which contains all records. Instead would it be possible to split the files such that each output xml file contains only 1000 records.", "links": [], "channel": "scrapy"},
{"date": "2014-05-29T12:38:51.569977+00:00", "nick": "Moki", "message": "could someone explain how to split output xml files based on number of records. for example 1000 per file .", "links": [], "channel": "scrapy"},
{"date": "2014-05-29T12:38:59.571497+00:00", "nick": "Moki", "message": "While crawling whole site, scrapy creates single output file which contains all records. Instead would it be possible to split the files such that each output xml file contains only 1000 records.", "links": [], "channel": "scrapy"},
{"date": "2014-05-29T13:36:29.964255+00:00", "nick": "umair", "message": "Moki: I think you would require custom feed exporter AFAIK", "links": [], "channel": "scrapy"},
{"date": "2014-05-29T13:37:22.549537+00:00", "nick": "nikolaosk", "message": "Moki: you can't just split xml", "links": [], "channel": "scrapy"},
{"date": "2014-05-29T13:37:52.844129+00:00", "nick": "nikolaosk", "message": "it may become invalid", "links": [], "channel": "scrapy"},
{"date": "2014-05-29T13:38:26.915495+00:00", "nick": "nikolaosk", "message": "no, it _will_ become", "links": [], "channel": "scrapy"},
{"date": "2014-05-29T13:39:19.770458+00:00", "nick": "nikolaosk", "message": "oh, I see you are talking about records, my bad", "links": [], "channel": "scrapy"},
{"date": "2014-05-29T13:42:18.306241+00:00", "nick": "nikolaosk", "message": "Moki: https://github.com/scrapy/scrapy/blob/master/sc...", "links": ["https://github.com/scrapy/scrapy/blob/master/scrapy/contrib/exporter/__init__.py#L116-L160"], "channel": "scrapy"},
{"date": "2014-05-29T13:42:21.807874+00:00", "nick": "nikolaosk", "message": "read this", "links": [], "channel": "scrapy"},
{"date": "2014-05-29T13:43:21.943678+00:00", "nick": "nikolaosk", "message": "I think you will need to move almost all the logic and state under export_item", "links": [], "channel": "scrapy"},
{"date": "2014-05-29T13:43:58.407265+00:00", "nick": "nikolaosk", "message": "when I say under I mean from a call stack perspective", "links": [], "channel": "scrapy"},
{"date": "2014-05-29T13:44:12.167588+00:00", "nick": "nikolaosk", "message": "not just write all the code in one method", "links": [], "channel": "scrapy"},
{"date": "2014-05-29T13:44:20.958341+00:00", "nick": "nikolaosk", "message": "writting*", "links": [], "channel": "scrapy"},
{"date": "2014-05-29T14:05:44.379119+00:00", "nick": "Moki", "message": "Thanks nikolaosk for information", "links": [], "channel": "scrapy"},
{"date": "2014-05-29T14:06:14.307747+00:00", "nick": "Moki", "message": "I am now with python, would you provide me exact change of code which I need to use for this custom requirement ?", "links": [], "channel": "scrapy"},
{"date": "2014-05-29T14:08:49.498881+00:00", "nick": "nikolaosk", "message": "s/now/new/ ?", "links": [], "channel": "scrapy"},
{"date": "2014-05-29T14:09:35.427724+00:00", "nick": "nikolaosk", "message": "I can't, I am too busy to write/test stuff right now", "links": [], "channel": "scrapy"},
{"date": "2014-05-29T16:58:02.647921+00:00", "nick": "nixfreak", "message": "question I want to create a web crawler that will search specific search engines based on keywords, is this possible with scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-05-29T16:58:54.207933+00:00", "nick": "nixfreak", "message": "after I find the sites based on keywords then i will isolate them to scrape", "links": [], "channel": "scrapy"},
{"date": "2014-05-29T16:58:57.130086+00:00", "nick": "scrapy_support", "message": "It's possible, but u can look for APIs of this search engine if exist", "links": [], "channel": "scrapy"},
{"date": "2014-05-29T16:59:40.400782+00:00", "nick": "nixfreak", "message": "I'm looking to search multile search engines at once", "links": [], "channel": "scrapy"},
{"date": "2014-05-29T16:59:52.481309+00:00", "nick": "nixfreak", "message": "multile = multiple", "links": [], "channel": "scrapy"},
{"date": "2014-05-29T17:00:46.154547+00:00", "nick": "nixfreak", "message": "If scrapy isn't going to work out could you point me in right direction how I am implement this?", "links": [], "channel": "scrapy"},
{"date": "2014-05-29T17:02:23.999002+00:00", "nick": "scrapy_support", "message": "scrapy is the right framework who can do this perfectly", "links": [], "channel": "scrapy"},
{"date": "2014-05-29T17:04:19.364697+00:00", "nick": "nixfreak", "message": "ok great so if it is possible to write a crawler using scrapy to search multiple search engines would I have to figure out every API based on that search engine?", "links": [], "channel": "scrapy"},
{"date": "2014-05-30T23:26:56.328683+00:00", "nick": "ncavet", "message": "Hi all...I'm looking to hire a scrapy developer for a project I'm working on. I'd love to know if you know of anyone who'd be interested.", "links": [], "channel": "scrapy"},
{"date": "2014-05-31T01:19:34.395516+00:00", "nick": "Pajamas", "message": "can't run scrapy..", "links": [], "channel": "scrapy"},
{"date": "2014-05-31T01:19:46.100754+00:00", "nick": "Pajamas", "message": "AttributeError: 'module' object has no attribute 'PY2'", "links": [], "channel": "scrapy"},
{"date": "2014-05-31T02:06:20.687835+00:00", "nick": "Pajamas", "message": "got it", "links": [], "channel": "scrapy"},
{"date": "2014-05-31T03:03:05.845682+00:00", "nick": "Digenis", "message": "lol, I was about to ask if you were tryong this in javascript", "links": [], "channel": "scrapy"},
{"date": "2014-05-31T03:03:24.359414+00:00", "nick": "Digenis", "message": "you know, because of you nick, and pyjamas", "links": [], "channel": "scrapy"},
{"date": "2014-05-31T05:23:16.704083+00:00", "nick": "dpn`", "message": "ahoyhoy, any of the SH crew still awake?", "links": [], "channel": "scrapy"},
{"date": "2014-06-02T02:37:16.637693+00:00", "nick": "cornjuliox", "message": "hey all i'm trying to scrape this site right here: https://login.employmentplus.com/Avionte/Portal... but I'm having a bit of trouble getting my spider to follow the 'view detail' link", "links": ["https://login.employmentplus.com/Avionte/Portals/JobBoard/JobSearch.aspx?CompanyID=EmploymentPlus"], "channel": "scrapy"},
{"date": "2014-06-02T02:38:27.676993+00:00", "nick": "cornjuliox", "message": "i take a look at the site in chrome, and I noticed two requests appear to be sent when I click the link - a POST request that has no response and a GET to a completely different URL", "links": [], "channel": "scrapy"},
{"date": "2014-06-02T02:38:51.623881+00:00", "nick": "cornjuliox", "message": "its the GET request that has the data for the actual detail page but I can't for the life of me figure out how it was generated", "links": [], "channel": "scrapy"},
{"date": "2014-06-02T02:39:16.740698+00:00", "nick": "cornjuliox", "message": "coudl anyone lend a hand?", "links": [], "channel": "scrapy"},
{"date": "2014-06-03T19:41:57.278323+00:00", "nick": "Digenis", "message": "any thoughts on #741? the new item loader filtering behavior", "links": [], "channel": "scrapy"},
{"date": "2014-06-03T19:56:32.738184+00:00", "nick": "Digenis", "message": "it does break spiders of mine", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T01:36:40.148473+00:00", "nick": "`brain", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T01:37:30.125789+00:00", "nick": "`brain", "message": "Do scrapy shell sessions automatically handle cookies?  If so, how do I inspect cookies contents while in shell?", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T01:38:58.544320+00:00", "nick": "`brain", "message": "When examining my target site with firefox, I can see: Cookie:ASP.NET_SessionId=cw20isvdk3alihbyh3ha0bc0", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T01:39:53.008084+00:00", "nick": "`brain", "message": "but when I scrapy shell <targetURL> and print the response headers, I don't see the cookie", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T02:39:47.292738+00:00", "nick": "nyov", "message": "`brain: I don't think you'll find them in response.headers. can you try response.meta?", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T03:48:34.296108+00:00", "nick": "KojitBeta", "message": "i'm getting a scrapy error, 'No module named pyopenssl', But i have installed pyopenssl 0.12", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T03:49:35.227996+00:00", "nick": "KojitBeta", "message": "have i done something wrong?", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:20:33.399217+00:00", "nick": "`brain", "message": "very simply, am trying to POST what seems like a simple two-parameter request", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:21:10.701089+00:00", "nick": "`brain", "message": "I've read a lot on stackexchange, debugging with firefox I think I have determined what needs to be done, but am running into this 400 error", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:21:12.488466+00:00", "nick": "nikolaosk", "message": "then why are you using request.replace?", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:21:33.253463+00:00", "nick": "`brain", "message": "OK - your question makes me wonder if I have started off wrong :)", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:21:56.800847+00:00", "nick": "`brain", "message": "I do : scrapy shell <siteurl>", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:21:57.264137+00:00", "nick": "nikolaosk", "message": "not necessarily", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:22:31.472378+00:00", "nick": "nikolaosk", "message": "and then you select fields to use for the request body I guess", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:22:51.730700+00:00", "nick": "`brain", "message": "from there, I do : request = request.replace(method='POST', url=<URLofServiceToPOST>)", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:23:20.950801+00:00", "nick": "`brain", "message": "then I construct the header via : request = request.replace(headers={", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:24:30.390801+00:00", "nick": "`brain", "message": "the values of which I copied from firefox, with the exception of the cookie value, that I (just) got from response.headers", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:25:04.698361+00:00", "nick": "`brain", "message": "then, to pass POST params, I do this (which might be wrong!!)", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:25:07.528473+00:00", "nick": "`brain", "message": "payload = {\"eventSessionID\":116073,\"eventSessionWebSiteSetupViewID\":191}", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:25:14.572429+00:00", "nick": "`brain", "message": "request = request.replace(body=json.dumps(payload))", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:25:26.832265+00:00", "nick": "`brain", "message": "(which I \"learned\" from stackexchage)", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:25:38.555460+00:00", "nick": "`brain", "message": "finally, I do : fetch(request)", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:26:43.529101+00:00", "nick": "`brain", "message": "I'm not leaving out the particulars due to confidentiality, but rather because I want to understand where I'm going wrong", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:28:11.616940+00:00", "nick": "`brain", "message": "is my technique wrong?", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:32:35.135251+00:00", "nick": "nikolaosk", "message": "I see", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:32:50.799774+00:00", "nick": "nikolaosk", "message": "the site wants a json body", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:33:00.978448+00:00", "nick": "`brain", "message": "I'm not sure of that", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:33:29.898264+00:00", "nick": "`brain", "message": "I have inferred from what I have read on SE, and from seeing this in debug in browser: 'Accept': 'application/json, text/javascript, */*; q=0.01'", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:34:46.280470+00:00", "nick": "nikolaosk", "message": "yes, if I was an http server I would say it that way", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:35:16.237347+00:00", "nick": "nikolaosk", "message": "I but I got lost", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:35:21.558621+00:00", "nick": "`brain", "message": "the request payload in chrome dev tools -> XHR -> headers looks like this: {\"eventSessionID\":116114,\"eventSessionWebSiteSetupViewID\":191}", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:36:06.441153+00:00", "nick": "`brain", "message": "is it OK to post the URL of target site in here?", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:36:14.687168+00:00", "nick": "`brain", "message": "it is a safe, public site", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:36:14.846107+00:00", "nick": "nikolaosk", "message": "`brain: did you make sure you change those numbers in session id cookies?", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:36:19.109167+00:00", "nick": "nikolaosk", "message": "don't answer", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:36:21.884990+00:00", "nick": "nikolaosk", "message": "do make sure", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:36:33.540353+00:00", "nick": "nikolaosk", "message": "if not log out and log in from the site in question", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:36:49.876426+00:00", "nick": "nikolaosk", "message": "I mean change before posting here", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:36:56.716211+00:00", "nick": "nikolaosk", "message": "and anywere", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:37:02.635978+00:00", "nick": "nikolaosk", "message": "anywhere, sorry", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:37:35.495915+00:00", "nick": "`brain", "message": "well, I inspected the value by doing : print response.headers", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:38:00.132557+00:00", "nick": "`brain", "message": "(I'm not logged into the site, and it does not require login, it just sets a sessionid in cookie)", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:38:30.106955+00:00", "nick": "`brain", "message": "I copied the cookie name/value pair and used it when crafting the request headers", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:39:26.098787+00:00", "nick": "nikolaosk", "message": "do you copy all the headers from the browser?", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:39:31.444474+00:00", "nick": "`brain", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:39:34.182246+00:00", "nick": "`brain", "message": "ALL", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:39:41.379786+00:00", "nick": "`brain", "message": "assuming that was the right thing to do", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:39:51.280583+00:00", "nick": "`brain", "message": "all I don't pass is the cookie value from browser", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:40:10.402496+00:00", "nick": "nikolaosk", "message": "well, cookies are headers too", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:41:29.931189+00:00", "nick": "`brain", "message": "right - what I meant is I \"copy/paste\" from browser to manually crafted headers, except I use the response.body's cookie value", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:42:52.911644+00:00", "nick": "nikolaosk", "message": "response.body?", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:43:01.240856+00:00", "nick": "`brain", "message": "from shell session", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:43:06.613381+00:00", "nick": "nikolaosk", "message": "do you mean form data?", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:43:42.332597+00:00", "nick": "`brain", "message": "sorry - response.*header*", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:45:29.869311+00:00", "nick": "nikolaosk", "message": "why do you fetch without the headers first", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:45:39.392615+00:00", "nick": "nikolaosk", "message": "when you pass the url as a parameter to scrapy shell", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:45:52.072035+00:00", "nick": "`brain", "message": "I don't know?", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:46:01.140889+00:00", "nick": "nikolaosk", "message": "have you tried not to and using fetch(Request(...", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:46:36.090331+00:00", "nick": "`brain", "message": "I assume that to start a \"session\" clean, mimicking the browser, I would hit the URL first via scrapy shell URL", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:47:04.701325+00:00", "nick": "`brain", "message": "if I don't, how will I know my sessionID to pass via request header cookie?", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T13:47:27.124547+00:00", "nick": "`brain", "message": "(I have NOT tried what you suggested though)", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:01:41.240466+00:00", "nick": "`brain`", "message": "bah - internets", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:02:16.000741+00:00", "nick": "`brain`", "message": "OK - trying to do this via : scrapy shell <noURLhere>", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:03:20.420231+00:00", "nick": "`brain`", "message": "when I try to create myrequest = Request(..., I get NameError: name 'Request' is not defined", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:05:33.638563+00:00", "nick": "nikolaosk", "message": "from scrapy.http import Request", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:10:05.149191+00:00", "nick": "`brain`", "message": "thanks!  No luck :(   400 Bad Request", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:10:48.378544+00:00", "nick": "`brain`", "message": "maybe my technique in forming the request headers is somehow mangling the header data?  I'm pretty much copy / pasting, and adding in single quotes around everything", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:11:45.027113+00:00", "nick": "`brain`", "message": "i.e. if in browser I see Content-Length: 62 I do 'Content-Length': '62'", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:13:18.810747+00:00", "nick": "nikolaosk", "message": "oups", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:13:25.800401+00:00", "nick": "nikolaosk", "message": "drop this header", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:13:41.732782+00:00", "nick": "nikolaosk", "message": "also", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:14:34.220985+00:00", "nick": "nikolaosk", "message": "try requesting to such a service http://http-echo.jgate.de/", "links": ["http://http-echo.jgate.de/"], "channel": "scrapy"},
{"date": "2014-06-04T14:14:47.520495+00:00", "nick": "`brain`", "message": "what's that?", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:15:11.197630+00:00", "nick": "nikolaosk", "message": "it just echoes back your request wraped in a body", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:15:25.094332+00:00", "nick": "`brain`", "message": "cool!", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:16:08.681626+00:00", "nick": "`brain`", "message": "also - your first point about dropping header content-length, do you mean to specifically not pass that during request (why? browser had it)", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:16:20.411711+00:00", "nick": "nikolaosk", "message": "yes, not pass it", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:16:35.620208+00:00", "nick": "nikolaosk", "message": "as you may be guessing", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:16:50.324383+00:00", "nick": "nikolaosk", "message": "it is filled in by some middleware", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:18:58.633416+00:00", "nick": "`brain`", "message": "fascinating!  (I am not guessing - I'm stumbling and learning - my FIRST (this) site to scrape with scrapy, turned out the be the \"non-trivial\" kind)", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:19:19.156864+00:00", "nick": "`brain`", "message": "(and I'm doing it for The Wife, so there is \"management pressure\")", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:20:34.266878+00:00", "nick": "`brain`", "message": "DEBUG: Crawled (405) <POST http://http-echo.jgate.de/&gt;", "links": ["http://http-echo.jgate.de/&amp;gt"], "channel": "scrapy"},
{"date": "2014-06-04T14:21:27.079174+00:00", "nick": "`brain`", "message": "ha, it didn't like POST, GET seems to have worked", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:21:42.052035+00:00", "nick": "nikolaosk", "message": "hm", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:22:05.645600+00:00", "nick": "nikolaosk", "message": "some friend of mine knows a url with another one, that returns json", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:24:23.220289+00:00", "nick": "nikolaosk", "message": "not responding", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:24:30.125954+00:00", "nick": "nikolaosk", "message": "but try this https://github.com/bcantoni/echotest", "links": ["https://github.com/bcantoni/echotest"], "channel": "scrapy"},
{"date": "2014-06-04T14:25:11.109505+00:00", "nick": "nikolaosk", "message": "Usage has the urls to that service", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:25:23.741610+00:00", "nick": "nikolaosk", "message": "and https://github.com/bcantoni/echotest#usage", "links": ["https://github.com/bcantoni/echotest#usage"], "channel": "scrapy"},
{"date": "2014-06-04T14:25:49.327951+00:00", "nick": "nikolaosk", "message": "if you can't make anything out of it try the other services listed there https://github.com/bcantoni/echotest#echo-webse...", "links": ["https://github.com/bcantoni/echotest#echo-webservice"], "channel": "scrapy"},
{"date": "2014-06-04T14:27:38.218240+00:00", "nick": "`brain`", "message": "http-echo, the one you first posted works via browser, but when I try via scrapy shell, I get (I think) no meaningful content in response.body", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:28:33.045178+00:00", "nick": "nikolaosk", "message": "not meaningful? try another content encoding", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:29:31.320948+00:00", "nick": "nikolaosk", "message": "sorry, content type I mean", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:29:59.686541+00:00", "nick": "`brain`", "message": "i spoke poorly: not meaningful meant : what I got back in response.body was \" The easiest way to get your ideas online.\"", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:30:10.586215+00:00", "nick": "`brain`", "message": "maybe an ad banner or something", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:30:36.429332+00:00", "nick": "`brain`", "message": "that was via : print response.body", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:31:30.714742+00:00", "nick": "`brain`", "message": "<head>", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:31:30.928595+00:00", "nick": "`brain`", "message": "<title>JGate Maintenance</title>", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:31:43.580511+00:00", "nick": "`brain`", "message": "like my request didn't go to the right place?", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:32:21.062032+00:00", "nick": "`brain`", "message": ">>> print request.url", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:32:21.284517+00:00", "nick": "`brain`", "message": "http://http-echo.jgate.de/", "links": ["http://http-echo.jgate.de/"], "channel": "scrapy"},
{"date": "2014-06-04T14:32:36.849640+00:00", "nick": "`brain`", "message": "now I'm really confused :)", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:34:05.603822+00:00", "nick": "`brain`", "message": "I feel like I'm missing something fundamental in my understanding", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:35:48.748511+00:00", "nick": "nikolaosk", "message": "I don't get it", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:35:57.046268+00:00", "nick": "`brain`", "message": "HOLD ON!!", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:36:17.654603+00:00", "nick": "`brain`", "message": "I think I got a 200 response from targetURL after trimming down the header I was constructing!!!", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:36:31.528516+00:00", "nick": "`brain`", "message": "YES!", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:36:41.774452+00:00", "nick": "`brain`", "message": "Thank you!", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:36:43.364571+00:00", "nick": "nikolaosk", "message": "maybe some header was not handled by the app", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:37:05.151768+00:00", "nick": "`brain`", "message": "after your suggestion to NOT send content length, I decided to NOT send a bunch of other things", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:39:48.846219+00:00", "nick": "`brain`", "message": "my (flawed) thinking was that I must pass all header values that browser passes", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:40:26.502818+00:00", "nick": "`brain`", "message": "you suggested that some middleware was handling content-length, so I figured other things (like cookies) were also being handled", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:40:57.276382+00:00", "nick": "`brain`", "message": "I kept host, user-agent, accept, content-tyep, x-requested-with, referer and connectoin", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:41:01.386517+00:00", "nick": "nikolaosk", "message": "cookies?", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:41:21.819971+00:00", "nick": "nikolaosk", "message": "did the echo service fail because of cookies?", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:41:28.639669+00:00", "nick": "nikolaosk", "message": "weird", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:41:34.132079+00:00", "nick": "`brain`", "message": "I never got echo to work!!", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:41:51.248129+00:00", "nick": "`brain`", "message": "I got TargetURL to work - the thing I came in here trying to scrape", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:42:06.023255+00:00", "nick": "nikolaosk", "message": "ok", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:46:42.362256+00:00", "nick": "`brain`", "message": "I'm not sure what I learned, other than, the browser debug header is not the \"the required\" header", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T14:47:02.253743+00:00", "nick": "`brain`", "message": "maybe I need to understand what is and is not passed via middleware?", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T17:47:27.406064+00:00", "nick": "desophos", "message": "hi, i'm having some xpath trouble. i'm trying to do something like sel.css('pre tr') but i can't seem to get anything starting with 'pre'. this is the webpage i'm using as an example: http://l0dup05.larc.nasa.gov/opendap/tesl1l2l3/...", "links": ["http://l0dup05.larc.nasa.gov/opendap/tesl1l2l3/TES/TL1BN.005/2004.09.21/contents.html"], "channel": "scrapy"},
{"date": "2014-06-04T17:47:57.335614+00:00", "nick": "desophos", "message": "there are two tables and i want to get the one inside the <pre> tag", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T17:48:23.569135+00:00", "nick": "desophos", "message": "i can do sel.css('table') and it gets both tables, but sel.css('pre table') gets nothing", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T17:48:59.386545+00:00", "nick": "desophos", "message": "i don't know if this is a scrapy-specific problem, but this is inside a scrapy spider", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T17:49:06.537492+00:00", "nick": "desophos", "message": "and i'm using the scrapy shell to test this", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T17:54:43.196284+00:00", "nick": "csalazar", "message": "well, you can use xpath instead of css: //pre/table", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T17:56:31.272943+00:00", "nick": "desophos", "message": "doesn't work", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T17:56:36.528711+00:00", "nick": "desophos", "message": "same result", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T17:56:59.871746+00:00", "nick": "desophos", "message": "(empty)", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T18:00:08.625693+00:00", "nick": "desophos", "message": "so i decided to just manually grab the first table, but that produced another error", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T18:00:13.301397+00:00", "nick": "nikolaosk", "message": "do you mean //pre//table", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T18:00:23.693394+00:00", "nick": "nikolaosk", "message": "how did the table end up in there anyway?", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T18:00:38.613657+00:00", "nick": "nikolaosk", "message": "you know what the pre tag is for right?", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T18:00:53.883684+00:00", "nick": "desophos", "message": "clearly the people who created this page did not", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T18:01:07.451462+00:00", "nick": "nikolaosk", "message": "lol, happens all the time", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T18:02:10.379842+00:00", "nick": "csalazar", "message": "it seems a bug in the selector", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T18:02:28.394041+00:00", "nick": "desophos", "message": "anyway, i get this error when i try to initialize a Selector from HTML from sel.css('table').extract()[0]: exceptions.TypeError: cannot create weak reference to 'unicode' object", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T18:02:48.756296+00:00", "nick": "nikolaosk", "message": "desophos: scrapy --version?", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T18:02:55.698677+00:00", "nick": "csalazar", "message": "In [8]: sel.xpath(\"//pre\").extract()", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T18:02:55.961368+00:00", "nick": "csalazar", "message": "Out[8]: [u'<pre>\\n         </pre>']", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T18:03:09.702169+00:00", "nick": "desophos", "message": "0.22.2", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T18:03:47.712218+00:00", "nick": "desophos", "message": "csalazar: yeah, but try getting any of its children", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T18:04:07.590601+00:00", "nick": "nikolaosk", "message": "I think it would worth trying with lxml directly, but not by me, not now, I have work to do", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T18:06:41.793208+00:00", "nick": "csalazar", "message": "the selector uses lxml and there's a difference between response.body and sel.extract() related to <pre>", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T18:07:10.043366+00:00", "nick": "csalazar", "message": "desophos, how are you initializing the selector?", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T18:07:26.479505+00:00", "nick": "desophos", "message": "Selector(sel.css('table').extract()[0])", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T18:07:33.831623+00:00", "nick": "desophos", "message": "i also tried separating it", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T18:07:41.830260+00:00", "nick": "nikolaosk", "message": "aha", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T18:07:51.515775+00:00", "nick": "desophos", "message": "so table = sel.css('table').extract()[0]; Selector(table)", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T18:08:09.155812+00:00", "nick": "nikolaosk", "message": "sel.xpath return SelectorList instance", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T18:08:20.488148+00:00", "nick": "nikolaosk", "message": "wait, what?", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T18:08:23.065977+00:00", "nick": "nikolaosk", "message": "extract()?", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T18:08:51.869102+00:00", "nick": "nikolaosk", "message": "aha, I see why", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T18:08:53.875660+00:00", "nick": "desophos", "message": "yeah, extract() to get the list of children so i can index it", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T18:09:08.370194+00:00", "nick": "nikolaosk", "message": "the content as text, yes", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T18:09:23.513755+00:00", "nick": "csalazar", "message": "you can use new_selector = sel.css('table')", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T18:09:33.927996+00:00", "nick": "csalazar", "message": "then continue doing new_selector.css()", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T18:11:04.715811+00:00", "nick": "desophos", "message": "ah my mistake, i can just index the selector list", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T18:11:09.368681+00:00", "nick": "desophos", "message": "let me try that and see if it works", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T18:12:24.921776+00:00", "nick": "csalazar", "message": "anyway, you can do it in the way you explained, but use argument \"text=table\"", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T18:12:39.520916+00:00", "nick": "csalazar", "message": "because first argument in Selector is response", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T18:13:13.634016+00:00", "nick": "desophos", "message": "hmm, i tried that but i must have done something wrong because i got the same error", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T18:13:25.198051+00:00", "nick": "desophos", "message": "maybe i had two i guess", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T18:13:46.031278+00:00", "nick": "desophos", "message": "anyway, thanks for your help, pretty sure the issue is resolved", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T18:14:22.887269+00:00", "nick": "csalazar", "message": "it worked for me", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T18:14:42.764564+00:00", "nick": "csalazar", "message": "In [9]: table = sel.css('table').extract()[0]", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T18:14:51.403278+00:00", "nick": "csalazar", "message": "In [13]: Selector(text=table)", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T18:14:51.639418+00:00", "nick": "csalazar", "message": "Out[13]: <Selector xpath=None data=u'<html><body><table border=\"0\" width=\"100'>", "links": [], "channel": "scrapy"},
{"date": "2014-06-04T18:26:14.137138+00:00", "nick": "desophos", "message": "great, it's working now! thanks :)", "links": [], "channel": "scrapy"},
{"date": "2014-06-06T21:25:25.482201+00:00", "nick": "fpghost84", "message": "Hi, I'm scraping a website that sets a cookie by javascipt (a sort of \"challenge\" cookie called \"this-is-human\". I have no problem scraping the javascript, grabbing the cookie value and passing it to the request with cookies={'this-is-human':'value'}, but then on the next request is appears scrapy has forgetten about this cookie, and it is no longer being passed on. Do I have manually pass it each time?", "links": [], "channel": "scrapy"},
{"date": "2014-06-08T12:44:05.452947+00:00", "nick": "rdefeo", "message": "Hi!", "links": [], "channel": "scrapy"},
{"date": "2014-06-08T12:45:08.177248+00:00", "nick": "rdefeo", "message": "Have done some work with scrapy, and currently use scrapyd but I really want to have a separate process that manages the queuing of requests for scrapy to work on.", "links": [], "channel": "scrapy"},
{"date": "2014-06-08T12:45:35.901318+00:00", "nick": "rdefeo", "message": "Any suggestions?", "links": [], "channel": "scrapy"},
{"date": "2014-06-08T13:57:58.917411+00:00", "nick": "srek", "message": "Hi all", "links": [], "channel": "scrapy"},
{"date": "2014-06-08T15:39:24.317831+00:00", "nick": "hugo___", "message": "hi all :) I was wondering if anyone could help me with failing Request (failure: twisted.web.http._DataLoss). The page can be accessed with wget and gives a 200 OK response. But from within scrapy, it fails. In chrome network debugger i can see the possible problem: http://cl.ly/image/1K3c3o0I1O1O Any thoughts on how to get around this?", "links": ["http://cl.ly/image/1K3c3o0I1O1O"], "channel": "scrapy"},
{"date": "2014-06-09T09:29:41.187618+00:00", "nick": "saurabhzx", "message": "any body know how to implement more than one spider in one scrapy project or is it reliable to make one project per spider?", "links": [], "channel": "scrapy"},
{"date": "2014-06-09T09:54:50.736499+00:00", "nick": "saurabhzx", "message": "any body know how to implement more than one spider in one scrapy project or is it reliable to make one project per spider?", "links": [], "channel": "scrapy"},
{"date": "2014-06-09T17:33:02.770907+00:00", "nick": "zcc39298", "message": "hi i'm trying to do the tutorial on scrapy's documentation. but copying and pasting the text exactly, i am getting an indentation error even though i made sure all indents are tabs. what's wrong with this: https://pastebin.mozilla.org/5380943", "links": ["https://pastebin.mozilla.org/5380943"], "channel": "scrapy"},
{"date": "2014-06-09T17:34:11.883314+00:00", "nick": "zcc39298", "message": "here's the error https://pastebin.mozilla.org/5380946", "links": ["https://pastebin.mozilla.org/5380946"], "channel": "scrapy"},
{"date": "2014-06-09T19:04:34.074604+00:00", "nick": "alexenko", "message": "any chance someone can say why even though parse_links lines 54 and 65 yields parse_page, parse_page never gets executed?", "links": [], "channel": "scrapy"},
{"date": "2014-06-09T19:04:44.775605+00:00", "nick": "alexenko", "message": "https://gist.github.com/alexenko/3025b6747d576b...", "links": ["https://gist.github.com/alexenko/3025b6747d576b29dc62"], "channel": "scrapy"},
{"date": "2014-06-09T19:14:31.350563+00:00", "nick": "nyov", "message": "alexenko: it looks good from here. are you sure it doesn't execute?", "links": [], "channel": "scrapy"},
{"date": "2014-06-09T19:18:48.271627+00:00", "nick": "alexenko", "message": "yeah the print \"invode parse_page\" never gets printed in console", "links": [], "channel": "scrapy"},
{"date": "2014-06-09T19:22:27.077244+00:00", "nick": "alexenko", "message": "if i change line 78 to return item, it will return the first item parsed on the call from line 54", "links": [], "channel": "scrapy"},
{"date": "2014-06-09T19:23:31.737494+00:00", "nick": "alexenko", "message": "90% certain it has something to do with the generators being lazy", "links": [], "channel": "scrapy"},
{"date": "2014-06-09T19:45:16.049199+00:00", "nick": "alexenko", "message": "yeah it was something with the generators, rewrote it to use scrapy.spider.Spider instead of scrapy.contrib.spiders.CrawlSpider and it works now", "links": [], "channel": "scrapy"},
{"date": "2014-06-09T19:47:35.920245+00:00", "nick": "nyov", "message": "alexenko: i think it may have to do with you overriding parse_start_url, and not returning a list. also, not sure if you can yield requests at that early point", "links": [], "channel": "scrapy"},
{"date": "2014-06-09T19:53:58.577217+00:00", "nick": "alexenko", "message": "yeah, i went by an example on stackoverflow and apparently i couldn't hack it to do what i needed", "links": [], "channel": "scrapy"},
{"date": "2014-06-09T21:29:45.921425+00:00", "nick": "zcc39298", "message": "does anyone know how i can just get the data=100.00 part and not all the rest of the output in front of it: https://pastebin.mozilla.org/5381798", "links": ["https://pastebin.mozilla.org/5381798"], "channel": "scrapy"},
{"date": "2014-06-09T22:19:58.191737+00:00", "nick": "yhager", "message": "zcc39298: sel.xpath(...).extract()[0]", "links": [], "channel": "scrapy"},
{"date": "2014-06-09T22:24:05.381471+00:00", "nick": "zcc39298", "message": "yhager: thank you!", "links": [], "channel": "scrapy"},
{"date": "2014-06-09T23:06:33.895577+00:00", "nick": "inverio", "message": "hi all", "links": [], "channel": "scrapy"},
{"date": "2014-06-09T23:06:38.001887+00:00", "nick": "inverio", "message": "just one question", "links": [], "channel": "scrapy"},
{"date": "2014-06-09T23:07:08.649449+00:00", "nick": "inverio", "message": "just want to start with bot crawlers, anyone can give my any hint where to start, language, site, whatever?", "links": [], "channel": "scrapy"},
{"date": "2014-06-09T23:07:11.568092+00:00", "nick": "inverio", "message": "thanks in advanc", "links": [], "channel": "scrapy"},
{"date": "2014-06-09T23:07:12.147474+00:00", "nick": "inverio", "message": "e", "links": [], "channel": "scrapy"},
{"date": "2014-06-09T23:07:42.222379+00:00", "nick": "inverio", "message": "Hi!", "links": [], "channel": "scrapy"},
{"date": "2014-06-09T23:07:49.121769+00:00", "nick": "inverio", "message": "just one question", "links": [], "channel": "scrapy"},
{"date": "2014-06-09T23:07:54.948243+00:00", "nick": "inverio", "message": "just want to start with bot crawlers, anyone can give my any hint where to start, language, site, whatever?", "links": [], "channel": "scrapy"},
{"date": "2014-06-09T23:07:58.318130+00:00", "nick": "inverio", "message": "thanks in advance", "links": [], "channel": "scrapy"},
{"date": "2014-06-09T23:17:27.524469+00:00", "nick": "Pajamas", "message": "how do i call rules from my custom parse function to follow links?", "links": [], "channel": "scrapy"},
{"date": "2014-06-09T23:18:38.037720+00:00", "nick": "toothrot", "message": "Pajamas, do you mean you're overriding the actual 'parse' method?", "links": [], "channel": "scrapy"},
{"date": "2014-06-09T23:26:55.597462+00:00", "nick": "Pajamas", "message": "Yes, i'm using parse_items but it does not continue with my set Rules", "links": [], "channel": "scrapy"},
{"date": "2014-06-09T23:27:05.071775+00:00", "nick": "Pajamas", "message": "Rules only works if I use the default parse", "links": [], "channel": "scrapy"},
{"date": "2014-06-10T00:58:06.631984+00:00", "nick": "toothrot", "message": "Pajamas, you said yes, (but) that you're using 'parse_items'; then you said that bit about default 'parse', so i'm not clear on what you've done", "links": [], "channel": "scrapy"},
{"date": "2014-06-10T00:58:31.623032+00:00", "nick": "toothrot", "message": "default parse handles the rules. you need to involve it in some way.", "links": [], "channel": "scrapy"},
{"date": "2014-06-10T01:05:52.350110+00:00", "nick": "Pajamas", "message": "Thanks, is there a way to append a variable that is retrived from the first crawled page to a list of urls to crawl?", "links": [], "channel": "scrapy"},
{"date": "2014-06-10T01:06:23.993585+00:00", "nick": "Pajamas", "message": "So, i go to example.com and get id, and subsequent crawls would be (i.e.) example.com?id= + id", "links": ["http://example.com"], "channel": "scrapy"},
{"date": "2014-06-10T09:45:29.590354+00:00", "nick": "vishal__", "message": "hello guys", "links": [], "channel": "scrapy"},
{"date": "2014-06-10T09:45:39.003358+00:00", "nick": "baazzilhassan", "message": "hello", "links": [], "channel": "scrapy"},
{"date": "2014-06-10T09:46:01.361980+00:00", "nick": "vishal__", "message": "hey i am building price comparision site", "links": [], "channel": "scrapy"},
{"date": "2014-06-10T09:46:09.265946+00:00", "nick": "vishal__", "message": "anyone have any expirience in this?", "links": [], "channel": "scrapy"},
{"date": "2014-06-10T09:48:36.212589+00:00", "nick": "baazzilhassan", "message": "what is your questions ?", "links": [], "channel": "scrapy"},
{"date": "2014-06-10T15:37:11.603479+00:00", "nick": "Pajamas", "message": "y", "links": [], "channel": "scrapy"},
{"date": "2014-06-10T15:48:18.911308+00:00", "nick": "zcc324", "message": "i'm having trouble getting my spider to run. can someone take a look at this URL style and the crawl rules and see where i went wrong? when irun the spider it opens up then closes with a finished message without crawling anything. https://pastebin.mozilla.org/5385909", "links": ["https://pastebin.mozilla.org/5385909"], "channel": "scrapy"},
{"date": "2014-06-10T15:50:38.413224+00:00", "nick": "baazzilhassan", "message": "do u crawl links or sitemaps ?", "links": [], "channel": "scrapy"},
{"date": "2014-06-10T15:51:33.907560+00:00", "nick": "zcc324", "message": "in this particular case i was going to have it try to crawl the sitemap but is it easier to crawl links?", "links": [], "channel": "scrapy"},
{"date": "2014-06-10T15:51:53.030768+00:00", "nick": "zcc324", "message": "all of the URLs match that pattern, with just the \"1234\" part different for each one", "links": [], "channel": "scrapy"},
{"date": "2014-06-10T15:52:02.299690+00:00", "nick": "baazzilhassan", "message": "yeh", "links": [], "channel": "scrapy"},
{"date": "2014-06-10T15:52:21.113033+00:00", "nick": "baazzilhassan", "message": "but it's depend what a base spider to use", "links": [], "channel": "scrapy"},
{"date": "2014-06-10T15:53:30.160511+00:00", "nick": "baazzilhassan", "message": "if u use a sitemap the use this http://doc.scrapy.org/en/latest/topics/spiders....", "links": ["http://doc.scrapy.org/en/latest/topics/spiders.html#sitemapspider"], "channel": "scrapy"},
{"date": "2014-06-10T15:53:56.266809+00:00", "nick": "baazzilhassan", "message": "if u want to crawl links use this: http://doc.scrapy.org/en/latest/topics/spiders....", "links": ["http://doc.scrapy.org/en/latest/topics/spiders.html#scrapy.contrib.spiders.CrawlSpider"], "channel": "scrapy"},
{"date": "2014-06-10T15:54:38.298875+00:00", "nick": "zcc324", "message": "cool", "links": [], "channel": "scrapy"},
{"date": "2014-06-10T15:54:55.247174+00:00", "nick": "zcc324", "message": "i'll check them out. shukran", "links": [], "channel": "scrapy"},
{"date": "2014-06-10T15:55:05.960709+00:00", "nick": "baazzilhassan", "message": "marhaba :)", "links": [], "channel": "scrapy"},
{"date": "2014-06-10T15:55:46.941119+00:00", "nick": "zcc324", "message": "ahlan :)", "links": [], "channel": "scrapy"},
{"date": "2014-06-10T15:56:36.676845+00:00", "nick": "baazzilhassan", "message": "what is your nationality ?", "links": [], "channel": "scrapy"},
{"date": "2014-06-10T15:59:52.370462+00:00", "nick": "zcc324", "message": "american but i'm trying to learn arabic", "links": [], "channel": "scrapy"},
{"date": "2014-06-10T16:00:27.024700+00:00", "nick": "baazzilhassan", "message": "ohh that's nice :)", "links": [], "channel": "scrapy"},
{"date": "2014-06-10T16:00:37.618761+00:00", "nick": "baazzilhassan", "message": "I'm from Morocco", "links": [], "channel": "scrapy"},
{"date": "2014-06-10T16:01:10.252135+00:00", "nick": "zcc324", "message": "cool. i love middle eastern culture and food", "links": [], "channel": "scrapy"},
{"date": "2014-06-10T16:01:42.473120+00:00", "nick": "baazzilhassan", "message": "hmmm,", "links": [], "channel": "scrapy"},
{"date": "2014-06-10T16:02:14.649019+00:00", "nick": "baazzilhassan", "message": "Morocco is in north africa, south of spanish", "links": [], "channel": "scrapy"},
{"date": "2014-06-10T16:03:47.244092+00:00", "nick": "zcc324", "message": "i know where it is. lol", "links": [], "channel": "scrapy"},
{"date": "2014-06-10T16:04:01.636669+00:00", "nick": "baazzilhassan", "message": "not everybody :p", "links": [], "channel": "scrapy"},
{"date": "2014-06-10T16:04:04.686508+00:00", "nick": "zcc324", "message": "'near east'", "links": [], "channel": "scrapy"},
{"date": "2014-06-10T16:04:11.818508+00:00", "nick": "baazzilhassan", "message": "yeh", "links": [], "channel": "scrapy"},
{"date": "2014-06-10T16:04:51.511522+00:00", "nick": "baazzilhassan", "message": "so, if u have some problems on regex to extract links I'm here, I can help :)", "links": [], "channel": "scrapy"},
{"date": "2014-06-10T16:06:58.053653+00:00", "nick": "zcc324", "message": "thanks i'm trying to get the sitemap spider to work", "links": [], "channel": "scrapy"},
{"date": "2014-06-10T16:14:53.410378+00:00", "nick": "zcc324", "message": "can i crawl links if the links end in =1234 but it's not always 4 numbers, but varying numbers?", "links": [], "channel": "scrapy"},
{"date": "2014-06-10T16:15:13.493960+00:00", "nick": "zcc324", "message": "4-7 numbers, etc", "links": [], "channel": "scrapy"},
{"date": "2014-06-10T19:47:47.748105+00:00", "nick": "testing123", "message": "Hello there...has anyone run into the following error using scrapy: 'OffsiteMiddleware' object has no attribute 'host_regex' ?", "links": [], "channel": "scrapy"},
{"date": "2014-06-10T22:11:08.708481+00:00", "nick": "koell", "message": "wait [o__o] is using scrapy as well? :3", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T08:51:25.244392+00:00", "nick": "kjam", "message": "good morning! i was hoping someone in here has worked with multipart form data (not necessarily uploads, just old forms that use JS to submit) and scrapy?", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T09:01:56.438430+00:00", "nick": "koell", "message": "kjam: idk if this will work with scrapy then, im still a beginner but you may look in phantomJS then", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T09:08:26.856665+00:00", "nick": "kjam", "message": "yes, i was hoping there was an implementation in the FormResponse.from_response but it doesnt look lik eit", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T09:08:42.679514+00:00", "nick": "kjam", "message": "maybe selenium with scrapy is the ticket :)", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T09:08:53.505857+00:00", "nick": "koell", "message": "kjam: man, you really have to handle such ugly form? :D", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T09:09:05.751848+00:00", "nick": "kjam", "message": "ha, sadly :)", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T09:09:53.414819+00:00", "nick": "koell", "message": "kjam: selenium?", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T09:28:34.393245+00:00", "nick": "kjam", "message": "koell: selenium the webdriver resource. its useful with some js submitted forms", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T14:58:13.849035+00:00", "nick": "crlane", "message": "Hi!", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T14:59:08.026882+00:00", "nick": "crlane", "message": "is someone available to help with a question?", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T15:02:23.849856+00:00", "nick": "crlane", "message": "hello?", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T15:16:32.009775+00:00", "nick": "samtc", "message": "crlane: if you want help you have to ask your question", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T15:16:57.664809+00:00", "nick": "crlane", "message": "ok, sorry", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T15:17:14.610358+00:00", "nick": "crlane", "message": "I am using scrapyd on multiple servers to run a lot of spiders", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T15:17:21.528182+00:00", "nick": "crlane", "message": "but I am getting a strange error", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T15:17:38.941789+00:00", "nick": "crlane", "message": "OffsiteMiddleware' object has no attribute 'host_regex'", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T15:18:59.125770+00:00", "nick": "crlane", "message": "I am passing each spider a list of urls, then traversing 1 level deep on those urls, limiting to those domains", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T15:20:35.606453+00:00", "nick": "crlane", "message": "it looks like it's getting stuck in the limiting of domains in the OffsiteMiddleware", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T15:22:35.606286+00:00", "nick": "crlane", "message": "Ok, no go there.", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T15:22:57.321733+00:00", "nick": "crlane", "message": "I'm also having an issue with getting the logging and items directories to change", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T15:24:55.092053+00:00", "nick": "crlane", "message": "I've changed the paths in scrapyd.conf and placed that in the home directory of my project before deploying", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T15:25:50.275648+00:00", "nick": "samtc", "message": "scrapy.conf or scrapy.cfg?", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T15:26:02.784040+00:00", "nick": "crlane", "message": "scrapyd.conf", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T15:26:26.403163+00:00", "nick": "crlane", "message": "I've also tried placing that at /etc/scrapyd.conf.d", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T15:26:54.321023+00:00", "nick": "crlane", "message": "* /etc/scrapyd/conf.d/", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T15:27:00.048163+00:00", "nick": "samtc", "message": "I think it's scrapy/scrapyd.cfg not .conf", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T15:27:03.820792+00:00", "nick": "samtc", "message": "err", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T15:27:19.291208+00:00", "nick": "samtc", "message": "I think it's .cfg not .conf", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T15:27:24.624998+00:00", "nick": "crlane", "message": "yes for prject configuration", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T15:27:40.272102+00:00", "nick": "crlane", "message": "but this is for where jobs deployed to scrapyd will store items and logs", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T15:27:53.001102+00:00", "nick": "samtc", "message": "ok", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T15:28:17.571393+00:00", "nick": "crlane", "message": "sorry should have metioned tat", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T15:28:18.598249+00:00", "nick": "crlane", "message": "that", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T15:31:09.047737+00:00", "nick": "crlane", "message": "are there any other ways to change the storing of items/logs in scrapyd?", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T15:42:39.054979+00:00", "nick": "crlane", "message": "here's my scrapyd.conf", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T15:42:50.341983+00:00", "nick": "crlane", "message": "[scrapyd]", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T15:43:13.540370+00:00", "nick": "crlane", "message": "logs_dir = /mnt/scraped-data/logs", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T15:43:13.710852+00:00", "nick": "crlane", "message": "items_dir = /mnt/scraped-data/items", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T15:44:51.599185+00:00", "nick": "crlane", "message": "however, the logs are still being set as /var/log/scrapyd", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T16:22:37.228734+00:00", "nick": "crlane", "message": "am I doing something wrong with my scrapyd.conf?", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T18:41:37.437944+00:00", "nick": "zcc345", "message": "can anyone please check out this error and help me figure out what's going on? https://pastebin.mozilla.org/5392217", "links": ["https://pastebin.mozilla.org/5392217"], "channel": "scrapy"},
{"date": "2014-06-11T18:48:05.342577+00:00", "nick": "samtc__", "message": "referer not in headers?", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T18:48:19.188363+00:00", "nick": "samtc__", "message": "request.headers.get('Referer') return None", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T18:48:48.902053+00:00", "nick": "samtc__", "message": "if 'sitemap' not in request.headers.get('Referer', ''):", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T18:52:33.829553+00:00", "nick": "zcc345", "message": "thanks i'm trying that", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T18:55:00.276395+00:00", "nick": "zcc345", "message": "what's strange is that this spider was working last week. could something have changed since then that broke it?", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T18:57:52.917838+00:00", "nick": "samtc__", "message": "referer is not in the headers", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T18:59:09.851574+00:00", "nick": "samtc__", "message": "I don't have much detail... maybe you're redirected to an https url?", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T19:01:06.888967+00:00", "nick": "zcc345", "message": "DEBUG: Retrying <GET http://www.URL.com/sitemap.xml&gt; (failed 1 times): User timeout caused connection failure: Getting http://www.URL.com/sitemap.xml took longer than 180 seconds..", "links": ["http://www.URL.com/sitemap.xml&amp;gt", "http://www.URL.com/sitemap.xml"], "channel": "scrapy"},
{"date": "2014-06-11T19:01:39.300896+00:00", "nick": "zcc345", "message": "looks like it's timing out now. but your fix seems to have fixed the referer thing", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T19:08:47.547629+00:00", "nick": "zcc345", "message": "on the results when i try crawling one link the region is blank. instead of url.com/product?region=XX it just has /product?region=  with a blank", "links": ["http://url.com/product?region=XX"], "channel": "scrapy"},
{"date": "2014-06-11T19:16:17.700972+00:00", "nick": "crlane", "message": "if anyone else has the issue I referenced above, it can be fixed by editing the file at /etc/scrapyd/conf.d/000-default. that seems to be the only way to get scrapyd to use nondefault log/item director settings", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T20:54:55.431894+00:00", "nick": "zcc345", "message": "i'm getting errors when I try to crawl pages which do not contain a field my spider looks for. isn't the way to handle this to use .extract()[0] or [None] or am i missing something?", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T20:55:10.376969+00:00", "nick": "zcc345", "message": "exceptions.IndexError: list index out of range", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T20:57:07.483381+00:00", "nick": "baazzilhassan_", "message": "use this: ''.join(sel.xpath('//selector').extract())", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T20:59:02.556320+00:00", "nick": "baazzilhassan_", "message": "zcc345: use this: ''.join(sel.xpath('//selector').extract())", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T20:59:31.012775+00:00", "nick": "zcc345", "message": "baazzilhassan_: thanks i will try that", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T20:59:45.537299+00:00", "nick": "baazzilhassan_", "message": "zcc345: :)", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T21:01:44.116660+00:00", "nick": "zcc345", "message": "baazzilhassan_: so if i have something like div[@class = 'pDprop'][1]//text()\").extract()[0] do i add what you said to another line to fix it or change what i have?", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T21:47:05.551952+00:00", "nick": "zcc345", "message": "baazzilhassan_: thanks for your help, it worked", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T21:47:41.799877+00:00", "nick": "baazzilhassan_", "message": "zcc345: welcome :)", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T22:02:10.873523+00:00", "nick": "nScrapyn", "message": "is it always this quiet in here?", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T22:18:33.342991+00:00", "nick": "zcc345", "message": "anyone know how to implement this in a scrapy spider. the country needs to be selected before the page returns the data i want. https://pastebin.mozilla.org/5393097", "links": ["https://pastebin.mozilla.org/5393097"], "channel": "scrapy"},
{"date": "2014-06-11T22:27:12.033987+00:00", "nick": "fpghost84", "message": "zcc345: Can be done with something like Selenium+scrapy, but I don't think with just scrapy alone (could be wrong)", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T22:27:43.983922+00:00", "nick": "fpghost84", "message": "zcc345: can you not parse the country, then replicate the POST in scrapy?", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T22:28:39.111172+00:00", "nick": "nScrapyn", "message": "I've only just started using scrapy but I was googling and reading some of the docs and there doesn't really seem to be anything to do what he wants", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T22:28:56.298034+00:00", "nick": "nScrapyn", "message": "so idk", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T22:29:01.214868+00:00", "nick": "fpghost84", "message": "I'm thinking use FormRequest, and pass it the formdata you scraped from options", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T22:29:09.633466+00:00", "nick": "fpghost84", "message": "I'm no expert though...", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T22:30:28.607605+00:00", "nick": "baazzilhassan_", "message": "nScrapyn: what is your problem ?", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T22:31:39.870336+00:00", "nick": "fpghost84", "message": "Something like yield FormRequest(url=\"http://www.URL.com/redirec...;,      formdata={'country': 'Argentina'},   callback=self.after_post)]", "links": ["http://FormRequest(url=\"http://www.URL.com/redirect.php\""], "channel": "scrapy"},
{"date": "2014-06-11T22:31:59.479731+00:00", "nick": "fpghost84", "message": "plus any other headers you find are needed through something like firebug perhaps...", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T22:41:17.125925+00:00", "nick": "zcc345", "message": "hmm. well thanks anyways. i can keep reading up on it", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T23:06:40.961430+00:00", "nick": "zcc345", "message": "fpghost84: i'm getting exceptions.NameError: global name 'FormRequest' is not defined", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T23:07:06.822970+00:00", "nick": "zcc345", "message": "where am i supposed to stick this bit of code?", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T23:07:46.861341+00:00", "nick": "fpghost84", "message": "zcc345: make sure you import it first: from scrapy.http import FormRequest", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T23:09:12.836080+00:00", "nick": "fpghost84", "message": "zcc345: Also check out: http://doc.scrapy.org/en/latest/topics/request-...", "links": ["http://doc.scrapy.org/en/latest/topics/request-response.html#formrequest-objects"], "channel": "scrapy"},
{"date": "2014-06-11T23:10:34.768891+00:00", "nick": "zcc345", "message": "fpghost84: thanks", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T23:19:45.967286+00:00", "nick": "zcc345", "message": "sorry to keep asking questions but i tried to follow the documentation and am getting exceptions.AttributeError: 'TocrisSpider' object has no attribute 'after_post'", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T23:20:36.647580+00:00", "nick": "zcc345", "message": "i think i'll try selenium as an alternate method of doing this", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T23:20:43.108848+00:00", "nick": "baazzilhassan_", "message": "self.after_post", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T23:22:02.366673+00:00", "nick": "zcc345", "message": "ya, i have callback=self.after_post)]", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T23:22:59.276127+00:00", "nick": "baazzilhassan_", "message": "then declare your callback", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T23:23:13.460134+00:00", "nick": "baazzilhassan_", "message": "def after_post(self, response)", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T23:25:00.096844+00:00", "nick": "zcc345", "message": "baazzilhassan_: oh i was putting it inside of another function. thanks", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T23:25:55.113373+00:00", "nick": "baazzilhassan_", "message": ":)", "links": [], "channel": "scrapy"},
{"date": "2014-06-11T23:30:49.977775+00:00", "nick": "zcc345", "message": "regards", "links": [], "channel": "scrapy"},
{"date": "2014-06-12T06:27:01.974558+00:00", "nick": "eN_Joy", "message": "i am scraping a chinese site, in the console i am seeing the log contains the scraped item in the form: ` {'title': u'\\u4ed9\\u513f/\\u6e23\\u513f',...}`, i know this is only a displayed format, the item saved into database with correct encoding, besides, i can alway add a line in pipelines to `print item['title']` to have the console output `\u4ed9\u513f/\u6e23\u513f`, however, is there an ultimate solution to have scrapy console to directly output utf-8?", "links": [], "channel": "scrapy"},
{"date": "2014-06-12T06:40:13.344531+00:00", "nick": "dpn`", "message": "eN_Joy, the item with the \\uXXX stuff is showing the python representation of the object", "links": [], "channel": "scrapy"},
{"date": "2014-06-12T06:40:35.007482+00:00", "nick": "dpn`", "message": "python will always show the ugly representation of the object there as it's supposed to help with debugging", "links": [], "channel": "scrapy"},
{"date": "2014-06-12T06:41:01.556345+00:00", "nick": "dpn`", "message": "the equivalent of doing repr([\"\u4ed9\u513f/\u6e23\u513f\"])", "links": [], "channel": "scrapy"},
{"date": "2014-06-12T06:41:57.899142+00:00", "nick": "dpn`", "message": ">>> repr([\"\u4ed9\u513f/\u6e23\u513f\"])  ->    \"['\\\\xe4\\\\xbb\\\\x99\\\\xe5\\\\x84\\\\xbf/\\\\xe6\\\\xb8\\\\xa3\\\\xe5\\\\x84\\\\xbf']\"", "links": [], "channel": "scrapy"},
{"date": "2014-06-12T06:42:40.235937+00:00", "nick": "dpn`", "message": "if you'd like to change that formatting you would probably have to add your own logging handler", "links": [], "channel": "scrapy"},
{"date": "2014-06-12T06:43:44.430954+00:00", "nick": "eN_Joy", "message": "well, for now, i just add few lines of `print item['field']` to help me determine if i have scraped correctly", "links": [], "channel": "scrapy"},
{"date": "2014-06-12T06:44:54.617122+00:00", "nick": "dpn`", "message": "you could use the logging mechanism too", "links": [], "channel": "scrapy"},
{"date": "2014-06-12T06:44:55.667848+00:00", "nick": "eN_Joy", "message": "\\uXXX is hard to read;-)", "links": [], "channel": "scrapy"},
{"date": "2014-06-12T06:45:11.847946+00:00", "nick": "eN_Joy", "message": "that's what i am trying to accomplish...", "links": [], "channel": "scrapy"},
{"date": "2014-06-12T06:45:44.274388+00:00", "nick": "dpn`", "message": "log.msg(\"scraped: %s\" % item['field'])", "links": [], "channel": "scrapy"},
{"date": "2014-06-12T06:46:02.052560+00:00", "nick": "dpn`", "message": "I thought you were wanting to change the way the built in log observer was working", "links": [], "channel": "scrapy"},
{"date": "2014-06-12T06:46:18.124356+00:00", "nick": "dpn`", "message": "if you don't mind two messages for each item then print or log.msg will do", "links": [], "channel": "scrapy"},
{"date": "2014-06-12T06:46:37.616223+00:00", "nick": "eN_Joy", "message": "i see,", "links": [], "channel": "scrapy"},
{"date": "2014-06-12T06:47:24.221539+00:00", "nick": "eN_Joy", "message": "i remember i could suppress the output from the builtin message and only keep the output from my own `print` clause", "links": [], "channel": "scrapy"},
{"date": "2014-06-12T07:45:25.548700+00:00", "nick": "robin__", "message": "hi guys, any knows thats how i can set referer for ImagesPipeline?", "links": [], "channel": "scrapy"},
{"date": "2014-06-12T07:46:13.415209+00:00", "nick": "robin__", "message": "seems ImagesPipeline will not set Referer automatic while downloading image file.", "links": [], "channel": "scrapy"},
{"date": "2014-06-12T16:33:48.112819+00:00", "nick": "ccb4n", "message": "to use selenium with my scrapy spider do i just download it here? https://pypi.python.org/pypi/selenium", "links": ["https://pypi.python.org/pypi/selenium"], "channel": "scrapy"},
{"date": "2014-06-12T20:36:48.149695+00:00", "nick": "joeobrien", "message": "Can anybody help me figure out why my scrapy spider isn't sending the data to the database", "links": [], "channel": "scrapy"},
{"date": "2014-06-12T22:34:46.097815+00:00", "nick": "Pajamas", "message": "How do i prevent skip to the next node in parse_node on meeting a condition?", "links": [], "channel": "scrapy"},
{"date": "2014-06-12T22:35:50.272879+00:00", "nick": "Pajamas", "message": "Some nodes should skip and not go to pipeline", "links": [], "channel": "scrapy"},
{"date": "2014-06-13T00:12:04.002459+00:00", "nick": "Pajamas", "message": "got it", "links": [], "channel": "scrapy"},
{"date": "2014-06-13T19:03:22.789826+00:00", "nick": "zcc345", "message": "i am a novice and before i try and look into selenium to do this, i just wanted to know if anyone knows whether scrapy is capable of selecting country from a drop-down list. the data i want to scrape doesn't appear on the page until country is selected https://pastebin.mozilla.org/5404227", "links": ["https://pastebin.mozilla.org/5404227"], "channel": "scrapy"},
{"date": "2014-06-13T19:04:30.613886+00:00", "nick": "zcc345", "message": "this was my attempt at it in scrapy https://pastebin.mozilla.org/5404231", "links": ["https://pastebin.mozilla.org/5404231"], "channel": "scrapy"},
{"date": "2014-06-13T19:49:47.688166+00:00", "nick": "l34kc1M", "message": "hey", "links": [], "channel": "scrapy"},
{"date": "2014-06-13T19:50:24.856020+00:00", "nick": "l34kc1M", "message": "how are you guys doing ? Im new to web scraping would like to ask some questions about scrapy ?", "links": [], "channel": "scrapy"},
{"date": "2014-06-13T21:30:43.693624+00:00", "nick": "brain", "message": "zcc345: I too am a newb, and my first scrapy project was all about javascript, etc.", "links": [], "channel": "scrapy"},
{"date": "2014-06-13T21:31:43.943116+00:00", "nick": "brain", "message": "what worked for me, which is the general approach, is to play with the site in firefox or chrome with firebug or developer tools open, so that you can analyze what the site is doing (usually some http POST)", "links": [], "channel": "scrapy"},
{"date": "2014-06-13T21:32:18.471462+00:00", "nick": "brain", "message": "you can then make scrapy send those POST requests, and analyze the response, in which you might have what you are looking for", "links": [], "channel": "scrapy"},
{"date": "2014-06-13T21:33:22.263362+00:00", "nick": "brain", "message": "here is a good article: http://stackoverflow.com/questions/8550114/can-...", "links": ["http://stackoverflow.com/questions/8550114/can-scrapy-be-used-to-scrape-dynamic-content-from-websites-that-are-using-ajax"], "channel": "scrapy"},
{"date": "2014-06-13T21:34:01.125066+00:00", "nick": "brain", "message": "look at the second answer - a full walkthrough from user Badarau Petru", "links": [], "channel": "scrapy"},
{"date": "2014-06-13T21:51:09.913367+00:00", "nick": "zcc345", "message": "brain: thanks. in this case i can see how in firefox the xpath works after the country is selected, but looking before and after posting the country, i am too much of a newb to figure out what's going on", "links": [], "channel": "scrapy"},
{"date": "2014-06-13T21:51:33.855604+00:00", "nick": "zcc345", "message": "i've been testing it out using the developer console in FF", "links": [], "channel": "scrapy"},
{"date": "2014-06-13T22:01:29.458735+00:00", "nick": "brain", "message": "can you share link?", "links": [], "channel": "scrapy"},
{"date": "2014-06-13T22:01:34.343588+00:00", "nick": "brain", "message": "to target", "links": [], "channel": "scrapy"},
{"date": "2014-06-13T22:01:59.869699+00:00", "nick": "brain", "message": "the xpath will help you find the element that you want, *after* it is loaded.", "links": [], "channel": "scrapy"},
{"date": "2014-06-13T22:02:36.240366+00:00", "nick": "brain", "message": "you want to watch in FF console as you are clicking the dropdown/whatever, to see what is being sent by browser to site, likely via http POST", "links": [], "channel": "scrapy"},
{"date": "2014-06-13T22:04:55.383760+00:00", "nick": "zcc345", "message": "thanks yea", "links": [], "channel": "scrapy"},
{"date": "2014-06-13T22:05:12.161258+00:00", "nick": "zcc345", "message": "the link you sent actually has some good info i can use", "links": [], "channel": "scrapy"},
{"date": "2014-06-13T22:08:11.125792+00:00", "nick": "brain", "message": "it is excellent - you just have to get your head around the concepts", "links": [], "channel": "scrapy"},
{"date": "2014-06-13T22:08:44.750818+00:00", "nick": "brain", "message": "OK - so at this stage, select your country is blank", "links": [], "channel": "scrapy"},
{"date": "2014-06-13T22:13:48.790448+00:00", "nick": "zcc345", "message": "ya", "links": [], "channel": "scrapy"},
{"date": "2014-06-13T22:14:09.575840+00:00", "nick": "zcc345", "message": "once you select country, it sets a cookie", "links": [], "channel": "scrapy"},
{"date": "2014-06-13T22:14:55.333841+00:00", "nick": "brain", "message": "you can see that after you hit submit, you are posting a GET to TARGET.js?1402697109", "links": [], "channel": "scrapy"},
{"date": "2014-06-13T22:15:14.166064+00:00", "nick": "brain", "message": "and that at the end of the cookie, there is country=290", "links": [], "channel": "scrapy"},
{"date": "2014-06-13T22:15:57.353475+00:00", "nick": "zcc345", "message": "hmm ok", "links": [], "channel": "scrapy"},
{"date": "2014-06-13T22:16:24.868859+00:00", "nick": "brain", "message": "are you trying to get the prices of one part for all countries?", "links": [], "channel": "scrapy"},
{"date": "2014-06-13T22:16:51.524733+00:00", "nick": "brain", "message": "or many parts for one country?  Your approach will determine what you want to iterate over in scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-06-13T22:16:52.070207+00:00", "nick": "zcc345", "message": "no just for '290'", "links": [], "channel": "scrapy"},
{"date": "2014-06-13T22:17:02.125513+00:00", "nick": "zcc345", "message": "only that one country", "links": [], "channel": "scrapy"},
{"date": "2014-06-13T22:17:34.634780+00:00", "nick": "brain", "message": "so you should be able to set your country=290 when you make your initial request", "links": [], "channel": "scrapy"},
{"date": "2014-06-13T22:18:15.447571+00:00", "nick": "brain", "message": "is there a place here to unset your country?", "links": [], "channel": "scrapy"},
{"date": "2014-06-13T22:18:41.288082+00:00", "nick": "zcc345", "message": "i tried to do this like this: https://pastebin.mozilla.org/5404231 (setting the country)", "links": ["https://pastebin.mozilla.org/5404231"], "channel": "scrapy"},
{"date": "2014-06-13T22:19:03.059882+00:00", "nick": "brain", "message": "anywho - to make it a bit more robust, you can get the value for your country by parsing that initial dropdown list, looking for your country name and getting the option value corresponding to use in your cookie", "links": [], "channel": "scrapy"},
{"date": "2014-06-13T22:19:07.798919+00:00", "nick": "zcc345", "message": "at the very bottom of the page, there is a 'change country' link if that's what you mean", "links": [], "channel": "scrapy"},
{"date": "2014-06-13T22:20:07.826882+00:00", "nick": "zcc345", "message": "ya i know the value is 290 but not sure how to have scrapy select that other that my attempt using FormRequest.", "links": [], "channel": "scrapy"},
{"date": "2014-06-13T22:20:21.553080+00:00", "nick": "brain", "message": "I'm not smart enough to know, but I'd say that using formdata like that means that there is a FORM param with name 'option value'", "links": [], "channel": "scrapy"},
{"date": "2014-06-13T22:20:25.541569+00:00", "nick": "brain", "message": "(which there isn't)", "links": [], "channel": "scrapy"},
{"date": "2014-06-13T22:21:52.562097+00:00", "nick": "zcc345", "message": "it says in the page <option value=\"290\">", "links": [], "channel": "scrapy"},
{"date": "2014-06-13T22:22:00.180750+00:00", "nick": "zcc345", "message": "and a bunch of other values for different countries", "links": [], "channel": "scrapy"},
{"date": "2014-06-13T22:22:06.811089+00:00", "nick": "zcc345", "message": "that's what i called it option value", "links": [], "channel": "scrapy"},
{"date": "2014-06-13T22:22:23.606336+00:00", "nick": "brain", "message": "yeah, that just HTML for what that dropdown will send as a value, when the corresponding country is selected", "links": [], "channel": "scrapy"},
{"date": "2014-06-13T22:22:51.010340+00:00", "nick": "brain", "message": "on the backend, their webserver will read the value 290, and return the prices for that country", "links": [], "channel": "scrapy"},
{"date": "2014-06-13T22:23:26.478305+00:00", "nick": "zcc345", "message": "ya", "links": [], "channel": "scrapy"},
{"date": "2014-06-13T22:24:15.708263+00:00", "nick": "brain", "message": "http://stackoverflow.com/questions/10667202/how...", "links": ["http://stackoverflow.com/questions/10667202/how-to-overwrite-use-cookies-in-scrapy"], "channel": "scrapy"},
{"date": "2014-06-13T22:33:08.969192+00:00", "nick": "brain", "message": "I think, you need to make your request with that cookie value set", "links": [], "channel": "scrapy"},
{"date": "2014-06-13T22:36:41.317498+00:00", "nick": "zcc345", "message": "i tried sticking this in there but no luck https://pastebin.mozilla.org/5404984", "links": ["https://pastebin.mozilla.org/5404984"], "channel": "scrapy"},
{"date": "2014-06-13T22:40:53.320424+00:00", "nick": "brain", "message": "yeah - you're (at least) forgetting that country=290", "links": [], "channel": "scrapy"},
{"date": "2014-06-13T22:41:03.225400+00:00", "nick": "brain", "message": "the \"name\" is USA, the \"value\" is 290", "links": [], "channel": "scrapy"},
{"date": "2014-06-13T22:41:18.395252+00:00", "nick": "brain", "message": "that's not scrapy, that's just HTML, forms, and option", "links": [], "channel": "scrapy"},
{"date": "2014-06-13T22:41:24.039807+00:00", "nick": "brain", "message": "k, bbl, good luck!", "links": [], "channel": "scrapy"},
{"date": "2014-06-13T22:42:54.907683+00:00", "nick": "zcc345", "message": "thanks i tried the 290 also", "links": [], "channel": "scrapy"},
{"date": "2014-06-13T22:43:05.864363+00:00", "nick": "zcc345", "message": "see you later", "links": [], "channel": "scrapy"},
{"date": "2014-06-14T04:27:00.413875+00:00", "nick": "eN_Joy", "message": "to obtain all innerhtml from within a div, what's the difference between `sel.xpath('//div[@id=\"content\"]/*&... and `sel.xpath('//div[@id=\"content\"]/no...", "links": ["mailto:`sel.xpath('//div[@id=\"content\"]/*').extract()`", "mailto:`sel.xpath('//div[@id=\"content\"]/node()').extract()`?"], "channel": "scrapy"},
{"date": "2014-06-14T13:42:00.263400+00:00", "nick": "Volis", "message": "Hello", "links": [], "channel": "scrapy"},
{"date": "2014-06-14T13:43:13.886767+00:00", "nick": "scrapy_freelance", "message": "Volis: Hi", "links": [], "channel": "scrapy"},
{"date": "2014-06-14T13:43:41.627540+00:00", "nick": "Volis", "message": "I was trying to scrape data from here: http://www.esuvidha.info/", "links": ["http://www.esuvidha.info/"], "channel": "scrapy"},
{"date": "2014-06-14T13:43:49.304551+00:00", "nick": "Volis", "message": "but this has died unexpectedly", "links": [], "channel": "scrapy"},
{"date": "2014-06-14T13:44:21.572391+00:00", "nick": "scrapy_freelance", "message": "okey", "links": [], "channel": "scrapy"},
{"date": "2014-06-14T13:44:48.057866+00:00", "nick": "Volis", "message": "Can anyone tell me whats wrong with the website?", "links": [], "channel": "scrapy"},
{"date": "2014-06-14T13:45:30.184870+00:00", "nick": "Volis", "message": "I'm a complete beginner with data scraping and web in general.", "links": [], "channel": "scrapy"},
{"date": "2014-06-14T13:45:34.194663+00:00", "nick": "scrapy_freelance", "message": "give me the roll numero", "links": [], "channel": "scrapy"},
{"date": "2014-06-14T13:45:40.099595+00:00", "nick": "scrapy_freelance", "message": "and name to enter", "links": [], "channel": "scrapy"},
{"date": "2014-06-14T13:45:46.616849+00:00", "nick": "Volis", "message": "scrapy_freelance, 12ejcec910", "links": [], "channel": "scrapy"},
{"date": "2014-06-14T13:45:59.179605+00:00", "nick": "Volis", "message": "Name too?", "links": [], "channel": "scrapy"},
{"date": "2014-06-14T13:46:05.739456+00:00", "nick": "Volis", "message": "scrapy_freelance, ok", "links": [], "channel": "scrapy"},
{"date": "2014-06-14T13:46:56.124813+00:00", "nick": "Volis", "message": "Here is the source of the page that keeps on redirecting: http://pastebin.mozilla.org/5408002", "links": ["http://pastebin.mozilla.org/5408002"], "channel": "scrapy"},
{"date": "2014-06-14T13:47:07.233432+00:00", "nick": "Volis", "message": "it also has the results", "links": [], "channel": "scrapy"},
{"date": "2014-06-14T13:53:21.004864+00:00", "nick": "toothrot", "message": "what in the world is this?", "links": [], "channel": "scrapy"},
{"date": "2014-06-14T13:54:07.196801+00:00", "nick": "Volis", "message": "toothrot, What do you mean?", "links": [], "channel": "scrapy"},
{"date": "2014-06-14T19:10:04.076408+00:00", "nick": "`brain", "message": "after populating some items, I would like to \"go back\" and do something with some of the items, is there a way to iterate over the collection of items?", "links": [], "channel": "scrapy"},
{"date": "2014-06-14T19:10:15.961863+00:00", "nick": "`brain", "message": "is there even such a thing as \"the collection of items\" ?", "links": [], "channel": "scrapy"},
{"date": "2014-06-14T19:11:42.954295+00:00", "nick": "`brain", "message": "for example, i have item[\"number\"] and item[\"url\"] - maybe if item[\"number\"] is odd, I will go and issues a request() to item[\"url\"]", "links": [], "channel": "scrapy"},
{"date": "2014-06-14T19:12:10.134306+00:00", "nick": "`brain", "message": "*must* I do this when I'm populating the items instead of when I have done my first pass?", "links": [], "channel": "scrapy"},
{"date": "2014-06-14T19:34:48.748315+00:00", "nick": "Digenis_", "message": "back where?", "links": [], "channel": "scrapy"},
{"date": "2014-06-14T19:35:02.396061+00:00", "nick": "Digenis_", "message": "do you mean after yielding them?", "links": [], "channel": "scrapy"},
{"date": "2014-06-14T19:35:43.744182+00:00", "nick": "Digenis", "message": "there is no straightforward way", "links": [], "channel": "scrapy"},
{"date": "2014-06-14T19:36:33.916423+00:00", "nick": "Digenis", "message": "if I am not mistaken you want to put all the validation logic (eg about \"odd numbers\") in one place", "links": [], "channel": "scrapy"},
{"date": "2014-06-14T19:39:18.164943+00:00", "nick": "Digenis", "message": "I 'll check later, I 'd like to hear this", "links": [], "channel": "scrapy"},
{"date": "2014-06-14T19:41:31.306967+00:00", "nick": "`brain", "message": "interesting", "links": [], "channel": "scrapy"},
{"date": "2014-06-14T19:41:39.717729+00:00", "nick": "`brain", "message": "maybe I'm too linear a thinker?", "links": [], "channel": "scrapy"},
{"date": "2014-06-14T19:41:57.087852+00:00", "nick": "`brain", "message": "let's pretend that I won't \"know\" about even/oddness until later", "links": [], "channel": "scrapy"},
{"date": "2014-06-14T19:42:19.746600+00:00", "nick": "`brain", "message": "I tried adding this:", "links": [], "channel": "scrapy"},
{"date": "2014-06-14T19:42:21.930350+00:00", "nick": "`brain", "message": "for count,block in enumerate(blocks):", "links": [], "channel": "scrapy"},
{"date": "2014-06-14T19:42:21.932982+00:00", "nick": "`brain", "message": "try:", "links": [], "channel": "scrapy"},
{"date": "2014-06-14T19:42:22.049050+00:00", "nick": "`brain", "message": "item[\"count\"] = count", "links": [], "channel": "scrapy"},
{"date": "2014-06-14T19:42:22.051705+00:00", "nick": "`brain", "message": "item[\"sessionTitle\"] = block.xpath(\".//table[1]/tr/td[2]/text()\").extract()", "links": [], "channel": "scrapy"},
{"date": "2014-06-14T19:42:57.973093+00:00", "nick": "`brain", "message": "but I'm too dumb to know if this will allow me to later say: for x in range(0, count)", "links": [], "channel": "scrapy"},
{"date": "2014-06-14T19:43:35.380862+00:00", "nick": "`brain", "message": "\"print item x's URL\"", "links": [], "channel": "scrapy"},
{"date": "2014-06-14T19:44:01.319099+00:00", "nick": "`brain", "message": "are my items dicts?", "links": [], "channel": "scrapy"},
{"date": "2014-06-14T19:47:15.359697+00:00", "nick": "`brain", "message": "if this were perl, and my datastructure was a hash of hashes", "links": [], "channel": "scrapy"},
{"date": "2014-06-14T19:47:47.090261+00:00", "nick": "`brain", "message": "I would get the hash who's key was my counter, and then get the value who's key was URL", "links": [], "channel": "scrapy"},
{"date": "2014-06-14T19:48:49.272243+00:00", "nick": "`brain", "message": "but in this case,(understanding that I'm new to Python and scrapy), it feels like I'm instantiating Items, populating them, and then \"throwing them away\"", "links": [], "channel": "scrapy"},
{"date": "2014-06-14T19:51:28.297347+00:00", "nick": "`brain", "message": "my use case, is that I'm scraping information recursively, and I want to associate things found on child pages to their parent", "links": [], "channel": "scrapy"},
{"date": "2014-06-14T23:56:25.879162+00:00", "nick": "average", "message": "ananana: stalking you", "links": [], "channel": "scrapy"},
{"date": "2014-06-14T23:56:45.493239+00:00", "nick": "average", "message": "ananana: sup ?", "links": [], "channel": "scrapy"},
{"date": "2014-06-14T23:57:05.685775+00:00", "nick": "ananana", "message": "average: using scrapy right now; do you have any questions about it?", "links": [], "channel": "scrapy"},
{"date": "2014-06-14T23:57:23.007609+00:00", "nick": "average", "message": "ananana: you're excessively on-topic", "links": [], "channel": "scrapy"},
{"date": "2014-06-14T23:58:00.774033+00:00", "nick": "average", "message": "ananana: but yeah, I was thinking about switching some of my code to scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-06-14T23:58:03.101459+00:00", "nick": "average", "message": "not sure yet", "links": [], "channel": "scrapy"},
{"date": "2014-06-14T23:58:03.817888+00:00", "nick": "ananana", "message": "average: this channel is logged so i'm trying to be good", "links": [], "channel": "scrapy"},
{"date": "2014-06-14T23:58:28.009524+00:00", "nick": "ananana", "message": "average: it should say so in the welcome message or something", "links": [], "channel": "scrapy"},
{"date": "2014-06-15T09:01:37.076009+00:00", "nick": "Digenis", "message": "`brain: read the documentation about the meta attribute of the request object", "links": [], "channel": "scrapy"},
{"date": "2014-06-15T09:01:42.766254+00:00", "nick": "Digenis", "message": "if you haven't already", "links": [], "channel": "scrapy"},
{"date": "2014-06-15T09:02:22.621720+00:00", "nick": "Digenis", "message": "you can pass information to each following request through it", "links": [], "channel": "scrapy"},
{"date": "2014-06-15T09:09:31.879337+00:00", "nick": "average", "message": "hey Digenis , what are you working on ?", "links": [], "channel": "scrapy"},
{"date": "2014-06-15T09:10:10.111617+00:00", "nick": "average", "message": "Digenis: u there ?", "links": [], "channel": "scrapy"},
{"date": "2014-06-15T13:03:15.888525+00:00", "nick": "Digenis", "message": "average: what do you mean?", "links": [], "channel": "scrapy"},
{"date": "2014-06-15T13:25:28.454952+00:00", "nick": "techdragon", "message": "Does anyone know how to use an item pipeline with portia? ... and how to use portia with scrapyd ?", "links": [], "channel": "scrapy"},
{"date": "2014-06-15T13:29:23.689807+00:00", "nick": "techdragon", "message": "I can easily use the item pipeline in code i write myself, same with scrapyd... I was really hoping I could use portia in order to whip up quick scrapers without coding. But if portia doesn't work with item_pipelines or scraped then I'll have to write every scraper by hand :-(", "links": [], "channel": "scrapy"},
{"date": "2014-06-15T13:34:28.305749+00:00", "nick": "techdragon", "message": "hrm, ok i think i can see how to use the item_pipeline in portia, I need to fork portia and add it to the defaults which is ok for an item pipeline, but I'm still blocked on how i can use scraped with portia...", "links": [], "channel": "scrapy"},
{"date": "2014-06-15T17:47:39.784293+00:00", "nick": "toothrot", "message": "techdragon, why would you fork? just change the settings", "links": [], "channel": "scrapy"},
{"date": "2014-06-15T21:28:35.995003+00:00", "nick": "geardev", "message": "i want to run a scrapy task using celery every so often, does anyone know of a similar project doing this?", "links": [], "channel": "scrapy"},
{"date": "2014-06-15T21:28:47.154516+00:00", "nick": "geardev", "message": "i'm looking for something to do in the next few hours, and this seems like a feasable project", "links": [], "channel": "scrapy"},
{"date": "2014-06-16T15:14:16.380395+00:00", "nick": "`brain", "message": "Digenis: Thanks - i saw the meta info in the docs.  I think that is related, but not what I want to do.  I found a snippet on stackexchange that looks like it will handle my requirement.  the (pseudo) code was", "links": [], "channel": "scrapy"},
{"date": "2014-06-16T15:14:34.492466+00:00", "nick": "`brain", "message": "itemArray = []", "links": [], "channel": "scrapy"},
{"date": "2014-06-16T15:15:03.818513+00:00", "nick": "`brain", "message": "foreach item in items do", "links": [], "channel": "scrapy"},
{"date": "2014-06-16T15:15:23.300281+00:00", "nick": "`brain", "message": "item[\"URL\"] = something", "links": [], "channel": "scrapy"},
{"date": "2014-06-16T15:15:26.418422+00:00", "nick": "`brain", "message": "...", "links": [], "channel": "scrapy"},
{"date": "2014-06-16T15:15:40.365878+00:00", "nick": "`brain", "message": "itemArray.append(item)", "links": [], "channel": "scrapy"},
{"date": "2014-06-16T15:16:04.768799+00:00", "nick": "`brain", "message": "this will give me a list of item that I (think) I can then step through to do more things with", "links": [], "channel": "scrapy"},
{"date": "2014-06-16T15:16:24.822071+00:00", "nick": "`brain", "message": "this might not be the best way to do things, but it matches my currently limited thinking style", "links": [], "channel": "scrapy"},
{"date": "2014-06-16T22:56:12.662639+00:00", "nick": "tty101", "message": "can anyone take a look at this. it runs on one computer but not another. i thought it's probably an indent error somewhere but everything looks ok https://pastebin.mozilla.org/5420757", "links": ["https://pastebin.mozilla.org/5420757"], "channel": "scrapy"},
{"date": "2014-06-16T22:56:27.517114+00:00", "nick": "tty101", "message": "when i run it, it says: exceptions.UnbloundLocalError: local variable 'site' referenced before assignment", "links": [], "channel": "scrapy"},
{"date": "2014-06-18T16:11:18.934126+00:00", "nick": "km81", "message": "last week, i wrote and tested a scrapy sitemap spider which worked but all the sudden this week it stopped working and is timing out. it doesn't appear that the website has gone through any changes. does anyone know why this might be happening? it seems pretty slow to even read the sitemap as you can see from the timestamps.", "links": [], "channel": "scrapy"},
{"date": "2014-06-18T16:11:21.992482+00:00", "nick": "km81", "message": "https://pastebin.mozilla.org/5430598", "links": ["https://pastebin.mozilla.org/5430598"], "channel": "scrapy"},
{"date": "2014-06-18T18:14:00.917189+00:00", "nick": "km81", "message": "any ideas? i just need to get pointed in the right direction", "links": [], "channel": "scrapy"},
{"date": "2014-06-18T18:33:54.233283+00:00", "nick": "nyov", "message": "km81: well, if your ;) code didn't change, and the environment didn't change (slow internet connection, possible DNS lookup failures) - then *maybe* the website added some anti-bot protection (tarpitting your connections)", "links": [], "channel": "scrapy"},
{"date": "2014-06-18T18:36:11.842180+00:00", "nick": "nyov", "message": "another likely possibility is SitemapSpider's memory usage. with big sitemap lists, it builds up a massive number of requests in-memory before starting a crawl. maybe it just becomes too slow then and times out", "links": [], "channel": "scrapy"},
{"date": "2014-06-18T18:37:00.710045+00:00", "nick": "nyov", "message": "see if you have enough memory. that's all i can say on the topic", "links": [], "channel": "scrapy"},
{"date": "2014-06-19T14:00:59.487772+00:00", "nick": "Roux_taff", "message": "hi there", "links": [], "channel": "scrapy"},
{"date": "2014-06-19T14:01:19.495245+00:00", "nick": "Roux_taff", "message": "I 'm struggling to find a way to run a final callback after the end of a spider's run", "links": [], "channel": "scrapy"},
{"date": "2014-06-19T14:01:28.302524+00:00", "nick": "Roux_taff", "message": "any idea how to do that ?", "links": [], "channel": "scrapy"},
{"date": "2014-06-19T14:01:44.518245+00:00", "nick": "Roux_taff", "message": "defining the __del__ function of the spider maybe?", "links": [], "channel": "scrapy"},
{"date": "2014-06-19T14:15:48.526533+00:00", "nick": "nikolaosk", "message": "no way", "links": [], "channel": "scrapy"},
{"date": "2014-06-19T14:16:02.920388+00:00", "nick": "nikolaosk", "message": "define the spider.close method", "links": [], "channel": "scrapy"},
{"date": "2014-06-19T14:16:21.611843+00:00", "nick": "nikolaosk", "message": "are you sure you want this after the end?", "links": [], "channel": "scrapy"},
{"date": "2014-06-19T14:16:34.549649+00:00", "nick": "Roux_taff", "message": "yes i want to remove a cookie file that was created at the init", "links": [], "channel": "scrapy"},
{"date": "2014-06-19T14:17:19.907891+00:00", "nick": "Roux_taff", "message": "what do you mean by spider.close ? Spider doesn't have such function so I don\"t see how it can be called?", "links": [], "channel": "scrapy"},
{"date": "2014-06-19T14:17:22.029810+00:00", "nick": "nikolaosk", "message": "ok", "links": [], "channel": "scrapy"},
{"date": "2014-06-19T14:17:32.861929+00:00", "nick": "nikolaosk", "message": "well it wasn't documented", "links": [], "channel": "scrapy"},
{"date": "2014-06-19T14:17:47.078347+00:00", "nick": "nikolaosk", "message": "although it's old", "links": [], "channel": "scrapy"},
{"date": "2014-06-19T14:18:02.081437+00:00", "nick": "nikolaosk", "message": "it hooks on the spider_closed signal", "links": [], "channel": "scrapy"},
{"date": "2014-06-19T14:18:06.500249+00:00", "nick": "nikolaosk", "message": "so", "links": [], "channel": "scrapy"},
{"date": "2014-06-19T14:18:11.142357+00:00", "nick": "nikolaosk", "message": "define it", "links": [], "channel": "scrapy"},
{"date": "2014-06-19T14:18:16.932675+00:00", "nick": "nikolaosk", "message": "it's a method", "links": [], "channel": "scrapy"},
{"date": "2014-06-19T14:18:20.283477+00:00", "nick": "nikolaosk", "message": "not a function", "links": [], "channel": "scrapy"},
{"date": "2014-06-19T14:18:29.562323+00:00", "nick": "Roux_taff", "message": "yeah i meant method", "links": [], "channel": "scrapy"},
{"date": "2014-06-19T14:18:37.314295+00:00", "nick": "Roux_taff", "message": "is this coming from object_ref ?", "links": [], "channel": "scrapy"},
{"date": "2014-06-19T14:18:54.482939+00:00", "nick": "Roux_taff", "message": "since there's no such method in  https://github.com/scrapy/scrapy/blob/master/sc...", "links": ["https://github.com/scrapy/scrapy/blob/master/scrapy/spider.py"], "channel": "scrapy"},
{"date": "2014-06-19T14:19:01.245371+00:00", "nick": "nikolaosk", "message": "and do the deletion inside this method", "links": [], "channel": "scrapy"},
{"date": "2014-06-19T14:19:15.532867+00:00", "nick": "nikolaosk", "message": "yes, there isn't any", "links": [], "channel": "scrapy"},
{"date": "2014-06-19T14:19:49.263152+00:00", "nick": "nikolaosk", "message": "scrapy does a getattr(spider, 'closed', None)", "links": [], "channel": "scrapy"},
{"date": "2014-06-19T14:20:34.150926+00:00", "nick": "Roux_taff", "message": "all right thx", "links": [], "channel": "scrapy"},
{"date": "2014-06-19T14:20:40.412973+00:00", "nick": "Roux_taff", "message": "sounds a bit weird indeed :D", "links": [], "channel": "scrapy"},
{"date": "2014-06-19T14:21:00.854812+00:00", "nick": "nikolaosk", "message": "look in the topics in git if you want", "links": [], "channel": "scrapy"},
{"date": "2014-06-19T14:21:08.443409+00:00", "nick": "nikolaosk", "message": "are the recent merged pull requests", "links": [], "channel": "scrapy"},
{"date": "2014-06-19T14:21:24.686641+00:00", "nick": "nikolaosk", "message": "I think I have a link to the line", "links": [], "channel": "scrapy"},
{"date": "2014-06-19T14:22:07.326783+00:00", "nick": "nikolaosk", "message": "no forget it", "links": [], "channel": "scrapy"},
{"date": "2014-06-19T14:22:17.761344+00:00", "nick": "nikolaosk", "message": "found it in the source already", "links": [], "channel": "scrapy"},
{"date": "2014-06-19T14:22:22.713760+00:00", "nick": "nikolaosk", "message": "scrapy.spidermanager", "links": [], "channel": "scrapy"},
{"date": "2014-06-19T14:22:33.919692+00:00", "nick": "nikolaosk", "message": "small file", "links": [], "channel": "scrapy"},
{"date": "2014-06-19T14:22:38.113658+00:00", "nick": "nikolaosk", "message": "last method", "links": [], "channel": "scrapy"},
{"date": "2014-06-19T14:32:23.771527+00:00", "nick": "Roux_taff", "message": "indeed all the way at the bottom", "links": [], "channel": "scrapy"},
{"date": "2014-06-19T14:32:31.841533+00:00", "nick": "Roux_taff", "message": "thanks again, I would never have found it otherwise! :)", "links": [], "channel": "scrapy"},
{"date": "2014-06-19T14:45:55.636722+00:00", "nick": "nikolaosk", "message": "you are welcome", "links": [], "channel": "scrapy"},
{"date": "2014-06-19T14:46:37.521812+00:00", "nick": "nikolaosk", "message": "remember __del__ is not the opposite of __init__", "links": [], "channel": "scrapy"},
{"date": "2014-06-19T15:40:41.545693+00:00", "nick": "odigem", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2014-06-19T15:40:53.996373+00:00", "nick": "odigem", "message": "how to make sel from string?", "links": [], "channel": "scrapy"},
{"date": "2014-06-19T16:03:21.841540+00:00", "nick": "medecau", "message": "odigem: can you refrase that?", "links": [], "channel": "scrapy"},
{"date": "2014-06-19T21:03:03.743252+00:00", "nick": "rodmar", "message": "hey guys", "links": [], "channel": "scrapy"},
{"date": "2014-06-19T21:03:21.118059+00:00", "nick": "rodmar", "message": "I have a simple problem and I am looking for the solution", "links": [], "channel": "scrapy"},
{"date": "2014-06-19T21:03:37.020140+00:00", "nick": "rodmar", "message": "I am trying to scrape information from all members of a forum", "links": [], "channel": "scrapy"},
{"date": "2014-06-19T21:03:58.770130+00:00", "nick": "nixfreak", "message": "Trying to create a spider that searches on keywords instead of categories", "links": [], "channel": "scrapy"},
{"date": "2014-06-19T21:04:14.027093+00:00", "nick": "rodmar", "message": "I know the patter of the url example.forums.com/members/1  example.forums.com/members/2 example.forums.com/members/..", "links": ["http://example.forums.com/members/1", "http://example.forums.com/members/2", "http://example.forums.com/members/."], "channel": "scrapy"},
{"date": "2014-06-19T21:04:15.515536+00:00", "nick": "nixfreak", "message": "so i am using the google webapi", "links": [], "channel": "scrapy"},
{"date": "2014-06-19T21:04:50.699158+00:00", "nick": "rodmar", "message": "and I want to test the links until I reach a 404 meaning I already got all members", "links": [], "channel": "scrapy"},
{"date": "2014-06-19T21:04:56.369526+00:00", "nick": "rodmar", "message": "how to I go about doing that?", "links": [], "channel": "scrapy"},
{"date": "2014-06-19T21:05:40.067796+00:00", "nick": "rodmar", "message": "I need to go to example.forums.com/members/1 save the item and then go to example.forums.com/members/2 until 404", "links": ["http://example.forums.com/members/1", "http://example.forums.com/members/2"], "channel": "scrapy"},
{"date": "2014-06-19T21:06:11.209652+00:00", "nick": "rodmar", "message": "all the solutions I found is to create an item with information from more then one url", "links": [], "channel": "scrapy"},
{"date": "2014-06-19T21:07:37.841143+00:00", "nick": "rodmar", "message": "the problem is that I do not know the last url", "links": [], "channel": "scrapy"},
{"date": "2014-06-19T23:11:20.178121+00:00", "nick": "chad_c", "message": "Hi!  I'm looking for examples of using jsonlinesitemexporter.", "links": [], "channel": "scrapy"},
{"date": "2014-06-19T23:11:50.108812+00:00", "nick": "chad_c", "message": "I'd really like to export items to couchdb", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T14:08:38.910807+00:00", "nick": "Roux_taff", "message": "ahoy", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T14:08:45.876609+00:00", "nick": "Roux_taff", "message": "new question :)", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T14:09:21.259546+00:00", "nick": "baazzilhassan", "message": "?", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T14:09:23.412480+00:00", "nick": "Roux_taff", "message": "Is there a way from within a scrapyd spider to get the exact path of scrapyd's log directory?", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T14:11:43.526768+00:00", "nick": "baazzilhassan", "message": "Roux_taff, in setthings .py you set this variable logs_dir", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T14:11:44.418320+00:00", "nick": "baazzilhassan", "message": ".", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T14:11:46.747628+00:00", "nick": "baazzilhassan", "message": "?", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T14:12:27.227106+00:00", "nick": "Roux_taff", "message": "I'm talking about scrapyd the daemon server", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T14:12:58.118327+00:00", "nick": "Roux_taff", "message": "logs_dir is set globally within /etc/scrapyd/conf.d/", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T14:13:02.468467+00:00", "nick": "Roux_taff", "message": "as logs_dir indeed", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T14:13:18.486819+00:00", "nick": "Roux_taff", "message": "I would like to access this variable from within a spider and am wondering how", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T14:14:19.462733+00:00", "nick": "baazzilhassan", "message": "Roux_taff, self.crawl.settings['logs_dir']", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T14:14:41.523250+00:00", "nick": "Roux_taff", "message": "ah cool thanks will try", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T14:15:28.235648+00:00", "nick": "baazzilhassan", "message": "Roux_taff, Oh sorry I make a typo, self.crawler.settings['logs_dir']", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T14:15:41.633535+00:00", "nick": "baazzilhassan", "message": "crawler not crawl", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T14:15:51.658794+00:00", "nick": "baazzilhassan", "message": "welcome", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T14:16:43.379947+00:00", "nick": "Roux_taff", "message": "yep i fixed it myself :)", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T14:16:53.478766+00:00", "nick": "baazzilhassan", "message": "good :)", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T14:24:34.130429+00:00", "nick": "Roux_taff", "message": "except it is only accessible from start_requests and not already within __init__ but i'll deal with it :)", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T14:36:47.244176+00:00", "nick": "Roux_taff", "message": "mmm doesn't work in fact", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T14:36:52.693563+00:00", "nick": "Roux_taff", "message": "it returns None :/", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T14:37:18.774854+00:00", "nick": "baazzilhassan", "message": "in which call ? start_requests ?", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T14:37:27.826730+00:00", "nick": "Roux_taff", "message": "yep", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T14:37:40.396794+00:00", "nick": "Roux_taff", "message": "crawler.settings returns the settings.py content from my spider's egg", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T14:37:43.983371+00:00", "nick": "Roux_taff", "message": "not the scrapyd config", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T14:48:31.665625+00:00", "nick": "baazzilhassan", "message": "Roux_taff, okey, mybe this:", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T14:48:51.297223+00:00", "nick": "baazzilhassan", "message": "from scrapyd.config import Config", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T14:49:08.775298+00:00", "nick": "baazzilhassan", "message": "and in start_requests", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T14:51:19.457341+00:00", "nick": "baazzilhassan", "message": "Config.get('logs_dir')", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T14:56:22.936289+00:00", "nick": "baazzilhassan", "message": "Roux_taff, ?", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T15:17:10.732936+00:00", "nick": "Roux_taff", "message": "thx baazzilhassan I'm going to try right now", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T15:28:58.341095+00:00", "nick": "Roux_taff", "message": "seems to work!", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T15:29:03.079629+00:00", "nick": "Roux_taff", "message": "thanks a lot again baazzilhassan", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T15:29:38.303017+00:00", "nick": "baazzilhassan", "message": "welcome :)", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T21:25:33.162294+00:00", "nick": "chad_c", "message": "hello, anyone around?", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T21:30:53.828692+00:00", "nick": "crlane", "message": "chad_c: happy to help if you have a question", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T21:31:05.338811+00:00", "nick": "chad_c", "message": "crlane: ! thanks", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T21:31:20.075202+00:00", "nick": "chad_c", "message": "I'm trying to get some data into a db and I'm using jsonlinesexporter", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T21:31:29.971542+00:00", "nick": "chad_c", "message": "I *think* that's what I want to use as I'm trying to stream the data into couchdb", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T21:32:04.716775+00:00", "nick": "crlane", "message": "seems reasonable", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T21:32:06.995981+00:00", "nick": "chad_c", "message": "how can I parse the fields in the output stream?", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T21:32:37.623526+00:00", "nick": "chad_c", "message": "I don't want to dump each line into the database, but I want to parse it into specific fields in my documents", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T21:32:50.202180+00:00", "nick": "chad_c", "message": "does that sounds crazy?", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T21:32:52.987892+00:00", "nick": "chad_c", "message": "*sound", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T21:33:57.091534+00:00", "nick": "crlane", "message": "I don't think so, but it seems at first glance like it might be easier to change the way the Item is stored", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T21:34:07.958571+00:00", "nick": "crlane", "message": "is that not an option?", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T21:34:39.032812+00:00", "nick": "chad_c", "message": "so in other words, just store the entire line in the doc?", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T21:36:05.728702+00:00", "nick": "crlane", "message": "so the jsonlinesexporter will give you one json document for each item your spider retrieves", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T21:36:35.513023+00:00", "nick": "crlane", "message": "if you only want certain fields or want to make sure the fields in the document are stored a certain way, you could just change the way the Items are defined or found", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T21:37:02.793914+00:00", "nick": "chad_c", "message": "so let's just say I take the json doc from jsonlinesexporter and I put that into my couchdb document", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T21:37:24.275629+00:00", "nick": "chad_c", "message": "at this point, I'm just trying to get some morsel of data I've scraped exported somehow", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T21:37:27.028498+00:00", "nick": "chad_c", "message": "http://pastebin.com/nbFizXQi", "links": ["http://pastebin.com/nbFizXQi"], "channel": "scrapy"},
{"date": "2014-06-20T21:38:04.655467+00:00", "nick": "chad_c", "message": "http://pastebin.com/fMHYPN9X", "links": ["http://pastebin.com/fMHYPN9X"], "channel": "scrapy"},
{"date": "2014-06-20T21:38:49.479660+00:00", "nick": "chad_c", "message": "this is all test stuff anyway", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T21:39:11.195506+00:00", "nick": "chad_c", "message": "but I just want to get the 'doc' from jsonlinesexporter into a field of one of my couchdb docs", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T21:40:32.931646+00:00", "nick": "crlane", "message": "oh ok, I see what you mean", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T21:40:35.097942+00:00", "nick": "chad_c", "message": "so take the item from jsonlinesexporter and access the string content", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T21:40:50.122108+00:00", "nick": "chad_c", "message": "if I sound naive, it's because I am, but I'm trying to wrap my head around this", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T21:41:42.095815+00:00", "nick": "chad_c", "message": "the only reason why I'm using couch is because of the http interface and the fact that I don't know how to structure my data yet", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T21:42:04.320282+00:00", "nick": "crlane", "message": "I think I have an idea...just a sec", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T21:42:26.026070+00:00", "nick": "crlane", "message": "yeah, not familiar with couchdb, but it looks a lot like elasticsearch (and probably uses the same Lucene technology under the hood)", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T21:44:12.351419+00:00", "nick": "chad_c", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T21:49:29.951158+00:00", "nick": "crlane", "message": "i think you're going to have to do something a bit more complicated", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T21:49:55.464769+00:00", "nick": "chad_c", "message": "haha, well, I don't think I've done anything complicated just yet", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T21:50:05.428489+00:00", "nick": "chad_c", "message": "just learning how the pipeline works", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T21:50:42.187983+00:00", "nick": "chad_c", "message": "I just thought I could get the info to a database without generating files on the scrapy server", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T21:50:48.729888+00:00", "nick": "chad_c", "message": "seems reasonable", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T21:51:51.852154+00:00", "nick": "crlane", "message": "rather than using the jsonlines exporter, just use the Item itself", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T21:52:29.712460+00:00", "nick": "crlane", "message": "build a dict using the items and json.dumps it", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T21:53:20.312487+00:00", "nick": "chad_c", "message": "ah okay, well that give me some new search terms", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T21:53:26.794159+00:00", "nick": "chad_c", "message": "I'm new to python", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T21:53:28.377232+00:00", "nick": "crlane", "message": "it isn't using the fancy stuff, the exporter and json encoder, but from what I understand it should work for your purpose", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T21:53:33.727894+00:00", "nick": "crlane", "message": "ok, let me give you a code sample", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T21:53:51.964450+00:00", "nick": "chad_c", "message": "and this would still go in my pipelines.py correct?", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T21:54:02.723864+00:00", "nick": "chad_c", "message": "I at least have my db connection info in there, so I would assume this would all be part of the pipeline", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T21:54:19.376294+00:00", "nick": "crlane", "message": "yeah, it should", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T21:55:41.959159+00:00", "nick": "chad_c", "message": "is fields_to_export not a dict?  not to interrupt your thought process", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T21:55:56.238287+00:00", "nick": "chad_c", "message": "oh nm, it's just a list", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T21:59:43.208478+00:00", "nick": "crlane", "message": "did you try just doing it like this?", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T21:59:43.892194+00:00", "nick": "crlane", "message": "http://pastebin.com/mPASiEnG", "links": ["http://pastebin.com/mPASiEnG"], "channel": "scrapy"},
{"date": "2014-06-20T22:00:06.278336+00:00", "nick": "crlane", "message": "I'm not sure if you can access things from the item like that, but it seems you should", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T22:00:15.785113+00:00", "nick": "chad_c", "message": "that was my thought too, but I don't think I had the syntax correct", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T22:00:17.958036+00:00", "nick": "chad_c", "message": "let me try it real quick", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T22:01:11.091534+00:00", "nick": "crlane", "message": "actually that's not going to work either", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T22:02:16.106850+00:00", "nick": "crlane", "message": "forgot some commas and stuff", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T22:02:16.659351+00:00", "nick": "crlane", "message": "http://pastebin.com/510w1KGn", "links": ["http://pastebin.com/510w1KGn"], "channel": "scrapy"},
{"date": "2014-06-20T22:02:35.331355+00:00", "nick": "crlane", "message": "and export_item doesn't actually return the json, it writes it to file", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T22:03:02.337087+00:00", "nick": "chad_c", "message": "gotcha", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T22:03:04.051434+00:00", "nick": "chad_c", "message": "okay let me try this", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T22:04:09.695530+00:00", "nick": "chad_c", "message": "exceptions.TypeError: unhashable type: 'list'", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T22:04:44.024039+00:00", "nick": "scrapy_freelance", "message": "see this article can be helpful", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T22:04:46.090960+00:00", "nick": "scrapy_freelance", "message": "http://blog.jbinfo.io/post/89242990659/how-to-c...", "links": ["http://blog.jbinfo.io/post/89242990659/how-to-create-a-scrapy-csv-exporter-with-a-custom"], "channel": "scrapy"},
{"date": "2014-06-20T22:05:27.498322+00:00", "nick": "crlane", "message": "yeah, it's trying to create a dictionary of something that is stored as a list...without knowing how the item is defined, I don't know", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T22:05:31.964791+00:00", "nick": "chad_c", "message": "yeah", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T22:05:46.752052+00:00", "nick": "crlane", "message": "subclassing an exporter maybe the best option", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T22:05:53.653725+00:00", "nick": "crlane", "message": "sorry I couldn't be of more help", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T22:06:21.525238+00:00", "nick": "chad_c", "message": "no that's fine, this has me thinking about other ways to approach the problem", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T22:06:38.768304+00:00", "nick": "chad_c", "message": "scrapy_freelance: thanks for the suggestion", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T22:06:58.577214+00:00", "nick": "chad_c", "message": "I just don't want to create a file, I just want to take the item, turn it into json, and shove it into a couchdb document", "links": [], "channel": "scrapy"},
{"date": "2014-06-20T22:12:45.932254+00:00", "nick": "chad_c", "message": "I'll figure it out and check back later :)", "links": [], "channel": "scrapy"},
{"date": "2014-06-21T08:59:36.493694+00:00", "nick": "HowardwLo", "message": "Hi, anyone up at this hour? :) if response.meta[\u2018item\u2019] is throwing a weird error, says KeyError, but isnt it suppose to fail silently?", "links": [], "channel": "scrapy"},
{"date": "2014-06-21T09:01:38.764883+00:00", "nick": "HowardwLo", "message": "nvm im high", "links": [], "channel": "scrapy"},
{"date": "2014-06-21T17:26:48.085528+00:00", "nick": "bzzie", "message": "Hi, i have a list of start_urls i want to open and follow via a CrawlSpider. But heres the catch: I do want for each start_url a allowed_domain so that the crawler stays within the site. It might by that urlA is somehow connected to urlB (when i give a whole list of allowed domains the crawler might jump from a to b). Is there a simple way to do so (not creating a bash script looping over a spider with ONE url and ONE domain).", "links": [], "channel": "scrapy"},
{"date": "2014-06-21T17:27:34.987794+00:00", "nick": "bzzie", "message": "I was thinking of putting the Rule() into the start_requests or make_requests_from_url method so that i EACH url gets its own set of rules (with own domain allowed).. is this possible?", "links": [], "channel": "scrapy"},
{"date": "2014-06-21T17:46:49.699336+00:00", "nick": "bzzie", "message": "Here is the idea: http://i.stack.imgur.com/GRK8W.png", "links": ["http://i.stack.imgur.com/GRK8W.png"], "channel": "scrapy"},
{"date": "2014-06-21T17:47:51.437528+00:00", "nick": "bzzie", "message": "refering to http://stackoverflow.com/questions/24316900/cra... (gues the explanation is not so good)", "links": ["http://stackoverflow.com/questions/24316900/crawl-multiple-domains-with-scrapy-without-criss-cross"], "channel": "scrapy"},
{"date": "2014-06-21T19:48:34.292071+00:00", "nick": "Adam_", "message": "Hi I've some troubles with the tutorial -> \"scrapy crawl dmoz\" return me NameError: name 'sel' is not defined", "links": [], "channel": "scrapy"},
{"date": "2014-06-21T19:50:21.970467+00:00", "nick": "Adam_", "message": "I copy-paste the code of the tutorial :/", "links": [], "channel": "scrapy"},
{"date": "2014-06-22T12:54:41.291001+00:00", "nick": "geekforever", "message": "want to scrape all the kindle book links from the below web page  and then visit each of the page to scrape more details...", "links": [], "channel": "scrapy"},
{"date": "2014-06-22T12:54:52.895774+00:00", "nick": "geekforever", "message": "this is the link : http://www.amazon.com/s/ref=sr_pg_3?rh=n%3A1331...", "links": ["http://www.amazon.com/s/ref=sr_pg_3?rh=n%3A133140011%2Cn%3A%21133141011%2Cn%3A154606011%2Cn%3A668010011%2Cn%3A158591011%2Cn%3A158592011&amp=&page=3&amp=&bbn=158591011&amp=&ie=UTF8&amp=&qid=1403264902"], "channel": "scrapy"},
{"date": "2014-06-22T12:55:07.532644+00:00", "nick": "geekforever", "message": "this is the code which I tried :", "links": [], "channel": "scrapy"},
{"date": "2014-06-22T12:55:07.854892+00:00", "nick": "geekforever", "message": "http://pastebin.com/LYAYXYce", "links": ["http://pastebin.com/LYAYXYce"], "channel": "scrapy"},
{"date": "2014-06-22T12:55:27.808761+00:00", "nick": "geekforever", "message": "can anyone help me out?", "links": [], "channel": "scrapy"},
{"date": "2014-06-22T15:32:52.923528+00:00", "nick": "chad_c", "message": "geekforever: are you getting an error or something?  That may help others chime in.  It\u2019s usually pretty empty in here.", "links": [], "channel": "scrapy"},
{"date": "2014-06-22T15:33:12.459146+00:00", "nick": "chad_c", "message": "Amazon scraping is pretty intersting to me", "links": [], "channel": "scrapy"},
{"date": "2014-06-22T15:33:17.746720+00:00", "nick": "chad_c", "message": "*interesting", "links": [], "channel": "scrapy"},
{"date": "2014-06-22T22:41:05.636442+00:00", "nick": "eN_Joy", "message": "general question for opinions: i have dozens of spiders, each requires certain DOM manipulation (spider domain specific) of some field, it's possible to use either a pipleline or item loader processor, which way would you go?", "links": [], "channel": "scrapy"},
{"date": "2014-06-22T22:42:51.913820+00:00", "nick": "Lhassan", "message": "can u explain more I dont understand", "links": [], "channel": "scrapy"},
{"date": "2014-06-22T22:51:53.696475+00:00", "nick": "eN_Joy", "message": "for example, the datetime field that you need to convert string to python (or whatever) datetime object, this comes with so many different format, %Y%M%D:%H....stuff, typically denpends on domain", "links": [], "channel": "scrapy"},
{"date": "2014-06-22T22:53:02.311497+00:00", "nick": "eN_Joy", "message": "you then need to post process after the spider finishes its job, this post processing can be done in either pipeline, or you can use item loader processors to clean them", "links": [], "channel": "scrapy"},
{"date": "2014-06-22T22:54:19.536162+00:00", "nick": "eN_Joy", "message": "another good example is for the content, you may want to pop (delete) some certain child elements, get rid of excessive <br /> tags, etc...", "links": [], "channel": "scrapy"},
{"date": "2014-06-22T22:55:13.029365+00:00", "nick": "eN_Joy", "message": "my question was: would you rather to use pipeline or processor?", "links": [], "channel": "scrapy"},
{"date": "2014-06-22T23:04:13.010675+00:00", "nick": "toothrot", "message": "i'm currently going the 'item loader' route (even though i don't use item loaders, rather something similar of my own creation (from before item loaders existed))", "links": [], "channel": "scrapy"},
{"date": "2014-06-23T01:04:14.180151+00:00", "nick": "chad_c", "message": "hello everyone \u2014 anyone have experience with using pipelines to access specific items?", "links": [], "channel": "scrapy"},
{"date": "2014-06-23T09:53:57.560247+00:00", "nick": "xificurC", "message": "I'm trying to scrape this site http://www.obedovat.sk/denne-menu?latitude=48.1... to learn a bit of scrapy. Each lunch menu is wrapped in a section with class facility. However when I run the scrapy shell on this site and call sel.xpath('//section[@class=\"facility\"]') I get an empty list as a result. What can be the cause? The lower level parts of the section can be retrieved but this doesnt work", "links": ["http://www.obedovat.sk/denne-menu?latitude=48.1405986&amp=&longitude=17.1248325"], "channel": "scrapy"},
{"date": "2014-06-23T11:12:05.013660+00:00", "nick": "xificurC", "message": "found that the class name is \"facility hidden\" and firebug truncated the rest", "links": [], "channel": "scrapy"},
{"date": "2014-06-23T11:13:31.041293+00:00", "nick": "baazzilhassan", "message": "xificurC, Hi, what is your question ?", "links": [], "channel": "scrapy"},
{"date": "2014-06-23T11:14:32.445257+00:00", "nick": "xificurC", "message": "baazzilhassan: nothing anymore, I was wondering why the mentioned xpath call doesn't return anything but an empty list", "links": [], "channel": "scrapy"},
{"date": "2014-06-23T11:14:49.052710+00:00", "nick": "xificurC", "message": "and found out it's because the class name was wrong", "links": [], "channel": "scrapy"},
{"date": "2014-06-23T11:15:02.212898+00:00", "nick": "xificurC", "message": "now im wondering, are class names allowed to contain spaces?", "links": [], "channel": "scrapy"},
{"date": "2014-06-23T11:15:14.328322+00:00", "nick": "xificurC", "message": "cause it seemed firebug truncated it", "links": [], "channel": "scrapy"},
{"date": "2014-06-23T11:15:58.253949+00:00", "nick": "baazzilhassan", "message": "xificurC, no, in HTML and CSS specifications an element can have multiple classes", "links": [], "channel": "scrapy"},
{"date": "2014-06-23T11:16:26.882414+00:00", "nick": "baazzilhassan", "message": "but with xpath in scrapy it's complexe", "links": [], "channel": "scrapy"},
{"date": "2014-06-23T11:16:33.728503+00:00", "nick": "dpn`", "message": "class attributes can have spaces", "links": [], "channel": "scrapy"},
{"date": "2014-06-23T11:16:44.489564+00:00", "nick": "baazzilhassan", "message": "yeh", "links": [], "channel": "scrapy"},
{"date": "2014-06-23T11:16:46.706657+00:00", "nick": "dpn`", "message": "they are generally interpreted as being separate classes on the one element", "links": [], "channel": "scrapy"},
{"date": "2014-06-23T11:17:05.272064+00:00", "nick": "dpn`", "message": "that depends on the parser though", "links": [], "channel": "scrapy"},
{"date": "2014-06-23T11:17:10.105711+00:00", "nick": "baazzilhassan", "message": "<div class=\"facility hidden\"></div>", "links": [], "channel": "scrapy"},
{"date": "2014-06-23T11:17:32.811266+00:00", "nick": "dpn`", "message": "if you use an xml parser on html it likely won't match on a single class if there are multiple classes", "links": [], "channel": "scrapy"},
{"date": "2014-06-23T11:17:49.580145+00:00", "nick": "dpn`", "message": "(don't use an xml parser on html)", "links": [], "channel": "scrapy"},
{"date": "2014-06-23T11:17:50.431172+00:00", "nick": "dpn`", "message": ":D]", "links": [], "channel": "scrapy"},
{"date": "2014-06-23T11:17:51.991901+00:00", "nick": "xificurC", "message": "scrapy matched on \"facility hidden\", didn't match on \"facility\"", "links": [], "channel": "scrapy"},
{"date": "2014-06-23T11:17:54.130854+00:00", "nick": "dpn`", "message": "-]", "links": [], "channel": "scrapy"},
{"date": "2014-06-23T11:18:00.978356+00:00", "nick": "baazzilhassan", "message": "sel.xpath('//div[contains(@class, \"facility\")]')", "links": [], "channel": "scrapy"},
{"date": "2014-06-23T11:18:11.914486+00:00", "nick": "dpn`", "message": "yeah.. what baazzilhassan said", "links": [], "channel": "scrapy"},
{"date": "2014-06-23T11:18:45.891120+00:00", "nick": "xificurC", "message": "baazzilhassan: that returns an empty list too", "links": [], "channel": "scrapy"},
{"date": "2014-06-23T11:19:02.207790+00:00", "nick": "dpn`", "message": "did you use //section ?", "links": [], "channel": "scrapy"},
{"date": "2014-06-23T11:19:06.484857+00:00", "nick": "baazzilhassan", "message": "give me the link to website to test", "links": [], "channel": "scrapy"},
{"date": "2014-06-23T11:19:10.707453+00:00", "nick": "dpn`", "message": "not //dev as baazzilhassan said", "links": [], "channel": "scrapy"},
{"date": "2014-06-23T11:19:14.383921+00:00", "nick": "dpn`", "message": "div*", "links": [], "channel": "scrapy"},
{"date": "2014-06-23T11:19:20.182652+00:00", "nick": "dpn`", "message": "baazzilhassan,", "links": [], "channel": "scrapy"},
{"date": "2014-06-23T11:19:20.815287+00:00", "nick": "dpn`", "message": "<xificurC> I'm trying to scrape this site http://www.obedovat.sk/denne-menu?latitude=48.1... to learn a bit of scrapy. Each lunch menu is wrapped in a section with class facility. However when I run the scrapy shell on this site and call sel.xpath('//section[@class=\"facility\"]') I get an empty list as a result. What can be the cause? The lower level parts of the section can be retrieved but this doesnt work", "links": ["http://www.obedovat.sk/denne-menu?latitude=48.1405986&amp=&longitude=17.1248325"], "channel": "scrapy"},
{"date": "2014-06-23T11:19:24.388195+00:00", "nick": "xificurC", "message": "yeah sorry", "links": [], "channel": "scrapy"},
{"date": "2014-06-23T11:19:29.162825+00:00", "nick": "xificurC", "message": "should be section", "links": [], "channel": "scrapy"},
{"date": "2014-06-23T11:19:29.538685+00:00", "nick": "dpn`", "message": "original question - just before you arrived", "links": [], "channel": "scrapy"},
{"date": "2014-06-23T11:19:33.373490+00:00", "nick": "xificurC", "message": "works with that", "links": [], "channel": "scrapy"},
{"date": "2014-06-23T11:19:59.060319+00:00", "nick": "xificurC", "message": "a lot of sugar in selectors", "links": [], "channel": "scrapy"},
{"date": "2014-06-23T11:20:49.426997+00:00", "nick": "baazzilhassan", "message": "xificurC, Good", "links": [], "channel": "scrapy"},
{"date": "2014-06-23T11:21:49.753241+00:00", "nick": "xificurC", "message": "thanks for the tips so far", "links": [], "channel": "scrapy"},
{"date": "2014-06-23T11:22:44.072368+00:00", "nick": "baazzilhassan", "message": "welcome :)", "links": [], "channel": "scrapy"},
{"date": "2014-06-23T12:31:59.226748+00:00", "nick": "xificurC", "message": "this site im trying to scrape says it uses utf-8 but when I retrieve the data I get e.g. '\u00e1' changed to '\u00c3\u00a1'", "links": [], "channel": "scrapy"},
{"date": "2014-06-23T12:38:10.124128+00:00", "nick": "xificurC", "message": "my bad again, the data is right, excel is stupid :)", "links": [], "channel": "scrapy"},
{"date": "2014-06-23T12:39:59.388901+00:00", "nick": "baazzilhassan", "message": "xificurC, put this code in spider and item files in the begening of", "links": [], "channel": "scrapy"},
{"date": "2014-06-23T12:40:02.019615+00:00", "nick": "baazzilhassan", "message": "# -*- coding: utf-8 -*-", "links": [], "channel": "scrapy"},
{"date": "2014-06-23T19:19:56.234046+00:00", "nick": "noob3", "message": "I'm trying to install scrapy on a 64 bit ec2 ubuntu instance. I have python 2.7.5 and pip 1.5.6 installed, but 'pip install scrapy' exits with errors.", "links": [], "channel": "scrapy"},
{"date": "2014-06-23T19:20:18.221439+00:00", "nick": "noob3", "message": "The error seems to be cffi? Has anyone seen this problem before?", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T05:50:22.475523+00:00", "nick": "HowardwLo", "message": "when you do a -o something.json -t json , is the output in binary?", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T05:50:53.372353+00:00", "nick": "HowardwLo", "message": "\u2026 i guess if its binary you wont be able to understand it?", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T06:35:49.139099+00:00", "nick": "baazzilhassan", "message": "HowardwLo, it's in json format", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T06:36:29.176820+00:00", "nick": "HowardwLo", "message": "baazzilhassan: how about \u2018wb\u2019 or \u2018w\u2019?", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T06:45:25.672819+00:00", "nick": "baazzilhassan", "message": "HowardwLo, what do u mean by wb or w ?", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T06:45:39.262358+00:00", "nick": "HowardwLo", "message": "baazzilhassan: when you do a json dump to a file", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T06:45:49.200997+00:00", "nick": "HowardwLo", "message": "theres a \u2018w\u2019 option or \u2018wb\u2019 option for binary", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T06:45:54.838085+00:00", "nick": "HowardwLo", "message": "im wondering which one i should be using", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T06:46:10.771847+00:00", "nick": "HowardwLo", "message": "i heard windows OS needs wb?", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T06:53:55.991424+00:00", "nick": "baazzilhassan", "message": "HowardwLo, there is no support of benary export", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T06:54:00.431442+00:00", "nick": "baazzilhassan", "message": "see this http://doc.scrapy.org/en/latest/topics/feed-exp...", "links": ["http://doc.scrapy.org/en/latest/topics/feed-exports.html#json"], "channel": "scrapy"},
{"date": "2014-06-24T06:54:12.773166+00:00", "nick": "baazzilhassan", "message": "the only supported format is:", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T06:55:23.448581+00:00", "nick": "baazzilhassan", "message": "JSON, JSON lines, CSV, XML", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T09:12:21.820559+00:00", "nick": "flyingtriangle", "message": "I have a crawlspider and I want to authenticate to a page, then continue as normal scraping that page's links. I created a custom start_requests() function to login, but on the response from the login the crawler stops despite me having rules set up with SgmlLinkExtractor() going to another callback. How do I login then continue crawling pages as it did prior to overriding start_requests()?", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T09:16:20.837286+00:00", "nick": "baazzilhassan", "message": "flyingtriangle, did you a parse method on your spider ?", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T09:16:42.145766+00:00", "nick": "flyingtriangle", "message": "oh I figured it out", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T09:17:21.263913+00:00", "nick": "flyingtriangle", "message": "I had a callback in the options of the Request object that was created post-login", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T09:17:30.063226+00:00", "nick": "flyingtriangle", "message": "I took that out and now I see it's defaulting to the rules", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T09:17:36.927751+00:00", "nick": "baazzilhassan", "message": "in the Rule section set a callback that are differente to parse", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T09:17:44.734175+00:00", "nick": "flyingtriangle", "message": "I'm probably not explaining that very well", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T09:18:05.694850+00:00", "nick": "baazzilhassan", "message": "because scrapy use this internally to parse the response to find links to follow", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T09:18:06.493849+00:00", "nick": "flyingtriangle", "message": "but it's all good now, just had to remove the callback parameter", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T09:18:18.027177+00:00", "nick": "flyingtriangle", "message": "thanks for the fast response baazzilhassan", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T09:18:30.051746+00:00", "nick": "baazzilhassan", "message": "flyingtriangle, good, welcome :)", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T11:01:08.940240+00:00", "nick": "koell", "message": "do u have an example for scrapy putting the result into a mysql database?", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T11:12:48.424520+00:00", "nick": "baazzilhassan", "message": "koell, not but googline for a mysql pipline", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T11:14:33.884691+00:00", "nick": "baazzilhassan", "message": "koell, see this project https://github.com/darkrho/dirbot-mysql", "links": ["https://github.com/darkrho/dirbot-mysql"], "channel": "scrapy"},
{"date": "2014-06-24T11:14:40.668853+00:00", "nick": "koell", "message": "oh pipeline yeah, thats how it is called :)", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T11:15:22.935889+00:00", "nick": "baazzilhassan", "message": "for more infos: googling with scrapy mysql pipline", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T11:15:44.174427+00:00", "nick": "koell", "message": "baazzilhassan: hahaha :D", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T11:16:00.717080+00:00", "nick": "baazzilhassan", "message": ":)", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T13:50:03.753940+00:00", "nick": "search-db", "message": "Hi!", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T13:51:03.939394+00:00", "nick": "search-db", "message": "Windows user here, I've downloaded scrapy 0.22.2 and extracted it, but how do I run this thing exactly? does it require python?", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T13:51:55.814265+00:00", "nick": "baazzilhassan", "message": "search-db, http://doc.scrapy.org/en/latest/intro/install.h...", "links": ["http://doc.scrapy.org/en/latest/intro/install.html#platform-specific-installation-notes"], "channel": "scrapy"},
{"date": "2014-06-24T13:52:23.722325+00:00", "nick": "search-db", "message": "thx.", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T13:52:47.273633+00:00", "nick": "baazzilhassan", "message": "welcome :)", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T15:18:10.673485+00:00", "nick": "fpghost84", "message": "How can I best parse the data this cURL: http://paste.ubuntu.com/7695540/ produces?", "links": ["http://paste.ubuntu.com/7695540/"], "channel": "scrapy"},
{"date": "2014-06-24T15:18:20.561712+00:00", "nick": "fpghost84", "message": "json.loads doesn't seem to like it", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T15:18:30.050508+00:00", "nick": "fpghost84", "message": "it's like a string of nested lists", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T15:18:52.982737+00:00", "nick": "fpghost84", "message": "(the response header  says json though)", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T15:21:18.893624+00:00", "nick": "baazzilhassan", "message": "fpghost84, I dont understand ?", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T15:21:31.652026+00:00", "nick": "baazzilhassan", "message": "where is the data you want to parse", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T15:21:51.786393+00:00", "nick": "fpghost84", "message": "the data is the response from that cURL, I thought it might be easier to give it like that", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T15:22:06.159604+00:00", "nick": "fpghost84", "message": "I can give you a pastebin of data itself though if you prefer?", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T15:24:38.514395+00:00", "nick": "baazzilhassan", "message": "(6) Could not resolve host: betting.pokerheaven.com", "links": ["http://betting.pokerheaven.com"], "channel": "scrapy"},
{"date": "2014-06-24T15:25:16.828178+00:00", "nick": "fpghost84", "message": "The data is here: http://paste.ubuntu.com/7695495/", "links": ["http://paste.ubuntu.com/7695495/"], "channel": "scrapy"},
{"date": "2014-06-24T15:25:56.440937+00:00", "nick": "baazzilhassan", "message": "[[12732,\"Australia - Victorian Cup\",7220010,300011850,0,[[0,\"Full Time\",0,\"2014-06-25T09:30:00.0000000\",[\"1x2\",\"Asians\",\"Over/Under\"],0,[1,1,1],3],[157,\"Draw No Bet\",300,\"2014-06-25T09:30:00.0000000\",[\"1x2\",\"Asians\",\"Over/Under\"],0,0,4],[200,\"Total Goals\",110,\"2014-06-25T09:30:00.0000000\",[\"1x2\",\"Asians\",\"Over/Under\"],0,0,22],[38,\"Odd/Even Match Goals\",740,\"2014-06-25T09:30:00.0000000\",[\"Odd/Even\",null,null],1,0,4],[60,\"Correct Score\",270,\"2014-0", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T15:25:56.457457+00:00", "nick": "baazzilhassan", "message": "6-25T09:30:00.0000000\",[\"Correct Score\",null,null],1,0,0],[61,\"Double Chance\",250,\"2014-06-25T09:30:00.0000000\",[\"Double Chance\",null,null],1,0,0],[62,\"Halftime/Fulltime\",290,\"2014-06-25T09:30:00.0000000\",[\"Halftime/Fulltime\",null,null],1,0,0],[89,\"Half With Most Goals\",880,\"2014-06-25T09:30:00.0000000\",[null,null,null],1,0,0],[119,\"Number Of Goals\",830,\"2014-06-25T09:30:00.0000000\",[null,null,null],1,0,0],[120,\"Number Of Goals 1st Half\",860,\"201", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T15:25:56.829396+00:00", "nick": "baazzilhassan", "message": "4-06-25T09:30:00.0000000\",[null,null,null],1,0,0],[143,\"Three Way Handicap\",380,\"2014-06-25T09:30:00.0000000\",[null,null,null],1,0,0],[144,\"Correct Score 1st Half\",280,\"2014-06-25T09:30:00.0000000\",[null,null,null],1,0,0],[145,\"Double Chance 1st Half\",260,\"2014-06-25T09:30:00.0000000\",[null,null,null],1,0,0],[1,\"1st Half\",200,\"2014-06-25T09:30:00.0000000\",[\"1x2\",\"Asians\",\"Over/Under\"],0,[1,1,1],3]],0,0],[13858,\"Russia - Premier Liga\",41870110,200", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T15:26:01.842801+00:00", "nick": "baazzilhassan", "message": "0910,0,0,0,[[8,\"Outright\",3270,,,1,1,3]]],[12215,\"Ireland - Premier\",28200070,2000920,0,[[0,\"Full Time\",0,\"2014-06-27T18:45:00.0000000\",[\"1x2\",\"Asians\",\"Over/Under\"],0,[1,1,1],3],[157,\"Draw No Bet\",300,\"2014-06-27T18:45:00.0000000\",[\"1x2\",\"Asians\",\"Over/Under\"],0,0,4],[200,\"Total Goals\",110,\"2014-06-27T18:45:00.0000000\",[\"1x2\",\"Asians\",\"Over/Under\"],0,0,22],[7,\"Total Team Goals\",310,\"2014-06-27T18:45:00.0000000\",[\"1x2\",\"Asians\",\"Over/Under\"],0,0,", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T15:26:06.831539+00:00", "nick": "baazzilhassan", "message": "6],[7,\"Total Team Goals\",310,\"2014-06-27T18:45:00.0000000\",[\"1x2\",\"Asians\",\"Over/Under\"],0,0,6],[20,\"Last Team To Score\",360,\"2014-06-27T18:45:00.0000000\",[\"1x2\",\"Asians\",\"Over/Under\"],0,0,9],[65,\"Time Of 1st Goal\",910,\"2014-06-27T18:45:00.0000000\"", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T15:26:56.843200+00:00", "nick": "fpghost84", "message": "yep....", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T15:30:31.079164+00:00", "nick": "fpghost84", "message": "so any ideas? Why does json.loads have issues?", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T15:40:04.122451+00:00", "nick": "fpghost84", "message": "Looks like its the empty \", , , \" sections causing the json issues", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T15:40:13.732520+00:00", "nick": "baazzilhassan", "message": "fpghost84, yeh", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T15:40:15.959270+00:00", "nick": "fpghost84", "message": "(from online json lint site)", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T15:40:27.345121+00:00", "nick": "fpghost84", "message": "hmm...", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T15:40:30.822507+00:00", "nick": "baazzilhassan", "message": "how did you generate this ??", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T15:40:42.585587+00:00", "nick": "fpghost84", "message": "it's by mimicing ajax request to the site", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T15:40:48.870040+00:00", "nick": "fpghost84", "message": "it's the data the server returns", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T15:41:12.540220+00:00", "nick": "baazzilhassan", "message": "are you looking if there is a format parameter's", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T15:41:32.364051+00:00", "nick": "fpghost84", "message": "the response header claims json encoding...", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T15:41:41.180864+00:00", "nick": "baazzilhassan", "message": "that u pass to the server and return response ?", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T15:42:11.347521+00:00", "nick": "fpghost84", "message": "well, the headers that might be relevant seem to be", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T15:42:12.737523+00:00", "nick": "fpghost84", "message": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T15:42:19.561049+00:00", "nick": "fpghost84", "message": "and", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T15:42:20.419427+00:00", "nick": "fpghost84", "message": "Content-Typeapplication/x-www-form-urlencoded", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T15:43:19.401775+00:00", "nick": "baazzilhassan", "message": "but this not json ??", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T15:43:21.593943+00:00", "nick": "fpghost84", "message": "but even when viewing the response in firbug of the ajax request (i.e. the genuine request the site itself makes back to the server) the response is just the same", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T15:43:33.167925+00:00", "nick": "fpghost84", "message": "(i.e. full of ', , , ' sections)", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T15:43:45.821619+00:00", "nick": "fpghost84", "message": "well doesn't seem to be strictly json because of those sections...", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T15:44:11.468584+00:00", "nick": "baazzilhassan", "message": "not that means that it accept this", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T15:44:30.004918+00:00", "nick": "baazzilhassan", "message": "give me the mink to the server ?", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T15:44:34.638875+00:00", "nick": "baazzilhassan", "message": "link*", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T15:44:40.545397+00:00", "nick": "fpghost84", "message": "http://betting.pokerheaven.com/soccer/", "links": ["http://betting.pokerheaven.com/soccer/"], "channel": "scrapy"},
{"date": "2014-06-24T15:45:22.732639+00:00", "nick": "fpghost84", "message": "if you're using Firebug, search the response for something like \"League A\" and you'll quickly come across the request I' replicating", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T15:52:16.519962+00:00", "nick": "baazzilhassan", "message": "fpghost84, this is not a json", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T15:52:30.181590+00:00", "nick": "baazzilhassan", "message": "it's a javascript array", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T15:52:43.742795+00:00", "nick": "fpghost84", "message": "OK", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T15:52:48.960831+00:00", "nick": "baazzilhassan", "message": "if u put it in firebug console and execute it", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T15:52:59.458606+00:00", "nick": "baazzilhassan", "message": "it return a javascript array", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T15:53:11.651969+00:00", "nick": "fpghost84", "message": "I see", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T15:53:20.757080+00:00", "nick": "fpghost84", "message": "is there a nice way to parse these in python/", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T15:53:22.415107+00:00", "nick": "fpghost84", "message": "?", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T16:02:50.487490+00:00", "nick": "baazzilhassan", "message": "fpghost84, see this http://stackoverflow.com/questions/3601864/pyth...", "links": ["http://stackoverflow.com/questions/3601864/python-load-text-as-python-object/3602436#3602436"], "channel": "scrapy"},
{"date": "2014-06-24T16:08:53.235168+00:00", "nick": "kjam", "message": "I have a CrawlSpider Rule question. How do I get the CrawlSpider to queue links that match more than one rule while still allowing for follow=True. Basically, I need to process many paginated results and then yeild those urls (with their continued pagination matches) to process results with a callback.", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T16:26:43.805145+00:00", "nick": "search-db", "message": "which version of python should i get for scrapy? 2.7 or 3.4 ? 3.4 is smaller file size, so would that be okay? and for lmxl the latest is 3.3.5 .. do i get the py2.6 or py2.7 or py3.2 one? thx.", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T17:05:47.783276+00:00", "nick": "search-db", "message": "I require assistance", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T17:05:49.952184+00:00", "nick": "search-db", "message": "is anyone around", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T21:37:28.388346+00:00", "nick": "fpghost84", "message": "Is it possible in scrapy to pass along the item object? Like if I need to build some keys in it from one request, but then  make another request to get the remainder. Is it possible that the item I made in the first request can persist?", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T21:37:52.055300+00:00", "nick": "fpghost84", "message": "(I know I can use meta to pass along some things, but in this case it would not be very practical)", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T21:39:45.032826+00:00", "nick": "fpghost84", "message": "I guess I could just make item outside any class method, and append as I go along?", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T21:40:43.336101+00:00", "nick": "Lhassan", "message": "fpghost84: yeh =, it is possible, pass the item on the meta dict of each request and get it on the response meta dict", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T21:41:21.941505+00:00", "nick": "Lhassan", "message": "Request(url, meta={'key': 'value'})", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T21:41:30.793999+00:00", "nick": "Lhassan", "message": "in the response", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T21:41:43.411169+00:00", "nick": "Lhassan", "message": "response.meta['key']", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T21:44:26.901267+00:00", "nick": "fpghost84", "message": "Lhassan: yeah I knew about the meta way, but actually it doesn't work gre for me in this case. The situation is I scrape a page, grab lots of individual football events (so I have a list of eventname, date, market_id) essentially). Then I next need to make another request for all market_ids in this list the response of which is basically [ (market_id, odds_data), (market_id,odds_data),.....]. So in my final callback I would need all the lis", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T21:44:27.020897+00:00", "nick": "fpghost84", "message": "t of events data to pair up with the market data", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T21:45:19.530510+00:00", "nick": "fpghost84", "message": "(the request for odds data is made for all market ids at once i.e. with a GETstr like 123213&21312321&2321312&2321312)", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T21:45:48.829039+00:00", "nick": "fpghost84", "message": "passing everything in the meta key could be done I guess, but it would mean the meta data is pretty large", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T21:46:16.113304+00:00", "nick": "Lhassan", "message": "okey, yeh u are right", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T21:46:20.527063+00:00", "nick": "fpghost84", "message": "can I not just give the spider class an items =[] attribute?", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T21:46:33.434554+00:00", "nick": "fpghost84", "message": "then in my first request throw some half built items into it", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T21:46:46.744868+00:00", "nick": "fpghost84", "message": "and in my second request, retrive them by id, and continue building?", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T21:47:00.862882+00:00", "nick": "Lhassan", "message": "did you have a start_requests method on your spider", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T21:47:04.422760+00:00", "nick": "fpghost84", "message": "yeah", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T21:47:35.841456+00:00", "nick": "Lhassan", "message": "you know what", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T21:48:10.621100+00:00", "nick": "Lhassan", "message": "your are right the dict can be large, but it keep things organized", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T21:48:21.145211+00:00", "nick": "Lhassan", "message": "so I prefer the dict method", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T21:49:03.221413+00:00", "nick": "fpghost84", "message": "maybe...", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T21:50:15.287128+00:00", "nick": "Lhassan", "message": "because if you put it in start_requests", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T21:50:56.092511+00:00", "nick": "Lhassan", "message": "you have to initialize the item in start_request and in the method when you return the item", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T21:51:16.081411+00:00", "nick": "Lhassan", "message": "you know two differente places", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T21:51:51.203626+00:00", "nick": "fpghost84", "message": "actually I should mention that start requests is even further back in my requests tree...I just spoke of my last two requests to try and simplify above. So item wouldn't need to be init in start_requests", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T21:52:06.800947+00:00", "nick": "fpghost84", "message": "but yeah, maybe this dict thing is simpler even if it ends up quite big...", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T21:54:29.767390+00:00", "nick": "fpghost84", "message": "it can be a nested dictionary right?", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T21:56:05.418623+00:00", "nick": "Lhassan", "message": "yeh", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T21:56:14.634313+00:00", "nick": "Lhassan", "message": "I will send an example", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T21:56:33.451517+00:00", "nick": "fpghost84", "message": "thanks alot", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T21:57:17.206574+00:00", "nick": "Lhassan", "message": "http://pastebin.mozilla.org/5404741", "links": ["http://pastebin.mozilla.org/5404741"], "channel": "scrapy"},
{"date": "2014-06-24T21:57:33.809759+00:00", "nick": "Lhassan", "message": "welcome :)", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T21:58:18.956808+00:00", "nick": "fpghost84", "message": "I'll give this a go...", "links": [], "channel": "scrapy"},
{"date": "2014-06-24T22:11:21.869307+00:00", "nick": "Lhassan", "message": "fpghost84: good", "links": [], "channel": "scrapy"},
{"date": "2014-06-25T20:08:44.308677+00:00", "nick": "chad_c", "message": "hello all \u2014 how can I pass a dict item (my url) to another parse method?", "links": [], "channel": "scrapy"},
{"date": "2014-06-25T20:08:59.890804+00:00", "nick": "chad_c", "message": "I scrape an index page to get URLs based on a series of xpath rules", "links": [], "channel": "scrapy"},
{"date": "2014-06-25T20:09:21.871380+00:00", "nick": "chad_c", "message": "now I want to go visit that url and scrape the page in another method", "links": [], "channel": "scrapy"},
{"date": "2014-06-25T20:20:50.784894+00:00", "nick": "`brain", "message": "the generic approach is to handle the \"first level\" collection in your parse handler, and then pass the results in subsequent callbacks to another handler", "links": [], "channel": "scrapy"},
{"date": "2014-06-25T20:25:28.994477+00:00", "nick": "chad_c", "message": "`brain: so that\u2019s how I have it now, I\u2019m just unsure of the syntax", "links": [], "channel": "scrapy"},
{"date": "2014-06-25T20:25:51.670111+00:00", "nick": "chad_c", "message": "so I have an items collection and I\u2019m populating it based on xpath info", "links": [], "channel": "scrapy"},
{"date": "2014-06-25T20:25:58.606072+00:00", "nick": "chad_c", "message": "one of those items is the URL", "links": [], "channel": "scrapy"},
{"date": "2014-06-25T20:26:25.808569+00:00", "nick": "chad_c", "message": "so, python-noob question, how do I pass an item in that collection in the correct format to an additional parse method?", "links": [], "channel": "scrapy"},
{"date": "2014-06-25T20:27:22.106654+00:00", "nick": "chad_c", "message": "right now I get an exception even doing a json.dumps(item[\u2018myurl\u2019]) to another method: exceptions.ValueError: Missing scheme in request url: %5B%5D", "links": [], "channel": "scrapy"},
{"date": "2014-06-25T20:27:34.221512+00:00", "nick": "chad_c", "message": "so I\u2019m ASSuming the string is not formatted properly", "links": [], "channel": "scrapy"},
{"date": "2014-06-25T20:28:19.745596+00:00", "nick": "chad_c", "message": "\u2026.and pastebin is down", "links": [], "channel": "scrapy"},
{"date": "2014-06-25T20:28:23.607406+00:00", "nick": "chad_c", "message": "damn you free service!", "links": [], "channel": "scrapy"},
{"date": "2014-06-25T20:28:49.417724+00:00", "nick": "`brain", "message": "well, in your first parse(self,response) you will do something that says \"if I'm dealing with a link, call another parsing function\": yield Request(myItem'sLink, callback=parse_scraped_links)", "links": [], "channel": "scrapy"},
{"date": "2014-06-25T20:29:25.908757+00:00", "nick": "`brain", "message": "and then in parse_scraped_links, define what to do with the results of the link - there are good examples on SE:", "links": [], "channel": "scrapy"},
{"date": "2014-06-25T20:29:34.550520+00:00", "nick": "`brain", "message": "http://stackoverflow.com/questions/12397259/scr...", "links": ["http://stackoverflow.com/questions/12397259/scrapy-recursive-crawling-with-different-xpathselector"], "channel": "scrapy"},
{"date": "2014-06-25T20:30:25.941913+00:00", "nick": "chad_c", "message": "ah okay let me take a look at that example \u2014 I couldn\u2019t put in the correct search terms!", "links": [], "channel": "scrapy"},
{"date": "2014-06-25T20:30:30.647398+00:00", "nick": "chad_c", "message": "thanks `brain", "links": [], "channel": "scrapy"},
{"date": "2014-06-25T20:31:04.515069+00:00", "nick": "`brain", "message": "np - I too am a n00b, and have been working on similar projects, and am learning only enough python to get by as I go", "links": [], "channel": "scrapy"},
{"date": "2014-06-25T20:31:28.848924+00:00", "nick": "chad_c", "message": "^^ yeah that\u2019s my story so far", "links": [], "channel": "scrapy"},
{"date": "2014-06-25T20:32:11.736188+00:00", "nick": "chad_c", "message": "everytime I get a little bit of something to work though, that feeling haha.  I just realize after my first working iteration that I need to take the index I\u2019ve made and browse each url in that index for more info.  So then I broke everything in the process of trying to do that.", "links": [], "channel": "scrapy"},
{"date": "2014-06-26T15:15:04.767775+00:00", "nick": "leechy", "message": "Does anyone have any experience scraping from a website with NTLM authentication? I was able to scrape using httpNtlmAuth and requests but I would like to test out Scrapy.", "links": [], "channel": "scrapy"},
{"date": "2014-06-26T15:55:12.675405+00:00", "nick": "nikolaosk", "message": "didn't want to spam in github but dangra +1 for #764", "links": [], "channel": "scrapy"},
{"date": "2014-06-26T15:55:54.837786+00:00", "nick": "dangra", "message": "hey, thanks nikolaosk", "links": [], "channel": "scrapy"},
{"date": "2014-06-26T17:52:06.745438+00:00", "nick": "Madhusudhan", "message": "Hi Everybody, I am new to the web crawler and looking to the basics how the web crawler works in python. Can anyone help me in getting the right docs,so I can try to code it", "links": [], "channel": "scrapy"},
{"date": "2014-06-26T18:06:11.505579+00:00", "nick": "nyov", "message": "http://doc.scrapy.org", "links": ["http://doc.scrapy.org"], "channel": "scrapy"},
{"date": "2014-06-27T00:16:31.889757+00:00", "nick": "chad_c", "message": "anyone around to help out with passing parameters to an additional parse function?  I\u2019m been researching for days and I think I\u2019m running around in circles.", "links": [], "channel": "scrapy"},
{"date": "2014-06-27T02:24:04.646846+00:00", "nick": "nyov", "message": "chad_c: just add parameters to it? def additional_parse(self, response, mydata)", "links": [], "channel": "scrapy"},
{"date": "2014-06-27T04:02:17.687924+00:00", "nick": "Pheronius", "message": "hey guys -- I am absolutely stumped on trying navigate multiple pages", "links": [], "channel": "scrapy"},
{"date": "2014-06-27T04:02:37.849363+00:00", "nick": "Pheronius", "message": "clearly, the way that I'm thinking about it is off", "links": [], "channel": "scrapy"},
{"date": "2014-06-27T04:04:17.050224+00:00", "nick": "Pheronius", "message": "so, as an example, I'm trying to scrape text from imsdb.com", "links": ["http://imsdb.com"], "channel": "scrapy"},
{"date": "2014-06-27T04:07:05.849795+00:00", "nick": "Pheronius", "message": "this is what I'm working with at the moment: http://pastebin.com/74YqSZnk", "links": ["http://pastebin.com/74YqSZnk"], "channel": "scrapy"},
{"date": "2014-06-27T04:08:21.844178+00:00", "nick": "Pheronius", "message": "basically, I can get a list of urls from the top page, but I've tried dozens of different things (defining a second part, e.g., parse_2, and using a yield Request, etc.) and little tweaks to everything that I've tried, and I can't really figure out how to get it to navigate to the urls that it pulls from the start page", "links": [], "channel": "scrapy"},
{"date": "2014-06-27T04:27:12.866549+00:00", "nick": "nyov", "message": "Pheronius: you yield a Request or an Item inside a parse method or return a list of them, which returns things to the twisted reactor loop.", "links": [], "channel": "scrapy"},
{"date": "2014-06-27T04:27:51.047436+00:00", "nick": "nyov", "message": "returned requests will then be handled again by the downloader, which hands responses to your defined parse method again", "links": [], "channel": "scrapy"},
{"date": "2014-06-27T04:29:57.048367+00:00", "nick": "nyov", "message": "also, you don't sleep() like this in twisted. that'll freeze the whole reactor, and potentially time out your running downloads/connections. this is twisted and you'll have to learn it's twisted async way of doing things", "links": [], "channel": "scrapy"},
{"date": "2014-06-27T04:30:55.165194+00:00", "nick": "nyov", "message": "which means you can't do this \"response = Request(url=LinkFullString)\" thing. because it will actually be \"request = Request(url=LinkFullString)\"", "links": [], "channel": "scrapy"},
{"date": "2014-06-27T04:31:27.048095+00:00", "nick": "nyov", "message": "(then you do \"yield request\" next)", "links": [], "channel": "scrapy"},
{"date": "2014-06-27T04:32:32.595616+00:00", "nick": "nyov", "message": "it may help to think of parse() as a recursive function there", "links": [], "channel": "scrapy"},
{"date": "2014-06-27T04:44:19.375428+00:00", "nick": "Pheronius", "message": "thanks nyov", "links": [], "channel": "scrapy"},
{"date": "2014-06-27T04:44:29.299856+00:00", "nick": "Pheronius", "message": "that makes a bit more sense", "links": [], "channel": "scrapy"},
{"date": "2014-06-27T04:45:02.664806+00:00", "nick": "Pheronius", "message": "I think that in my head, I've been trying to code things in a nested fashion that kind of maps onto the nested structure of the site itself", "links": [], "channel": "scrapy"},
{"date": "2014-06-27T04:54:55.734527+00:00", "nick": "nyov", "message": "Pheronius: no problem to do that. you just yield requests (inside a loop) or return them, with a different callback method", "links": [], "channel": "scrapy"},
{"date": "2014-06-27T04:55:19.915225+00:00", "nick": "nyov", "message": "yield Request(url=somescript, callback=parse_scripts)", "links": [], "channel": "scrapy"},
{"date": "2014-06-27T04:56:16.043696+00:00", "nick": "nyov", "message": "and for constructing absolute url's correctly, you may want to consider using urljoin, from the urlparse package", "links": [], "channel": "scrapy"},
{"date": "2014-06-27T04:56:50.675510+00:00", "nick": "nyov", "message": "from urlparse import urljoin", "links": [], "channel": "scrapy"},
{"date": "2014-06-27T04:57:16.298092+00:00", "nick": "nyov", "message": "yield Request(urljoin(response.url, my_url_here), callback)", "links": [], "channel": "scrapy"},
{"date": "2014-06-27T15:30:56.358173+00:00", "nick": "fpghost84", "message": "Is is possible with scrapy xpath to use nth-child (or otherwise) to select all but the first table row?", "links": [], "channel": "scrapy"},
{"date": "2014-06-27T15:34:47.740695+00:00", "nick": "csalazar", "message": "position()>1", "links": [], "channel": "scrapy"},
{"date": "2014-06-27T15:44:44.152708+00:00", "nick": "fpghost84", "message": "csalazar: thanks!", "links": [], "channel": "scrapy"},
{"date": "2014-06-27T19:26:51.651255+00:00", "nick": "Raji", "message": "Hi", "links": [], "channel": "scrapy"},
{"date": "2014-06-27T19:27:09.503871+00:00", "nick": "Raji", "message": "I need help in scraping", "links": [], "channel": "scrapy"},
{"date": "2014-06-27T19:27:35.439096+00:00", "nick": "Raji", "message": "Is anybody there to help me?", "links": [], "channel": "scrapy"},
{"date": "2014-06-27T19:35:24.699113+00:00", "nick": "Raji", "message": "Form submit using firefox works fine but the same form submit through scrapy is not working.", "links": [], "channel": "scrapy"},
{"date": "2014-06-27T19:35:29.164844+00:00", "nick": "Raji", "message": "I got the HTTP header form firefox submit using POST.", "links": [], "channel": "scrapy"},
{"date": "2014-06-27T19:35:32.425777+00:00", "nick": "Raji", "message": "Is ther a way to get the HTTP header of the submitted from through scrapy?", "links": [], "channel": "scrapy"},
{"date": "2014-06-27T19:45:10.471642+00:00", "nick": "Raji", "message": "Hi", "links": [], "channel": "scrapy"},
{"date": "2014-06-27T20:04:18.298837+00:00", "nick": "koell", "message": "=)", "links": [], "channel": "scrapy"},
{"date": "2014-06-27T21:03:36.218204+00:00", "nick": "Pheronius", "message": "nyov: Thanks again for the help yesterday. I've got things working a bit better now, although I am stumped yet again", "links": [], "channel": "scrapy"},
{"date": "2014-06-27T21:03:51.189245+00:00", "nick": "Pheronius", "message": "I'm sure that it's something obvious that I'm overlooking, but I can't figure out where the failure is here", "links": [], "channel": "scrapy"},
{"date": "2014-06-27T21:04:09.761222+00:00", "nick": "Pheronius", "message": "So, this is following the nested \"yield Request\" thing that was brought up yesterday", "links": [], "channel": "scrapy"},
{"date": "2014-06-27T21:04:31.007962+00:00", "nick": "Pheronius", "message": "My second yield never calls, and I can't figure out why", "links": [], "channel": "scrapy"},
{"date": "2014-06-27T21:04:31.588726+00:00", "nick": "Pheronius", "message": "http://pastebin.com/iZv2RNqD", "links": ["http://pastebin.com/iZv2RNqD"], "channel": "scrapy"},
{"date": "2014-06-27T21:04:46.483292+00:00", "nick": "Pheronius", "message": "I've put a handful of print statements in to test it along the way", "links": [], "channel": "scrapy"},
{"date": "2014-06-27T21:05:41.426740+00:00", "nick": "Pheronius", "message": "the first yield comes out fine and through the syntax builds a legit url to yield to a third callback (parse_script)", "links": [], "channel": "scrapy"},
{"date": "2014-06-27T21:05:55.528624+00:00", "nick": "Pheronius", "message": "however, it seems to always just skip right over it, no matter what I've tried", "links": [], "channel": "scrapy"},
{"date": "2014-06-28T00:26:43.691547+00:00", "nick": "yhager", "message": "Pheronius: I am not sure it causes your problem, but don't overwrite 'parse' when using a CrawlSpider. I think in your case you need to derive from Spider instead", "links": [], "channel": "scrapy"},
{"date": "2014-06-28T00:29:41.611213+00:00", "nick": "yhager", "message": "Pheronius: Oh, I know why it doesn't call parse_script. You're trying to use the same url twice, and it's being removed by the duplicate url filter.", "links": [], "channel": "scrapy"},
{"date": "2014-06-28T00:30:09.082364+00:00", "nick": "yhager", "message": "Pheronius: add dont_filter=True to the Request you yield in parse_lvl_2, if that's what you want to do.", "links": [], "channel": "scrapy"},
{"date": "2014-06-28T00:32:56.843833+00:00", "nick": "yhager", "message": "actually, no, it's not the same url. I am not sure why it's not working for you. would be simpler to just use a crawlspider, and define a rule with allow=('/scripts') that calls your callback.", "links": [], "channel": "scrapy"},
{"date": "2014-06-28T01:11:57.628084+00:00", "nick": "Pheronius", "message": "yhager: thanks for the feedback", "links": [], "channel": "scrapy"},
{"date": "2014-06-28T01:12:27.857396+00:00", "nick": "Pheronius", "message": "I'm going to be moving in the rule direction sooner or later, but I'm absolutely baffled by why this isn't working", "links": [], "channel": "scrapy"},
{"date": "2014-06-28T01:12:55.631988+00:00", "nick": "Pheronius", "message": "I haven't actually tinkered with the rules yet -- from a quick glance, I couldn't quite figure out what the format of them was. Is it regex?", "links": [], "channel": "scrapy"},
{"date": "2014-06-28T01:16:04.595145+00:00", "nick": "Pheronius", "message": "hmm.. yeah... definitely regex", "links": [], "channel": "scrapy"},
{"date": "2014-06-28T01:38:57.843306+00:00", "nick": "nyov", "message": "RTFM. seriously, read the docs. or the source, it's just python anyway, then it'll be very obvious why it doesn't work", "links": [], "channel": "scrapy"},
{"date": "2014-06-28T01:39:29.317672+00:00", "nick": "nyov", "message": "Pheronius: 1. CrawlSpider works on Rules, you have no rules defined", "links": [], "channel": "scrapy"},
{"date": "2014-06-28T01:39:38.559161+00:00", "nick": "nyov", "message": "Pheronius: 1. CrawlSpider works on parse, you override parse", "links": [], "channel": "scrapy"},
{"date": "2014-06-28T01:39:44.409692+00:00", "nick": "nyov", "message": "*2nd", "links": [], "channel": "scrapy"},
{"date": "2014-06-28T01:41:37.075059+00:00", "nick": "nyov", "message": "3. yield ____ is useless, when ____ is undefined. use \"pass\" or just leave it", "links": [], "channel": "scrapy"},
{"date": "2014-06-28T01:41:55.099304+00:00", "nick": "nyov", "message": "(leave it out, that is)", "links": [], "channel": "scrapy"},
{"date": "2014-06-28T01:41:57.864686+00:00", "nick": "Pheronius", "message": "That's nice that you assume that I've *not* been reading the docs incessantly. I don't typically develop in python, and since I'm not familiar with any of the packages that scrapy rides on, it seemed like it would be worthwhile to... you know... *ask* someone about it. Which, as the docs state, this is the place to get help", "links": [], "channel": "scrapy"},
{"date": "2014-06-28T01:42:03.631783+00:00", "nick": "Pheronius", "message": "and the _____ is just a placeholder", "links": [], "channel": "scrapy"},
{"date": "2014-06-28T01:42:08.158544+00:00", "nick": "Pheronius", "message": "so, thanks for your input", "links": [], "channel": "scrapy"},
{"date": "2014-06-28T01:44:07.968164+00:00", "nick": "yhager", "message": "Pheronius: at least try not to to overwrite 'parse', or not to use CrawlSpider.", "links": [], "channel": "scrapy"},
{"date": "2014-06-28T01:44:12.513921+00:00", "nick": "nyov", "message": "sorry if this sounded rude. a lot of people have this issue with CrawlSpider the first time, after all.", "links": [], "channel": "scrapy"},
{"date": "2014-06-28T01:45:00.622944+00:00", "nick": "nyov", "message": "but if you had read the docs for CrawlSpider, you should have read the big fat  warning here http://doc.scrapy.org/en/latest/topics/spiders....", "links": ["http://doc.scrapy.org/en/latest/topics/spiders.html?highlight=crawlspider#crawling-rules"], "channel": "scrapy"},
{"date": "2014-06-28T01:45:47.463513+00:00", "nick": "nyov", "message": "you must at least call the parent class to keep parse working, using super()", "links": [], "channel": "scrapy"},
{"date": "2014-06-28T01:46:14.135529+00:00", "nick": "Pheronius", "message": "yhager: I had originally used the basic Spider and wasn't getting any depth to it. I had swapped to CrawlSpider with some rules as an attempt to see how far I could get, and it went a bit deeper -- I must have spaced on swapping back to the higher-order crawler when I pastebinned it", "links": [], "channel": "scrapy"},
{"date": "2014-06-28T01:46:40.561820+00:00", "nick": "yhager", "message": "so it doesn't work with a regular spider either?", "links": [], "channel": "scrapy"},
{"date": "2014-06-28T01:47:14.374350+00:00", "nick": "Pheronius", "message": "Nope. Although I assume that in that earlier version, something that I wrote borked up", "links": [], "channel": "scrapy"},
{"date": "2014-06-28T01:47:44.186906+00:00", "nick": "Pheronius", "message": "The version that I slapped on pastebin is a bit different than what I had done with the regular spider after a ton of attempted tweaks", "links": [], "channel": "scrapy"},
{"date": "2014-06-28T01:48:01.469006+00:00", "nick": "Pheronius", "message": "that's my fault for leaving it that way when I pastebinned it", "links": [], "channel": "scrapy"},
{"date": "2014-06-28T01:49:18.196911+00:00", "nick": "Pheronius", "message": "also, version 0.22.2, if that helps", "links": [], "channel": "scrapy"},
{"date": "2014-06-28T01:51:14.330755+00:00", "nick": "nyov", "message": "22.2+ also has scrapy.selector.Selector instead of scrapy.selector.HtmlXPathSelector. though both still work in that version", "links": [], "channel": "scrapy"},
{"date": "2014-06-28T01:51:40.288833+00:00", "nick": "Pheronius", "message": "yeah, I had tried swapping between the two and didn't see any difference in functionality", "links": [], "channel": "scrapy"},
{"date": "2014-06-28T01:51:44.907657+00:00", "nick": "nyov", "message": "(so you can remove the old one)", "links": [], "channel": "scrapy"},
{"date": "2014-06-28T01:51:59.351041+00:00", "nick": "nyov", "message": "it's just a rename", "links": [], "channel": "scrapy"},
{"date": "2014-06-28T01:52:06.332884+00:00", "nick": "nyov", "message": "mostly", "links": [], "channel": "scrapy"},
{"date": "2014-06-28T01:53:08.666018+00:00", "nick": "nyov", "message": "anyway, the code looks good. except for the manual url-stiching-together. which may break, if some of the site URLs are absolute", "links": [], "channel": "scrapy"},
{"date": "2014-06-28T01:53:32.174466+00:00", "nick": "nyov", "message": "you may still want to consider using \"urljoin\" or something similar", "links": [], "channel": "scrapy"},
{"date": "2014-06-28T01:54:20.378839+00:00", "nick": "Pheronius", "message": "yeah, I still need to grab urlparse", "links": [], "channel": "scrapy"},
{"date": "2014-06-28T05:39:33.040466+00:00", "nick": "Raji", "message": "Hi", "links": [], "channel": "scrapy"},
{"date": "2014-06-28T05:40:12.194601+00:00", "nick": "Raji", "message": "Can u help to sort out my issue?", "links": [], "channel": "scrapy"},
{"date": "2014-06-28T05:40:28.723001+00:00", "nick": "Raji", "message": "anybody is there??", "links": [], "channel": "scrapy"},
{"date": "2014-06-28T05:48:45.490619+00:00", "nick": "Raji", "message": "Form submit using firefox works fine but the same form submit through scrapy is not working.", "links": [], "channel": "scrapy"},
{"date": "2014-06-28T05:49:17.351088+00:00", "nick": "Raji", "message": "I got the HTTP header from firefox submit using POST", "links": [], "channel": "scrapy"},
{"date": "2014-06-28T05:50:10.421032+00:00", "nick": "Raji", "message": "Is ther a way to get the HTTP header of the submitted form through scrapy?", "links": [], "channel": "scrapy"},
{"date": "2014-06-28T06:16:47.155556+00:00", "nick": "nyov", "message": "Raji: you want the HTTP headers of a POST request? client or server?", "links": [], "channel": "scrapy"},
{"date": "2014-06-28T06:18:48.208273+00:00", "nick": "nyov", "message": "well, the server's should be in response.headers and the client's in response.request.headers", "links": [], "channel": "scrapy"},
{"date": "2014-06-28T06:19:30.628801+00:00", "nick": "Raji", "message": "Ok", "links": [], "channel": "scrapy"},
{"date": "2014-06-28T17:31:38.456265+00:00", "nick": "fpghost84", "message": "Hi, I'm trying to scrape this site: https://www.bet-at-home.com/en/sport Upon choosing a league on left hand side, a POST req is made, which I tried to replicate. The problem is even doing \"copy as cURL\" in firebug and trying to cURL freezes up and eventually gives SSL read error. Would anyone know what the problem is?", "links": ["https://www.bet-at-home.com/en/sport"], "channel": "scrapy"},
{"date": "2014-06-28T17:36:03.434110+00:00", "nick": "yasoob", "message": "hi!", "links": [], "channel": "scrapy"},
{"date": "2014-06-28T17:39:50.019732+00:00", "nick": "yasoob", "message": "hello?", "links": [], "channel": "scrapy"},
{"date": "2014-06-28T17:44:39.377510+00:00", "nick": "fpghost84", "message": "Hi", "links": [], "channel": "scrapy"},
{"date": "2014-06-28T17:49:01.172852+00:00", "nick": "yasoob", "message": "how many users are here? O.o", "links": [], "channel": "scrapy"},
{"date": "2014-06-28T17:54:02.059536+00:00", "nick": "nyov", "message": "unless you're blind, your irc client should tell you", "links": [], "channel": "scrapy"},
{"date": "2014-06-28T18:42:02.113226+00:00", "nick": "fpghost84", "message": "After making a duplicate request in the scrapy shell, and getting the filtered msg, is there anyway to quite without killing the terminal? (it seems ctrl+d or c has no effect)", "links": [], "channel": "scrapy"},
{"date": "2014-06-29T05:49:33.738363+00:00", "nick": "dokma", "message": "I'd like to create and maintain a database of all .hr websites. Is scrapy the right tool?", "links": [], "channel": "scrapy"},
{"date": "2014-06-29T07:35:24.785685+00:00", "nick": "Rok", "message": "Hi! I'm new to linux, python & scrapy. Just installed ubuntu 14.04 & python 2.7.6 is the current version. I am trying to install scrapy and it says \"unable to locate packgae scrapy-0.24\"", "links": [], "channel": "scrapy"},
{"date": "2014-06-29T07:35:33.638401+00:00", "nick": "Rok", "message": "can anyone help?", "links": [], "channel": "scrapy"},
{"date": "2014-06-30T00:31:50.736580+00:00", "nick": "Doginal", "message": "hey, i am trying to run scrapy on my mac 10.9.3, i keep getting trace back errors when i run the bench command. not sure if i should rm python and try installing again or what i should do. heres the tracebacks http://pastebin.com/V0THagrW", "links": ["http://pastebin.com/V0THagrW"], "channel": "scrapy"},
{"date": "2014-06-30T07:40:43.495849+00:00", "nick": "BADOS", "message": "Hey :)", "links": [], "channel": "scrapy"},
{"date": "2014-06-30T07:40:45.489541+00:00", "nick": "BADOS", "message": "Anyone here?", "links": [], "channel": "scrapy"},
{"date": "2014-06-30T07:53:02.827281+00:00", "nick": "nkuttler", "message": "i have a spider that works fine for single pages, but i need to recurse the multi page archive index that links to the singles. what's the best practice for this, does somebody have an example?", "links": [], "channel": "scrapy"},
{"date": "2014-06-30T07:56:00.054668+00:00", "nick": "nkuttler", "message": "i mean, should i just write another spider for the index, and pass that data somehow into start_urls, put it into one spider, etc...", "links": [], "channel": "scrapy"},
{"date": "2014-06-30T13:53:54.812834+00:00", "nick": "`brain`", "message": "nkuttler: in your code, you most likely are calling parse() to do some work", "links": [], "channel": "scrapy"},
{"date": "2014-06-30T13:54:35.248156+00:00", "nick": "nkuttler", "message": "`brain`: yeah", "links": [], "channel": "scrapy"},
{"date": "2014-06-30T13:55:15.304954+00:00", "nick": "`brain`", "message": "in your parse() you can write code to hand off things that are URLs to a second (to be written by you) parse_found_URLs via a yield(URL to pass) from within parse()", "links": [], "channel": "scrapy"},
{"date": "2014-06-30T13:55:26.437093+00:00", "nick": "`brain`", "message": "search on stackexchange for scrapy recursive", "links": [], "channel": "scrapy"},
{"date": "2014-06-30T13:55:47.042846+00:00", "nick": "nkuttler", "message": "oh, ok. makes sense", "links": [], "channel": "scrapy"},
{"date": "2014-06-30T13:55:59.050636+00:00", "nick": "`brain`", "message": "this is such a normal request, that it should be in your face in the scrapy docs, but isnt", "links": [], "channel": "scrapy"},
{"date": "2014-06-30T13:56:17.612255+00:00", "nick": "`brain`", "message": "I bet its in the top 10 questions people join here to ask", "links": [], "channel": "scrapy"},
{"date": "2014-06-30T13:56:28.149640+00:00", "nick": "nkuttler", "message": "yeah, i assumed so", "links": [], "channel": "scrapy"},
{"date": "2014-06-30T13:56:41.603234+00:00", "nick": "nkuttler", "message": "i checked so, but there are different examples etc", "links": [], "channel": "scrapy"},
{"date": "2014-06-30T13:57:38.650092+00:00", "nick": "`brain`", "message": "I came here first looking for something else, but eventually ran into your question, and was surprised to find that the tutorials are lacking - I guess the right thing to do is to help by writing a better tutorial, but I consider myslef a newb", "links": [], "channel": "scrapy"},
{"date": "2014-06-30T21:11:24.326620+00:00", "nick": "desophos", "message": "hi, i'm trying to figure out how to distribute a scrapy project with some python scripts", "links": [], "channel": "scrapy"},
{"date": "2014-06-30T21:11:40.668475+00:00", "nick": "desophos", "message": "i don't want my users to have to go through the ordeal of installing scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-06-30T21:12:51.088694+00:00", "nick": "desophos", "message": "is there a way to somehow distribute the scrapy library with a package?", "links": [], "channel": "scrapy"},
{"date": "2014-06-30T21:16:15.824994+00:00", "nick": "desophos", "message": "i guess i would have to include precompiled dependencies or something?", "links": [], "channel": "scrapy"},
{"date": "2014-06-30T21:40:08.226161+00:00", "nick": "desophos", "message": "urgh", "links": [], "channel": "scrapy"},
{"date": "2014-06-30T21:42:56.296127+00:00", "nick": "desophos", "message": "https://groups.google.com/forum/#!topic/scrapy-...", "links": ["https://groups.google.com/forum/#!topic/scrapy-users/9w9t8hIMm4w"], "channel": "scrapy"},
{"date": "2014-06-30T21:52:27.675450+00:00", "nick": "desophos", "message": "anyone?", "links": [], "channel": "scrapy"},
{"date": "2014-06-30T22:40:19.599799+00:00", "nick": "desophos", "message": "i guess i'll have to build my own installer...", "links": [], "channel": "scrapy"},
{"date": "2014-06-30T22:40:32.804111+00:00", "nick": "desophos", "message": "i find it hard to believe that nobody has ever had to distribute a scrapy project", "links": [], "channel": "scrapy"},
{"date": "2014-06-30T22:56:41.705119+00:00", "nick": "`brain", "message": "perl has (had?) a thingy called perl2exe that would bundle in perl, the libraries you were using, and your script so that you could distribute on win32", "links": [], "channel": "scrapy"},
{"date": "2014-06-30T22:57:13.785980+00:00", "nick": "`brain", "message": "actually, perl didn't \"have it\", someone wrote it and if you paid, you could remove his banner", "links": [], "channel": "scrapy"},
{"date": "2014-06-30T22:57:22.127968+00:00", "nick": "`brain", "message": "is there a python2exe ?", "links": [], "channel": "scrapy"},
{"date": "2014-06-30T22:57:36.719805+00:00", "nick": "desophos", "message": "yes, py2exe", "links": [], "channel": "scrapy"},
{"date": "2014-06-30T22:57:43.303861+00:00", "nick": "desophos", "message": "but apparently scrapy has a wheel", "links": [], "channel": "scrapy"},
{"date": "2014-06-30T23:04:18.340768+00:00", "nick": "toothr", "message": "`brain, pyinstaller, py2exe, cx_Freeze, (nuitka, cython, can also do it)", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T07:50:30.496607+00:00", "nick": "vab", "message": "hi Guys", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T16:34:45.752333+00:00", "nick": "Kiran__", "message": "hello !", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T16:36:45.475175+00:00", "nick": "Kiran__", "message": "Are there sample examples for scrapy contracts ? I am confused as how to use them. Here is the pastebin : http://pastebin.com/ynhb07kG", "links": ["http://pastebin.com/ynhb07kG"], "channel": "scrapy"},
{"date": "2014-07-01T16:46:01.360403+00:00", "nick": "Kiran__", "message": "Ok i think I got it . Need to modify scrapy settings", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T18:08:49.945526+00:00", "nick": "VooDooNOFX_", "message": "Is there an example which shows a scrapy config where you need to heavily use a linkextractor to find the items you want to scrape? For example, suppose I wanted to scrape the page of every ebay seller, I might locate every auction first (and dump that into a mongodb), then locate the seller.", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T20:43:14.410595+00:00", "nick": "jakobdm", "message": "hey", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T20:44:51.833536+00:00", "nick": "jakobdm", "message": "Has anyone used scrapy on a Raspberry Pi? The crawling itself is adequately fast, but it takes roughly thirty seconds before \"Scrapy started\" is logged...", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T21:49:26.457579+00:00", "nick": "VooDooNOFX_", "message": "jakobdm: what's your setup like? venv, how fast is the SD card, etc?", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T21:53:00.537109+00:00", "nick": "jakobdm", "message": "Venv with nothing but scrapy (and dependencies) installed, the SD card is very fast (3.5", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T21:53:19.562325+00:00", "nick": "jakobdm", "message": "3.5 MBps on random small writes, 20 MBps sequentially)", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T21:53:51.924768+00:00", "nick": "jakobdm", "message": "The Pi runs Arch Linux ARM, no GUI or any fancy packages", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T21:58:21.103853+00:00", "nick": "VooDooNOFX_", "message": "Lemme test it on my pidora pi", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T21:58:54.639288+00:00", "nick": "jakobdm", "message": "Cool, thank you", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T22:00:19.903180+00:00", "nick": "VooDooNOFX_", "message": "I've noticed a lot of utilities hang for exorbitant amounts of time, like yum, etc when they're doing disk IO.", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T22:02:17.863286+00:00", "nick": "jakobdm", "message": "True, I've bought one of the better SD cards when I noticed that and it runs smoothly now. I also checked top while running scrapy, there's not much waiting on I/O", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T22:11:25.276536+00:00", "nick": "jakobdm", "message": "Uh, running 'scrapy' without any arguments also takes about thirty seconds", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T22:28:15.161541+00:00", "nick": "jakobdm", "message": "Mh I've done some profiling, it's \"from twisted import reactor\" that takes the bulk of the time", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T22:39:20.954973+00:00", "nick": "jakobdm", "message": "I gotta take off, thanks for your help!", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:15:26.745277+00:00", "nick": "yoLo_", "message": "i need help installing scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:15:35.354913+00:00", "nick": "yoLo_", "message": "can anybody help ?", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:15:57.430975+00:00", "nick": "yoLo_", "message": "i used pip but i got some error during installation", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:16:59.717625+00:00", "nick": "VooDooNOFX_", "message": "yoLo_: Show the errors :)", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:17:05.242889+00:00", "nick": "VooDooNOFX_", "message": "(in a pastebin)", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:17:53.980581+00:00", "nick": "yoLo_", "message": "http://bpaste.net/show/uhauWZQwLz2zESx595SZ/", "links": ["http://bpaste.net/show/uhauWZQwLz2zESx595SZ/"], "channel": "scrapy"},
{"date": "2014-07-01T23:18:06.793000+00:00", "nick": "yoLo_", "message": ":)", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:23:26.078623+00:00", "nick": "yoLo_", "message": "VooDooNOFX_, so... think you can help ?", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:23:51.076129+00:00", "nick": "VooDooNOFX_", "message": "You need to pip install it with sudo provledges", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:24:09.176696+00:00", "nick": "VooDooNOFX_", "message": "You don't ahve permissiong to modify the system /usr/local/lib/python2.7 folder without it", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:24:34.882692+00:00", "nick": "yoLo_", "message": "that's what i thought...", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:24:47.525608+00:00", "nick": "yoLo_", "message": "so have you used scrapy before /", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:24:49.096638+00:00", "nick": "yoLo_", "message": "?", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:31:54.644678+00:00", "nick": "yoLo_", "message": "VooDooNOFX_,", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:32:28.349299+00:00", "nick": "VooDooNOFX_", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:32:34.618886+00:00", "nick": "yoLo_", "message": "http://bpaste.net/show/940cjECVqKCkHOUfyIdl/", "links": ["http://bpaste.net/show/940cjECVqKCkHOUfyIdl/"], "channel": "scrapy"},
{"date": "2014-07-01T23:33:01.324712+00:00", "nick": "VooDooNOFX_", "message": "yoLo_: you're missing python-dev (or python-devel) for you installation", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:33:25.354359+00:00", "nick": "VooDooNOFX_", "message": "You're going to need to install some other packages too, but I cannot recall them righ tnow", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:35:11.238305+00:00", "nick": "yoLo_", "message": "ohh installing essential python-dev", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:41:03.687194+00:00", "nick": "yoLo_", "message": "VooDooNOFX_,  it should work now..", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:41:26.588244+00:00", "nick": "yoLo_", "message": "it says Requirement already satisfied", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:41:48.737213+00:00", "nick": "VooDooNOFX_", "message": "do \"pip install --upgrade scrapy\"", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:41:54.158307+00:00", "nick": "VooDooNOFX_", "message": "that'll force a rebuild", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:41:58.551653+00:00", "nick": "yoLo_", "message": "ok", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:42:48.325140+00:00", "nick": "yoLo_", "message": "still get the same result", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:42:53.280508+00:00", "nick": "yoLo_", "message": "Requirement already satisfied", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:43:53.196280+00:00", "nick": "VooDooNOFX_", "message": "then you should be good :)", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:44:07.239152+00:00", "nick": "yoLo_", "message": "have you used it before ?", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:44:36.223306+00:00", "nick": "VooDooNOFX_", "message": "Yes, i'm indexing etsy.com as we speak :)", "links": ["http://etsy.com"], "channel": "scrapy"},
{"date": "2014-07-01T23:44:57.570431+00:00", "nick": "yoLo_", "message": "after 3 months of some struggle.. i built a simple basic webcrawler with beautiful soup..", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:45:13.544834+00:00", "nick": "yoLo_", "message": "then when i got to #web", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:45:22.829372+00:00", "nick": "yoLo_", "message": "and #python", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:45:45.860837+00:00", "nick": "yoLo_", "message": "they suggested that scrapy is a complet crawler", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:45:51.341740+00:00", "nick": "VooDooNOFX_", "message": "It is", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:46:00.669602+00:00", "nick": "VooDooNOFX_", "message": "as big or little as you want it o be", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:46:07.322209+00:00", "nick": "VooDooNOFX_", "message": "*to be", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:46:35.198969+00:00", "nick": "yoLo_", "message": "problem is that i'm a beginner and i need some advise on how to use", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:46:54.530718+00:00", "nick": "yoLo_", "message": "my intention is to use it along side Whoosh", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:47:38.802217+00:00", "nick": "yoLo_", "message": "I'm trying to build a basic search engine", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:48:36.771358+00:00", "nick": "VooDooNOFX_", "message": "Ok. Truthfully, there's never anything \"basic\" about a search engine, but go ahead", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:48:44.921614+00:00", "nick": "yoLo_", "message": "there is a difference between crawling and indexing right ?", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:49:16.882191+00:00", "nick": "VooDooNOFX_", "message": "They are the same; synonyms.", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:50:20.822655+00:00", "nick": "yoLo_", "message": "i'm very confused with those \"index\" most specifically", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:50:23.793612+00:00", "nick": "VooDooNOFX_", "message": "I mean, technically, they're 2 parts of the same process", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:50:28.946517+00:00", "nick": "yoLo_", "message": "what do you mean by indexing ?", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:50:34.210302+00:00", "nick": "VooDooNOFX_", "message": "but it's not important for a beginner", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:51:07.144595+00:00", "nick": "VooDooNOFX_", "message": "Indexing is assigning some weight to your discovered links.", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:51:14.698273+00:00", "nick": "VooDooNOFX_", "message": "crawling is discovering the links", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:52:11.480147+00:00", "nick": "yoLo_", "message": "well crawling i understand", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:52:23.557565+00:00", "nick": "yoLo_", "message": "assigning some weight ?", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:52:24.362280+00:00", "nick": "VooDooNOFX_", "message": "so it's a 2 step process: 1) crawl the web, looking for links. 2) Allow searching of that data (indexing that data)", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:53:11.755247+00:00", "nick": "yoLo_", "message": "indexing is when you inspect a data ?", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:53:26.929613+00:00", "nick": "yoLo_", "message": "as either it's an image or pdf or mp3 file ?", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:53:38.170927+00:00", "nick": "VooDooNOFX_", "message": "yoLo_: for example, say you index just the top 10 websites out there (youtube, facebook, etc), when a user searches for something that they all have, how do you know which to show first?", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:53:52.433778+00:00", "nick": "VooDooNOFX_", "message": "like, say you searched for the term \"Video\".", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:54:04.624006+00:00", "nick": "VooDooNOFX_", "message": "facebook and youtube both have video, so who gets shown first?", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:54:14.703808+00:00", "nick": "yoLo_", "message": "youtube", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:54:20.290334+00:00", "nick": "VooDooNOFX_", "message": "why?", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:54:32.719952+00:00", "nick": "yoLo_", "message": "it's a video streaming site", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:54:42.919745+00:00", "nick": "VooDooNOFX_", "message": "so is facebook...", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:55:00.038806+00:00", "nick": "yoLo_", "message": "yes but priority goes to youtube", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:55:10.121604+00:00", "nick": "yoLo_", "message": "well you're right", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:55:15.865912+00:00", "nick": "yoLo_", "message": "but am just confused..", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:55:25.369204+00:00", "nick": "VooDooNOFX_", "message": "so, you arbitrarily assigned a \"weight\" to one result.", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:56:12.494317+00:00", "nick": "yoLo_", "message": "VooDooNOFX_, would i need sql for indexing when building a search engine ?", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:56:59.785297+00:00", "nick": "VooDooNOFX_", "message": "yoLo_: you would need someplace to store all this data. SQL isn't necessarily the answer, but many search engines do use it.", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:57:33.144517+00:00", "nick": "VooDooNOFX_", "message": "you could also use NoSQL", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:57:39.424747+00:00", "nick": "VooDooNOFX_", "message": "like Mongo, cassandra, etc", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:57:55.181783+00:00", "nick": "VooDooNOFX_", "message": "or could use a flat file system, like a bunch of .text file all over a hard drive", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:58:02.456266+00:00", "nick": "VooDooNOFX_", "message": "I mean, the possibilities are endless", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:58:23.794751+00:00", "nick": "yoLo_", "message": "more confusing... yesterday some professionals said sql isn't required..", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:58:39.838704+00:00", "nick": "yoLo_", "message": "it will only slow stuff down", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:59:26.735301+00:00", "nick": "VooDooNOFX_", "message": "It's one solution, yes.", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:59:30.102437+00:00", "nick": "VooDooNOFX_", "message": "there are others", "links": [], "channel": "scrapy"},
{"date": "2014-07-01T23:59:43.214866+00:00", "nick": "VooDooNOFX_", "message": "faster and slower isn't the only criteria for a search engine", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:02:17.149244+00:00", "nick": "yoLo_", "message": "VooDooNOFX_, so when scrapy craws a web, it also retrieves the website url an stores it right ?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:02:50.605372+00:00", "nick": "VooDooNOFX_", "message": "It retrieves the HTML page from the site, and lets you decide what to do with it.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:02:58.447705+00:00", "nick": "VooDooNOFX_", "message": "some people only extract parts of the page, and store just parts.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:03:01.671028+00:00", "nick": "VooDooNOFX_", "message": "others store the whole page", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:05:10.960676+00:00", "nick": "yoLo_", "message": "so it stores some few key words from that page right", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:06:49.884890+00:00", "nick": "VooDooNOFX_", "message": "Yes, mostly", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:07:02.845616+00:00", "nick": "yoLo_", "message": "i see...", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:07:20.044821+00:00", "nick": "VooDooNOFX_", "message": "Guess at which keywords truly represent the page, index those, etc", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:07:57.476421+00:00", "nick": "VooDooNOFX_", "message": "It's a constant cat and mouse game, people adding keywords in white text color on a white background that don't show on teh page, but your spider will see to cheat the systme.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:10:28.669646+00:00", "nick": "yoLo_", "message": "so i have to manually tell scrapy to index a a specific type of keywords /", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:11:54.627023+00:00", "nick": "VooDooNOFX_", "message": "scrapy is only going to retrieve the page for you.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:12:08.589913+00:00", "nick": "VooDooNOFX_", "message": "You then setup the Spider to extract specific parts of the page", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:12:12.959205+00:00", "nick": "VooDooNOFX_", "message": "and possibly store them somewhere", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:12:56.209167+00:00", "nick": "VooDooNOFX_", "message": "You generally use an xpath for this, if you know it. Search Engines can't though, since they have no concept of the structure of the pages they're visiting until they visit it.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:13:19.572292+00:00", "nick": "VooDooNOFX_", "message": "i.e. I benefit from knowing about how to parse facebook", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:13:33.750905+00:00", "nick": "VooDooNOFX_", "message": "because i've been to facebook before. I know which div tags to index, etc.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:14:41.892116+00:00", "nick": "yoLo_", "message": "and the storage is where the search engine site will look up when it gets a query", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:14:43.044661+00:00", "nick": "VooDooNOFX_", "message": "but you're visiting somebrandnewsite.com, somewhere you've never been, and have no idea about the structure of this page.", "links": ["http://somebrandnewsite.com"], "channel": "scrapy"},
{"date": "2014-07-02T00:15:19.611669+00:00", "nick": "VooDooNOFX_", "message": "so you have to guess at how the page looks. They better your guesses, the better your search results.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:16:08.716650+00:00", "nick": "VooDooNOFX_", "message": "And yes, your search engine will look into this massive database of sites you've already visited", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:16:20.279887+00:00", "nick": "yoLo_", "message": "hmm...", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:17:07.902506+00:00", "nick": "yoLo_", "message": "what would you suggest for database", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:17:08.003563+00:00", "nick": "VooDooNOFX_", "message": "Today, there's an estimated 992 million websites (domains) in use. How many can you index?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:17:27.116470+00:00", "nick": "VooDooNOFX_", "message": "yoLo_: that depends on what databases you've used in the past.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:17:33.976088+00:00", "nick": "yoLo_", "message": "never used one", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:18:30.304707+00:00", "nick": "VooDooNOFX_", "message": "Any of the popular ones can be made to do this simple task (mysql, postgresql, microsoft sql server), and so can some NoSQL databases (MongoDB, cassandra, HBase, etc)", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:18:50.417764+00:00", "nick": "yoLo_", "message": ":|", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:18:50.973656+00:00", "nick": "yoLo_", "message": "right now my project is in it's alpha stage", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:18:51.074768+00:00", "nick": "yoLo_", "message": "am brainstorming.. on things i will require..", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:18:58.302220+00:00", "nick": "VooDooNOFX_", "message": "You will obviously run into scaling issues once you've indexed millions of websites, possibly billions.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:19:04.982040+00:00", "nick": "yoLo_", "message": "so basically, indexing is when you look for keywords on a site..", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:19:26.124986+00:00", "nick": "VooDooNOFX_", "message": "no.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:19:58.184459+00:00", "nick": "VooDooNOFX_", "message": "You make a web crawler (a.k.a Spider, searcher, etc) that looks for sites to visit, extracts some unique data about the site and stores it in a database.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:20:18.758694+00:00", "nick": "yoLo_", "message": "since you're very good with scrapy , i'd assume you know a lot about python orm and django", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:20:43.901674+00:00", "nick": "VooDooNOFX_", "message": "The act of storing it effectively so it can be queried is called \"indexing\".", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:20:51.841973+00:00", "nick": "VooDooNOFX_", "message": "the term Index is actually a database term", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:21:09.643157+00:00", "nick": "VooDooNOFX_", "message": "so think of that it must exist in the database before it can be indexed.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:21:34.844276+00:00", "nick": "VooDooNOFX_", "message": "I'm familiar with them, yes.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:22:15.644747+00:00", "nick": "yoLo_", "message": "hmmm..", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:22:55.023222+00:00", "nick": "yoLo_", "message": "don't you store what has been indexed in the database ?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:23:10.699934+00:00", "nick": "yoLo_", "message": "or the site is indexed in the database ?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:24:51.295986+00:00", "nick": "VooDooNOFX_", "message": "It's a pedantic differenece.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:25:00.814019+00:00", "nick": "VooDooNOFX_", "message": "You put something about the site into a database.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:25:07.119950+00:00", "nick": "yoLo_", "message": "ok", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:25:10.575452+00:00", "nick": "VooDooNOFX_", "message": "the act of it being put there assigns it to an index.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:25:15.993459+00:00", "nick": "VooDooNOFX_", "message": "therefore, it's been indexed.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:26:13.505898+00:00", "nick": "yoLo_", "message": "the act of storing those keywords into the database", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:26:24.924811+00:00", "nick": "VooDooNOFX_", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:26:42.117627+00:00", "nick": "VooDooNOFX_", "message": "because every item in a database is given an index which is used to extract that item back out later", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:26:48.102054+00:00", "nick": "VooDooNOFX_", "message": "usually, it's a number, like a numeric index.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:26:57.117934+00:00", "nick": "VooDooNOFX_", "message": "but it can be different (like in mongodb)", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:33:17.268649+00:00", "nick": "yoLo_", "message": "interesting", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:33:32.741489+00:00", "nick": "yoLo_", "message": "what program would you suggest that i use for database", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:34:54.209917+00:00", "nick": "VooDooNOFX_", "message": "Depends. There is some utility in learning a structured database tool like MySQL.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:35:06.782122+00:00", "nick": "VooDooNOFX_", "message": "But, i haven't used mysql in years for anything production.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:36:24.201259+00:00", "nick": "VooDooNOFX_", "message": "but I do have a postgres install at work with 1/2 billion records, and we also have a mongodb system with 1/4 billion records.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:36:32.317025+00:00", "nick": "VooDooNOFX_", "message": "So, the answer may be \"learn both\"", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:36:34.649190+00:00", "nick": "VooDooNOFX_", "message": "start with SQL", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:36:40.954001+00:00", "nick": "VooDooNOFX_", "message": "like MySQL, or PostgreSQL", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:37:00.979978+00:00", "nick": "yoLo_", "message": "what about orm or django ?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:39:27.989206+00:00", "nick": "yoLo_", "message": "VooDooNOFX_, how do i start scrapy..", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:39:34.319240+00:00", "nick": "VooDooNOFX_", "message": "django is just the web front-end", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:39:49.028133+00:00", "nick": "VooDooNOFX_", "message": "there are alternatives also, Django just wraps some of the sharp edges in foam", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:39:57.895227+00:00", "nick": "VooDooNOFX_", "message": "what web experience do you already have?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:41:31.228886+00:00", "nick": "yoLo_", "message": "i know python", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:41:42.445523+00:00", "nick": "yoLo_", "message": "but has no clue how to use scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:42:10.333874+00:00", "nick": "VooDooNOFX_", "message": "then django seems like a good starting point.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:42:23.479864+00:00", "nick": "VooDooNOFX_", "message": "django uses its own ORM, but other frameworks use sqlalchemy", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:42:51.430033+00:00", "nick": "VooDooNOFX_", "message": "building a spider will be the start for you from everythging then", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:42:57.640245+00:00", "nick": "VooDooNOFX_", "message": "Work on getting a basic spider up and running", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:43:24.910146+00:00", "nick": "VooDooNOFX_", "message": "have you done the tutorial yet?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:45:56.351785+00:00", "nick": "yoLo_", "message": "there are some few things i don't understand in the tutorial", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:46:08.756302+00:00", "nick": "VooDooNOFX_", "message": "well, this is as good as place as any to ask :)", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:47:31.162228+00:00", "nick": "yoLo_", "message": "for example expath", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:48:25.153626+00:00", "nick": "VooDooNOFX_", "message": "you mean xpath", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:49:08.977596+00:00", "nick": "yoLo_", "message": "yea", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:49:15.963754+00:00", "nick": "VooDooNOFX_", "message": "ok, do you have google chrome?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:49:20.286599+00:00", "nick": "yoLo_", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:49:53.084938+00:00", "nick": "VooDooNOFX_", "message": "open it up, and head to themeforest.net", "links": ["http://themeforest.net"], "channel": "scrapy"},
{"date": "2014-07-02T00:50:10.690068+00:00", "nick": "VooDooNOFX_", "message": "are you familiar with the Chrome Developer TOols?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:50:58.337875+00:00", "nick": "yoLo_", "message": "ok", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:51:25.802661+00:00", "nick": "yoLo_", "message": "i already have it installed", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:51:31.211796+00:00", "nick": "VooDooNOFX_", "message": "In this can't, don't go to themeforest.net. Instead, go to http://www.dmoz.org (so it's like the tutorial)", "links": ["http://themeforest.net", "http://www.dmoz.org"], "channel": "scrapy"},
{"date": "2014-07-02T00:51:53.698143+00:00", "nick": "VooDooNOFX_", "message": "Open that site in Chrome", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:52:10.558202+00:00", "nick": "VooDooNOFX_", "message": "(which, as a developer, should probably be your default browser from now on)", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:52:26.645347+00:00", "nick": "yoLo_", "message": "ahaha...", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:52:36.971455+00:00", "nick": "VooDooNOFX_", "message": "So, you at dmoz?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:52:40.212198+00:00", "nick": "yoLo_", "message": "if my search engine could look like that i'd be very happy", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:52:44.441859+00:00", "nick": "yoLo_", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:52:51.513782+00:00", "nick": "VooDooNOFX_", "message": "Well, it can, and probably will, eventually.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:53:12.327278+00:00", "nick": "VooDooNOFX_", "message": "So, press F12 to open the developer console, and go to the tab called \"Console\"", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:53:25.946383+00:00", "nick": "VooDooNOFX_", "message": "here we can enter any xpath commands, and it will return results, or not.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:54:19.054266+00:00", "nick": "VooDooNOFX_", "message": "xpath is a language used to navigate through documents in an XML doc. HTML is basically XML, so it can be used here too.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:54:28.945651+00:00", "nick": "VooDooNOFX_", "message": "So, let's do something super simple.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:54:38.337665+00:00", "nick": "VooDooNOFX_", "message": "Put this into console, and hit enter: $x(\"//html\")", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:54:49.220305+00:00", "nick": "VooDooNOFX_", "message": "$x means \"Google Chrome\"", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:55:05.168915+00:00", "nick": "VooDooNOFX_", "message": "the xpath here is \"//html\"", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:55:23.177999+00:00", "nick": "yoLo_", "message": "k", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:55:33.109008+00:00", "nick": "VooDooNOFX_", "message": "this will search the current page, starting at the root (<html>) node, and look for anything that matches the tagname \"html\"", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:55:42.681479+00:00", "nick": "VooDooNOFX_", "message": "of course, the first main tag matches, and it's returned in a list to you", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:55:57.911764+00:00", "nick": "VooDooNOFX_", "message": "[ -> <html>...</html>]", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:55:59.070498+00:00", "nick": "VooDooNOFX_", "message": "yes?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:56:35.158226+00:00", "nick": "yoLo_", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:56:46.583321+00:00", "nick": "VooDooNOFX_", "message": "Good, you've used xpath. Let's try something more complex.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:57:02.195365+00:00", "nick": "VooDooNOFX_", "message": "We'll use xpath to search for all <a href> links in the page all at once", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:57:20.146260+00:00", "nick": "VooDooNOFX_", "message": "$x(\"//a\")", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:57:28.543361+00:00", "nick": "VooDooNOFX_", "message": "This should return an Array of 103 items", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:58:28.766020+00:00", "nick": "yoLo_", "message": "hmm", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:58:39.064584+00:00", "nick": "yoLo_", "message": "i have a [ ] that's it", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:58:44.186345+00:00", "nick": "yoLo_", "message": "wait", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:59:00.317729+00:00", "nick": "VooDooNOFX_", "message": "Then you did something not right", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:59:03.327513+00:00", "nick": "yoLo_", "message": "my array is 310", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T00:59:49.964769+00:00", "nick": "VooDooNOFX_", "message": "What's your xpath query?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:00:24.883467+00:00", "nick": "yoLo_", "message": ":| query ?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:00:38.323165+00:00", "nick": "yoLo_", "message": "where do i find this ?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:00:47.142099+00:00", "nick": "VooDooNOFX_", "message": "the part inside the $x(...)", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:00:57.703522+00:00", "nick": "yoLo_", "message": "\"//a\"", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:01:19.579902+00:00", "nick": "VooDooNOFX_", "message": "ok, I have 103: $x(\"//a\").length == 103", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:01:36.749817+00:00", "nick": "VooDooNOFX_", "message": "anyway, look at one of them.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:02:28.125374+00:00", "nick": "VooDooNOFX_", "message": "So, an xpath query starts with //, meaning start at the beginning of my document", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:02:47.576389+00:00", "nick": "VooDooNOFX_", "message": "then it limits items further than this, based on whatever tag name you're looking for.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:03:08.141434+00:00", "nick": "VooDooNOFX_", "message": "so //a shows all <a> tags. //br shows all <br> tags.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:03:19.242926+00:00", "nick": "yoLo_", "message": "ok", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:04:02.875345+00:00", "nick": "yoLo_", "message": "ohh", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:04:03.658430+00:00", "nick": "yoLo_", "message": "ok", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:04:07.375639+00:00", "nick": "yoLo_", "message": "i got 103", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:04:29.067247+00:00", "nick": "yoLo_", "message": "i didn't use the query on he right site", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:04:39.639733+00:00", "nick": "VooDooNOFX_", "message": "yoLo_: yes, that's important :)", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:05:14.091330+00:00", "nick": "VooDooNOFX_", "message": "So, notice this. Not all the <a> links on this page are part of the directory, right", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:05:16.186166+00:00", "nick": "VooDooNOFX_", "message": "?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:05:36.754248+00:00", "nick": "yoLo_", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:05:39.785967+00:00", "nick": "VooDooNOFX_", "message": "If I follow every single link, i'll end up following the foreign language links at the bottom too, and my search engine isn't setup for this yet.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:05:55.172745+00:00", "nick": "VooDooNOFX_", "message": "So, we're going to limit *which* <a> tags we care about with an xpath", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:06:45.305689+00:00", "nick": "VooDooNOFX_", "message": "Looking at the HTML doc (by right clicking on one of the links, and chooseing \"inspect element\"), we can see that a few div tags up is a <div id=\"catalog\">", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:06:55.805753+00:00", "nick": "yoLo_", "message": "i probably should get a book or pdf of this xpath language", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:07:08.370550+00:00", "nick": "VooDooNOFX_", "message": "and if I hover my mouse over it, that part of the page highlights blue, yes?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:07:16.343039+00:00", "nick": "yoLo_", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:07:26.994105+00:00", "nick": "VooDooNOFX_", "message": "And the blue seems to cover only the links I want.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:08:03.042376+00:00", "nick": "VooDooNOFX_", "message": "So, out of all the HTML in the page, we only want <a> links which are under the <div id=\"catalog\"> section.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:08:28.581846+00:00", "nick": "yoLo_", "message": "yea", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:09:02.234828+00:00", "nick": "VooDooNOFX_", "message": "So, that looks like this: $x('//div[@id=\"catalogs\"]')", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:09:15.425258+00:00", "nick": "VooDooNOFX_", "message": "so, starting at the top of the page, look for all <div> tags.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:09:24.041534+00:00", "nick": "VooDooNOFX_", "message": "For each div tag, look for a property called \"id\"", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:09:29.605998+00:00", "nick": "VooDooNOFX_", "message": "and it must equal \"catalogs\"", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:09:37.216980+00:00", "nick": "VooDooNOFX_", "message": "if all of these are true, return the result to the user.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:10:23.067417+00:00", "nick": "yoLo_", "message": "k", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:10:30.998002+00:00", "nick": "VooDooNOFX_", "message": "So, you should get a single result, yes?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:10:45.502917+00:00", "nick": "VooDooNOFX_", "message": "Now, let's enhance that a little bit", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:11:38.868597+00:00", "nick": "VooDooNOFX_", "message": "$x('//div[@id=\"catalogs\"]/div/span/a')", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:11:51.548216+00:00", "nick": "VooDooNOFX_", "message": "So, just like before, find that catalogs div.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:11:51.687647+00:00", "nick": "yoLo_", "message": "haha", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:11:55.254097+00:00", "nick": "yoLo_", "message": "nice", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:11:57.617156+00:00", "nick": "VooDooNOFX_", "message": "Then, under it, find another div", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:12:01.874422+00:00", "nick": "VooDooNOFX_", "message": "then under that, find a span", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:12:02.280307+00:00", "nick": "yoLo_", "message": "this is fun..", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:12:07.046424+00:00", "nick": "VooDooNOFX_", "message": "then under that, find an <a>", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:12:36.766489+00:00", "nick": "VooDooNOFX_", "message": "You should end up with 15 links. These are the header links in the page (arts, business, etc)", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:12:40.910705+00:00", "nick": "yoLo_", "message": "hmm..", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:12:44.324763+00:00", "nick": "yoLo_", "message": "for some reason", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:13:02.372883+00:00", "nick": "yoLo_", "message": "$x('//div[@id=\"catalogs\"]/div/span/a') has only yield  [ ] as a result", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:13:24.346602+00:00", "nick": "VooDooNOFX_", "message": "are you on the dmoz page when you hit enter?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:13:33.365690+00:00", "nick": "yoLo_", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:14:18.425346+00:00", "nick": "VooDooNOFX_", "message": "Then it should have worked... Check your work...", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:14:43.985792+00:00", "nick": "VooDooNOFX_", "message": "you should be on the dmoz.org homepage still", "links": ["http://dmoz.org"], "channel": "scrapy"},
{"date": "2014-07-02T01:16:19.827017+00:00", "nick": "yoLo_", "message": "i am", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:16:54.179053+00:00", "nick": "VooDooNOFX_", "message": "then I dunno...", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:17:16.733523+00:00", "nick": "VooDooNOFX_", "message": "Break it down, slash by slash to see where it breaks.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:17:20.037779+00:00", "nick": "VooDooNOFX_", "message": "$x('//div[@id=\"catalogs\"]')", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:17:26.773059+00:00", "nick": "VooDooNOFX_", "message": "$x('//div[@id=\"catalogs\"]/div')", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:17:36.413548+00:00", "nick": "VooDooNOFX_", "message": "$x('//div[@id=\"catalogs\"]/div/a'), and so on...", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:18:56.036750+00:00", "nick": "yoLo_", "message": "still get [ ]  :|", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:19:04.806291+00:00", "nick": "yoLo_", "message": "are you using windows or linux .", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:19:05.578917+00:00", "nick": "yoLo_", "message": "?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:19:13.339779+00:00", "nick": "VooDooNOFX_", "message": "xpath doesn't care about platform", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:19:32.601457+00:00", "nick": "yoLo_", "message": "perhaps the devtool it the problem", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:19:37.817169+00:00", "nick": "VooDooNOFX_", "message": "Start over. open a new browser to http://www.dmoz.org", "links": ["http://www.dmoz.org"], "channel": "scrapy"},
{"date": "2014-07-02T01:19:52.432554+00:00", "nick": "VooDooNOFX_", "message": "then open console, and enter $x('//div[@id=\"catalogs\"]/div/span/a')", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:20:18.476334+00:00", "nick": "yoLo_", "message": "everything seems fine until i type the @id=\"catalog\"", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:20:35.468110+00:00", "nick": "VooDooNOFX_", "message": "Are you using the correct quotes?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:20:54.414236+00:00", "nick": "yoLo_", "message": "yes i do", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:21:01.015578+00:00", "nick": "yoLo_", "message": "to make sure i even copy paste", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:21:06.403309+00:00", "nick": "VooDooNOFX_", "message": "$x(<single_quote>div[@id=<double_quote> ...", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:21:15.984037+00:00", "nick": "yoLo_", "message": "ohh ok", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:21:17.949829+00:00", "nick": "yoLo_", "message": "it's working now", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:21:54.386676+00:00", "nick": "VooDooNOFX_", "message": "So, there's your *very) brief xpath tutorial", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:21:59.062822+00:00", "nick": "yoLo_", "message": "$x(<single_quote>div[@id=<double_quote>", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:21:59.825608+00:00", "nick": "yoLo_", "message": "SyntaxError: Unexpected token <", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:22:15.076731+00:00", "nick": "VooDooNOFX_", "message": "don't cut/paste that. I was typing it out for you.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:26:58.582081+00:00", "nick": "yoLo_", "message": "how did you learned xpath ?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:27:05.313958+00:00", "nick": "yoLo_", "message": "books, pdf ?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:28:25.070383+00:00", "nick": "VooDooNOFX_", "message": "from online tutorials, like most things.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:28:59.951950+00:00", "nick": "VooDooNOFX_", "message": "I usually hate w3schools, but their tutorial is pretty decent. You can also look on youtube if you need more lengthy help", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:34:28.365699+00:00", "nick": "yoLo_", "message": "so all i need is xpath", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:34:39.081790+00:00", "nick": "yoLo_", "message": "what about xml and xslt ?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:34:48.285694+00:00", "nick": "VooDooNOFX_", "message": "you won't need either for this.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:38:43.809816+00:00", "nick": "yoLo_", "message": "good..", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:38:45.146687+00:00", "nick": "VooDooNOFX_", "message": "There's also some chrome/firefox extensions that will help you identify the xpath required for a specific div on a page", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:38:55.964486+00:00", "nick": "yoLo_", "message": "ok", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:40:09.009455+00:00", "nick": "VooDooNOFX_", "message": "Ok, i'm out. have fun with your new-found scrapy abilities :)", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:40:19.964968+00:00", "nick": "yoLo_", "message": "thanks..", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:40:51.611900+00:00", "nick": "yoLo_", "message": "if i only knew thise channel existed", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:41:05.324970+00:00", "nick": "yoLo_", "message": "you guys know alot on indexing and scraping", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:42:15.429302+00:00", "nick": "yoLo_", "message": "i will be coming here more often...", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:43:29.571048+00:00", "nick": "VooDooNOFX_", "message": "#python is also a good resource.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:43:48.153736+00:00", "nick": "VooDooNOFX_", "message": "for when this channel is silent.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:43:56.598455+00:00", "nick": "yoLo_", "message": "yes it is... but not when it comes to crawling the internet or index", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:44:23.280444+00:00", "nick": "VooDooNOFX_", "message": "well, i'm in both :)", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:44:30.906984+00:00", "nick": "yoLo_", "message": "great..", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:44:56.515622+00:00", "nick": "HowardwLo", "message": "Howdy! How do you build a start/pause into scrapy?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:45:14.555705+00:00", "nick": "HowardwLo", "message": "and an updater ?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:45:28.902719+00:00", "nick": "HowardwLo", "message": "would oyu copy down some unique id into a json file or something?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:45:47.335893+00:00", "nick": "yoLo_", "message": ":|", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:46:11.470098+00:00", "nick": "yoLo_", "message": "well i'll go over the tutorial until i get it", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:47:24.214791+00:00", "nick": "yoLo_", "message": "i was wondering how many servers and machines i'd need to start my search engine", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:56:05.107790+00:00", "nick": "yoLo_", "message": "wish i could help HowardwLo i just started using scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:56:53.658350+00:00", "nick": "HowardwLo", "message": "yoLo_: thanks, ya wish scrapy irc was more alive :)", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:57:13.871874+00:00", "nick": "yoLo_", "message": "few people here are good...", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:57:31.018386+00:00", "nick": "HowardwLo", "message": "you shoudl check out #web for server help", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:57:34.458204+00:00", "nick": "yoLo_", "message": "i guess room was made recently", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:57:43.061786+00:00", "nick": "yoLo_", "message": "i have", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:57:47.251774+00:00", "nick": "HowardwLo", "message": "no luck?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:57:50.243145+00:00", "nick": "yoLo_", "message": "the're confusing", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:57:58.107331+00:00", "nick": "HowardwLo", "message": "confusing?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:58:13.376186+00:00", "nick": "yoLo_", "message": "they all suggest different things", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:58:23.529648+00:00", "nick": "HowardwLo", "message": "there is no clear winner", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:58:38.959567+00:00", "nick": "HowardwLo", "message": "but alteast they suggest somethings so you can research on your own", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:58:44.514588+00:00", "nick": "yoLo_", "message": "and i also noticed that there are a lot of factors i need to take into account", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:58:50.522094+00:00", "nick": "HowardwLo", "message": "oh ya :)", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:59:10.029436+00:00", "nick": "HowardwLo", "message": "usually, i dont plan too much, and just do it", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:59:13.514351+00:00", "nick": "HowardwLo", "message": "you will learn along the way", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T01:59:58.491736+00:00", "nick": "HowardwLo", "message": "good luck", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T02:01:07.266493+00:00", "nick": "yoLo_", "message": "i will getting about 10 servers for starts", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T02:03:12.819898+00:00", "nick": "HowardwLo", "message": "you ahve that many users already?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T02:03:45.172221+00:00", "nick": "yoLo_", "message": "nope", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T02:03:50.073737+00:00", "nick": "yoLo_", "message": "just incase...", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T02:03:55.759150+00:00", "nick": "HowardwLo", "message": "??? waste of money", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T02:03:56.836571+00:00", "nick": "yoLo_", "message": "probably not 10", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T02:04:04.789704+00:00", "nick": "HowardwLo", "message": "get what you need, upgrade when you need to", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T02:04:06.384529+00:00", "nick": "yoLo_", "message": "maybe 5", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T02:04:28.311532+00:00", "nick": "HowardwLo", "message": "why buy a dedicated one to begin with? just rent hosting", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T02:04:28.970398+00:00", "nick": "yoLo_", "message": "HowardwLo, you already got scrapy working ?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T02:04:37.448986+00:00", "nick": "HowardwLo", "message": "once you need more, you can get more dedicated servers....", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T02:04:40.214970+00:00", "nick": "yoLo_", "message": "like a domain name ?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T02:04:47.074973+00:00", "nick": "HowardwLo", "message": "you dont even have a domain yet???", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T02:04:50.720952+00:00", "nick": "yoLo_", "message": "i do", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T02:04:59.741549+00:00", "nick": "HowardwLo", "message": "how many people use your product right now", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T02:05:05.105876+00:00", "nick": "HowardwLo", "message": "everyday", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T02:05:19.105072+00:00", "nick": "yoLo_", "message": "i havn't started with the server", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T02:05:20.687618+00:00", "nick": "yoLo_", "message": ":|", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T02:05:21.173779+00:00", "nick": "HowardwLo", "message": "and yes, i have scrapy working (not sure how)", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T02:05:24.608359+00:00", "nick": "HowardwLo", "message": "you have 0 users", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T02:05:26.986356+00:00", "nick": "HowardwLo", "message": "and you want 10 servers?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T02:05:33.228836+00:00", "nick": "HowardwLo", "message": "10 servers can handle what, 10 million users?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T02:05:38.997250+00:00", "nick": "yoLo_", "message": "nope i don't even have the server", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T02:05:47.453272+00:00", "nick": "yoLo_", "message": "i'm still planning", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T02:05:50.731635+00:00", "nick": "HowardwLo", "message": "linode.com", "links": ["http://linode.com"], "channel": "scrapy"},
{"date": "2014-07-02T02:05:56.426384+00:00", "nick": "yoLo_", "message": "everything is in alpha stage", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T02:05:56.527239+00:00", "nick": "HowardwLo", "message": "buy a 10 dollar a month option", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T02:06:00.754767+00:00", "nick": "HowardwLo", "message": "start there", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T02:06:29.367798+00:00", "nick": "HowardwLo", "message": "dont waste money on even 1 server", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T02:06:52.207883+00:00", "nick": "HowardwLo", "message": "its like you\u2019re buying a car for your daughter, when you dont even have a wife yet", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T02:07:22.199567+00:00", "nick": "yoLo_", "message": "i already have a domain with godaddy.. will i be able to transfer it to linode?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T02:07:43.878122+00:00", "nick": "HowardwLo", "message": "server hosting is in addition domain", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T02:08:10.141108+00:00", "nick": "HowardwLo", "message": "you have to tell godaddy which ip to point to", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T02:08:20.787588+00:00", "nick": "HowardwLo", "message": "linode, and other server hosting, gives you the ip", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T02:08:34.270707+00:00", "nick": "HowardwLo", "message": "its two diff things. you have to read more guides", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T02:08:55.539506+00:00", "nick": "HowardwLo", "message": "IMO, you\u2019 shouldn\u2019t even be in #scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T02:08:59.773960+00:00", "nick": "HowardwLo", "message": "thats development", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T02:09:15.338581+00:00", "nick": "HowardwLo", "message": "you\u2019re not even there yet", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T02:09:16.314682+00:00", "nick": "yoLo_", "message": "lol", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T02:09:27.563565+00:00", "nick": "HowardwLo", "message": "you\u2019re planning too much :P", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T02:09:35.838710+00:00", "nick": "yoLo_", "message": "i will get there eventually we all gotta start somewhere", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T02:09:50.766653+00:00", "nick": "HowardwLo", "message": "by all means, dont forget to start by thinking too much", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T02:09:59.931175+00:00", "nick": "yoLo_", "message": "i always do plan too much but i eventually get there before things are done", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T02:21:28.359134+00:00", "nick": "yoLo_", "message": "HowardwLo, so linode is a cloud service ?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T02:21:53.831878+00:00", "nick": "HowardwLo", "message": "yoLo_: cloud hosting", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T02:22:14.582337+00:00", "nick": "HowardwLo", "message": "one of many", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T02:25:52.998215+00:00", "nick": "yoLo_", "message": "so i wouldn't be able to transfer my domain name....", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T02:30:06.334904+00:00", "nick": "yoLo_", "message": "anyways.. brb", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T02:56:43.293110+00:00", "nick": "HowardwLo", "message": "any tips on building a start/pause scraper? and an update scraper that adds items that havent been scraped before?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:30:21.667918+00:00", "nick": "VooDooNOFX_", "message": "HowardwLo: to use persistence (which pauses jobs), you have to start the job with it enabled: scrapy crawl somespider -s JOBDIR=crawls/somespider-1", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:30:52.019023+00:00", "nick": "VooDooNOFX_", "message": "Then you send SIGHUP, or ctrl+c or some term code, and it will resume where it left off next time.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:31:10.366946+00:00", "nick": "HowardwLo", "message": "VooDooNOFX_: what if the object its scraping changes?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:31:20.072814+00:00", "nick": "HowardwLo", "message": "i\u2019m scraping a list that changes from time to time", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:31:28.085550+00:00", "nick": "HowardwLo", "message": "additional items, less items", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:31:49.350464+00:00", "nick": "VooDooNOFX_", "message": "Can you explain further? That's a different problem than restarting the job", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:32:01.418828+00:00", "nick": "HowardwLo", "message": "say im scraping a list of addresses from a website", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:32:18.861193+00:00", "nick": "HowardwLo", "message": "this list updates itself over time, sometimes peopel add addresses, other times the address are removed", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:32:29.284384+00:00", "nick": "HowardwLo", "message": "and say its organized alphabetically", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:32:41.323197+00:00", "nick": "HowardwLo", "message": "if i scrape from A,  and pause at B", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:32:54.378552+00:00", "nick": "HowardwLo", "message": "and someone adds an address under A, will the persistence catch the new A ?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:33:05.789259+00:00", "nick": "HowardwLo", "message": "i guess thats also updating.....", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:33:12.135045+00:00", "nick": "VooDooNOFX_", "message": "Yeah, each spider will only get to see the page URL that it should hit. There's no concept of partial page spiders", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:33:35.010627+00:00", "nick": "VooDooNOFX_", "message": "So, each page is a unique web request, and the spider will only pause in-between web-requests", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:33:45.796883+00:00", "nick": "HowardwLo", "message": "ah i see", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:34:04.101207+00:00", "nick": "HowardwLo", "message": "how would you build an updater into it?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:34:10.156622+00:00", "nick": "VooDooNOFX_", "message": "So, if the page changes in between the stop and restart, you'll be possibly missing items, or have duplicate items (if it used to appear at the bottom of page 1, but is now at the top of page 2)", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:34:35.772628+00:00", "nick": "VooDooNOFX_", "message": "I'm imaging the ebay listing grid, where new auctions are added constantly.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:34:47.119718+00:00", "nick": "VooDooNOFX_", "message": "I scan page 1 and 2, but pause for 2 hours.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:35:07.863206+00:00", "nick": "VooDooNOFX_", "message": "When I go back to page 3, it will be a different page than it used to be before (depending on the site actually)", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:35:28.616280+00:00", "nick": "HowardwLo", "message": "right", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:35:50.947186+00:00", "nick": "VooDooNOFX_", "message": "for example, etsy.com embeds the last ID for each page.", "links": ["http://etsy.com"], "channel": "scrapy"},
{"date": "2014-07-02T06:36:05.036176+00:00", "nick": "VooDooNOFX_", "message": "this allows it to start on the next page at a specific ID, so no loss of items.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:36:13.074680+00:00", "nick": "VooDooNOFX_", "message": "But most sites don't do that.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:36:48.507859+00:00", "nick": "HowardwLo", "message": "ya...", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:36:57.938774+00:00", "nick": "HowardwLo", "message": "in that case, would i just keep a json file with the ID\u2019s in it?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:37:06.433167+00:00", "nick": "HowardwLo", "message": "and everytime I scrape, cross check?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:37:10.994307+00:00", "nick": "HowardwLo", "message": "kind of inefficient...", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:37:13.074013+00:00", "nick": "VooDooNOFX_", "message": "Yes, or some sort of database.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:37:41.189067+00:00", "nick": "VooDooNOFX_", "message": "Keep the last seen ID, and start scanning pages from the beginning until you see that ID appear in the list (assuming the list is sorted chronologically)", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:38:09.926220+00:00", "nick": "HowardwLo", "message": "unforunately not, new updates can go anywhere", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:38:18.879324+00:00", "nick": "VooDooNOFX_", "message": "Then it's a total rescan each time.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:38:29.573375+00:00", "nick": "VooDooNOFX_", "message": "You can use something like MongoDB with upsert", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:38:50.332285+00:00", "nick": "VooDooNOFX_", "message": "this will allow it to just update when you see old data, and insert when you see new data", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:39:08.085859+00:00", "nick": "HowardwLo", "message": "whew, thats a whole nother beast isnt it? mongodb?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:39:30.020040+00:00", "nick": "VooDooNOFX_", "message": "Sure, I mean it's a common place to store data; a database :)", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:39:47.454824+00:00", "nick": "HowardwLo", "message": "ya", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:39:50.726141+00:00", "nick": "HowardwLo", "message": "i just hate messing with the db", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:39:56.059996+00:00", "nick": "HowardwLo", "message": "and matching up keys and everything", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:40:03.115649+00:00", "nick": "VooDooNOFX_", "message": "or you can use mysql, or amazon s3", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:40:07.539628+00:00", "nick": "VooDooNOFX_", "message": "or redis, or whatever", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:40:15.956747+00:00", "nick": "VooDooNOFX_", "message": "backends are the easy part.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:40:17.738076+00:00", "nick": "HowardwLo", "message": "db much faster than a json file?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:40:31.055610+00:00", "nick": "VooDooNOFX_", "message": "It's more memory effecient.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:40:47.410852+00:00", "nick": "VooDooNOFX_", "message": "b+ trees are much faster as balancing than a huge json structure in memory.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:41:09.743806+00:00", "nick": "HowardwLo", "message": ":/", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:41:19.595245+00:00", "nick": "VooDooNOFX_", "message": "I mean, you could even use sqlite for it, and that's pretty much a flat file database, mmapped to memory", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:41:44.250546+00:00", "nick": "HowardwLo", "message": "using the ebay example", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:41:47.521095+00:00", "nick": "HowardwLo", "message": "each bid has a unique id", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:41:51.521062+00:00", "nick": "HowardwLo", "message": "and unique url", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:42:03.512937+00:00", "nick": "HowardwLo", "message": "if i use persistance, it\u2019ll catch duplicate responses right?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:42:12.933785+00:00", "nick": "HowardwLo", "message": "so if i\u2019ve visited a bid before, it\u2019ll know?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:42:21.731530+00:00", "nick": "HowardwLo", "message": "auction* i should say, not bid", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:43:16.729132+00:00", "nick": "VooDooNOFX_", "message": "Yes, actually scrapy can decide if it's already visited a URL (during an individual scan at least)", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:43:20.949893+00:00", "nick": "flyingtriangle", "message": "How do I prevent a request from percent encoding the URL?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:43:41.728892+00:00", "nick": "HowardwLo", "message": "flyingtriangle you can decode it again, is what i do :)", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:43:45.027854+00:00", "nick": "VooDooNOFX_", "message": "flyingtriangle: you don't. can you explain the real problem you're having better, perhaps with some pastebins?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:44:19.449271+00:00", "nick": "flyingtriangle", "message": "HowardwLo, how do you do that? Monkey patch somehow? VooDooNOFX_ I'm writing an XSS finder and I want to send the URL unencoded", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:44:44.694623+00:00", "nick": "HowardwLo", "message": "flyingtriangle: i use python, theres a url library that handles decode encode", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:44:55.259306+00:00", "nick": "HowardwLo", "message": "VooDooNOFX_: i guess that\u2019ll have to do", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:44:57.642762+00:00", "nick": "VooDooNOFX_", "message": "flyingtriangle: ah, tricky. You don't \"unencode\" the url. instead, you send arbitrary bytes in the header.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:45:13.363936+00:00", "nick": "HowardwLo", "message": "ah really?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:45:30.012518+00:00", "nick": "HowardwLo", "message": "im amateur status flyingtriangle, dont heed my advice :D", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:46:31.897378+00:00", "nick": "VooDooNOFX_", "message": "flyingtriangle: you sound like you're trying to autodiscover form vulnerabilities and xss issues, so you're going to need to craft your http reqests (gets and posts) carefully. In python, you can craft your Request however you want.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:47:28.838370+00:00", "nick": "VooDooNOFX_", "message": "flyingtriangle: https://github.com/scrapy/scrapy/blob/master/sc...", "links": ["https://github.com/scrapy/scrapy/blob/master/scrapy/http/request/__init__.py#L20"], "channel": "scrapy"},
{"date": "2014-07-02T06:48:14.927589+00:00", "nick": "VooDooNOFX_", "message": "There's a request being made, in python, using scrapy. You don't have *too* much control, is it's being escape_ajax-ified (on line 52)", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:48:24.103538+00:00", "nick": "VooDooNOFX_", "message": "So, you might want to superclass this for yourself", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:49:00.886572+00:00", "nick": "flyingtriangle", "message": "VooDooNOFX_, ok interesting, it looks like I might be able to monkeypath line 57 there", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:49:01.564341+00:00", "nick": "flyingtriangle", "message": "self._set_url(url.encode(self.encoding))", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:49:36.218068+00:00", "nick": "flyingtriangle", "message": "monkeypatch* I'm not sure what a superclass is but I'm googling it. I'd imagine it's where you just recreate that class in your project directory and import that Request instead?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:50:02.665699+00:00", "nick": "VooDooNOFX_", "message": "flyingtriangle: it's where you inherit all the underlying functions from another class, just overriding one class for yourself.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:51:12.228857+00:00", "nick": "VooDooNOFX_", "message": "overriding one *method* for yourself", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:52:24.966589+00:00", "nick": "flyingtriangle", "message": "Cool thanks", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T06:52:39.597904+00:00", "nick": "VooDooNOFX_", "message": "sure", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T07:22:23.638287+00:00", "nick": "flyingtriangle", "message": "VooDooNOFX_, am I on the right track if I just eliminate line 57 from https://github.com/scrapy/scrapy/blob/master/sc...", "links": ["https://github.com/scrapy/scrapy/blob/master/scrapy/http/request/__init__.py#L20?"], "channel": "scrapy"},
{"date": "2014-07-02T07:24:41.574025+00:00", "nick": "flyingtriangle", "message": "Since it's not an ajax url ever", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T07:29:53.425856+00:00", "nick": "VooDooNOFX_", "message": "flyingtriangle: well, that's not getting you towards your goal, but yes, you can remove that line since your requests aren't ajax.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T07:30:19.600546+00:00", "nick": "VooDooNOFX_", "message": "But remember having scrapy like that will have non-intended consequences. Some we can help with, others we won't know what you did...", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T07:31:11.560512+00:00", "nick": "VooDooNOFX_", "message": "In short, you need to craft your own Request class, then have you application call that one.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T07:31:39.874706+00:00", "nick": "flyingtriangle", "message": "I guess I'm just unclear on exactly where scrapy is percent encoding the URL within that function", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T07:32:29.255496+00:00", "nick": "VooDooNOFX_", "message": "flyingtriangle: give me a concrete example where you think it's percent encoding the URL.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T07:33:11.957319+00:00", "nick": "flyingtriangle", "message": "VooDooNOFX_, like in the Request code?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T07:35:44.230149+00:00", "nick": "flyingtriangle", "message": "I figured since __init__ contained: self._set_url(url), that I should be checking _set_url for % encoding but I'm not really sure where it is encoding stuff in that function. The place it looks like it's % encoding is in canonicalize_url() of scrapy.utils.url", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T07:36:49.701460+00:00", "nick": "flyingtriangle", "message": "query = urllib.urlencode(keyvals)", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T07:36:54.437170+00:00", "nick": "VooDooNOFX_", "message": "flyingtriangle: show me an actual scrap session that's doing this.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T07:36:59.077425+00:00", "nick": "flyingtriangle", "message": "Ah", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T07:37:00.332838+00:00", "nick": "VooDooNOFX_", "message": "pastebin something", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T07:38:13.674091+00:00", "nick": "VooDooNOFX_", "message": "because https://github.com/scrapy/scrapy/blob/master/sc... should be the line that's actually doing it, but we'll see", "links": ["https://github.com/scrapy/scrapy/blob/master/scrapy/utils/url.py#L38"], "channel": "scrapy"},
{"date": "2014-07-02T07:38:59.084060+00:00", "nick": "flyingtriangle", "message": "2014-07-02 03:37:57-0400 [xss_spider] DEBUG: Scraped from <200 https://google-gruyere.appspot.com/916751114721...;     The uid= should be 9zqjx()=<>9zqjx>", "links": ["https://google-gruyere.appspot.com/916751114721/snippets.gtl?uid=9zqjx%28%29%3D%3C%3E9zqjx&gt="], "channel": "scrapy"},
{"date": "2014-07-02T07:39:22.452010+00:00", "nick": "VooDooNOFX_", "message": "flyingtriangle: it is.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T07:39:43.498842+00:00", "nick": "VooDooNOFX_", "message": "Have you ever seen what an HTTP header looks like?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T07:40:46.303311+00:00", "nick": "flyingtriangle", "message": "I have, I want the GET header to actually say <> rather than %3c and %3e. Maybe I'm wrong that there's any difference?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T07:42:48.745504+00:00", "nick": "VooDooNOFX_", "message": "There's not.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T07:44:08.258014+00:00", "nick": "VooDooNOFX_", "message": "all items are urlencoded in the actual http packet, and decoded by the webserver on the other end.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T07:44:15.813000+00:00", "nick": "flyingtriangle", "message": "Ah....", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T07:44:17.035532+00:00", "nick": "flyingtriangle", "message": "interesting", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T07:44:27.906138+00:00", "nick": "flyingtriangle", "message": "Thanks voodoo you just saved me a lot of time barking up the wrong tree", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T07:44:32.632996+00:00", "nick": "flyingtriangle", "message": "VooDooNOFX_, *", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T07:44:49.616940+00:00", "nick": "VooDooNOFX_", "message": ":D", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T07:45:30.152904+00:00", "nick": "VooDooNOFX_", "message": "btw, appspot.com domains are unlikely to be sql injected targets, but may be xss targets.", "links": ["http://appspot.com"], "channel": "scrapy"},
{"date": "2014-07-02T07:45:56.442771+00:00", "nick": "VooDooNOFX_", "message": "since they're google app engine hosted apps, with bigtable behind them.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T07:46:13.880537+00:00", "nick": "VooDooNOFX_", "message": "but it wouldn't surprise me to find eval() peppered throughout appspot.com applications.", "links": ["http://appspot.com"], "channel": "scrapy"},
{"date": "2014-07-02T07:46:25.787807+00:00", "nick": "flyingtriangle", "message": "That link is from a purposely vulnerable web app for testing purposes", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T07:46:32.896880+00:00", "nick": "flyingtriangle", "message": "kinda like dvwa", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T07:46:50.154059+00:00", "nick": "VooDooNOFX_", "message": "ah, nice.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T07:48:04.598615+00:00", "nick": "VooDooNOFX_", "message": "ok. might.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T07:48:06.333839+00:00", "nick": "VooDooNOFX_", "message": "night*", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T07:48:27.293548+00:00", "nick": "flyingtriangle", "message": "Have a good one!", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T07:51:13.346985+00:00", "nick": "sunshine`", "message": "Hi", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T07:51:21.946692+00:00", "nick": "sunshine`", "message": "anybody here?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T07:51:44.396366+00:00", "nick": "flyingtriangle", "message": "Sure but I don't know much", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T11:00:44.347702+00:00", "nick": "HowardwLo", "message": "VooDooNOFX_: Still here? :)", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T11:03:30.885905+00:00", "nick": "HowardwLo", "message": "im wondering if persistance will continue after error messages :)", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T11:11:58.005288+00:00", "nick": "YatharthROCK", "message": "I have a home page from which I want to extract links. I want to then follow those links and scrape the actual data from them.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T11:12:29.836881+00:00", "nick": "YatharthROCK", "message": "I can do both the steps individually, and connect them via a JSON file' but I want to know how do it properly.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T11:12:49.209946+00:00", "nick": "HowardwLo", "message": "i use a callback", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T11:12:57.182675+00:00", "nick": "HowardwLo", "message": "the first parse grabs the links", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T11:13:00.447041+00:00", "nick": "HowardwLo", "message": "and makes a request to them", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T11:13:09.953918+00:00", "nick": "HowardwLo", "message": "callback=self.page_parse or something", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T11:13:17.037002+00:00", "nick": "YatharthROCK", "message": "HowardwLo: Do you mean I should return a Request object whose parameter is a second parsing function?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T11:13:56.158449+00:00", "nick": "HowardwLo", "message": "YatharthROCK: ya, request = Request(link, callback = self.pageparse)", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T11:14:04.111875+00:00", "nick": "HowardwLo", "message": "yield request or return request after", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T11:14:22.687635+00:00", "nick": "HowardwLo", "message": "you put your yield/return Item in the pageparse", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T11:14:26.259548+00:00", "nick": "YatharthROCK", "message": "HowardwLo: But wait, I need many such requests", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T11:14:33.169693+00:00", "nick": "YatharthROCK", "message": "Oh, i should yield them one by one.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T11:14:37.079145+00:00", "nick": "HowardwLo", "message": "then your first parse should be a loop", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T11:14:47.368532+00:00", "nick": "HowardwLo", "message": "ya, use yield not return in that case", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T11:15:07.772581+00:00", "nick": "YatharthROCK", "message": "so sth like: `for link in links: yield Request(link, callback=parse_page)`", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T11:15:19.266885+00:00", "nick": "YatharthROCK", "message": "Cool.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T11:15:45.317924+00:00", "nick": "YatharthROCK", "message": "HowardwLo: Thanks.", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T11:15:49.486065+00:00", "nick": "HowardwLo", "message": "YatharthROCK: yup real easy, np", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T11:15:53.138113+00:00", "nick": "YatharthROCK", "message": "Are you a dev, BTW?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T11:16:01.430905+00:00", "nick": "HowardwLo", "message": "barely, still learning", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T11:16:33.596254+00:00", "nick": "YatharthROCK", "message": "HowardwLo: I meant, do you contribute to Scrapy?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T11:16:37.775723+00:00", "nick": "HowardwLo", "message": "YatharthROCK: oh no", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T11:16:40.807833+00:00", "nick": "HowardwLo", "message": "just a user", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T11:17:03.459006+00:00", "nick": "YatharthROCK", "message": "HowardwLo: Oh, fine. Anyway, thanks again!", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T11:17:09.770040+00:00", "nick": "HowardwLo", "message": "np", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T11:27:44.517761+00:00", "nick": "YatharthROCK", "message": "HowardwLo: Also, how could I extract the href arg from an anchor element using CSS selectors?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T15:09:51.394124+00:00", "nick": "mucahit", "message": "Hi!", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T15:10:06.847318+00:00", "nick": "mucahit", "message": "I want to ask a question about scraping not specificaly about scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T15:10:27.424527+00:00", "nick": "mucahit", "message": "I want to scrape a website however it uses javascript to redirect to another page", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T15:11:00.363996+00:00", "nick": "mucahit", "message": "how can I detect the url which redirected into ?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T15:41:57.974541+00:00", "nick": "YatharthROCK", "message": "mucahit: response.url as opposed to response.request.url", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T16:55:51.343322+00:00", "nick": "YatharthROCK", "message": "How can I OR CSS selectors together in the same way I can do with '|' using XPath?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T16:56:18.307245+00:00", "nick": "YatharthROCK", "message": "HowardwLo: Can you help with this?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T20:47:50.577600+00:00", "nick": "HowardwLo", "message": "how well does scrapy persistance handle errors?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T21:56:28.444761+00:00", "nick": "rodmar", "message": "hi all", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T21:56:33.266993+00:00", "nick": "rodmar", "message": "I have a small question", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T21:56:51.203710+00:00", "nick": "rodmar", "message": "I am using a basespider and a generator for the start urls", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T21:57:15.632013+00:00", "nick": "rodmar", "message": "My question is how to I tell my spider to stop once a given condition is met?", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T21:59:12.454905+00:00", "nick": "rodmar", "message": "here is the code for my spider", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T21:59:13.449184+00:00", "nick": "rodmar", "message": "http://fpaste.org/115175/04338314/", "links": ["http://fpaste.org/115175/04338314/"], "channel": "scrapy"},
{"date": "2014-07-02T22:11:27.696039+00:00", "nick": "rodmar", "message": "nevermind solved it", "links": [], "channel": "scrapy"},
{"date": "2014-07-02T23:01:55.889923+00:00", "nick": "yoLo_", "message": "VooDooNOFX_, hey", "links": [], "channel": "scrapy"},
{"date": "2014-07-03T19:50:53.015638+00:00", "nick": "yoLo_", "message": "VooDooNOFX_, !!!!", "links": [], "channel": "scrapy"},
{"date": "2014-07-03T21:23:33.400487+00:00", "nick": "yoLo_", "message": "anybody uses scrapy ?", "links": [], "channel": "scrapy"},
{"date": "2014-07-04T00:17:22.827073+00:00", "nick": "chad_c", "message": "hi!  I have a dumb question: if I extract a url using xpath, how do I pass that extracted text as a url into a request function?", "links": [], "channel": "scrapy"},
{"date": "2014-07-04T00:17:30.471418+00:00", "nick": "chad_c", "message": "I guess that\u2019s more of a python question than a scrapy question, maybe?", "links": [], "channel": "scrapy"},
{"date": "2014-07-04T00:17:41.794182+00:00", "nick": "chad_c", "message": "or a request call, I should say", "links": [], "channel": "scrapy"},
{"date": "2014-07-04T00:18:13.534884+00:00", "nick": "chad_c", "message": "in other words, if I have item[\u2018url\u2019] = xpath.extract()", "links": [], "channel": "scrapy"},
{"date": "2014-07-04T00:18:36.441842+00:00", "nick": "chad_c", "message": "I want to call request(item[\u2018url\u2019],\u2026)", "links": [], "channel": "scrapy"},
{"date": "2014-07-04T01:06:21.699747+00:00", "nick": "yoLo_", "message": "VooDooNOFX_,  :|", "links": [], "channel": "scrapy"},
{"date": "2014-07-04T03:32:27.214835+00:00", "nick": "Bing", "message": "hello,", "links": [], "channel": "scrapy"},
{"date": "2014-07-04T03:33:06.833292+00:00", "nick": "Guest71184", "message": "hello, mates.", "links": [], "channel": "scrapy"},
{"date": "2014-07-04T05:02:35.005629+00:00", "nick": "Bing", "message": "hello", "links": [], "channel": "scrapy"},
{"date": "2014-07-04T05:02:50.895747+00:00", "nick": "Guest89939", "message": "hello, mates.", "links": [], "channel": "scrapy"},
{"date": "2014-07-04T05:03:54.901284+00:00", "nick": "Guest89939", "message": "I have a question in stackoverflow, here is the link, does anybody have comments on this issue, Thanks in advance.", "links": [], "channel": "scrapy"},
{"date": "2014-07-04T05:04:16.311892+00:00", "nick": "Guest89939", "message": "http://stackoverflow.com/questions/24566281/scr...", "links": ["http://stackoverflow.com/questions/24566281/scrapy-how-to-catch-download-error-and-try-download-it-again"], "channel": "scrapy"},
{"date": "2014-07-04T05:22:37.039862+00:00", "nick": "lolcat__", "message": "fanybody?", "links": [], "channel": "scrapy"},
{"date": "2014-07-04T11:17:54.858643+00:00", "nick": "jakobdm", "message": "VooDooNOFX_: Just in case you're still interested ;), downgrading pyOpenSSL to 0.13 significantly speeded up importing the Twisted reactor / starting scrapy. 26 seconds with 0.14, 6 seconds with 0.13 on my Pi.", "links": [], "channel": "scrapy"},
{"date": "2014-07-04T13:21:36.310995+00:00", "nick": "`brain`", "message": "anyone here have infor on how to use pydepta?", "links": [], "channel": "scrapy"},
{"date": "2014-07-04T13:21:50.602569+00:00", "nick": "`brain`", "message": "some functional example?", "links": [], "channel": "scrapy"},
{"date": "2014-07-04T15:15:09.696869+00:00", "nick": "`brain`", "message": "OK - just read that scrapy != scrapely", "links": [], "channel": "scrapy"},
{"date": "2014-07-04T15:15:59.209803+00:00", "nick": "Fjal", "message": "Hi all", "links": [], "channel": "scrapy"},
{"date": "2014-07-04T15:16:45.416223+00:00", "nick": "Fjal", "message": "quick question, for some reason my crawler repeats same item in the category =S", "links": [], "channel": "scrapy"},
{"date": "2014-07-04T15:17:14.900215+00:00", "nick": "Fjal", "message": "like if there are 3 items in the category it will only get the last name 3 time", "links": [], "channel": "scrapy"},
{"date": "2014-07-04T15:17:31.321477+00:00", "nick": "Fjal", "message": "can someone help me figure out the problem ? =)", "links": [], "channel": "scrapy"},
{"date": "2014-07-05T13:43:51.328962+00:00", "nick": "mazzy_", "message": "Hi", "links": [], "channel": "scrapy"},
{"date": "2014-07-05T13:43:59.380017+00:00", "nick": "mazzy_", "message": "someone could give me an hand", "links": [], "channel": "scrapy"},
{"date": "2014-07-05T13:44:17.242811+00:00", "nick": "mazzy_", "message": "to figure out a problem", "links": [], "channel": "scrapy"},
{"date": "2014-07-05T13:44:42.549928+00:00", "nick": "mazzy_", "message": "I'm trying to scraping a web page", "links": [], "channel": "scrapy"},
{"date": "2014-07-05T13:44:43.272840+00:00", "nick": "mazzy_", "message": "I", "links": [], "channel": "scrapy"},
{"date": "2014-07-05T13:44:48.663419+00:00", "nick": "mazzy_", "message": "'m pretty sure", "links": [], "channel": "scrapy"},
{"date": "2014-07-05T13:44:58.118185+00:00", "nick": "mazzy_", "message": "I have done everything fine", "links": [], "channel": "scrapy"},
{"date": "2014-07-05T13:45:21.466323+00:00", "nick": "mazzy_", "message": "This is the code", "links": [], "channel": "scrapy"},
{"date": "2014-07-05T13:45:23.243740+00:00", "nick": "mazzy_", "message": "I have used", "links": [], "channel": "scrapy"},
{"date": "2014-07-05T13:45:27.212034+00:00", "nick": "mazzy_", "message": "def parse(self, response):          sel = Selector(response)          models = sel.xpath('//div[@class=\"hgroup\"]/a')", "links": [], "channel": "scrapy"},
{"date": "2014-07-05T13:46:44.969303+00:00", "nick": "mazzy_", "message": "to get info from this page", "links": [], "channel": "scrapy"},
{"date": "2014-07-05T13:46:52.280026+00:00", "nick": "mazzy_", "message": "www.quattroruote.it/listino/abarth", "links": ["http://www.quattroruote.it/listino/abarth"], "channel": "scrapy"},
{"date": "2014-07-05T13:47:11.514906+00:00", "nick": "mazzy_", "message": "unfortunately the var models is empty", "links": [], "channel": "scrapy"},
{"date": "2014-07-05T13:47:22.550582+00:00", "nick": "mazzy_", "message": "someone of you could help me?", "links": [], "channel": "scrapy"},
{"date": "2014-07-05T21:51:29.531844+00:00", "nick": "HowardwLo", "message": "hello !", "links": [], "channel": "scrapy"},
{"date": "2014-07-05T21:52:05.031237+00:00", "nick": "HowardwLo", "message": "I usually do a try: finally: to record the last item I scraped so I can pick up where I left off (with selenium). Is there a way to do the same with a spider for scrapy?", "links": [], "channel": "scrapy"},
{"date": "2014-07-05T21:52:22.663825+00:00", "nick": "HowardwLo", "message": "in case of errors and what not", "links": [], "channel": "scrapy"},
{"date": "2014-07-05T21:52:29.839392+00:00", "nick": "HowardwLo", "message": "it\u2019ll exit and record where it is", "links": [], "channel": "scrapy"},
{"date": "2014-07-06T05:35:02.740775+00:00", "nick": "TheRinger", "message": "can anyone point me toward doc on configuring scrapy with tor", "links": [], "channel": "scrapy"},
{"date": "2014-07-06T09:01:07.923658+00:00", "nick": "kevc", "message": "does scrapy not support keep-alive?", "links": [], "channel": "scrapy"},
{"date": "2014-07-06T10:07:18.273038+00:00", "nick": "TheRinger", "message": "pretty sure you can do anything you can program it to", "links": [], "channel": "scrapy"},
{"date": "2014-07-06T11:12:24.096779+00:00", "nick": "kevc", "message": "seems you need a custom http client factory, mostly just a little surprised it's not already built in", "links": [], "channel": "scrapy"},
{"date": "2014-07-06T11:15:06.856792+00:00", "nick": "kevc", "message": "is there any way to annotate a request id to all log messages?", "links": [], "channel": "scrapy"},
{"date": "2014-07-06T15:43:27.257416+00:00", "nick": "Digenis", "message": "pablohof: by what criteria is list of companies using scrapy sorted?", "links": [], "channel": "scrapy"},
{"date": "2014-07-06T15:46:01.160464+00:00", "nick": "Digenis", "message": "already looked into the file's history. didn't make help me", "links": [], "channel": "scrapy"},
{"date": "2014-07-06T17:17:48.001243+00:00", "nick": "Digenis", "message": "pablohof: you probably timed out before seeing it", "links": [], "channel": "scrapy"},
{"date": "2014-07-06T17:17:53.064237+00:00", "nick": "Digenis", "message": "by what criteria is list of companies using scrapy sorted?", "links": [], "channel": "scrapy"},
{"date": "2014-07-06T17:17:56.528061+00:00", "nick": "Digenis", "message": "already looked into the file's history. didn't make help me", "links": [], "channel": "scrapy"},
{"date": "2014-07-06T17:31:51.181798+00:00", "nick": "pablohof", "message": "Digenis: it's not sorted by any specific criteria", "links": [], "channel": "scrapy"},
{"date": "2014-07-06T17:31:58.020077+00:00", "nick": "pablohof", "message": "lately we've been adding new ones to the bottom", "links": [], "channel": "scrapy"},
{"date": "2014-07-06T17:32:52.134231+00:00", "nick": "pablohof", "message": "but there's a lot of companies there, we should probably curate that list and arrange it better", "links": [], "channel": "scrapy"},
{"date": "2014-07-06T17:33:28.108189+00:00", "nick": "pablohof", "message": "maybe separate in a few categories, and remove the ones that are no longer operating", "links": [], "channel": "scrapy"},
{"date": "2014-07-06T17:33:33.857150+00:00", "nick": "pablohof", "message": "(or using scrapy)", "links": [], "channel": "scrapy"},
{"date": "2014-07-06T17:34:38.030626+00:00", "nick": "Digenis", "message": "I told my cto about the list and he suggested I send a PR", "links": [], "channel": "scrapy"},
{"date": "2014-07-06T17:34:55.786339+00:00", "nick": "Digenis", "message": "but I don't know where to add it", "links": [], "channel": "scrapy"},
{"date": "2014-07-06T17:35:11.872969+00:00", "nick": "Digenis", "message": "I guess at the end of the list", "links": [], "channel": "scrapy"},
{"date": "2014-07-06T17:51:20.704233+00:00", "nick": "pablohof", "message": "yeah, just add it to the end of the list", "links": [], "channel": "scrapy"},
{"date": "2014-07-06T17:52:23.114562+00:00", "nick": "pablohof", "message": "another simpler way to organize that list would be to highlight some features companies who have made a greater impact on scrapy (either by blogging or contributing to scrapy)", "links": [], "channel": "scrapy"},
{"date": "2014-07-06T17:52:31.061482+00:00", "nick": "pablohof", "message": "featured*", "links": [], "channel": "scrapy"},
{"date": "2014-07-07T02:27:56.099704+00:00", "nick": "TheRinger", "message": "I'm trying to figure out how to make scrapy pull the info from inside a link it scrapes then go back to the orginal page and go to the next link and do the same.. should i make 2 different scrapers one that pulls the link and the other that grabs the links and scrapes them ?", "links": [], "channel": "scrapy"},
{"date": "2014-07-07T03:19:08.859651+00:00", "nick": "kevc", "message": "TheRinger: you just yeild a Request for each link", "links": [], "channel": "scrapy"},
{"date": "2014-07-07T03:34:47.737796+00:00", "nick": "TheRinger", "message": "ok, i've manage to build a spyder, but i think i just downloaded to much stuff and got banned from a site , good thing I am using a vps, I need to know how to execute the command line correctly, i'm using portiacrawl and i believe it uses scrapyd.. where can I find a good reference for how to dump the info to a file.. i think i just scraped a bunch of stuff and don't even have a data file i can see with the info on it", "links": [], "channel": "scrapy"},
{"date": "2014-07-07T03:37:08.867539+00:00", "nick": "TheRinger", "message": "yah I just used wget and i'm getting a 403 Forbidden, which is my bad cuz i never set any parameters before running my scraper.. i want it to run at a humanlike pace and download only so much data before taking a break etc. then dump it to my mysql db or a simple csv file", "links": [], "channel": "scrapy"},
{"date": "2014-07-07T03:38:06.426710+00:00", "nick": "TheRinger", "message": "kevc msg me if you aren't busy", "links": [], "channel": "scrapy"},
{"date": "2014-07-07T03:39:03.502376+00:00", "nick": "kevc", "message": "if you yeild an Item object scrapy will spit out JSON. Use \"scrapy crawl site --output=filename.json\"", "links": [], "channel": "scrapy"},
{"date": "2014-07-07T03:39:55.554937+00:00", "nick": "TheRinger", "message": "what should i use to slow it down", "links": [], "channel": "scrapy"},
{"date": "2014-07-07T03:41:38.412782+00:00", "nick": "kevc", "message": "set DOWNLOAD_DELAY=2 and CONCURRENT_REQUESTS_PER_DOMAIN=1 if you want something very safe", "links": [], "channel": "scrapy"},
{"date": "2014-07-07T03:43:02.693594+00:00", "nick": "TheRinger", "message": "awesome thanks dude i appreciate your help", "links": [], "channel": "scrapy"},
{"date": "2014-07-07T03:43:14.428466+00:00", "nick": "kevc", "message": "no problem", "links": [], "channel": "scrapy"},
{"date": "2014-07-07T03:43:52.436567+00:00", "nick": "TheRinger", "message": "guessing i'm going to need to use TOR or something or save a snapshot and hope for a new ip", "links": [], "channel": "scrapy"},
{"date": "2014-07-07T03:45:38.715053+00:00", "nick": "TheRinger", "message": "lease I got this far and was able to actually crawl something. I am pretty newb at python but Im trying to put together a local job scraper for my county, and this has been a lot of fun learning to build a scraper", "links": [], "channel": "scrapy"},
{"date": "2014-07-07T03:47:17.221174+00:00", "nick": "TheRinger", "message": "portia has made it incredibly easy for me to setup the right xml , i'm glad that I decided to use a remote vps to learn this so I wouldn't end up 403 forbidden from the internet before I figured all this out", "links": [], "channel": "scrapy"},
{"date": "2014-07-07T03:52:33.097732+00:00", "nick": "TheRinger", "message": "ok so DOWNLOAD_DELAY=2 and CONCURRENT_REQUESTS_PER_DOMAIN=1  would those be considered settings or arguments?", "links": [], "channel": "scrapy"},
{"date": "2014-07-07T03:53:39.845637+00:00", "nick": "TheRinger", "message": "so should i use NAME=VALUE ^^^^ or --settings=DOWNLOAD_DELAY=2\\", "links": [], "channel": "scrapy"},
{"date": "2014-07-07T03:53:55.262624+00:00", "nick": "kevc", "message": "put them in settings.py. I don't know anything about portia though", "links": [], "channel": "scrapy"},
{"date": "2014-07-07T03:54:58.256652+00:00", "nick": "TheRinger", "message": "it's the same as scrapyd with a frontend, here's the link .. http://vimeo.com/90592864  and github  https://github.com/scrapinghub/portia", "links": ["http://vimeo.com/90592864", "https://github.com/scrapinghub/portia"], "channel": "scrapy"},
{"date": "2014-07-07T03:55:22.133647+00:00", "nick": "kevc", "message": "oh, actually I saw that. I looked very interesting", "links": [], "channel": "scrapy"},
{"date": "2014-07-07T04:04:49.339527+00:00", "nick": "TheRinger", "message": "i'll send you a ip:port if you want to check it out you can use mine", "links": [], "channel": "scrapy"},
{"date": "2014-07-07T04:32:12.962435+00:00", "nick": "kevc", "message": "Thanks, I'm just going to set it up here actually, think my team would find it useful", "links": [], "channel": "scrapy"},
{"date": "2014-07-07T04:33:16.787302+00:00", "nick": "TheRinger", "message": "yah it's p. cool", "links": [], "channel": "scrapy"},
{"date": "2014-07-07T12:26:17.262806+00:00", "nick": "TheRinger", "message": "why is scrapyd always runningn?", "links": [], "channel": "scrapy"},
{"date": "2014-07-07T15:14:48.678113+00:00", "nick": "jakobdm", "message": "TheRinger: Because it's the scrapy _d_aemon. It's purpose is to sit around, waiting until someone tells it to run a job through the HTTP interface.", "links": [], "channel": "scrapy"},
{"date": "2014-07-07T15:41:19.643311+00:00", "nick": "jakobdm", "message": "*Its, dammit!", "links": [], "channel": "scrapy"},
{"date": "2014-07-07T16:24:58.921395+00:00", "nick": "TheRinger", "message": "anyone alive in here", "links": [], "channel": "scrapy"},
{"date": "2014-07-07T18:14:09.024186+00:00", "nick": "`brain", "message": "anyone have experience with pydepta ?", "links": [], "channel": "scrapy"},
{"date": "2014-07-07T18:14:45.946202+00:00", "nick": "`brain", "message": "I thought that since it is used with scrapLEY this was the wrong place to ask, but the scrapley docs say this is the right channel", "links": [], "channel": "scrapy"},
{"date": "2014-07-07T23:26:14.958569+00:00", "nick": "TheRinger", "message": "I've been playing with portia, but I'm having trouble getting it to pull both a url and a title from the same field , it doesn't add a new one at least that I can see it just replaces the current", "links": [], "channel": "scrapy"},
{"date": "2014-07-07T23:34:55.706095+00:00", "nick": "dokma", "message": "Is scrapy a good tool for the task of building a database of websites in a particular TLD for a sales department?", "links": [], "channel": "scrapy"},
{"date": "2014-07-07T23:41:29.117547+00:00", "nick": "dokma", "message": "what I'd like to do is just tell scrapy to get all websites in .hr TLD or that are in Croatian language and build me a database of their domains", "links": [], "channel": "scrapy"},
{"date": "2014-07-07T23:41:34.163030+00:00", "nick": "dokma", "message": "that's all", "links": [], "channel": "scrapy"},
{"date": "2014-07-07T23:55:32.090569+00:00", "nick": "Digenis", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2014-07-08T02:21:02.020082+00:00", "nick": "TheRinger", "message": "hey", "links": [], "channel": "scrapy"},
{"date": "2014-07-08T02:21:26.660860+00:00", "nick": "TheRinger", "message": "i need to setup a ping for when someone says something in here.. << --- so many questions i have...", "links": [], "channel": "scrapy"},
{"date": "2014-07-08T02:24:52.359562+00:00", "nick": "sunshine`", "message": "ls", "links": [], "channel": "scrapy"},
{"date": "2014-07-08T02:24:55.564356+00:00", "nick": "sunshine`", "message": "Hi", "links": [], "channel": "scrapy"},
{"date": "2014-07-08T02:25:03.763892+00:00", "nick": "neptune", "message": "ls", "links": [], "channel": "scrapy"},
{"date": "2014-07-08T02:25:08.973090+00:00", "nick": "neptune", "message": "Hi everybody", "links": [], "channel": "scrapy"},
{"date": "2014-07-08T02:42:54.218113+00:00", "nick": "TheRinger", "message": "yo", "links": [], "channel": "scrapy"},
{"date": "2014-07-08T03:16:13.246373+00:00", "nick": "kevc", "message": "dokma: scrapy could crawl the sites, but not sure where you'd get the list of sites in the .hr TLD", "links": [], "channel": "scrapy"},
{"date": "2014-07-08T05:05:30.920749+00:00", "nick": "TheRinger", "message": "hey dudes, i'm having a bit of trouble, i'm trying to scrape craigslist for jobs and pull the job title and the url , i'm using portia, but if I set the name tag then goto set the url tag one replaces the other and I'm not able to scrape the both.. what am I doing wrong..", "links": [], "channel": "scrapy"},
{"date": "2014-07-08T05:43:09.882054+00:00", "nick": "eN_Joy", "message": "general approach for modeling please: i am considering scraping dozens of site that have almost identical structure, i am particularly interested in scraping their tabulated infomation for each of the 200ish countries, so the row looks like: `country var1 var2...`, the goal is to build a pivot table on coutries, for example, user chooses `country1`, the page will list `var1` and/or `var2` in some sorted order, would you put each line for an item field", "links": [], "channel": "scrapy"},
{"date": "2014-07-08T05:44:21.988311+00:00", "nick": "kevc", "message": "eN_Joy: I'd normally suggest outputting whatever form it easiest to write the scraper code for", "links": [], "channel": "scrapy"},
{"date": "2014-07-08T05:44:52.108203+00:00", "nick": "kevc", "message": "and then transforming at a second stage processor, probably loading into an SQL database at that point", "links": [], "channel": "scrapy"},
{"date": "2014-07-08T05:47:36.675066+00:00", "nick": "eN_Joy", "message": "putting 200ish lines of field in items.py, and 200ish xpath's in spider, then pipe in database is very straightforward, just i kinda feel ugly;-) maybe at least i can build a list of countries, the `for loop` in items.py and spider? maybe?", "links": [], "channel": "scrapy"},
{"date": "2014-07-08T05:48:20.068260+00:00", "nick": "eN_Joy", "message": "at least the code will be shorter...", "links": [], "channel": "scrapy"},
{"date": "2014-07-08T05:51:33.892993+00:00", "nick": "kevc", "message": "scraper code is harder to write than code reading in JSON, so it's better to keep it succinct", "links": [], "channel": "scrapy"},
{"date": "2014-07-08T05:57:06.696216+00:00", "nick": "eN_Joy", "message": "hey i just found out scrapy added the line `# -*- coding: utf-8 -*-` automatically, is this a new thing in 0.24?", "links": [], "channel": "scrapy"},
{"date": "2014-07-08T06:12:02.772146+00:00", "nick": "Digenis", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2014-07-08T06:22:05.518550+00:00", "nick": "eN_Joy", "message": "Digenis: django should do this, too", "links": [], "channel": "scrapy"},
{"date": "2014-07-08T08:44:36.366455+00:00", "nick": "dokma", "message": "kevc, I was hopping to give scrapy a top .hr directory that contains thousands of Croatian web sites and have it find the rest of them same way Google does.", "links": [], "channel": "scrapy"},
{"date": "2014-07-08T09:03:20.873946+00:00", "nick": "kevc", "message": "dokma: google does a lot more than just link crawling, so not sure how far you would get trying to crawl all of .hr", "links": [], "channel": "scrapy"},
{"date": "2014-07-08T11:32:09.626714+00:00", "nick": "scmp", "message": "Hello. How do i issue a Request() from a CrawlSpider parse function that is tested for duplication by scrapy.dupefilter?", "links": [], "channel": "scrapy"},
{"date": "2014-07-08T11:41:59.202623+00:00", "nick": "nikolaosk", "message": "Request(dont_filter=True)", "links": [], "channel": "scrapy"},
{"date": "2014-07-08T11:42:33.570058+00:00", "nick": "nikolaosk", "message": "wait, tested you said, not (not tested)", "links": [], "channel": "scrapy"},
{"date": "2014-07-08T11:44:28.784275+00:00", "nick": "scmp", "message": "yeah, the issue is that my parse function yields a Request() and those requests are not filtered in a second run.", "links": [], "channel": "scrapy"},
{"date": "2014-07-08T11:46:49.668375+00:00", "nick": "TheRinger", "message": "https://github.com/tytek2012/Portia_Installer_S...", "links": ["https://github.com/tytek2012/Portia_Installer_Script"], "channel": "scrapy"},
{"date": "2014-07-08T12:00:34.093384+00:00", "nick": "nikolaosk", "message": "are you sure they are the same requests?", "links": [], "channel": "scrapy"},
{"date": "2014-07-08T12:01:19.861135+00:00", "nick": "nikolaosk", "message": "what if a query arg in it changes", "links": [], "channel": "scrapy"},
{"date": "2014-07-08T12:02:07.696446+00:00", "nick": "scmp", "message": "no, i think the issue seems to be scrapylib.deltafetch", "links": [], "channel": "scrapy"},
{"date": "2014-07-08T12:03:53.877015+00:00", "nick": "scmp", "message": "just realized it's different from the scrapy.dupefilter.. but produces similiar log messages", "links": [], "channel": "scrapy"},
{"date": "2014-07-08T12:05:31.737699+00:00", "nick": "scmp", "message": "https://github.com/scrapinghub/scrapylib/blob/m...", "links": ["https://github.com/scrapinghub/scrapylib/blob/master/scrapylib/deltafetch.py#L71"], "channel": "scrapy"},
{"date": "2014-07-08T12:07:57.878454+00:00", "nick": "scmp", "message": "could it be that the key is not set?", "links": [], "channel": "scrapy"},
{"date": "2014-07-08T12:12:23.157787+00:00", "nick": "scmp", "message": "to clarify, the scrapylib.deltafetch keeps requesting those urls, not scrapy.dupefilter", "links": [], "channel": "scrapy"},
{"date": "2014-07-08T12:37:20.941846+00:00", "nick": "scmp", "message": "Request(url=url, callback=cb, meta={'deltafetch_key': link.url}) seems to fix it", "links": [], "channel": "scrapy"},
{"date": "2014-07-08T12:57:05.566432+00:00", "nick": "scmp", "message": "unclear why. could be an issue with scrapy.utils.request.request_fingerprint and pdf files or redirects.", "links": [], "channel": "scrapy"},
{"date": "2014-07-08T15:36:57.164840+00:00", "nick": "kurtjx", "message": "hello Scrapy people!  I\u2019m wondering about duplicate-request filtering.  Is there an implementation of the duplicate filter that somehow caches previous response and when a duplicate request is made, just provides the cached response?", "links": [], "channel": "scrapy"},
{"date": "2014-07-09T03:13:49.260696+00:00", "nick": "azm", "message": "Could I use scrapy to gather usernames from this website? http://wastedonlol.com/", "links": ["http://wastedonlol.com/"], "channel": "scrapy"},
{"date": "2014-07-09T04:34:22.287915+00:00", "nick": "Digenis", "message": "dangra: https://github.com/scrapy/scrapy/pull/708#issue...", "links": ["https://github.com/scrapy/scrapy/pull/708#issuecomment-47985404"], "channel": "scrapy"},
{"date": "2014-07-09T04:34:52.828916+00:00", "nick": "Digenis", "message": "how do you know this? I can't find curita's branch about this", "links": [], "channel": "scrapy"},
{"date": "2014-07-09T04:35:02.421130+00:00", "nick": "Digenis", "message": "or PR or anything", "links": [], "channel": "scrapy"},
{"date": "2014-07-09T04:36:49.785876+00:00", "nick": "Digenis", "message": "is do the gsoc project specifications refer to it?", "links": [], "channel": "scrapy"},
{"date": "2014-07-09T04:37:16.081976+00:00", "nick": "Digenis", "message": "I wanted to report a bug found by a colleague of mine", "links": [], "channel": "scrapy"},
{"date": "2014-07-09T04:38:05.358205+00:00", "nick": "Digenis", "message": "in the start requests he yielded some items", "links": [], "channel": "scrapy"},
{"date": "2014-07-09T04:38:18.337787+00:00", "nick": "Digenis", "message": "obviously this doesn't work", "links": [], "channel": "scrapy"},
{"date": "2014-07-09T04:46:15.716822+00:00", "nick": "Digenis", "message": "sorry for the delay, phone rang", "links": [], "channel": "scrapy"},
{"date": "2014-07-09T04:46:18.380120+00:00", "nick": "Digenis", "message": "althoygh a proper error message in the log is needed as pablohof suggests", "links": [], "channel": "scrapy"},
{"date": "2014-07-09T04:46:50.005596+00:00", "nick": "Digenis", "message": "as when a parse callback yield neither an item or request", "links": [], "channel": "scrapy"},
{"date": "2014-07-09T04:46:59.444394+00:00", "nick": "Digenis", "message": "yields*", "links": [], "channel": "scrapy"},
{"date": "2014-07-09T04:48:04.182404+00:00", "nick": "Digenis", "message": "there are use cases, I think reported by chekunkov or nramirezuy", "links": [], "channel": "scrapy"},
{"date": "2014-07-09T04:48:20.735833+00:00", "nick": "Digenis", "message": "where items are used instead of log messages", "links": [], "channel": "scrapy"},
{"date": "2014-07-09T04:49:21.046736+00:00", "nick": "Digenis", "message": "in my colleague's case, he was trying to resume a crashed session by defining start_requests", "links": [], "channel": "scrapy"},
{"date": "2014-07-09T04:51:42.239304+00:00", "nick": "Digenis", "message": "nice, didn't notice the connection reset and kept talking", "links": [], "channel": "scrapy"},
{"date": "2014-07-09T04:52:40.478051+00:00", "nick": "Digenis", "message": "dangra: http://pastebin.kde.org/pni6blsof/fjr2gj", "links": ["http://pastebin.kde.org/pni6blsof/fjr2gj"], "channel": "scrapy"},
{"date": "2014-07-09T04:55:33.109710+00:00", "nick": "dangra", "message": "reading", "links": [], "channel": "scrapy"},
{"date": "2014-07-09T05:07:14.513368+00:00", "nick": "Digenis", "message": "I wanted to open a feature request for this, allowing items in start_requests", "links": [], "channel": "scrapy"},
{"date": "2014-07-09T05:07:57.978521+00:00", "nick": "Digenis", "message": "but it looks to me like there is already a plan to close this as described in the issue at github", "links": [], "channel": "scrapy"},
{"date": "2014-07-09T05:09:07.440719+00:00", "nick": "dangra", "message": "at first start_requests are only for requests, but I am open to discuss supporting items if you can prove it is useful", "links": [], "channel": "scrapy"},
{"date": "2014-07-09T05:09:43.517568+00:00", "nick": "dangra", "message": "open a feature request and describe your use case, other maintainers and people around scrapy can share their thoughts", "links": [], "channel": "scrapy"},
{"date": "2014-07-09T05:12:53.268925+00:00", "nick": "Digenis", "message": "indeed, yielding items would render the method name invalid", "links": [], "channel": "scrapy"},
{"date": "2014-07-09T05:12:58.627379+00:00", "nick": "Digenis", "message": "ok, I 'll open one", "links": [], "channel": "scrapy"},
{"date": "2014-07-09T05:32:43.312384+00:00", "nick": "nullne", "message": "halu  i am a newbie in scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-07-10T00:47:00.145439+00:00", "nick": "anviloro", "message": "Hi!", "links": [], "channel": "scrapy"},
{"date": "2014-07-10T00:47:41.432250+00:00", "nick": "anviloro", "message": "I'll start using scrapy to get json format requests, any idea is welcome :)", "links": [], "channel": "scrapy"},
{"date": "2014-07-10T03:06:22.964629+00:00", "nick": "FrankieDux", "message": "I'm getting an: HtmlResponse object has no attribute 'xpath' error when following the tutorial", "links": [], "channel": "scrapy"},
{"date": "2014-07-10T03:07:25.170922+00:00", "nick": "FrankieDux", "message": "I'm assuming something must have been changed and the tutorial doesn't reflect the changes made", "links": [], "channel": "scrapy"},
{"date": "2014-07-10T03:13:30.638472+00:00", "nick": "Digenis", "message": "FrankieDux: you have an older version of scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-07-10T03:13:45.510074+00:00", "nick": "Digenis", "message": "not .24", "links": [], "channel": "scrapy"},
{"date": "2014-07-10T03:24:08.725516+00:00", "nick": "FrankieDux", "message": "Digenis: that was the problem. thanks.", "links": [], "channel": "scrapy"},
{"date": "2014-07-10T14:32:20.974986+00:00", "nick": "nixfreak", "message": "Hi everyone can someone look at my spider for me? -- https://www.paste.to/toguxIzg", "links": ["https://www.paste.to/toguxIzg"], "channel": "scrapy"},
{"date": "2014-07-10T14:40:48.309529+00:00", "nick": "nikolaosk", "message": "nixfreak: lookup super in python doc and fix it", "links": [], "channel": "scrapy"},
{"date": "2014-07-10T14:47:40.611705+00:00", "nick": "nixfreak", "message": "ok thx", "links": [], "channel": "scrapy"},
{"date": "2014-07-10T20:10:54.483646+00:00", "nick": "Newbie007", "message": "Hey there, so I'm super new and would love some guidance.  I'm trying to make a spider that pulls data from a website and then proceeds onto the next page. I need it to change \"search=&type=4&filter_submit=Filter&page=2\" to page=3, then 4, etc. What's the best means of achieving that through my linkextractor? Any help is appreciated!", "links": [], "channel": "scrapy"},
{"date": "2014-07-11T09:16:44.844941+00:00", "nick": "orfeo", "message": "Bello all,", "links": [], "channel": "scrapy"},
{"date": "2014-07-11T09:19:04.782292+00:00", "nick": "orfeo", "message": "On scrapy 0.22 release how declare the Request method? import scrapy and then scrapy.Request(... Mamy thanks", "links": [], "channel": "scrapy"},
{"date": "2014-07-11T10:39:17.313208+00:00", "nick": "orfeo", "message": "Sorry, how can make a callback parse with scrapy 0.22. It's different about 0.24? Thanks", "links": [], "channel": "scrapy"},
{"date": "2014-07-11T10:40:27.671452+00:00", "nick": "orfeo", "message": "Hello", "links": [], "channel": "scrapy"},
{"date": "2014-07-11T10:45:34.280737+00:00", "nick": "orfeo", "message": "Sorry, but how can make to write a correct callback function on scrapy 0.22?", "links": [], "channel": "scrapy"},
{"date": "2014-07-11T10:48:28.752180+00:00", "nick": "nikolaosk", "message": "all the same, def parse_something(self, response)", "links": [], "channel": "scrapy"},
{"date": "2014-07-11T10:49:08.550913+00:00", "nick": "nikolaosk", "message": "the only thing that may confuse you is that at some point the default spider created by genspider command is the base Spider and not CrawlSpider", "links": [], "channel": "scrapy"},
{"date": "2014-07-11T10:49:32.870397+00:00", "nick": "nikolaosk", "message": "CrawlSpider's parse() method shouldn't be overriden", "links": [], "channel": "scrapy"},
{"date": "2014-07-11T10:49:41.902290+00:00", "nick": "nikolaosk", "message": "and another name should be picked", "links": [], "channel": "scrapy"},
{"date": "2014-07-11T10:54:04.557550+00:00", "nick": "orfeo", "message": "Thanks Nick, the correct flow is parse() obbligatory and then parse2() etc?", "links": [], "channel": "scrapy"},
{"date": "2014-07-11T11:02:07.764117+00:00", "nick": "nikolaosk", "message": "no", "links": [], "channel": "scrapy"},
{"date": "2014-07-11T11:02:25.275342+00:00", "nick": "nikolaosk", "message": "when you don't define a callback in the request", "links": [], "channel": "scrapy"},
{"date": "2014-07-11T11:02:31.273428+00:00", "nick": "nikolaosk", "message": "you get parse called as default", "links": [], "channel": "scrapy"},
{"date": "2014-07-11T19:54:24.448173+00:00", "nick": "t__", "message": "A complete noob here. Would anyone be able to help with a small bug im having?", "links": [], "channel": "scrapy"},
{"date": "2014-07-11T19:55:23.459272+00:00", "nick": "t__", "message": "i have these two lines", "links": [], "channel": "scrapy"},
{"date": "2014-07-11T19:55:25.778850+00:00", "nick": "t__", "message": "self.log('A response from %s just arrived!' % response.url) hxs = HtmlXPathSelector(response)", "links": [], "channel": "scrapy"},
{"date": "2014-07-11T19:56:07.887292+00:00", "nick": "t__", "message": "and get the error NameError: name 'hxs' is not defined", "links": [], "channel": "scrapy"},
{"date": "2014-07-11T19:56:15.590124+00:00", "nick": "t__", "message": "any ideas?", "links": [], "channel": "scrapy"},
{"date": "2014-07-11T19:56:43.888291+00:00", "nick": "t__", "message": "sorry the two lines are", "links": [], "channel": "scrapy"},
{"date": "2014-07-11T19:56:44.591663+00:00", "nick": "t__", "message": "hxs = HtmlXPathSelector(response)         title = hxs.select('//*[@id=\"product_title\"...", "links": ["mailto:hxs.select('//*[@id=\"product_title\"]/span[1]/').extract()"], "channel": "scrapy"},
{"date": "2014-07-12T02:01:46.103607+00:00", "nick": "esbee", "message": "yo", "links": [], "channel": "scrapy"},
{"date": "2014-07-12T02:03:23.889508+00:00", "nick": "esbee", "message": "whats the best way to scrape a database search interface? permutate through all combos?", "links": [], "channel": "scrapy"},
{"date": "2014-07-12T02:03:34.448976+00:00", "nick": "esbee", "message": "whats the googlefoo for answer?", "links": [], "channel": "scrapy"},
{"date": "2014-07-12T15:19:37.087143+00:00", "nick": "kajiChestno", "message": "mhm", "links": [], "channel": "scrapy"},
{"date": "2014-07-12T15:20:06.548982+00:00", "nick": "kajiChestno", "message": "mhm", "links": [], "channel": "scrapy"},
{"date": "2014-07-13T11:18:26.598021+00:00", "nick": "yaddaallover", "message": "Hey folks. Just wanted to share my joy. After DAYS of suffering in silence trying to get this to work and hours of youtube vids.. I came into this room to ask questions BUT I am finally getting somewhere anyway. Amazing what happens when you stick it out. Have a great day :)", "links": [], "channel": "scrapy"},
{"date": "2014-07-13T12:15:22.603795+00:00", "nick": "hannes|", "message": "Hi all, I am trying to follow the tutorial, on http://doc.scrapy.org/en/latest/intro/tutorial....", "links": ["http://doc.scrapy.org/en/latest/intro/tutorial.html"], "channel": "scrapy"},
{"date": "2014-07-13T12:15:46.024479+00:00", "nick": "hannes|", "message": "but I am not sure if I made it correctly, because I can't get it to work", "links": [], "channel": "scrapy"},
{"date": "2014-07-13T12:16:24.226897+00:00", "nick": "hannes|", "message": "Is it enough to edit the items.py and create the dmoz_spider.py", "links": [], "channel": "scrapy"},
{"date": "2014-07-13T12:16:41.929804+00:00", "nick": "hannes|", "message": "Or do I need to change something of those files to make it really work", "links": [], "channel": "scrapy"},
{"date": "2014-07-13T12:18:44.583323+00:00", "nick": "hannes|", "message": "items.py http://paste.fedoraproject.org/117647/14052538 and dmoz_spider.py http://paste.fedoraproject.org/117648/40525384", "links": ["http://paste.fedoraproject.org/117647/14052538", "http://paste.fedoraproject.org/117648/40525384"], "channel": "scrapy"},
{"date": "2014-07-13T12:19:37.179992+00:00", "nick": "hannes|", "message": "http://fpaste.org/117649/40525393/", "links": ["http://fpaste.org/117649/40525393/"], "channel": "scrapy"},
{"date": "2014-07-13T12:20:45.837720+00:00", "nick": "hannes|", "message": "that's the error on fedora 20 with the yum installed version", "links": [], "channel": "scrapy"},
{"date": "2014-07-13T12:56:45.816134+00:00", "nick": "jakobdm", "message": "hannes|: The repository version (0.22) is older than the version that the tutorial was written for (0.24)", "links": [], "channel": "scrapy"},
{"date": "2014-07-13T12:58:04.072950+00:00", "nick": "jakobdm", "message": "You could either update scrapy to 0.24 via pip, or look at the older version of the tutorial: At readthedocs, click on \"v: latest\" at the bottom left and select version 0.22", "links": [], "channel": "scrapy"},
{"date": "2014-07-13T12:59:20.392810+00:00", "nick": "jakobdm", "message": "(you'll then see that 'import scrapy' is exchanged with 'from scrapy.spider import Spider', and 'DmozSpider(scrapy.Spider)' is exchanged with 'DmozSpider(Spider)')", "links": [], "channel": "scrapy"},
{"date": "2014-07-13T13:08:44.619476+00:00", "nick": "hannes|", "message": "jakobdm, ah ok, thanks", "links": [], "channel": "scrapy"},
{"date": "2014-07-13T13:11:19.412333+00:00", "nick": "hannes|", "message": "works, :-)", "links": [], "channel": "scrapy"},
{"date": "2014-07-13T13:11:23.640462+00:00", "nick": "hannes|", "message": "thanks again jakobdm", "links": [], "channel": "scrapy"},
{"date": "2014-07-13T13:15:23.705207+00:00", "nick": "jakobdm", "message": "Sure, glad I could help. ;)", "links": [], "channel": "scrapy"},
{"date": "2014-07-14T10:42:07.178406+00:00", "nick": "scmp", "message": "Hi. Using a CrawlSpider i scrape date,url from a index page and url,title,etc from a detail page. Both functions create \"particial\" Items. What are the options to merge these Items?", "links": [], "channel": "scrapy"},
{"date": "2014-07-14T10:43:59.652744+00:00", "nick": "scmp", "message": "(In this case the date is only visible on the index page, not on the detail page)", "links": [], "channel": "scrapy"},
{"date": "2014-07-14T11:56:46.501683+00:00", "nick": "odigem", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2014-07-14T11:57:14.602954+00:00", "nick": "odigem", "message": "how to use scrapy outside spider?", "links": [], "channel": "scrapy"},
{"date": "2014-07-14T16:39:09.385410+00:00", "nick": "yaddaallover", "message": "Hi folks. I'm trying to get Scrapy to grab the text from a drop down menu. I've got as far as the text \"Choose size:\" but would like to grab the text that one step further.", "links": [], "channel": "scrapy"},
{"date": "2014-07-14T16:39:13.981055+00:00", "nick": "yaddaallover", "message": "This is the code from Scrapy http://pastebin.com/HwKT74wN that only grabs the \"Choose size:\" text", "links": ["http://pastebin.com/HwKT74wN"], "channel": "scrapy"},
{"date": "2014-07-14T16:39:47.091365+00:00", "nick": "yaddaallover", "message": "and this is the html on the page I am trying to scrape http://pastebin.com/ZnK48wjf", "links": ["http://pastebin.com/ZnK48wjf"], "channel": "scrapy"},
{"date": "2014-07-14T16:41:26.544727+00:00", "nick": "yaddaallover", "message": "I can see in the docs that it's FormRequest objects I need but my brain is not cooperating", "links": [], "channel": "scrapy"},
{"date": "2014-07-14T16:52:13.242353+00:00", "nick": "yaddaallover", "message": "omg I was editing the wrong Field.. I got it working. YAY. Nothing to see here ;)", "links": [], "channel": "scrapy"},
{"date": "2014-07-14T20:34:52.864154+00:00", "nick": "Doginal", "message": "hey guys, i am trying to ignore requests to social media, ie twitter, facebook. I have written a middleware but now my crawler isnt hitting any pages. heres the code from the middleware. http://pastebin.com/BCb3acJC   any ideas why this is stopping my crawl?", "links": ["http://pastebin.com/BCb3acJC"], "channel": "scrapy"},
{"date": "2014-07-14T21:14:04.911844+00:00", "nick": "jakobdm", "message": "Doginal: When the process_request method returns a Request, that Request will be scheduled and the original Request will stop being processed.", "links": [], "channel": "scrapy"},
{"date": "2014-07-14T21:14:50.894762+00:00", "nick": "jakobdm", "message": "You should change line 9 to \"return None\"", "links": [], "channel": "scrapy"},
{"date": "2014-07-14T21:17:50.337600+00:00", "nick": "Doginal", "message": "yea i did that and it fix it", "links": [], "channel": "scrapy"},
{"date": "2014-07-14T21:18:06.163991+00:00", "nick": "Doginal", "message": "thought i would try that, thanks jaobdm", "links": [], "channel": "scrapy"},
{"date": "2014-07-14T21:29:30.879817+00:00", "nick": "jakobdm", "message": "Sure, good you found it on your own. If I may give you a tip, you should not catch the IgnoreRequest exception (especially not with a catch-all \"except:\") as that defeats its purpose, which is that other downloader middlewares and finally the errback method of the Request are notified that the request was ignored.", "links": [], "channel": "scrapy"},
{"date": "2014-07-14T21:37:04.616930+00:00", "nick": "Doginal", "message": "okay, thanks jakobdm", "links": [], "channel": "scrapy"},
{"date": "2014-07-14T21:38:58.273495+00:00", "nick": "Doginal", "message": "hmm i am now getting alot of this traceback   raise SGMLParseError(message) sgmllib.SGMLParseError: expected name token at", "links": [], "channel": "scrapy"},
{"date": "2014-07-14T21:43:32.828154+00:00", "nick": "Doginal", "message": "is that the unicode breaking the SgmlLinkExtractor?", "links": [], "channel": "scrapy"},
{"date": "2014-07-14T22:26:17.595598+00:00", "nick": "jakobdm", "message": "Argh, that was totally misleading.", "links": [], "channel": "scrapy"},
{"date": "2014-07-14T23:41:02.299558+00:00", "nick": "Doginal", "message": "I can't seem to find anything on fixing this traceback:  raise SGMLParseError(message) sgmllib.SGMLParseError: expected name token at '<! SITEMENU END ->\\r\\n' if anyone answer me before i think my net dropped for a while", "links": [], "channel": "scrapy"},
{"date": "2014-07-15T10:00:43.141320+00:00", "nick": "peter_", "message": "Hi", "links": [], "channel": "scrapy"},
{"date": "2014-07-15T10:02:10.238313+00:00", "nick": "nlpeter", "message": "Hello", "links": [], "channel": "scrapy"},
{"date": "2014-07-15T10:02:19.243728+00:00", "nick": "nlpeter", "message": "can someone help me with my crawlspider please?", "links": [], "channel": "scrapy"},
{"date": "2014-07-15T10:03:22.766964+00:00", "nick": "nlpeter", "message": "i want my spider to follow links, which is working fine. however when i add my parse method it stops following links", "links": [], "channel": "scrapy"},
{"date": "2014-07-15T10:31:00.766309+00:00", "nick": "nlpeter", "message": "can someone help me with my spider?", "links": [], "channel": "scrapy"},
{"date": "2014-07-15T10:31:05.534619+00:00", "nick": "nlpeter", "message": "it is not following links", "links": [], "channel": "scrapy"},
{"date": "2014-07-15T10:31:11.863606+00:00", "nick": "nlpeter", "message": "https://github.com/petervdtoorn/scrapers/blob/m...", "links": ["https://github.com/petervdtoorn/scrapers/blob/master/Spiders/bouwmachineforum/bouwmachineforum/spiders/spider.py"], "channel": "scrapy"},
{"date": "2014-07-15T10:31:28.217207+00:00", "nick": "nlpeter", "message": "when i remove the parse method it works", "links": [], "channel": "scrapy"},
{"date": "2014-07-15T10:31:36.926380+00:00", "nick": "nlpeter", "message": "i dont understand why", "links": [], "channel": "scrapy"},
{"date": "2014-07-15T14:19:54.309099+00:00", "nick": "nixfreak", "message": "http://pastebin.ca/2821229", "links": ["http://pastebin.ca/2821229"], "channel": "scrapy"},
{"date": "2014-07-15T14:27:05.792286+00:00", "nick": "nixfreak", "message": "Hey guys good morning.. trying to scrape/spider google search results via keyword input here is the code http://pastebin.ca/2821229", "links": ["http://pastebin.ca/2821229"], "channel": "scrapy"},
{"date": "2014-07-15T14:35:34.591627+00:00", "nick": "rebsadran", "message": "are there any pre-requisites for scrapy aside from lxml?", "links": [], "channel": "scrapy"},
{"date": "2014-07-15T14:35:49.811035+00:00", "nick": "rebsadran", "message": "I'm trying to install it on pycharm virtualenv and I get this: http://paste.ubuntu.com/7798689/", "links": ["http://paste.ubuntu.com/7798689/"], "channel": "scrapy"},
{"date": "2014-07-15T14:37:37.460300+00:00", "nick": "jakobdm", "message": "rebsadran: You will also need libffi (see line 68)", "links": [], "channel": "scrapy"},
{"date": "2014-07-15T14:37:58.818209+00:00", "nick": "tomwardill", "message": "rebsadran: you need libffi (apt-get install libffi-dev if you\u2019re on a debian/ubuntu system)", "links": [], "channel": "scrapy"},
{"date": "2014-07-15T14:38:11.321021+00:00", "nick": "jakobdm", "message": "nixfreak: Your definition of __init__ has the wrong indentation level", "links": [], "channel": "scrapy"},
{"date": "2014-07-15T14:39:00.187621+00:00", "nick": "jakobdm", "message": "Your definition of the wndSpider class ends with line 10, then there's a (free-standing) function __init__ within which (and only therein) a function parse is defined.", "links": [], "channel": "scrapy"},
{"date": "2014-07-15T14:39:12.388126+00:00", "nick": "rebsadran", "message": "Thanks, how do you find some sense in these errors?", "links": [], "channel": "scrapy"},
{"date": "2014-07-15T14:39:31.507239+00:00", "nick": "rebsadran", "message": "I've been looking at it for good 10 minutes...", "links": [], "channel": "scrapy"},
{"date": "2014-07-15T14:40:39.735306+00:00", "nick": "jakobdm", "message": "It's mostly a matter of experience. In your case you could've also looked for lines that contain 'error'. ;)", "links": [], "channel": "scrapy"},
{"date": "2014-07-15T14:43:57.748212+00:00", "nick": "rebsadran", "message": "ok got new one, now I think it's this line: fatal error: openssl/aes.h: No such file or directory", "links": [], "channel": "scrapy"},
{"date": "2014-07-15T14:44:02.847097+00:00", "nick": "rebsadran", "message": "however I have openssl", "links": [], "channel": "scrapy"},
{"date": "2014-07-15T14:44:09.858034+00:00", "nick": "rebsadran", "message": "complete log: http://paste.opensuse.org/1095855", "links": ["http://paste.opensuse.org/1095855"], "channel": "scrapy"},
{"date": "2014-07-15T14:44:36.516108+00:00", "nick": "tomwardill", "message": "rebsadran: do you have libssl-dev ?", "links": [], "channel": "scrapy"},
{"date": "2014-07-15T14:46:06.723436+00:00", "nick": "rebsadran", "message": "now I do, trying to install again", "links": [], "channel": "scrapy"},
{"date": "2014-07-15T14:46:28.639806+00:00", "nick": "rebsadran", "message": "I don't get it why installing some packages is such a bitch. How do you even deploy stuff like this?", "links": [], "channel": "scrapy"},
{"date": "2014-07-15T14:47:21.017717+00:00", "nick": "rebsadran", "message": "finally got it installed, thanks!", "links": [], "channel": "scrapy"},
{"date": "2014-07-15T14:47:30.922745+00:00", "nick": "jakobdm", "message": "It's the price of compiling stuff yourself. ;)", "links": [], "channel": "scrapy"},
{"date": "2014-07-15T14:47:52.208761+00:00", "nick": "jakobdm", "message": "That's why there are repositories with precompiled packages.", "links": [], "channel": "scrapy"},
{"date": "2014-07-15T14:56:36.130102+00:00", "nick": "nixfreak", "message": "Thank you for the feedback", "links": [], "channel": "scrapy"},
{"date": "2014-07-15T14:58:25.428806+00:00", "nick": "jakobdm", "message": "No problem, does it work?", "links": [], "channel": "scrapy"},
{"date": "2014-07-15T14:59:04.487947+00:00", "nick": "nixfreak", "message": "Trying to figure it out now", "links": [], "channel": "scrapy"},
{"date": "2014-07-15T14:59:29.635870+00:00", "nick": "nixfreak", "message": "i was looking at an example in the scrapy pdf for manipulating the spider", "links": [], "channel": "scrapy"},
{"date": "2014-07-15T19:24:23.684924+00:00", "nick": "nixfreak", "message": "\\wc", "links": [], "channel": "scrapy"},
{"date": "2014-07-15T19:30:23.714348+00:00", "nick": "rebsadran", "message": "I'm currently starting out with scrapy and pycharm is failing to import classes from items, i.e. \"from tutorial.items import MinovaItem\" is marked as a failure even though it should work?", "links": [], "channel": "scrapy"},
{"date": "2014-07-15T19:31:06.346406+00:00", "nick": "rebsadran", "message": "the data structure is tutorial one /tutorial/tutorial/spider,items.py,pipelines.py etc.", "links": [], "channel": "scrapy"},
{"date": "2014-07-15T19:31:15.707436+00:00", "nick": "rebsadran", "message": "*project structure", "links": [], "channel": "scrapy"},
{"date": "2014-07-15T19:32:22.969021+00:00", "nick": "rebsadran", "message": "whole pycharm skeleton is kinda off by the looks of it", "links": [], "channel": "scrapy"},
{"date": "2014-07-15T19:41:37.029946+00:00", "nick": "fcanas", "message": "rebsadran, I saw this before and it was due to pycharm using the wrong python interpreter. Changing it to use python 2.7 in file-> settings -> project interpreter fixed it for me. Maybe?", "links": [], "channel": "scrapy"},
{"date": "2014-07-15T19:41:56.269306+00:00", "nick": "rebsadran", "message": "I'm running a virtual env of 2.7", "links": [], "channel": "scrapy"},
{"date": "2014-07-15T19:52:56.592220+00:00", "nick": "rebsadran", "message": "there's no auto complete for most of the stuff either", "links": [], "channel": "scrapy"},
{"date": "2014-07-15T19:53:00.310444+00:00", "nick": "rebsadran", "message": "really weird", "links": [], "channel": "scrapy"},
{"date": "2014-07-15T20:39:22.925805+00:00", "nick": "rebsadran", "message": "how can I make an xpath that would select the kilobite data in this example: http://paste.opensuse.org/65467657", "links": ["http://paste.opensuse.org/65467657"], "channel": "scrapy"},
{"date": "2014-07-15T20:40:05.311917+00:00", "nick": "rebsadran", "message": "/div[@id='specifications']/p[2] includes the \"Total size\" as wel", "links": [], "channel": "scrapy"},
{"date": "2014-07-15T21:00:53.105331+00:00", "nick": "yhager", "message": "rebsadran: use .re() on the selector to get what you want", "links": [], "channel": "scrapy"},
{"date": "2014-07-15T21:02:10.955865+00:00", "nick": "rebsadran", "message": "yhager: thnx", "links": [], "channel": "scrapy"},
{"date": "2014-07-15T21:02:41.957421+00:00", "nick": "rebsadran", "message": "is there a way to get rid of extra newlines, tabs and other garbage from the text() ?", "links": [], "channel": "scrapy"},
{"date": "2014-07-15T21:02:58.443289+00:00", "nick": "rebsadran", "message": "I have this string: \"\\n\\n11.86\\u00a0megabyte\"", "links": [], "channel": "scrapy"},
{"date": "2014-07-15T21:03:56.760059+00:00", "nick": "yhager", "message": "maybe normalize-space() in the xpath", "links": [], "channel": "scrapy"},
{"date": "2014-07-15T21:04:45.170560+00:00", "nick": "rebsadran", "message": "I thought about parsing it through re.sub(\"\\n+|\\t+|\\u00a0\",\" \", string)", "links": [], "channel": "scrapy"},
{"date": "2014-07-15T21:04:55.922060+00:00", "nick": "rebsadran", "message": "but that's a lot of trash code", "links": [], "channel": "scrapy"},
{"date": "2014-07-15T21:05:08.347224+00:00", "nick": "rebsadran", "message": "I'll try that", "links": [], "channel": "scrapy"},
{"date": "2014-07-15T21:06:01.312993+00:00", "nick": "rebsadran", "message": "yhager: error, invalid xpath with /normalize-space()", "links": [], "channel": "scrapy"},
{"date": "2014-07-15T21:06:48.442133+00:00", "nick": "yhager", "message": "re.sub(r'\\s+', ' ') should work.", "links": [], "channel": "scrapy"},
{"date": "2014-07-15T21:07:04.588637+00:00", "nick": "yhager", "message": "normalize-space(/div.../text())", "links": [], "channel": "scrapy"},
{"date": "2014-07-15T21:09:06.458717+00:00", "nick": "rebsadran", "message": "yeah normalize-space() seems to get rid of the string completely...", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T05:37:46.418240+00:00", "nick": "flyingtriangle", "message": "anyone know an easy way to prevent scrapy from url encoding requests?", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T09:27:21.489882+00:00", "nick": "rebsadran", "message": "I'm having trouble understanding Item loaders, so I have http://paste.kde.org/pjdtnzet1 and for nothing is either joined or edited at all, seems to be no errors either", "links": ["http://paste.kde.org/pjdtnzet1"], "channel": "scrapy"},
{"date": "2014-07-16T09:27:26.419139+00:00", "nick": "rebsadran", "message": "am I missing something?", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T09:27:37.436139+00:00", "nick": "rebsadran", "message": "I'm following the http://doc.scrapy.org/en/latest/topics/loaders.... tutorial", "links": ["http://doc.scrapy.org/en/latest/topics/loaders.html#topics-loaders"], "channel": "scrapy"},
{"date": "2014-07-16T09:53:10.142383+00:00", "nick": "nikolaosk", "message": "replace_entities?", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T09:56:13.789570+00:00", "nick": "rebsadran", "message": "oh I got it", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T09:56:22.526221+00:00", "nick": "rebsadran", "message": "they are defined in the spider itself as well", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T09:56:39.690353+00:00", "nick": "nikolaosk", "message": "are you sure replace_entities is what you want?", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T09:57:00.316728+00:00", "nick": "nikolaosk", "message": "it replaces encoded html such as &lt;", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T09:57:25.093580+00:00", "nick": "nikolaosk", "message": "why would you want to do this?", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T09:58:04.904583+00:00", "nick": "rebsadran", "message": "I'm just playing around right now", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T09:58:11.093814+00:00", "nick": "rebsadran", "message": "but what if I want just text string from it?", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T09:58:19.659810+00:00", "nick": "nikolaosk", "message": "try not to join", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T09:58:30.457431+00:00", "nick": "nikolaosk", "message": "to see the array itself", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T09:58:45.389361+00:00", "nick": "nikolaosk", "message": "maybe it has spaces you didn't notice", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T09:58:57.248605+00:00", "nick": "nikolaosk", "message": "but if you see Nothing or an empty array", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T09:59:14.215627+00:00", "nick": "nikolaosk", "message": "it means you are not adding anything to the item", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T09:59:22.743489+00:00", "nick": "nikolaosk", "message": "(not matching if you use .xpath or .css)", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:00:19.190127+00:00", "nick": "rebsadran", "message": "well yeah I got it working, originally I thought that you just define the item loader in the Items.py, but appears the whole spider structure uses it instead of the one provided in the orignal tutorial", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:00:51.129945+00:00", "nick": "nikolaosk", "message": "well, it's a mess if you ask me", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:01:15.572151+00:00", "nick": "rebsadran", "message": "what do you mean?", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:01:19.165706+00:00", "nick": "nikolaosk", "message": "no surprise people struggle with it", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:01:22.639255+00:00", "nick": "rebsadran", "message": "ooh", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:01:24.184433+00:00", "nick": "rebsadran", "message": "yeah", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:01:39.494456+00:00", "nick": "nikolaosk", "message": "item loaders try to separate parsing logic from crawling logic", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:02:01.468317+00:00", "nick": "rebsadran", "message": "I ussually just do it with bs4 and request/ asyncio, but I decided to give scrappy a go and so far it's a mixed bag", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:02:02.271271+00:00", "nick": "nikolaosk", "message": "but then you have all these calls to xpath upon them within the spider", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:04:09.180910+00:00", "nick": "nikolaosk", "message": "if you enjoy writting DSLs you can have a lot of fun with bs", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:04:48.400179+00:00", "nick": "rebsadran", "message": "DSL ?", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:04:53.287100+00:00", "nick": "nikolaosk", "message": "domain specific lang", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:05:10.367377+00:00", "nick": "rebsadran", "message": "ah", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:06:30.878057+00:00", "nick": "rebsadran", "message": "how do would you approach on purifying this : size = [\"\\n\", \"\\n1.25\\u00a0megabyte\"] ?", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:07:43.885238+00:00", "nick": "nikolaosk", "message": "join and strip", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:07:46.053467+00:00", "nick": "rebsadran", "message": "I tried Join() and re.sub away the newlines, however \\u00a0 is impossible to get rid for some reason", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:07:57.009573+00:00", "nick": "nikolaosk", "message": "or map strip join", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:08:10.053426+00:00", "nick": "nikolaosk", "message": "doesn't it strip?", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:08:24.290587+00:00", "nick": "nikolaosk", "message": "it does for me", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:08:41.215132+00:00", "nick": "nikolaosk", "message": "oh, I see", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:08:44.928288+00:00", "nick": "nikolaosk", "message": "a bytearray", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:08:56.732708+00:00", "nick": "nikolaosk", "message": "u\"\\n1.25\\u00a0megabyte\" strios", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:08:59.853246+00:00", "nick": "nikolaosk", "message": "strips*", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:09:13.265847+00:00", "nick": "nikolaosk", "message": "how did you end up with this string?", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:09:28.562730+00:00", "nick": "rebsadran", "message": "I'm doing the tutorial for Mininova scrape from scrapy docs", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:09:38.312469+00:00", "nick": "rebsadran", "message": "that's what I get with the xpath for size", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:09:50.083351+00:00", "nick": "nikolaosk", "message": "scrapy's selector methods return unicode, always afaik", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:11:28.182436+00:00", "nick": "rebsadran", "message": "well I'm not sure what's going on here then...", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:11:57.960055+00:00", "nick": "rebsadran", "message": "that's the spider: http://paste.kde.org/pixmw6u5u", "links": ["http://paste.kde.org/pixmw6u5u"], "channel": "scrapy"},
{"date": "2014-07-16T10:12:55.650641+00:00", "nick": "rebsadran", "message": ".replace(u'\\u00a0', '') works though", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:13:51.492495+00:00", "nick": "nikolaosk", "message": "no, don't rely on that, you want something else", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:15:06.273814+00:00", "nick": "rebsadran", "message": "alright, what do you suggest?", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:15:45.964176+00:00", "nick": "nikolaosk", "message": "https://github.com/scrapy/w3lib/blob/master/w3l...", "links": ["https://github.com/scrapy/w3lib/blob/master/w3lib/tests/test_html.py#L9-L46"], "channel": "scrapy"},
{"date": "2014-07-16T10:16:03.685583+00:00", "nick": "nikolaosk", "message": "from what I see in the tests, I wouldn't expect bytearray", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:16:15.034007+00:00", "nick": "nikolaosk", "message": "this array you said", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:16:34.192349+00:00", "nick": "nikolaosk", "message": "is returned by the input processor?", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:16:42.012041+00:00", "nick": "nikolaosk", "message": "I mean, is it?", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:17:08.583313+00:00", "nick": "rebsadran", "message": "Aye", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:17:26.077596+00:00", "nick": "rebsadran", "message": "http://paste.kde.org/pxm1magcw Items", "links": ["http://paste.kde.org/pxm1magcw"], "channel": "scrapy"},
{"date": "2014-07-16T10:18:38.738158+00:00", "nick": "nikolaosk", "message": "and if you remove replace_entities?", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:19:48.400923+00:00", "nick": "rebsadran", "message": "oh, it's gone", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:20:08.258013+00:00", "nick": "rebsadran", "message": "awesome, thanks", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:20:38.513697+00:00", "nick": "nikolaosk", "message": "wait, wait", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:20:59.502026+00:00", "nick": "nikolaosk", "message": "so, remove_entities was returning a non unicode string?", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:21:13.476410+00:00", "nick": "rebsadran", "message": "apparently", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:21:20.808594+00:00", "nick": "nikolaosk", "message": "a bug", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:21:53.401785+00:00", "nick": "nikolaosk", "message": "try this", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:22:37.437997+00:00", "nick": "nikolaosk", "message": "take the exact string that gets matched by your xpath expr", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:23:28.398901+00:00", "nick": "nikolaosk", "message": "do you have scrapy .24?", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:23:36.621525+00:00", "nick": "rebsadran", "message": "yeah", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:23:43.578482+00:00", "nick": "nikolaosk", "message": "try response.xpath(\"//div[@id='specifications']/p[2]/text()\")", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:23:53.494810+00:00", "nick": "nikolaosk", "message": "try response.xpath(\"//div[@id='specificatio...", "links": ["mailto:response.xpath(\"//div[@id='specifications']/p[2]/text()\").extract()"], "channel": "scrapy"},
{"date": "2014-07-16T10:24:16.590746+00:00", "nick": "rebsadran", "message": "you mean without ItemLoader?", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:24:35.470511+00:00", "nick": "nikolaosk", "message": "maybe let it asside", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:24:39.125834+00:00", "nick": "nikolaosk", "message": "give me a url", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:24:56.643274+00:00", "nick": "nikolaosk", "message": "or you can try it yourself", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:25:01.495709+00:00", "nick": "nikolaosk", "message": "from the shell", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:25:11.548008+00:00", "nick": "nikolaosk", "message": "scrapy shell \"url\"", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:25:22.046891+00:00", "nick": "nikolaosk", "message": "and then you drop in a python shell", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:25:26.854749+00:00", "nick": "rebsadran", "message": "well I had this exact xpath before I moved the whole thing to ItemLoader", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:25:34.991093+00:00", "nick": "rebsadran", "message": "same results", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:25:57.886776+00:00", "nick": "nikolaosk", "message": "try the above command on a shell", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:26:15.379015+00:00", "nick": "nikolaosk", "message": "it will fetch the url and set the response var", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:27:46.146977+00:00", "nick": "rebsadran", "message": "you mean in scrapy shell? I get this \"response.xpath(\"//div[@id='specifi...;", "links": ["mailto:\"response.xpath(\"//div[@id='specifications']/p[2]/text()\").extract()\""], "channel": "scrapy"},
{"date": "2014-07-16T10:28:27.865287+00:00", "nick": "rebsadran", "message": "damn", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:28:32.763652+00:00", "nick": "rebsadran", "message": "need to load the thing in right", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:28:46.201024+00:00", "nick": "rebsadran", "message": "on it", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:29:29.112973+00:00", "nick": "rebsadran", "message": "response:", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:29:30.699486+00:00", "nick": "rebsadran", "message": "response.xpath(\"//div[@id='specificatio...", "links": ["mailto:response.xpath(\"//div[@id='specifications']/p[2]/text()\").extract()"], "channel": "scrapy"},
{"date": "2014-07-16T10:29:32.444961+00:00", "nick": "rebsadran", "message": "damn", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:29:38.616097+00:00", "nick": "rebsadran", "message": "this one: [u'\\n', u'\\n807.15\\xa0kilobyte']", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:30:21.757079+00:00", "nick": "rebsadran", "message": "of this torrent: http://www.mininova.org/tor/13276871", "links": ["http://www.mininova.org/tor/13276871"], "channel": "scrapy"},
{"date": "2014-07-16T10:30:57.250646+00:00", "nick": "nikolaosk", "message": "well, doesn't return a bytearray for me", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:31:01.886683+00:00", "nick": "rebsadran", "message": "but so far it seems to be the same with every torrent so it's probably not the fault of html", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:31:24.675668+00:00", "nick": "rebsadran", "message": "umm, weird", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:31:39.349902+00:00", "nick": "rebsadran", "message": "I'm running it on Linux mint, py2.7 virtual env", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:31:49.453873+00:00", "nick": "nikolaosk", "message": "map(replace_entities, [u'\\n', u'\\n807.15\\xa0kilobyte'])  # => [u'\\n', u'\\n807.15\\xa0kilobyte']", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:34:15.129374+00:00", "nick": "nikolaosk", "message": "which version of w3lib do you have?", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:34:27.709613+00:00", "nick": "nikolaosk", "message": "$ pip show w3lib", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:34:36.793737+00:00", "nick": "rebsadran", "message": "1.6", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:34:50.474961+00:00", "nick": "nikolaosk", "message": "me too", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:35:07.871244+00:00", "nick": "nikolaosk", "message": "wait a sec", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:35:11.099971+00:00", "nick": "nikolaosk", "message": "your Join()", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:35:15.707421+00:00", "nick": "nikolaosk", "message": "doesn't have and arg", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:36:00.738833+00:00", "nick": "nikolaosk", "message": "no, it has a default, don't mind", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:39:02.578598+00:00", "nick": "rebsadran", "message": "well it seems to return <type 'list'>", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:41:33.896431+00:00", "nick": "rebsadran", "message": "bite list", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:41:41.270519+00:00", "nick": "rebsadran", "message": "umm", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:41:52.098799+00:00", "nick": "rebsadran", "message": "so it's fine ? http://paste.kde.org/pykwjyvgf", "links": ["http://paste.kde.org/pykwjyvgf"], "channel": "scrapy"},
{"date": "2014-07-16T10:42:03.585530+00:00", "nick": "rebsadran", "message": "u' is for unicode right?", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:57:31.451516+00:00", "nick": "nikolaosk", "message": "map(type, title)", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:57:49.957103+00:00", "nick": "nikolaosk", "message": "because you want the types of individual list items", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:58:01.214395+00:00", "nick": "nikolaosk", "message": "but you already know this from the repr string", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T10:58:08.344376+00:00", "nick": "nikolaosk", "message": "it shows u''", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T11:09:22.567737+00:00", "nick": "rebsadran", "message": "both are unicode", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T11:09:25.374360+00:00", "nick": "rebsadran", "message": "it seems", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T11:22:18.330377+00:00", "nick": "rebsadran", "message": "it seems there something wrong with my system", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T11:22:44.532546+00:00", "nick": "rebsadran", "message": "bs4 returns strings as byterays...", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T11:23:09.197602+00:00", "nick": "rebsadran", "message": "in a completely different virtual environment", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T12:18:09.773761+00:00", "nick": "bighat", "message": "hi,I got an error 'str' objects has no attribute 'iter' when I have a Rule like rules= (Rule(LxmlLinkExtractor(allow=(r'http://www.... = \"DobNextPrase\"),), it occured when \"for el in document.iter(etree.Element):\" executed", "links": ["http://Rule(LxmlLinkExtractor(allow=(r'http://www.xxx.com/discussion?start=%5Cd+/'),restrict_xpaths=('//span[@class=\"next\"]/a/@href')),callback"], "channel": "scrapy"},
{"date": "2014-07-16T12:18:31.436375+00:00", "nick": "bighat", "message": "= = what should I do.....", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T18:19:55.267566+00:00", "nick": "prakhar", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2014-07-16T19:24:16.463665+00:00", "nick": "daniel_cp", "message": "I have COOKIES_ENABLED set to False, but have one spider (out of many) where I need to use a cookiejar.  Is it possible to activate cookies on a per-spider or per-request basis, but keep the default project-side setting off?", "links": [], "channel": "scrapy"},
{"date": "2014-07-17T11:35:49.681182+00:00", "nick": "TheRinger", "message": "I'm trying to make a scraper that crawls google to find dropshippers something like includes product name & mentions dropshipping.. not sure how this would work does anyone know of any scrapy examples on how to get started ?", "links": [], "channel": "scrapy"},
{"date": "2014-07-17T16:27:38.896878+00:00", "nick": "cmdline1", "message": "Hi!", "links": [], "channel": "scrapy"},
{"date": "2014-07-17T16:27:54.393949+00:00", "nick": "cmdline1", "message": "igot the ImportError: No module named cmdline", "links": [], "channel": "scrapy"},
{"date": "2014-07-17T16:28:17.924168+00:00", "nick": "cmdline1", "message": "C:\\Python27\\Lib\\site-packages and C:\\Python27 already added in PATH", "links": [], "channel": "scrapy"},
{"date": "2014-07-17T16:29:47.026296+00:00", "nick": "cmdline1", "message": ">>> from scrapy.cmdline import execute    work fine", "links": [], "channel": "scrapy"},
{"date": "2014-07-17T16:29:55.792455+00:00", "nick": "cmdline1", "message": "any one can help ?", "links": [], "channel": "scrapy"},
{"date": "2014-07-17T16:55:47.717864+00:00", "nick": "cmdline1", "message": "can any one help this issue ? https://github.com/scrapy/scrapy/issues/807", "links": ["https://github.com/scrapy/scrapy/issues/807"], "channel": "scrapy"},
{"date": "2014-07-17T18:05:16.162000+00:00", "nick": "nixfreak", "message": "http://bpaste.net/show/FKSXdzbzfug8bOqVJ44v/ get the error ameError: name 'goog_search_list' is not defined", "links": ["http://bpaste.net/show/FKSXdzbzfug8bOqVJ44v/"], "channel": "scrapy"},
{"date": "2014-07-17T18:31:32.979265+00:00", "nick": "nixfreak", "message": "http://bpaste.net/show/FKSXdzbzfug8bOqVJ44v/  getting def error on goog_search_list", "links": ["http://bpaste.net/show/FKSXdzbzfug8bOqVJ44v/"], "channel": "scrapy"},
{"date": "2014-07-17T20:18:25.790344+00:00", "nick": "nkuttler", "message": "ananana: your connection is broken", "links": [], "channel": "scrapy"},
{"date": "2014-07-17T20:36:25.132579+00:00", "nick": "nixfreak", "message": "http://bpaste.net/show/dESIyeG6jmhfRsZiNw0J/ -- umping into debugger for post-mortem of exception 'unbalanced parenthesis':", "links": ["http://bpaste.net/show/dESIyeG6jmhfRsZiNw0J/"], "channel": "scrapy"},
{"date": "2014-07-17T20:36:28.077422+00:00", "nick": "nixfreak", "message": "> /home/nixfreak/.virtualenv/my_env/lib/python2.7/re.py(242)_compile()", "links": [], "channel": "scrapy"},
{"date": "2014-07-17T20:36:30.661730+00:00", "nick": "nixfreak", "message": "-> raise error, v # invalid expression", "links": [], "channel": "scrapy"},
{"date": "2014-07-17T20:37:15.958238+00:00", "nick": "nixfreak", "message": "Also that way I am handing a search word by putting it in the google http api is that correct?", "links": [], "channel": "scrapy"},
{"date": "2014-07-17T22:08:46.263450+00:00", "nick": "Doginal", "message": "hey all, i am trying to stop scrapy from crawling certain sites, ie twitter but i wrote an ignore request middleware, but the issue is that its still following sub domains. How can I stop it from following these subdomain links? do I need to just add every subdomain into my blacklist?", "links": [], "channel": "scrapy"},
{"date": "2014-07-17T22:12:03.966869+00:00", "nick": "jakobdm", "message": "Hi Doginal! You could use the urlparse module to extract the top-level domain and match that against your list of forbidden sites.", "links": [], "channel": "scrapy"},
{"date": "2014-07-17T22:15:46.032652+00:00", "nick": "jakobdm", "message": "Sorry, the urlparse module will not give you the top-level domain. It will however give you the network location part of the URI (that's everything inbetween \"http[s]://\" and the next slash), you could then check whether any of your forbidden domains is a substring of that net location.", "links": [], "channel": "scrapy"},
{"date": "2014-07-17T22:16:45.780180+00:00", "nick": "Doginal", "message": "okay, i am looking at urlparse right now", "links": [], "channel": "scrapy"},
{"date": "2014-07-17T22:22:31.139372+00:00", "nick": "Doginal", "message": "yea it looks like urlparse and searching for a substring is the way to go, thanks jaobdm", "links": [], "channel": "scrapy"},
{"date": "2014-07-17T22:23:12.254362+00:00", "nick": "jakobdm", "message": "Glad I could help!", "links": [], "channel": "scrapy"},
{"date": "2014-07-18T03:19:59.571922+00:00", "nick": "TheRinger", "message": "anyone alive in here", "links": [], "channel": "scrapy"},
{"date": "2014-07-18T11:07:25.234627+00:00", "nick": "flyingtriangle", "message": "I'm trying to prevent scrapy from url encoding characters in Request urls. I have tracked it down to url.py in Scrapy which calls w3lib's safe_url_string function. How can I monkey patch my spider to override the safe_url_string() function? Or just the _ALWAYS_SAFE_BYTES variable in w3lib's url.py? https://github.com/scrapy/w3lib/blob/master/w3l...", "links": ["https://github.com/scrapy/w3lib/blob/master/w3lib/url.py"], "channel": "scrapy"},
{"date": "2014-07-18T16:28:56.672418+00:00", "nick": "nixfreak", "message": "Hi good morning, I'm really stumped right now; here is my code - http://bpaste.net/show/MfW9hgApDqsSFee5JUEr/", "links": ["http://bpaste.net/show/MfW9hgApDqsSFee5JUEr/"], "channel": "scrapy"},
{"date": "2014-07-18T16:29:34.613442+00:00", "nick": "nixfreak", "message": "What I am trying to do is use a url prefix of google's search http api and combine it with keywords or searchstrings", "links": [], "channel": "scrapy"},
{"date": "2014-07-18T16:30:30.598620+00:00", "nick": "nixfreak", "message": "I'm probably doing this the wrong way but I'm learning", "links": [], "channel": "scrapy"},
{"date": "2014-07-18T17:27:58.407681+00:00", "nick": "nixfreak", "message": "I was just told on #python that what I'm trying to do is not going to work?", "links": [], "channel": "scrapy"},
{"date": "2014-07-18T19:18:11.052475+00:00", "nick": "nixfreak", "message": "bpaste.net/show/vkAAQYoi7BFFy4CROaAI/ how can I append a string to the end of an url?", "links": ["http://bpaste.net/show/vkAAQYoi7BFFy4CROaAI/"], "channel": "scrapy"},
{"date": "2014-07-18T21:37:44.126528+00:00", "nick": "nixfreak", "message": "Could someone plaese tell me why scrapy won't concat my search terms with a url ?", "links": [], "channel": "scrapy"},
{"date": "2014-07-18T21:38:00.387735+00:00", "nick": "nixfreak", "message": "http://www.bpaste.net/show/iex7fOkvxXRlNjRexTLL/", "links": ["http://www.bpaste.net/show/iex7fOkvxXRlNjRexTLL/"], "channel": "scrapy"},
{"date": "2014-07-18T23:22:57.647890+00:00", "nick": "TheRinger", "message": "try pinging", "links": [], "channel": "scrapy"},
{"date": "2014-07-19T19:11:54.825715+00:00", "nick": "T__", "message": "Hi. Is anyone here?", "links": [], "channel": "scrapy"},
{"date": "2014-07-19T19:12:02.953858+00:00", "nick": "gregor22", "message": "Hi, I try to write a small spider for a blog, but somehow scrapy doesn't follow all pages defined by the rules but only the ones available from the first page: https://gist.github.com/anonymous/60b912f1f9234...", "links": ["https://gist.github.com/anonymous/60b912f1f9234e5997ba"], "channel": "scrapy"},
{"date": "2014-07-19T19:12:10.372879+00:00", "nick": "gregor22", "message": "any help would be appreciated, thanks", "links": [], "channel": "scrapy"},
{"date": "2014-07-19T19:13:26.921590+00:00", "nick": "T__", "message": "Also does anyone know how to input arguments from a console into scrapy? when using \"scrapy crawl spider\"?", "links": [], "channel": "scrapy"},
{"date": "2014-07-19T19:19:18.241102+00:00", "nick": "gregor22", "message": "@T__ it' written in the documentation, have a look here: http://doc.scrapy.org/en/latest/topics/spiders....", "links": ["http://doc.scrapy.org/en/latest/topics/spiders.html#spider-arguments"], "channel": "scrapy"},
{"date": "2014-07-19T20:32:15.876668+00:00", "nick": "nick1", "message": "Hi all", "links": [], "channel": "scrapy"},
{"date": "2014-07-19T20:32:24.681025+00:00", "nick": "nick1", "message": "How can I debug why this returns \"killed\"?", "links": [], "channel": "scrapy"},
{"date": "2014-07-19T20:32:25.146734+00:00", "nick": "nick1", "message": "http://dpaste.com/1VDVHCM", "links": ["http://dpaste.com/1VDVHCM"], "channel": "scrapy"},
{"date": "2014-07-19T20:44:48.405836+00:00", "nick": "nick1", "message": "wait, it might have been a problem with having solr open at same time", "links": [], "channel": "scrapy"},
{"date": "2014-07-19T20:44:54.367734+00:00", "nick": "nick1", "message": "thanks anyways forks", "links": [], "channel": "scrapy"},
{"date": "2014-07-20T12:47:21.247957+00:00", "nick": "Rico", "message": "hi dear friends", "links": [], "channel": "scrapy"},
{"date": "2014-07-20T16:27:34.272501+00:00", "nick": "cryzed", "message": "Does anyone remember the name of the tool that ran a local proxy and enabled one to find XPaths for scraping with scrapy extremely easy?", "links": [], "channel": "scrapy"},
{"date": "2014-07-20T16:27:40.707759+00:00", "nick": "cryzed", "message": "I know it was related to this project I think...", "links": [], "channel": "scrapy"},
{"date": "2014-07-20T16:34:03.675629+00:00", "nick": "cryzed", "message": "Basically you could just click on elements in your browser and the (ideal) XPath was displayed", "links": [], "channel": "scrapy"},
{"date": "2014-07-20T16:36:33.729986+00:00", "nick": "cryzed", "message": "portia.", "links": [], "channel": "scrapy"},
{"date": "2014-07-20T16:36:36.423351+00:00", "nick": "cryzed", "message": "https://github.com/scrapinghub/portia", "links": ["https://github.com/scrapinghub/portia"], "channel": "scrapy"},
{"date": "2014-07-20T16:37:04.240185+00:00", "nick": "cryzed", "message": "thanks though", "links": [], "channel": "scrapy"},
{"date": "2014-07-21T02:05:02.875465+00:00", "nick": "truedon", "message": "#vagrant", "links": [], "channel": "scrapy"},
{"date": "2014-07-21T06:38:00.890701+00:00", "nick": "Doginal", "message": "hey everyone, how would i drop/delete a html cache if a keyword is not found?", "links": [], "channel": "scrapy"},
{"date": "2014-07-21T15:52:59.256102+00:00", "nick": "yoLo_", "message": "hello", "links": [], "channel": "scrapy"},
{"date": "2014-07-21T18:05:28.206376+00:00", "nick": "nixfreak", "message": "I'm getting really close to scraping duckduckgo using multi keyword searches but having some issues with xpath. I am using the non js page for duckduckgo and using this xpath to get the desired web results sel.xpath('//div[@class=\"results_links results_links_deep web-result\"]/div[@class=\"links_main links_deep\"]/div[@class=\"url\"]'...)", "links": ["mailto:links_deep\"]/div[@class=\"url\"]').extract("], "channel": "scrapy"},
{"date": "2014-07-21T18:06:03.891467+00:00", "nick": "nixfreak", "message": "i don't think i'm getting the correct results though , is this the channel to ask about xpath?", "links": [], "channel": "scrapy"},
{"date": "2014-07-22T08:58:20.946577+00:00", "nick": "flyingtriangle", "message": "I'm trying to make scrapy not URL encode requests. I see that this is done in scrapy.http.Requests importing w3lib.url and it's just the _ALWAYS_SAFE_BYTES variables I need to adjust in there. But how do I monkey patch that variable within my spider class?", "links": [], "channel": "scrapy"},
{"date": "2014-07-22T15:10:36.487892+00:00", "nick": "nixfreak", "message": "anyone here know a nick by name scapy support ?", "links": [], "channel": "scrapy"},
{"date": "2014-07-22T16:13:00.590168+00:00", "nick": "nixfreak", "message": "Does anyone know a nick by the name of scrapy support or just support ?", "links": [], "channel": "scrapy"},
{"date": "2014-07-22T16:14:21.509215+00:00", "nick": "baazzilhassan", "message": "Hi nixfreak, what is your question ?", "links": [], "channel": "scrapy"},
{"date": "2014-07-22T16:16:55.945500+00:00", "nick": "nixfreak", "message": "I talking to the individual about a project that I am working on", "links": [], "channel": "scrapy"},
{"date": "2014-07-22T16:19:16.088224+00:00", "nick": "nixfreak", "message": "of so anyway i am scraping the front page of certain search engines and using 30+ keywords to search from and i get the result back of those 30+ keywords", "links": [], "channel": "scrapy"},
{"date": "2014-07-22T16:19:37.662220+00:00", "nick": "nixfreak", "message": "but now I am unable to parse them out to dump to a json file", "links": [], "channel": "scrapy"},
{"date": "2014-07-22T16:19:57.759446+00:00", "nick": "nixfreak", "message": "i can print them on screen but not to a seperate file", "links": [], "channel": "scrapy"},
{"date": "2014-07-22T16:20:10.485563+00:00", "nick": "nixfreak", "message": "does that make sense ?", "links": [], "channel": "scrapy"},
{"date": "2014-07-22T16:38:19.104235+00:00", "nick": "toothrot", "message": "nixfreak, not really unless you explain what 'unable to parse' means. you don't know how? getting an error?", "links": [], "channel": "scrapy"},
{"date": "2014-07-22T20:13:15.171231+00:00", "nick": "ajoros", "message": "Hey gents, can someone view my r/learnpython post and tell me if they know of a solution. I've googled around and I'm still stumped. Here is the post: http://www.reddit.com/r/learnpython/comments/2b...", "links": ["http://www.reddit.com/r/learnpython/comments/2bfcdf/noob_post_anaconda_pip_install_scrapy_update/"], "channel": "scrapy"},
{"date": "2014-07-22T20:13:31.898374+00:00", "nick": "ajoros", "message": "It is titled: Noob post: Anaconda + \"pip install Scrapy --update\" failing. Error log included.", "links": [], "channel": "scrapy"},
{"date": "2014-07-22T20:51:27.408300+00:00", "nick": "ajoros", "message": "i  fixed it guys", "links": [], "channel": "scrapy"},
{"date": "2014-07-22T20:51:27.814724+00:00", "nick": "ajoros", "message": "nvm", "links": [], "channel": "scrapy"},
{"date": "2014-07-23T10:54:52.169710+00:00", "nick": "h8R", "message": "hey! is there any trick to combine multiple TD as node? I want to select two times 5 tds, and be able to loop over them as they are child of different element?", "links": [], "channel": "scrapy"},
{"date": "2014-07-23T10:56:07.928453+00:00", "nick": "h8R", "message": "in fact i want to select parts of one table as 5 different tables, so I can loop over them..", "links": [], "channel": "scrapy"},
{"date": "2014-07-23T10:56:58.492063+00:00", "nick": "baazzilhassan", "message": "h8R, can I see the html page ?", "links": [], "channel": "scrapy"},
{"date": "2014-07-23T10:57:30.207145+00:00", "nick": "h8R", "message": "sure", "links": [], "channel": "scrapy"},
{"date": "2014-07-23T11:00:54.033193+00:00", "nick": "h8R", "message": "baazzilhassan, http://gramophon.com/test/test1.html", "links": ["http://gramophon.com/test/test1.html"], "channel": "scrapy"},
{"date": "2014-07-23T11:01:11.238653+00:00", "nick": "h8R", "message": "baazzilhassan, http://gramophon.com/test/test.html -> here with only one item", "links": ["http://gramophon.com/test/test.html"], "channel": "scrapy"},
{"date": "2014-07-23T11:01:27.600375+00:00", "nick": "baazzilhassan", "message": "h8R, okey", "links": [], "channel": "scrapy"},
{"date": "2014-07-23T11:01:30.173176+00:00", "nick": "h8R", "message": "baazzilhassan, this selects them //table/tbody/tr/td[contains(text(), '\u041f\u043e\u0440\u0435\u0434\u0435\u043d')]/..|//table/tbody/tr/td[contains(text(), '\u041f\u043e\u0440\u0435\u0434\u0435\u043d')]/../self::tr/following-sibling::tr/td[not(@colspan='2')]", "links": [], "channel": "scrapy"},
{"date": "2014-07-23T11:01:51.772947+00:00", "nick": "h8R", "message": "but i want to loop over the different tables (and they are printed as TR in one table)", "links": [], "channel": "scrapy"},
{"date": "2014-07-23T11:02:50.673786+00:00", "nick": "h8R", "message": "i think i missing something fundamental in xpaths :D", "links": [], "channel": "scrapy"},
{"date": "2014-07-23T11:04:51.263692+00:00", "nick": "baazzilhassan", "message": "if you select all tds", "links": [], "channel": "scrapy"},
{"date": "2014-07-23T11:05:15.596488+00:00", "nick": "baazzilhassan", "message": "loop on all this TDs and check by index", "links": [], "channel": "scrapy"},
{"date": "2014-07-23T11:05:45.696226+00:00", "nick": "h8R", "message": "what you mean by index?", "links": [], "channel": "scrapy"},
{"date": "2014-07-23T11:06:24.813706+00:00", "nick": "baazzilhassan", "message": "when you extract all TDs, you have a list as result", "links": [], "channel": "scrapy"},
{"date": "2014-07-23T11:06:29.608121+00:00", "nick": "h8R", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2014-07-23T11:06:40.951054+00:00", "nick": "h8R", "message": "but then I don't know how they are grouped", "links": [], "channel": "scrapy"},
{"date": "2014-07-23T11:06:50.720710+00:00", "nick": "baazzilhassan", "message": "so index % 5 == 0", "links": [], "channel": "scrapy"},
{"date": "2014-07-23T11:07:07.595989+00:00", "nick": "h8R", "message": "nah, td counts is different", "links": [], "channel": "scrapy"},
{"date": "2014-07-23T11:07:23.314850+00:00", "nick": "h8R", "message": "or i'm not understanding something?", "links": [], "channel": "scrapy"},
{"date": "2014-07-23T11:15:28.440176+00:00", "nick": "baazzilhassan", "message": "h8R, forget the index methode", "links": [], "channel": "scrapy"},
{"date": "2014-07-23T11:15:48.573436+00:00", "nick": "baazzilhassan", "message": "h8R, I think this solution is simple", "links": [], "channel": "scrapy"},
{"date": "2014-07-23T11:16:13.725461+00:00", "nick": "baazzilhassan", "message": "get all TRs", "links": [], "channel": "scrapy"},
{"date": "2014-07-23T11:16:53.411868+00:00", "nick": "baazzilhassan", "message": "loop on all TRs and check if TR contain a TD with attribute colspan", "links": [], "channel": "scrapy"},
{"date": "2014-07-23T11:17:01.111764+00:00", "nick": "baazzilhassan", "message": "/td[@colspan]", "links": [], "channel": "scrapy"},
{"date": "2014-07-23T11:17:54.779895+00:00", "nick": "baazzilhassan", "message": "if true, then initialize a new ITEM", "links": [], "channel": "scrapy"},
{"date": "2014-07-23T11:18:16.163648+00:00", "nick": "h8R", "message": "yeah, that sounds like a good backup plan", "links": [], "channel": "scrapy"},
{"date": "2014-07-23T11:18:21.223554+00:00", "nick": "baazzilhassan", "message": "if false then pull data on the ITEM", "links": [], "channel": "scrapy"},
{"date": "2014-07-23T11:18:25.222781+00:00", "nick": "h8R", "message": "I was just curious how to achieve this", "links": [], "channel": "scrapy"},
{"date": "2014-07-23T11:19:48.992104+00:00", "nick": "baazzilhassan", "message": "simple, if colspan exist then return ITEM and initialize a new one", "links": [], "channel": "scrapy"},
{"date": "2014-07-23T11:21:38.640174+00:00", "nick": "h8R", "message": "baazzilhassan, thanks for the help, i'll test that now", "links": [], "channel": "scrapy"},
{"date": "2014-07-23T11:22:30.436514+00:00", "nick": "baazzilhassan", "message": "h8R, see this link http://stackoverflow.com/questions/14069236/scr...", "links": ["http://stackoverflow.com/questions/14069236/scrapy-single-spider-to-pass-multiple-item-classes-to-pipeline"], "channel": "scrapy"},
{"date": "2014-07-23T11:22:42.172451+00:00", "nick": "baazzilhassan", "message": "to know how te return multiple items", "links": [], "channel": "scrapy"},
{"date": "2014-07-23T11:24:18.838587+00:00", "nick": "baazzilhassan", "message": "create an empty list, for each item scrapped then append it to this list, in final return this list", "links": [], "channel": "scrapy"},
{"date": "2014-07-23T11:27:33.071226+00:00", "nick": "h8R", "message": "thanks a lot, baazzilhassan  + 1 beer", "links": [], "channel": "scrapy"},
{"date": "2014-07-23T11:28:10.400871+00:00", "nick": "baazzilhassan", "message": ":)", "links": [], "channel": "scrapy"},
{"date": "2014-07-23T16:31:39.467557+00:00", "nick": "odigem", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2014-07-23T16:31:49.647523+00:00", "nick": "odigem", "message": "how to get response cookies?", "links": [], "channel": "scrapy"},
{"date": "2014-07-23T16:44:08.625778+00:00", "nick": "odigem", "message": "how to add cookie to request?", "links": [], "channel": "scrapy"},
{"date": "2014-07-23T16:44:21.536556+00:00", "nick": "odigem", "message": "or update", "links": [], "channel": "scrapy"},
{"date": "2014-07-23T16:49:26.794752+00:00", "nick": "nikolaosk", "message": "odigem: https://scrapy.readthedocs.org/en/latest/topics...", "links": ["https://scrapy.readthedocs.org/en/latest/topics/request-response.html"], "channel": "scrapy"},
{"date": "2014-07-24T02:17:53.306311+00:00", "nick": "sunshine`", "message": "Hi", "links": [], "channel": "scrapy"},
{"date": "2014-07-24T02:17:56.486231+00:00", "nick": "sunshine`", "message": "Hi", "links": [], "channel": "scrapy"},
{"date": "2014-07-24T02:18:05.148120+00:00", "nick": "sunshine`", "message": "exit", "links": [], "channel": "scrapy"},
{"date": "2014-07-24T02:18:06.108815+00:00", "nick": "sunshine`", "message": "exit", "links": [], "channel": "scrapy"},
{"date": "2014-07-24T09:13:31.928961+00:00", "nick": "andrevmatos", "message": "Hi. I'm having issues with Image storage on S3. I've read the sources and followed http://stackoverflow.com/questions/10472274/how..., but still no luck, S3FilesStore seems to fail without reason", "links": ["http://stackoverflow.com/questions/10472274/how-to-store-scrapy-images-on-amazon-s3"], "channel": "scrapy"},
{"date": "2014-07-24T09:13:50.282541+00:00", "nick": "andrevmatos", "message": "anyone has any idea?", "links": [], "channel": "scrapy"},
{"date": "2014-07-24T18:30:37.780401+00:00", "nick": "Engin", "message": "ValueError: Invalid XPath: //select[@name=\"cboSeason\"]/option/value()", "links": [], "channel": "scrapy"},
{"date": "2014-07-24T18:31:12.880818+00:00", "nick": "Engin", "message": "Apparently value() is not available. I need the value attr of an option tag instead of its text(). Can I get it?", "links": [], "channel": "scrapy"},
{"date": "2014-07-24T18:31:27.377399+00:00", "nick": "Engin", "message": "I mean obviously I can but... an easy xpath way?", "links": [], "channel": "scrapy"},
{"date": "2014-07-24T18:34:19.056387+00:00", "nick": "Engin", "message": "aha /@value", "links": [], "channel": "scrapy"},
{"date": "2014-07-25T11:26:58.036315+00:00", "nick": "trivian", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2014-07-25T11:28:02.045051+00:00", "nick": "trivian", "message": "Can anyone point me to a code examplme that will allow me to persist my duplicate links from job to job using spider.state in the latest build?", "links": [], "channel": "scrapy"},
{"date": "2014-07-25T11:55:56.018713+00:00", "nick": "trivian", "message": "Let me rephrase that. Can (should) I use spider.state to persist duplicate URL data between batches? Is there a better way?", "links": [], "channel": "scrapy"},
{"date": "2014-07-25T11:56:29.353359+00:00", "nick": "trivian", "message": "I dont think that I can use the JOBDIR, can I? Will subsequent batch runs re-use that duplicate data?", "links": [], "channel": "scrapy"},
{"date": "2014-07-25T16:36:47.040457+00:00", "nick": "jefferson_scrape", "message": "Is it more preferable to do duplicate filtering in a pipeline or to specify it in settings as DUPEFILTER_CLASS?", "links": [], "channel": "scrapy"},
{"date": "2014-07-26T04:47:43.626734+00:00", "nick": "Hello", "message": "lkkkfafafaf", "links": [], "channel": "scrapy"},
{"date": "2014-07-26T04:47:45.643066+00:00", "nick": "Hello", "message": "Woah", "links": [], "channel": "scrapy"},
{"date": "2014-07-26T04:47:47.307379+00:00", "nick": "Hello", "message": "Sup guys", "links": [], "channel": "scrapy"},
{"date": "2014-07-26T04:47:53.658335+00:00", "nick": "Hello", "message": "I think this is my first IRC chat", "links": [], "channel": "scrapy"},
{"date": "2014-07-26T04:47:56.310429+00:00", "nick": "Hello", "message": "ever haha", "links": [], "channel": "scrapy"},
{"date": "2014-07-26T04:48:01.299045+00:00", "nick": "Hello", "message": "unless you count chat rooms", "links": [], "channel": "scrapy"},
{"date": "2014-07-27T13:48:53.259584+00:00", "nick": "avacsm", "message": "hi, is there a way to log each spider run into a separate file?", "links": [], "channel": "scrapy"},
{"date": "2014-07-27T13:49:34.080241+00:00", "nick": "avacsm", "message": "creating new observer in spider __init__ is not ideal, as one need to do it for each spider", "links": [], "channel": "scrapy"},
{"date": "2014-07-27T13:49:50.861106+00:00", "nick": "avacsm", "message": "*needs", "links": [], "channel": "scrapy"},
{"date": "2014-07-27T13:50:57.963031+00:00", "nick": "avacsm", "message": "i tried creating custom SpiderManager, but initializing new observer there, breaks the logging", "links": [], "channel": "scrapy"},
{"date": "2014-07-27T17:29:09.868853+00:00", "nick": "skua", "message": "hello", "links": [], "channel": "scrapy"},
{"date": "2014-07-27T17:29:14.940468+00:00", "nick": "skua", "message": "i am confused about Dynamic Creation of Item Classes", "links": [], "channel": "scrapy"},
{"date": "2014-07-27T17:29:19.161209+00:00", "nick": "skua", "message": "http://doc.scrapy.org/en/latest/topics/practice...", "links": ["http://doc.scrapy.org/en/latest/topics/practices.html"], "channel": "scrapy"},
{"date": "2014-07-27T17:29:23.182519+00:00", "nick": "skua", "message": "code at the bottom", "links": [], "channel": "scrapy"},
{"date": "2014-07-27T17:29:33.199735+00:00", "nick": "skua", "message": "where do I put it ?", "links": [], "channel": "scrapy"},
{"date": "2014-07-27T17:29:42.310202+00:00", "nick": "skua", "message": "items.py ?", "links": [], "channel": "scrapy"},
{"date": "2014-07-27T17:29:55.029213+00:00", "nick": "skua", "message": "need to load values from mysql", "links": [], "channel": "scrapy"},
{"date": "2014-07-27T18:29:15.577552+00:00", "nick": "skua", "message": "hey", "links": [], "channel": "scrapy"},
{"date": "2014-07-27T18:29:21.401454+00:00", "nick": "skua", "message": "who is good with scrapy?", "links": [], "channel": "scrapy"},
{"date": "2014-07-27T18:29:24.029121+00:00", "nick": "skua", "message": "please help!!!", "links": [], "channel": "scrapy"},
{"date": "2014-07-27T18:29:36.775658+00:00", "nick": "skua", "message": "google doesnt help", "links": [], "channel": "scrapy"},
{"date": "2014-07-27T23:49:56.559165+00:00", "nick": "man", "message": "Hii", "links": [], "channel": "scrapy"},
{"date": "2014-07-27T23:50:03.060341+00:00", "nick": "man", "message": "Guys", "links": [], "channel": "scrapy"},
{"date": "2014-07-27T23:50:32.991438+00:00", "nick": "Guest56697", "message": "can anyone tell me something about scrapp", "links": [], "channel": "scrapy"},
{"date": "2014-07-27T23:50:36.031122+00:00", "nick": "Guest56697", "message": "scrappy", "links": [], "channel": "scrapy"},
{"date": "2014-07-27T23:50:59.863480+00:00", "nick": "Guest56697", "message": "?", "links": [], "channel": "scrapy"},
{"date": "2014-07-27T23:51:05.851096+00:00", "nick": "Guest56697", "message": "anyone there ?", "links": [], "channel": "scrapy"},
{"date": "2014-07-27T23:55:11.810591+00:00", "nick": "Guest56697", "message": "any admin ?", "links": [], "channel": "scrapy"},
{"date": "2014-07-28T04:25:20.071429+00:00", "nick": "Rakan", "message": "Hello all", "links": [], "channel": "scrapy"},
{"date": "2014-07-28T04:26:00.271062+00:00", "nick": "Rakan", "message": "in a descendant class of ImagesPipeline, i am trying to yield requests to images", "links": [], "channel": "scrapy"},
{"date": "2014-07-28T04:26:12.230223+00:00", "nick": "Rakan", "message": "However, when a certain image fails... how do you retry forcing the image?", "links": [], "channel": "scrapy"},
{"date": "2014-07-28T04:26:22.828703+00:00", "nick": "Rakan", "message": "sorry, retry downloading the image", "links": [], "channel": "scrapy"},
{"date": "2014-07-28T04:28:47.247137+00:00", "nick": "Rakan", "message": "Does the RetryMiddleware do that in my case?", "links": [], "channel": "scrapy"},
{"date": "2014-07-28T18:41:41.163187+00:00", "nick": "exalt", "message": "Hello is there a python3 version of scrapy ?]", "links": [], "channel": "scrapy"},
{"date": "2014-07-29T04:11:20.059341+00:00", "nick": "SoFLy", "message": "hello there!", "links": [], "channel": "scrapy"},
{"date": "2014-07-29T04:11:55.962587+00:00", "nick": "SoFLy", "message": "i'm getting a exceptions.IndexError: list index out of range error on my code and having difficulty debugging what's wrong", "links": [], "channel": "scrapy"},
{"date": "2014-07-29T04:12:11.108763+00:00", "nick": "SoFLy", "message": "less to do with that i don't understand the python error and more to do with me not being an expert with scrapy.", "links": [], "channel": "scrapy"},
{"date": "2014-07-29T04:12:28.426390+00:00", "nick": "SoFLy", "message": "i'm quite comfortable with python", "links": [], "channel": "scrapy"},
{"date": "2014-07-29T04:16:24.916858+00:00", "nick": "SoFLy", "message": "the query works fine in scrapy shell", "links": [], "channel": "scrapy"},
{"date": "2014-07-29T04:16:30.804425+00:00", "nick": "SoFLy", "message": "just not when i run the spoder", "links": [], "channel": "scrapy"},
{"date": "2014-07-29T04:55:53.859874+00:00", "nick": "SoFLy", "message": "ok fixed those problems", "links": [], "channel": "scrapy"},
{"date": "2014-07-29T04:56:15.478805+00:00", "nick": "SoFLy", "message": "now a new confusing issue is how to request another page and then scrape data and yield the items", "links": [], "channel": "scrapy"},
{"date": "2014-07-29T05:22:33.318059+00:00", "nick": "SoFLy", "message": "ok now i can't get it to run on more than 1 of the url's in my list", "links": [], "channel": "scrapy"},
{"date": "2014-07-29T05:22:41.069371+00:00", "nick": "SoFLy", "message": "lol im just narrating -_-", "links": [], "channel": "scrapy"},
{"date": "2014-07-29T05:36:13.909683+00:00", "nick": "SoFLy", "message": "aand there we go. fixed it up.", "links": [], "channel": "scrapy"},
{"date": "2014-07-29T12:37:50.132791+00:00", "nick": "opskan", "message": "i wrote a simple spider to crawl certain subreddit but having some issues when it comes to handling paginated pages. When script runs, it collects the necessary information, at the end of the script it call request with the next page link ;which i made sure correctly formatted  even tried hardcoding the next page link but callback function still processes the initial response instead of processing the response from the request that was suppose to be made. He", "links": [], "channel": "scrapy"},
{"date": "2014-07-29T12:37:50.294121+00:00", "nick": "opskan", "message": "re is the related code http://stackoverflow.com/questions/25015292/rec...", "links": ["http://stackoverflow.com/questions/25015292/recursive-request-with-scrapy-does-not-yield-expected-result"], "channel": "scrapy"},
{"date": "2014-07-29T13:40:48.251827+00:00", "nick": "opskan", "message": "Hi!", "links": [], "channel": "scrapy"},
{"date": "2014-07-29T13:40:55.125522+00:00", "nick": "opskan", "message": "i wrote a simple spider to crawl certain subreddit but having some issues when it comes to handling paginated pages. When script runs, it collects the necessary information, at the end of the script it call request with the next page link ;which i made sure correctly formatted  even tried hardcoding the next page link but callback function still processes the initial response instead of processing the response from the request that was suppose to be made. He", "links": [], "channel": "scrapy"},
{"date": "2014-07-29T13:40:55.336905+00:00", "nick": "opskan", "message": "re is the related code http://stackoverflow.com/questions/25015292/rec...", "links": ["http://stackoverflow.com/questions/25015292/recursive-request-with-scrapy-does-not-yield-expected-result"], "channel": "scrapy"},
{"date": "2014-07-29T13:41:07.388563+00:00", "nick": "opskan", "message": "Anyone?", "links": [], "channel": "scrapy"},
{"date": "2014-07-29T13:42:39.196958+00:00", "nick": "nikolaosk", "message": "replace print statements with self.log", "links": [], "channel": "scrapy"},
{"date": "2014-07-29T13:42:52.219366+00:00", "nick": "nikolaosk", "message": "regarding your problem:", "links": [], "channel": "scrapy"},
{"date": "2014-07-29T13:42:54.598109+00:00", "nick": "nikolaosk", "message": "import pdb", "links": [], "channel": "scrapy"},
{"date": "2014-07-29T13:42:59.576255+00:00", "nick": "nikolaosk", "message": "pdb.set_trace()", "links": [], "channel": "scrapy"},
{"date": "2014-07-29T13:43:21.714198+00:00", "nick": "nikolaosk", "message": "the try to figure what gets matched and what not", "links": [], "channel": "scrapy"},
{"date": "2014-07-29T15:06:47.738077+00:00", "nick": "opskan", "message": "this is what is matched \"http://www.reddit.com/r/progresspics/?count=25&after=t3_2bz88l\" without the quotes. This is exactly the url that i needed to go to next page", "links": [], "channel": "scrapy"},
{"date": "2014-07-29T15:06:58.203820+00:00", "nick": "opskan", "message": "problem is it does not go through", "links": [], "channel": "scrapy"},
{"date": "2014-07-29T15:07:24.603846+00:00", "nick": "opskan", "message": "instead, it is running parse on the initial response once more.", "links": [], "channel": "scrapy"},
{"date": "2014-07-29T15:11:25.340609+00:00", "nick": "nikolaosk", "message": "doesn't reddit have an API or something?", "links": [], "channel": "scrapy"},
{"date": "2014-07-29T15:13:25.139127+00:00", "nick": "opskan", "message": "yeah but they have a very stringent limit, also this is just for me to learn more about scapy; just a side project per se to get familiar with scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-07-29T17:19:42.888079+00:00", "nick": "tightflks_", "message": "hi everyone", "links": [], "channel": "scrapy"},
{"date": "2014-07-29T17:20:35.176634+00:00", "nick": "tightflks_", "message": "im new to scrapy, and have a quick question\u2026i want to scrape a site, user has to login via their own login or use the social login options available, is this possible with scrapy?", "links": [], "channel": "scrapy"},
{"date": "2014-07-29T20:47:52.509402+00:00", "nick": "gantit", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2014-07-29T20:49:01.673581+00:00", "nick": "gantit", "message": "ani help me whit class DmozSpider(Spider):", "links": [], "channel": "scrapy"},
{"date": "2014-07-30T11:44:34.615322+00:00", "nick": "saransh__", "message": "scrapy's item pipeline", "links": [], "channel": "scrapy"},
{"date": "2014-07-30T11:44:36.381023+00:00", "nick": "saransh__", "message": "?", "links": [], "channel": "scrapy"},
{"date": "2014-07-30T12:37:08.071911+00:00", "nick": "saransh__", "message": "downloader middleware in scrapy ?", "links": [], "channel": "scrapy"},
{"date": "2014-07-30T15:06:15.785013+00:00", "nick": "skyscraper", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2014-07-30T15:06:36.433998+00:00", "nick": "skyscraper", "message": "can i get help on mysql", "links": [], "channel": "scrapy"},
{"date": "2014-07-30T18:07:13.626006+00:00", "nick": "saransh__", "message": "scrapy  downloader  middleware ?", "links": [], "channel": "scrapy"},
{"date": "2014-07-30T18:20:43.012179+00:00", "nick": "saransh__", "message": "hi i am struggling with this part", "links": [], "channel": "scrapy"},
{"date": "2014-07-30T18:21:03.985364+00:00", "nick": "saransh__", "message": "suppose if i request a url then how can i get it's middleware", "links": [], "channel": "scrapy"},
{"date": "2014-07-30T18:21:05.828127+00:00", "nick": "saransh__", "message": "?", "links": [], "channel": "scrapy"},
{"date": "2014-07-30T18:21:15.902113+00:00", "nick": "saransh__", "message": "one approach i followed is", "links": [], "channel": "scrapy"},
{"date": "2014-07-30T18:21:31.722122+00:00", "nick": "saransh__", "message": "i gave downloader middleware in the settings", "links": [], "channel": "scrapy"},
{"date": "2014-07-30T18:22:14.567924+00:00", "nick": "saransh__", "message": "and wrote my code in middlewares.py file", "links": [], "channel": "scrapy"},
{"date": "2014-07-30T18:23:02.929141+00:00", "nick": "saransh__", "message": "can anyone briefly describe all the steps I need to access the middleware ?", "links": [], "channel": "scrapy"},
{"date": "2014-07-30T18:55:00.671321+00:00", "nick": "saransh__", "message": "ImportError: Error loading object 'he_crawler.middlewares.QueryCount': Settings cannot be imported, because environment variable DJANGO_SETTINGS_MODULE is undefined", "links": [], "channel": "scrapy"},
{"date": "2014-07-31T08:30:10.865556+00:00", "nick": "handsomeyang", "message": "Hi, what's the relationship between \"scrapy shell\" and \"scrapy telnet console\"?", "links": [], "channel": "scrapy"},
{"date": "2014-07-31T11:25:21.272003+00:00", "nick": "fly2web", "message": "i want install scrapy. how can i do?", "links": [], "channel": "scrapy"},
{"date": "2014-07-31T12:12:48.520846+00:00", "nick": "nikolaosk", "message": "what have you tried?", "links": [], "channel": "scrapy"},
{"date": "2014-07-31T12:31:15.039478+00:00", "nick": "handsomeyang", "message": "Hi, what's the relationship between \"scrapy shell\" and \"scrapy telnet console\"?", "links": [], "channel": "scrapy"},
{"date": "2014-07-31T13:10:59.400040+00:00", "nick": "gregor_", "message": "Hi, just wanted to ask wheter the rules of a crawl spider apply after a form request, because somehow my spider doesn't follow links after login (I used LxmlLinkExtractor with restrict_xpaths", "links": [], "channel": "scrapy"},
{"date": "2014-07-31T13:17:43.220829+00:00", "nick": "gregor_", "message": "is there a way to manually call the rules of a CrawlSpider on a response to check whether they apply correctly?", "links": [], "channel": "scrapy"},
{"date": "2014-07-31T14:02:33.688677+00:00", "nick": "cr7", "message": "hey! i am new to scrapy and needed some help in scraping items from a single webpage", "links": [], "channel": "scrapy"},
{"date": "2014-07-31T14:04:38.683763+00:00", "nick": "cr7", "message": "i am scraping fixtures", "links": [], "channel": "scrapy"},
{"date": "2014-07-31T14:04:54.244902+00:00", "nick": "cr7", "message": "and every fixture has an id no.", "links": [], "channel": "scrapy"},
{"date": "2014-07-31T14:05:18.194779+00:00", "nick": "cr7", "message": "i want to get the teams who play each other and the dates of the matches", "links": [], "channel": "scrapy"},
{"date": "2014-07-31T14:05:23.734262+00:00", "nick": "cr7", "message": "anyone here?", "links": [], "channel": "scrapy"},
{"date": "2014-07-31T21:08:57.692947+00:00", "nick": "gregor22", "message": "Hello, I'm using a FormRequest to log in to a page within a CrawlSpider, but after it successfully logs in, it doesn't extract any links", "links": [], "channel": "scrapy"},
{"date": "2014-07-31T21:09:26.756867+00:00", "nick": "gregor22", "message": "Any help would be appreciated", "links": [], "channel": "scrapy"},
{"date": "2014-07-31T23:52:45.065596+00:00", "nick": "Doginal", "message": "hey all, any know if its possible to pass allowed_domains from scrapy-redis? or should i just read them from a text file?", "links": [], "channel": "scrapy"},
{"date": "2014-08-01T02:54:23.708426+00:00", "nick": "jonpon", "message": "Hey I have a few scrapy questions.... Is anyone available?", "links": [], "channel": "scrapy"},
{"date": "2014-08-01T03:06:14.190488+00:00", "nick": "jonpon", "message": "My question: I am new to scrapy so bear with me. When I run my scraper it only scrapes the start urls that I passed in. I have a link extractor rule and I am almost 100% sure it matches the links that are on the page. IS there some setting I need to enable for the spider to scraper other websites? Also if it finds a link that does not fit my rule will it follow it to find links that will fit my rule?", "links": [], "channel": "scrapy"},
{"date": "2014-08-01T15:18:09.696127+00:00", "nick": "infinow", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2014-08-01T20:04:53.235474+00:00", "nick": "rebsadran", "message": "I'm playing around with item loaders and I'm getting : \"exceptions.TypeError: 'ItemMeta' object does not support item assignment\" on line:\"return item_loader.load_item()\"", "links": [], "channel": "scrapy"},
{"date": "2014-08-01T20:04:58.103231+00:00", "nick": "rebsadran", "message": "what's going on here?", "links": [], "channel": "scrapy"},
{"date": "2014-08-01T20:12:53.672501+00:00", "nick": "rebsadran", "message": "solved it, was missing () when making itemloader object", "links": [], "channel": "scrapy"},
{"date": "2014-08-02T03:27:54.986108+00:00", "nick": "wishmaster", "message": "hello", "links": [], "channel": "scrapy"},
{"date": "2014-08-02T03:28:35.668101+00:00", "nick": "wishmaster", "message": "i would like to scrape a directory site to re import to another wordpress site any help", "links": [], "channel": "scrapy"},
{"date": "2014-08-02T07:02:10.703019+00:00", "nick": "Algorithmist", "message": "Hi", "links": [], "channel": "scrapy"},
{"date": "2014-08-02T11:04:38.738559+00:00", "nick": "rebsadran", "message": "How can I make a multileveled json file? i.e. I want to  have item meta data separate from the main data, the json structure would be item: {meta: {...}, data: {...}}", "links": [], "channel": "scrapy"},
{"date": "2014-08-02T14:07:25.911495+00:00", "nick": "Jamaica_", "message": "Hi", "links": [], "channel": "scrapy"},
{"date": "2014-08-02T18:17:12.040389+00:00", "nick": "rebsadran", "message": "hey, does linkExtractor return a set? as in if I extract page urls from every page will I end up with bunch of page 3s or only the unique urls are return?", "links": [], "channel": "scrapy"},
{"date": "2014-08-03T12:22:50.055651+00:00", "nick": "cetchmoh", "message": "hi there", "links": [], "channel": "scrapy"},
{"date": "2014-08-03T12:41:44.960499+00:00", "nick": "cetchmoh", "message": "I started porting a project of mine to scrapy - but there is one thing I cant wrap my head around.", "links": [], "channel": "scrapy"},
{"date": "2014-08-03T12:44:08.441478+00:00", "nick": "cetchmoh", "message": "My project uses half a dozen self written scrapers (urllib2, lxml, some homemade item queue). I start all of these in threads and have a mechanism to signal them that I need a quick update, or that they tell me when they have finished one run each. Now I read some articles about \"starting scrapy from a python script\" and some with threading, some with the multiprocessing module.", "links": [], "channel": "scrapy"},
{"date": "2014-08-03T12:45:32.869821+00:00", "nick": "cetchmoh", "message": "What would you guys recommend would be the correct approach? I like the idea of the item pipeline to access scraped items as soon as they are processed, but how can I control those scrapy instances (twisted, reactor) in the background and can get a signalling mechanism (finished, running, error?) for them besides using telnet or the web-api", "links": [], "channel": "scrapy"},
{"date": "2014-08-04T08:12:53.379643+00:00", "nick": "flyingtriangle", "message": "I'm having some conceptual difficulties", "links": [], "channel": "scrapy"},
{"date": "2014-08-04T08:16:27.491175+00:00", "nick": "flyingtriangle", "message": "Is it a good idea to have a request callback be somewhere other than the spider? I have an XSS spider so I send out a test request when there's parameters in the url. The response to that gets sent to be processed then another custom request is created. Once that request is created it also gets a custom callback and finally after its done within that callback, an item is created. What's the best way to do this? Like how can", "links": [], "channel": "scrapy"},
{"date": "2014-08-04T08:16:27.755061+00:00", "nick": "flyingtriangle", "message": "I offload each req's processing so it won't interfere with the spider?", "links": [], "channel": "scrapy"},
{"date": "2014-08-04T18:09:25.241454+00:00", "nick": "Jk_", "message": "Hi everyone. I am new to scrapy and cant figure out a problem. I wan to scrape from the Xpath    //*[@id=\"job-tasks\"]", "links": [], "channel": "scrapy"},
{"date": "2014-08-04T18:09:52.489498+00:00", "nick": "Jk_", "message": "sel.xpath('.//*[@id=\"job-tasks\"]... will not work", "links": ["mailto:sel.xpath('.//*[@id=\"job-tasks\"]').extract()"], "channel": "scrapy"},
{"date": "2014-08-04T23:43:15.238305+00:00", "nick": "Zost", "message": "hi guys, I have a question and I haven't  found an example for it yet.. If I make a Request(\"someurl\",callback=\"something\"), then in that callback, how can I extract all links with LinkExtractor?", "links": [], "channel": "scrapy"},
{"date": "2014-08-04T23:43:26.334232+00:00", "nick": "Zost", "message": "Is there any sample code?", "links": [], "channel": "scrapy"},
{"date": "2014-08-05T11:23:16.768353+00:00", "nick": "rmaja", "message": "hi!", "links": [], "channel": "scrapy"},
{"date": "2014-08-05T11:35:19.818452+00:00", "nick": "rmaja", "message": "how u scrap twitter without deleting your account?", "links": [], "channel": "scrapy"},
{"date": "2014-08-05T11:41:53.218741+00:00", "nick": "rmaja", "message": "i tried creating twitter accounts using selenium, but there are suspended after a couple of days max", "links": [], "channel": "scrapy"},
{"date": "2014-08-05T12:16:08.994106+00:00", "nick": "julio", "message": "hello guys", "links": [], "channel": "scrapy"},
{"date": "2014-08-05T12:16:49.923913+00:00", "nick": "julio", "message": "is it possible to use scrapyd functionalities upon a portia project ?", "links": [], "channel": "scrapy"},
{"date": "2014-08-05T13:15:33.233646+00:00", "nick": "julio", "message": "hello guys", "links": [], "channel": "scrapy"},
{"date": "2014-08-05T13:15:36.060968+00:00", "nick": "julio", "message": "is it possible to use scrapyd functionalities upon a portia project ?", "links": [], "channel": "scrapy"},
{"date": "2014-08-06T01:50:52.507151+00:00", "nick": "Doginal", "message": "hey guys, i am having some odd performance issues using allowed_domain and scrapy-redis. scarpy version 0.24.2 not sure why this is happening but i am only getting around 3pages/min and if i remove allowed_domains it goes back up to 200-300pages/min but cant controller where the crawler goes. I am not sure if this is a bug with scrapy or scrapy-redis any thoughts?", "links": [], "channel": "scrapy"},
{"date": "2014-08-06T04:48:31.907465+00:00", "nick": "vinayr", "message": "i'm not able to open the scrapy documentation http://doc.scrapy.org/en/latest/intro/tutorial....", "links": ["http://doc.scrapy.org/en/latest/intro/tutorial.html"], "channel": "scrapy"},
{"date": "2014-08-06T04:48:55.881118+00:00", "nick": "vinayr", "message": "i get a \"Server not found\" message", "links": [], "channel": "scrapy"},
{"date": "2014-08-06T09:36:55.476408+00:00", "nick": "ctolsen", "message": "Hi! I've just inherited a project so excuse me if my understanding of things so far is a bit obtuse. I have two spiders, one \"raw\" and one who uses PhantomJS to try to scrape links without javascriptless fallbacks. Whenever I run the latter, it seems to download fine to begin with (as does it if I just run \"scrapy fetch\" with it) but getting further into things I always get the following:", "links": [], "channel": "scrapy"},
{"date": "2014-08-06T09:37:00.260470+00:00", "nick": "ctolsen", "message": "selenium.common.exceptions.WebDriverException: Message: \"TypeError - 'undefined' is not a function (evaluating '_getTagName(currWindow).toLowerCase()')\"", "links": [], "channel": "scrapy"},
{"date": "2014-08-06T09:37:35.573267+00:00", "nick": "ctolsen", "message": "Does anyone know what I should be looking for to fix this? It seems to be thrown in ghostdriver.", "links": [], "channel": "scrapy"},
{"date": "2014-08-06T16:10:33.518963+00:00", "nick": "iammyr", "message": "Hi everyone! i'm going mad with an import-error i get when running my very simple basic test (it just calls scrapy to download a webpage content as in the scrapy tutorial)", "links": [], "channel": "scrapy"},
{"date": "2014-08-06T16:10:41.904360+00:00", "nick": "iammyr", "message": "the code runs perfectly fine within eclipse", "links": [], "channel": "scrapy"},
{"date": "2014-08-06T16:11:03.101620+00:00", "nick": "iammyr", "message": "but i'd like to run it successfully from within my virtualenv using tox", "links": [], "channel": "scrapy"},
{"date": "2014-08-06T16:11:24.953561+00:00", "nick": "iammyr", "message": "from the command line. but that's where i get \"ImportError: Error loading object 'scrapy.contrib.memusage.MemoryUsage': No module named OpenSSL\"", "links": [], "channel": "scrapy"},
{"date": "2014-08-06T16:11:42.184605+00:00", "nick": "iammyr", "message": "it's driving me mad. of course i already had python-openssl installed", "links": [], "channel": "scrapy"},
{"date": "2014-08-06T16:12:06.090719+00:00", "nick": "iammyr", "message": "through apt. then i also downloaded and compiled it explicitly using python2.7", "links": [], "channel": "scrapy"},
{"date": "2014-08-06T16:13:18.683536+00:00", "nick": "iammyr", "message": "i managed to solve other similar import-errors by moving folders but i've copied pyopenssl.py everywhere and still it is not retrieved by the interpreter. it is present in the folders included in sys.path", "links": [], "channel": "scrapy"},
{"date": "2014-08-06T16:13:44.370150+00:00", "nick": "iammyr", "message": "is there anyone who would know how to help? i'd be immensely thankful!", "links": [], "channel": "scrapy"},
{"date": "2014-08-06T17:03:30.863096+00:00", "nick": "hannes|", "message": "Hey all, is it possible to use scrapy on php websites which are using www.test.de/test.aspx?id=35091710 where the number changes", "links": ["http://www.test.de/test.aspx?id=35091710"], "channel": "scrapy"},
{"date": "2014-08-06T17:03:46.009440+00:00", "nick": "hannes|", "message": "I am always redirected to the start page of the website, when scraping", "links": [], "channel": "scrapy"},
{"date": "2014-08-06T17:06:09.799838+00:00", "nick": "hannes|", "message": "scraping works if I define one such website as the start url", "links": [], "channel": "scrapy"},
{"date": "2014-08-06T17:06:33.406901+00:00", "nick": "hannes|", "message": "http://paste.fedoraproject.org/123625/34477614", "links": ["http://paste.fedoraproject.org/123625/34477614"], "channel": "scrapy"},
{"date": "2014-08-06T17:06:49.088007+00:00", "nick": "hannes|", "message": "that's the spider", "links": [], "channel": "scrapy"},
{"date": "2014-08-06T17:19:38.538548+00:00", "nick": "hannes|", "message": "Just to be sure, I checked the website and the usage of crawlers is not explicitely forbidden", "links": [], "channel": "scrapy"},
{"date": "2014-08-06T20:02:46.309493+00:00", "nick": "julio", "message": "Hi! :)", "links": [], "channel": "scrapy"},
{"date": "2014-08-06T20:02:56.614638+00:00", "nick": "julio", "message": "Is it possible to export portia project to scrapyd ?", "links": [], "channel": "scrapy"},
{"date": "2014-08-06T22:47:14.536219+00:00", "nick": "Doginal", "message": "hey, guys i am trying to disable the offsite middleware but doing SPIDER_MIDDLEWARE = {", "links": [], "channel": "scrapy"},
{"date": "2014-08-06T22:47:14.716371+00:00", "nick": "Doginal", "message": "'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': None,", "links": [], "channel": "scrapy"},
{"date": "2014-08-06T22:47:14.848515+00:00", "nick": "Doginal", "message": "} is not working, any idea why?", "links": [], "channel": "scrapy"},
{"date": "2014-08-06T22:59:49.947893+00:00", "nick": "Doginal", "message": "sryy my client dropped off", "links": [], "channel": "scrapy"},
{"date": "2014-08-06T23:11:46.019980+00:00", "nick": "doginal_", "message": "any idea why i cant turn off the offsite middleware?", "links": [], "channel": "scrapy"},
{"date": "2014-08-06T23:45:28.685658+00:00", "nick": "doginal", "message": "does the allowed_domain list have a limit? and how does it process them one after the other, does this cause a performance issues with a large list?", "links": [], "channel": "scrapy"},
{"date": "2014-08-07T10:36:22.024340+00:00", "nick": "odigem", "message": "Hi", "links": [], "channel": "scrapy"},
{"date": "2014-08-07T10:36:35.442402+00:00", "nick": "odigem", "message": "how to disable cache errors?", "links": [], "channel": "scrapy"},
{"date": "2014-08-07T10:36:58.785160+00:00", "nick": "odigem", "message": "i enable http cache and all 503 errors stored in it O.o", "links": [], "channel": "scrapy"},
{"date": "2014-08-08T19:48:01.961949+00:00", "nick": "py0", "message": "You guys are my new friends for the coming months.", "links": [], "channel": "scrapy"},
{"date": "2014-08-08T20:47:24.317508+00:00", "nick": "nramirezuy", "message": "wut?", "links": [], "channel": "scrapy"},
{"date": "2014-08-09T21:05:35.780697+00:00", "nick": "odigem", "message": "how to return all items separately ?", "links": [], "channel": "scrapy"},
{"date": "2014-08-09T21:06:25.638908+00:00", "nick": "odigem", "message": "i['x']=sel.xpath('//a')  return i , i see all links in array, i want see one link per line for store to csv", "links": [], "channel": "scrapy"},
{"date": "2014-08-10T08:41:28.912325+00:00", "nick": "ScrapyNewbie", "message": "Hi guys I am a newbie scrapy user, do we have any experienced scrapy developers here? I am after a script that will crawl an entire website, follow all internal links on the website including subdomains example cnn.com and subdomain.cnn.com. I want all the pages on the website and also need it to extract all external links on those pages. I have around 1k sites I want to crawl. looking for a freelancer. Willing to pay for the dev", "links": ["http://cnn.com", "http://subdomain.cnn.com"], "channel": "scrapy"},
{"date": "2014-08-10T08:42:07.706236+00:00", "nick": "ScrapyNewbie", "message": "Doesnt need to follow external links just follow all internal and extract internal and external links", "links": [], "channel": "scrapy"},
{"date": "2014-08-10T15:29:54.250976+00:00", "nick": "odigem", "message": "HI", "links": [], "channel": "scrapy"},
{"date": "2014-08-10T15:54:38.563905+00:00", "nick": "odigem", "message": "how to disable ignore duplicates ?", "links": [], "channel": "scrapy"},
{"date": "2014-08-10T16:58:53.525955+00:00", "nick": "py0", "message": "scrapy won't support 3.4?", "links": [], "channel": "scrapy"},
{"date": "2014-08-10T18:20:24.645718+00:00", "nick": "py0", "message": "these errors occur from running the scrapy tutorial process http://bpaste.net/show/ofzlzJrb6CcDwPZEeROT/", "links": ["http://bpaste.net/show/ofzlzJrb6CcDwPZEeROT/"], "channel": "scrapy"},
{"date": "2014-08-10T18:20:53.510174+00:00", "nick": "py0", "message": "the isfile() is all me", "links": [], "channel": "scrapy"},
{"date": "2014-08-11T16:31:25.419749+00:00", "nick": "py0", "message": "When you run a scrapy shell and interactively use response.body.. is it supposed to come out as a huge pile of javascript garbage", "links": [], "channel": "scrapy"},
{"date": "2014-08-11T20:46:54.570573+00:00", "nick": "Jk__", "message": "Hi anyone there who could help me with an odd unexpected indent error?", "links": [], "channel": "scrapy"},
{"date": "2014-08-12T11:16:09.529338+00:00", "nick": "elmkarami", "message": "hi everyone !", "links": [], "channel": "scrapy"},
{"date": "2014-08-12T11:17:08.600718+00:00", "nick": "elmkarami", "message": "i am working on a project with scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-08-12T11:17:50.690505+00:00", "nick": "elmkarami", "message": "and i wanted to integrate sentry, I've used scrapy-sentry but it it does not at all", "links": [], "channel": "scrapy"},
{"date": "2014-08-12T11:18:16.311613+00:00", "nick": "elmkarami", "message": "i also tried to implement it using Extensions but it works only if an error occurred in the spider's callback (not pipelines.py, items.py)...", "links": [], "channel": "scrapy"},
{"date": "2014-08-12T11:18:38.948552+00:00", "nick": "elmkarami", "message": "is there any that i can log errors (in spiders, items, pipelines ...) to sentry, like in Django? Thanks", "links": [], "channel": "scrapy"},
{"date": "2014-08-12T11:40:50.762703+00:00", "nick": "elmkarami", "message": "can someone help me please !", "links": [], "channel": "scrapy"},
{"date": "2014-08-12T15:24:51.062224+00:00", "nick": "Immi", "message": "hello all", "links": [], "channel": "scrapy"},
{"date": "2014-08-12T15:25:24.975626+00:00", "nick": "Immi", "message": "I need some advise...on building a Meta travel search engine using scrapy...ex: kayak.com", "links": ["http://kayak.com"], "channel": "scrapy"},
{"date": "2014-08-12T15:25:43.600366+00:00", "nick": "Immi", "message": "Can anyone please help? if Scrapy would be a right choice", "links": [], "channel": "scrapy"},
{"date": "2014-08-12T15:25:44.544810+00:00", "nick": "Immi", "message": "?", "links": [], "channel": "scrapy"},
{"date": "2014-08-13T01:50:38.609572+00:00", "nick": "LewisMcM", "message": "anyone know ay any examples of a request being used to fill out a login form being used with CrawlSpider? all the exmples i can find are verry old and im struguling to get it continue crawling after the login", "links": [], "channel": "scrapy"},
{"date": "2014-08-13T01:59:13.284861+00:00", "nick": "LewisMcM", "message": "seems like i got it by hijacking parse_start_url", "links": [], "channel": "scrapy"},
{"date": "2014-08-13T01:59:25.304949+00:00", "nick": "LewisMcM", "message": "not sure thtats right but it seems to run", "links": [], "channel": "scrapy"},
{"date": "2014-08-15T00:42:48.512017+00:00", "nick": "jon__", "message": "is there any way to have scrapy only scrape links within a certain div? I am trying to scrape a webpage and I am getting a lot of linke I do not need at all.", "links": [], "channel": "scrapy"},
{"date": "2014-08-15T01:08:54.359932+00:00", "nick": "toothrot", "message": "of course there is", "links": [], "channel": "scrapy"},
{"date": "2014-08-15T01:10:05.356018+00:00", "nick": "toothrot", "message": "you need to say what components you are using", "links": [], "channel": "scrapy"},
{"date": "2014-08-15T01:10:19.305558+00:00", "nick": "toothrot", "message": "for instance LxmlLinkExtractor has a restrict_xpaths", "links": [], "channel": "scrapy"},
{"date": "2014-08-15T01:10:47.425663+00:00", "nick": "toothrot", "message": "i typically extract/build all of my own links though", "links": [], "channel": "scrapy"},
{"date": "2014-08-15T01:10:51.245673+00:00", "nick": "toothrot", "message": "jon__, ^", "links": [], "channel": "scrapy"},
{"date": "2014-08-15T01:11:07.088071+00:00", "nick": "toothrot", "message": "(i'm not recommending it, just saying there's a few ways to do it)", "links": [], "channel": "scrapy"},
{"date": "2014-08-15T01:14:32.473697+00:00", "nick": "jon__", "message": "ok. I think I by passed that by just creating an xpath that only matches links in a certain div", "links": [], "channel": "scrapy"},
{"date": "2014-08-15T01:54:02.626225+00:00", "nick": "sunshine`", "message": "Hi", "links": [], "channel": "scrapy"},
{"date": "2014-08-15T01:54:05.265245+00:00", "nick": "sunshine`", "message": "?", "links": [], "channel": "scrapy"},
{"date": "2014-08-15T02:19:07.374446+00:00", "nick": "marven", "message": "how can i do an \"async\" sleep on scrapy? i have a file with 1m+ urls to crawl and my spider dies since the Request objects pile up before they get evaluated. i need a way to delay reading the lines in the file when the total number of existing Request objects reaches a certain amount.", "links": [], "channel": "scrapy"},
{"date": "2014-08-15T02:23:17.788873+00:00", "nick": "toothrot", "message": "how have you determined too many Request objects causes the spider to die? what's the error?", "links": [], "channel": "scrapy"},
{"date": "2014-08-15T02:23:40.857412+00:00", "nick": "marven", "message": "i just get a", "links": [], "channel": "scrapy"},
{"date": "2014-08-15T02:23:45.973941+00:00", "nick": "marven", "message": "\"Killed\" message", "links": [], "channel": "scrapy"},
{"date": "2014-08-15T02:25:33.848289+00:00", "nick": "marven", "message": "i tried adding sleep() to the for loop of reading the lines, it just delays when the spider eventually dies", "links": [], "channel": "scrapy"},
{"date": "2014-08-15T02:27:25.146855+00:00", "nick": "marven", "message": "i could probably increase the sleep time to around 4s so that it matches the speed of evaluating the Requests but that isn't really a good fix", "links": [], "channel": "scrapy"},
{"date": "2014-08-15T03:22:50.731172+00:00", "nick": "marven", "message": "in case anyone is interested, solved it by reading the urls from the file whenever the spider is idle, no more memory problems :)", "links": [], "channel": "scrapy"},
{"date": "2014-08-15T12:35:00.237112+00:00", "nick": "petapetapeta", "message": "Hello. I am attempting to crawl a specific source using Scrapy 0.16.3 and I am encountering a weird error. When I try to debug it using pdb I get the following error: Exception AttributeError: \"'NoneType' object has no attribute 'path'\" in <generator object idGenerator at 0xd7e1a00> ignored", "links": [], "channel": "scrapy"},
{"date": "2014-08-15T12:35:18.047113+00:00", "nick": "petapetapeta", "message": "The error is thrown when this line is run:  pkg_resources.run_script('Scrapy==0.16.3', 'scrapy')", "links": [], "channel": "scrapy"},
{"date": "2014-08-15T12:39:56.484967+00:00", "nick": "nikolaosk", "message": "petapetapeta: how did you end up with scrapy version .16", "links": [], "channel": "scrapy"},
{"date": "2014-08-15T12:40:13.187594+00:00", "nick": "petapetapeta", "message": "nikolaosk: legacy software :/", "links": [], "channel": "scrapy"},
{"date": "2014-08-15T12:40:45.237992+00:00", "nick": "nikolaosk", "message": "oh", "links": [], "channel": "scrapy"},
{"date": "2014-08-15T12:40:59.304373+00:00", "nick": "petapetapeta", "message": "And it is a big task to update by now due to the way selectors have been changed in the newer versions (I've been told)", "links": [], "channel": "scrapy"},
{"date": "2014-08-15T12:41:26.978655+00:00", "nick": "nikolaosk", "message": "do you mean .xpath and .css methods ?", "links": [], "channel": "scrapy"},
{"date": "2014-08-15T12:41:28.726258+00:00", "nick": "petapetapeta", "message": "From what I can see it looks to be an internal scrapy error, but it doesn't seem right since I can crawl other sources", "links": [], "channel": "scrapy"},
{"date": "2014-08-15T12:41:32.170220+00:00", "nick": "nikolaosk", "message": "these are on .20", "links": [], "channel": "scrapy"},
{"date": "2014-08-15T12:41:52.802180+00:00", "nick": "petapetapeta", "message": "So I could potentially update to .19?", "links": [], "channel": "scrapy"},
{"date": "2014-08-15T12:42:40.255498+00:00", "nick": "petapetapeta", "message": "I would be that those are the ones. I have just taken over on this project and was told that it hadn't been updated due to the selectors behaving differently in the newer versions of Scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-08-15T12:42:47.937862+00:00", "nick": "nikolaosk", "message": ".19 is .20", "links": [], "channel": "scrapy"},
{"date": "2014-08-15T12:42:53.091731+00:00", "nick": "petapetapeta", "message": "*I would bet", "links": [], "channel": "scrapy"},
{"date": "2014-08-15T12:42:59.637256+00:00", "nick": "petapetapeta", "message": "That is a weird sentence -.-", "links": [], "channel": "scrapy"},
{"date": "2014-08-15T12:43:13.769776+00:00", "nick": "nikolaosk", "message": "do you have tests?", "links": [], "channel": "scrapy"},
{"date": "2014-08-15T12:44:00.897090+00:00", "nick": "petapetapeta", "message": "Nope. I can however see in the system that other sources are being crawled normally and that this one was crawled normally and suddenly stopped working", "links": [], "channel": "scrapy"},
{"date": "2014-08-15T12:44:59.072451+00:00", "nick": "petapetapeta", "message": "I have only attempted to debug using pdb and the telnet client, but that doesn't tell me anything else but the previously mentioned error", "links": [], "channel": "scrapy"},
{"date": "2014-08-16T18:06:08.038808+00:00", "nick": "x3000", "message": "hi all", "links": [], "channel": "scrapy"},
{"date": "2014-08-16T18:07:27.293865+00:00", "nick": "x3000", "message": "I've got a question regarding the scrapy startproject command.", "links": [], "channel": "scrapy"},
{"date": "2014-08-16T18:09:53.918655+00:00", "nick": "x3000", "message": "all the files that command creates are prefilled with code; the tutorial on the website show the code a little bit of from what's inside the prefilled files. Do I need to use the prefilled stuff or do I have to delete the contents / clear it to get the tutorial working?", "links": [], "channel": "scrapy"},
{"date": "2014-08-16T18:09:57.125212+00:00", "nick": "x3000", "message": "thanks q-", "links": [], "channel": "scrapy"},
{"date": "2014-08-16T18:13:24.763187+00:00", "nick": "hannes|", "message": "x3000, replace mostly. also make sure you are using the tutorial for your installed version", "links": [], "channel": "scrapy"},
{"date": "2014-08-16T18:21:48.545270+00:00", "nick": "x3000", "message": "thanks hannes, my scrapy version is -- Scrapy 0.24.4. The tutorial shos some output from January, '14 - to you think this tutorial isn't working anymore? I dunno but everytime I'm starting to run: \"scrapy crawl spider\" I am running into major errors.", "links": [], "channel": "scrapy"},
{"date": "2014-08-16T18:22:30.135453+00:00", "nick": "x3000", "message": "the tutorial shows, I mean.", "links": [], "channel": "scrapy"},
{"date": "2014-08-16T18:24:02.751483+00:00", "nick": "hannes|", "message": "not an expert on scrapy, to be honest, still learning as well", "links": [], "channel": "scrapy"},
{"date": "2014-08-16T19:30:51.952130+00:00", "nick": "jjmpsp", "message": "Any Scrapy devs here? I'm having a hard time understanding how to control my code when Scrapy performs an asynchronous page parse. What I want to do is load my parsing rules from a database when a page has been downloaded. Can anyone help? http://stackoverflow.com/questions/25342851/scr...", "links": ["http://stackoverflow.com/questions/25342851/scrapy-get-index-of-item-being-parsed"], "channel": "scrapy"},
{"date": "2014-08-16T20:47:38.867147+00:00", "nick": "toothrot", "message": "shame people give up so easily", "links": [], "channel": "scrapy"},
{"date": "2014-08-17T08:04:30.782506+00:00", "nick": "dineshs91", "message": "Hi", "links": [], "channel": "scrapy"},
{"date": "2014-08-17T08:31:51.351794+00:00", "nick": "dineshs91", "message": "How to use scrapy directly from source code ?", "links": [], "channel": "scrapy"},
{"date": "2014-08-17T08:37:59.199285+00:00", "nick": "nkuttler", "message": "dineshs91: what does that mean?", "links": [], "channel": "scrapy"},
{"date": "2014-08-17T08:38:59.996957+00:00", "nick": "dineshs91", "message": "I have cloned scrapy from github. How do I check it out ?", "links": [], "channel": "scrapy"},
{"date": "2014-08-17T08:39:37.744186+00:00", "nick": "nkuttler", "message": "you probably want to pip install it instead", "links": [], "channel": "scrapy"},
{"date": "2014-08-17T08:42:09.482407+00:00", "nick": "dineshs91", "message": "nkuttler:  Ok. But say I make some changes to the source, Is there a way I can verify those changes ?", "links": [], "channel": "scrapy"},
{"date": "2014-08-17T08:42:34.027852+00:00", "nick": "nkuttler", "message": "dineshs91: i haven't worked with scrapy source, but i'd assume the usual applies. run the test suite, etc", "links": [], "channel": "scrapy"},
{"date": "2014-08-17T08:44:21.459392+00:00", "nick": "dineshs91", "message": "nkuttler: thanks !!", "links": [], "channel": "scrapy"},
{"date": "2014-08-17T15:02:33.502325+00:00", "nick": "hck", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2014-08-17T15:02:55.161759+00:00", "nick": "hck", "message": "can I ask a question??", "links": [], "channel": "scrapy"},
{"date": "2014-08-17T15:03:06.489425+00:00", "nick": "hck", "message": "are there any master?", "links": [], "channel": "scrapy"},
{"date": "2014-08-17T15:03:45.196085+00:00", "nick": "hck", "message": "fuck all", "links": [], "channel": "scrapy"},
{"date": "2014-08-17T15:03:51.057098+00:00", "nick": "hck", "message": "aa", "links": [], "channel": "scrapy"},
{"date": "2014-08-18T19:58:47.865330+00:00", "nick": "newbsduser", "message": "hi, iam trying to write an xpath selector with python scrapy:  code:  https://gist.github.com/anonymous/e8f66b3e1c585...  ... i want to select <tr>...</tr> after <p>Handikap</p>...", "links": ["https://gist.github.com/anonymous/e8f66b3e1c5856baa687"], "channel": "scrapy"},
{"date": "2014-08-18T20:06:27.360958+00:00", "nick": "newbsduser", "message": "it worked", "links": [], "channel": "scrapy"},
{"date": "2014-08-18T20:06:33.232881+00:00", "nick": "newbsduser", "message": "hxs.select(\"//p[contains(text(),'Handikap')]/following-sibling::table/tr\")[0].select(\".//td\")", "links": [], "channel": "scrapy"},
{"date": "2014-08-18T20:06:40.943691+00:00", "nick": "newbsduser", "message": "thanks myself", "links": [], "channel": "scrapy"},
{"date": "2014-08-18T20:06:42.121151+00:00", "nick": "newbsduser", "message": "xD", "links": [], "channel": "scrapy"},
{"date": "2014-08-18T20:09:02.842327+00:00", "nick": "newbsduser", "message": "how can i extract <td>2 <span class=\"txt_yellow\">+1h</span'>   ... i want to get 2 +1h", "links": [], "channel": "scrapy"},
{"date": "2014-08-18T20:09:10.642010+00:00", "nick": "newbsduser", "message": "\"2 +1h\"", "links": [], "channel": "scrapy"},
{"date": "2014-08-18T20:41:31.654141+00:00", "nick": "yoLo_", "message": "what exactly is the crawl pipeline ?", "links": [], "channel": "scrapy"},
{"date": "2014-08-19T14:50:10.135827+00:00", "nick": "Roux_taff", "message": "hey there", "links": [], "channel": "scrapy"},
{"date": "2014-08-19T14:50:31.038150+00:00", "nick": "Roux_taff", "message": "I just tried to install portia", "links": [], "channel": "scrapy"},
{"date": "2014-08-19T14:50:42.147563+00:00", "nick": "Roux_taff", "message": "tried to build a spider with the web gui", "links": [], "channel": "scrapy"},
{"date": "2014-08-19T14:51:08.341958+00:00", "nick": "Roux_taff", "message": "then launched the portiacrawl bin (the doc misses some info on installation dependencies and path for tis btw)", "links": [], "channel": "scrapy"},
{"date": "2014-08-19T14:51:21.261322+00:00", "nick": "Roux_taff", "message": "it definitely crawled", "links": [], "channel": "scrapy"},
{"date": "2014-08-19T14:51:30.906161+00:00", "nick": "Roux_taff", "message": "but i can't find out where did the data get stored", "links": [], "channel": "scrapy"},
{"date": "2014-08-19T15:03:44.750673+00:00", "nick": "Roux_taff", "message": "ah found it", "links": [], "channel": "scrapy"},
{"date": "2014-08-19T15:03:54.348225+00:00", "nick": "Roux_taff", "message": "needed to add -o to the portiacrawl command", "links": [], "channel": "scrapy"},
{"date": "2014-08-19T15:04:21.432220+00:00", "nick": "Roux_taff", "message": "well it looks nice, the global doc could be improved but that's pretty cool", "links": [], "channel": "scrapy"},
{"date": "2014-08-19T15:05:43.268213+00:00", "nick": "Guest40338", "message": "Roux_taff: it would be nice if you create an issue describing the missing installation steps", "links": [], "channel": "scrapy"},
{"date": "2014-08-19T15:06:10.974382+00:00", "nick": "Roux_taff", "message": "I'll rather make a PR", "links": [], "channel": "scrapy"},
{"date": "2014-08-19T15:06:23.723182+00:00", "nick": "Guest40338", "message": "even better, thanks", "links": [], "channel": "scrapy"},
{"date": "2014-08-19T15:14:36.784897+00:00", "nick": "Roux_taff", "message": "done https://github.com/scrapinghub/portia/pull/89", "links": ["https://github.com/scrapinghub/portia/pull/89"], "channel": "scrapy"},
{"date": "2014-08-20T05:56:48.824419+00:00", "nick": "HowardwLo", "message": "howdy", "links": [], "channel": "scrapy"},
{"date": "2014-08-20T05:57:27.006424+00:00", "nick": "HowardwLo", "message": "If i want to reduce quality of images using PIL and optimize them for size, would I be doing that in a custom Images pipline?", "links": [], "channel": "scrapy"},
{"date": "2014-08-20T18:52:53.090608+00:00", "nick": "Kulbi", "message": "hey guys", "links": [], "channel": "scrapy"},
{"date": "2014-08-20T18:53:23.122239+00:00", "nick": "Kulbi", "message": "wonder if somebody could help me with one error I got trying to run a spider", "links": [], "channel": "scrapy"},
{"date": "2014-08-20T19:25:27.085352+00:00", "nick": "Kulbi", "message": "nevermind, I had a problem because of using Anaconda", "links": [], "channel": "scrapy"},
{"date": "2014-08-20T19:25:28.383559+00:00", "nick": "Kulbi", "message": "cheers", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T10:09:53.907245+00:00", "nick": "srek", "message": "Hi anyone awake?", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T11:00:51.204539+00:00", "nick": "rohitt", "message": "Hey guys!", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T11:01:29.102830+00:00", "nick": "rohitt", "message": "I've written a scraper for a website, which seems to be working fine -- for a while only, after which, it gets \"stuck\"", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T11:01:40.391664+00:00", "nick": "rohitt", "message": "The error is similar to -- https://groups.google.com/forum/#!topic/scrapy-...", "links": ["https://groups.google.com/forum/#!topic/scrapy-users/5gb5X2fmtCg"], "channel": "scrapy"},
{"date": "2014-08-21T11:01:57.963626+00:00", "nick": "rohitt", "message": "And I am using Scrapy 0.24.4", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T11:02:42.969968+00:00", "nick": "rohitt", "message": "Any guess?", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T13:21:26.973994+00:00", "nick": "srek", "message": "hi, i have a problem too try scrape a website, and i am stuck", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T13:22:51.123623+00:00", "nick": "srek", "message": "the problem is so easy i almost feel bad too ask", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T13:23:49.841200+00:00", "nick": "srek", "message": "if someone have some time to look at my crawler i can upload it some where", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T14:13:36.331078+00:00", "nick": "pig_raft", "message": "does scrapy not support the Xpath [ends-with] function?", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T14:16:21.925597+00:00", "nick": "pig_raft", "message": "[contains] works for my purposes, but [ends-with] would be the bulletproof solution", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T14:17:23.695876+00:00", "nick": "pig_raft", "message": "however, it throws ValueError: Invalid XPath:", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T14:24:10.360417+00:00", "nick": "nramirezuy", "message": "in Scrapy we use lxml for xpath resolving and it supports XPath 1.0, XSLT 1.0 and the EXSLT extensions through libxml2 (http://lxml.de/xpathxslt.html).", "links": ["http://lxml.de/xpathxslt.html"], "channel": "scrapy"},
{"date": "2014-08-21T14:24:10.651348+00:00", "nick": "nramirezuy", "message": "ends-with is part of XPath 2.0, XQuery 1.0 and XSLT 2.0, (http://www.w3schools.com/xpath/xpath_functions.asp)", "links": ["http://www.w3schools.com/xpath/xpath_functions.asp"], "channel": "scrapy"},
{"date": "2014-08-21T14:29:32.702936+00:00", "nick": "pig_raft", "message": "ah, ok. thanks.", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T14:35:15.771130+00:00", "nick": "nramirezuy", "message": "but you can use regular expressions, we add the namespace by default", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T14:44:42.694001+00:00", "nick": "pig_raft", "message": "i'll keep that in mind", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T17:04:52.789180+00:00", "nick": "nramirezuy", "message": "those batch are going to run in parallel?", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T17:05:27.635094+00:00", "nick": "vitorbaptista", "message": "hopefully :P", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T17:07:31.187941+00:00", "nick": "nramirezuy", "message": "then you have to create a hash of the urls, check this function to know how to do it properly https://github.com/scrapy/scrapy/blob/master/sc...", "links": ["https://github.com/scrapy/scrapy/blob/master/scrapy/utils/request.py#L19"], "channel": "scrapy"},
{"date": "2014-08-21T17:08:10.443176+00:00", "nick": "nramirezuy", "message": "you can use the hash to filter the urls on Foo", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T17:08:42.903095+00:00", "nick": "vitorbaptista", "message": "The thing is, Foo will be run multiple times, getting new data. First time it runs, it might get ids 1 and 2, then I run Bar and it gets the info for these two objects. Then, a few days later, I run Foo again and it finds that there's ids 1, 2 and 3 now. Then, when Bar is ran, I want it to notice that it already has ids 1 and 2, and download only id 3", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T17:08:50.192004+00:00", "nick": "vitorbaptista", "message": "humm", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T17:09:02.172546+00:00", "nick": "vitorbaptista", "message": "But where will I save the URLs that were already visited?", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T17:09:27.592295+00:00", "nick": "nramirezuy", "message": "save the hashes", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T17:09:57.601215+00:00", "nick": "vitorbaptista", "message": "My idea was using self.state for that, but I can't use it on .start_requests()", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T17:10:21.332493+00:00", "nick": "nramirezuy", "message": "if you are expecting too many use something like https://docs.python.org/2/library/anydbm.html", "links": ["https://docs.python.org/2/library/anydbm.html"], "channel": "scrapy"},
{"date": "2014-08-21T17:11:07.318051+00:00", "nick": "vitorbaptista", "message": "I could simply write a file on disk to keep the hashes I visited, but I was hoping scrapy provided something more convenient (like self.state)", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T17:11:11.790272+00:00", "nick": "nramirezuy", "message": "what is self.state?", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T17:11:33.614911+00:00", "nick": "vitorbaptista", "message": "http://doc.scrapy.org/en/latest/topics/jobs.htm...", "links": ["http://doc.scrapy.org/en/latest/topics/jobs.html#keeping-persistent-state-between-batches"], "channel": "scrapy"},
{"date": "2014-08-21T17:12:10.979344+00:00", "nick": "nramirezuy", "message": "are you using --JOBDIR ?", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T17:12:19.689830+00:00", "nick": "vitorbaptista", "message": "This simply is a dict that's available on parse() where you can put stuff into, and it'll persist across runs", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T17:12:21.863926+00:00", "nick": "vitorbaptista", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T17:12:31.064193+00:00", "nick": "vitorbaptista", "message": "it's working on .parse(), just not on .start_requests()", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T17:12:47.639850+00:00", "nick": "vitorbaptista", "message": "exceptions.AttributeError: 'VotacoesProposicoesSpider' object has no attribute 'state'", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T17:13:01.282546+00:00", "nick": "vitorbaptista", "message": "It seems it's initialized after .start_requests() was run", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T17:13:40.632882+00:00", "nick": "vitorbaptista", "message": "I even tried doing \"self.states = {'abc': 10}\" on .start_requests(), but it gets overwritten by the time I try to read it on .parse() :/", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T17:15:00.584151+00:00", "nick": "nramirezuy", "message": "can you try overriding the extension using the engine signal https://github.com/scrapy/scrapy/blob/master/sc...", "links": ["https://github.com/scrapy/scrapy/blob/master/scrapy/contrib/spiderstate.py"], "channel": "scrapy"},
{"date": "2014-08-21T17:16:48.331748+00:00", "nick": "nramirezuy", "message": "no I'm wrong, I'm not getting why is this happening spider_opened should be fired before start_requests", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T17:17:07.938229+00:00", "nick": "vitorbaptista", "message": "hummm", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T17:19:16.839283+00:00", "nick": "vitorbaptista", "message": "*Looking for where it's run...*", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T17:19:57.992154+00:00", "nick": "vitorbaptista", "message": "No, it seems it's right...", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T17:20:01.543163+00:00", "nick": "nramirezuy", "message": "a fast workaround could be crawling a random page on start_requests and using parse to do the actual start_requests", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T17:20:38.894081+00:00", "nick": "vitorbaptista", "message": "see https://github.com/scrapy/scrapy/blob/master/sc...", "links": ["https://github.com/scrapy/scrapy/blob/master/scrapy/core/engine.py#L218-L233"], "channel": "scrapy"},
{"date": "2014-08-21T17:20:56.920363+00:00", "nick": "vitorbaptista", "message": "the start_requests are run on line ~225", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T17:21:14.305064+00:00", "nick": "vitorbaptista", "message": "but spider_opened is just called on line ~232", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T17:22:37.759304+00:00", "nick": "vitorbaptista", "message": "OK, so I can't use self.state for this :/", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T17:22:53.333894+00:00", "nick": "vitorbaptista", "message": "I wonder if the Dupefilter doesn't work on multiple runs when using the JOBDIR", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T17:23:15.826543+00:00", "nick": "vitorbaptista", "message": "i.e. if I schedule the same URL twice using the same JOBDIR, will it be downloaded twice?", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T17:23:18.804279+00:00", "nick": "vitorbaptista", "message": "(Trying...)", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T17:31:26.307560+00:00", "nick": "nramirezuy", "message": "I have the last version of Scrapy pulled from Github and its working as you expect it", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T17:34:01.096837+00:00", "nick": "nramirezuy", "message": "Spider code: https://gist.github.com/nramirezuy/e25524e9f84e...", "links": ["https://gist.github.com/nramirezuy/e25524e9f84e3e8875bd"], "channel": "scrapy"},
{"date": "2014-08-21T17:34:01.376028+00:00", "nick": "nramirezuy", "message": "Logs: https://gist.github.com/nramirezuy/6dabd8faecfa...", "links": ["https://gist.github.com/nramirezuy/6dabd8faecfaac29dca1"], "channel": "scrapy"},
{"date": "2014-08-21T17:35:40.736443+00:00", "nick": "vitorbaptista", "message": "try setting the item_count on start_requests", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T17:35:42.663790+00:00", "nick": "vitorbaptista", "message": "like", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T17:35:48.195060+00:00", "nick": "vitorbaptista", "message": "self.state['items_count'] = 50", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T17:37:40.840126+00:00", "nick": "nramirezuy", "message": "it's working for me", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T17:38:38.445008+00:00", "nick": "nramirezuy", "message": "I moved self.state['items_count'] = self.state.get('items_count', 0) + 1 before the print and the first time it print 1 second run prints 2", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T17:38:49.022467+00:00", "nick": "vitorbaptista", "message": "hummm", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T17:39:05.548755+00:00", "nick": "vitorbaptista", "message": "I'm using 0.24.4, I'll try to update", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T17:39:14.076369+00:00", "nick": "nramirezuy", "message": "update your scrapy version pull master from github", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T17:40:13.043171+00:00", "nick": "nramirezuy", "message": "also this doesn't support parallel runs", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T17:40:38.354649+00:00", "nick": "nramirezuy", "message": "this state will allow you to just run serial batches", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T17:44:36.501668+00:00", "nick": "vitorbaptista", "message": "that's odd, I still have the same issue", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T17:44:53.572611+00:00", "nick": "vitorbaptista", "message": "installed through pip install git+git://github.com/scrapy/scrapy.git@master", "links": ["http://git+git://github.com/scrapy/scrapy.git@master"], "channel": "scrapy"},
{"date": "2014-08-21T17:45:16.756998+00:00", "nick": "nramirezuy", "message": "did you uninstalled your previous one?", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T17:45:36.965018+00:00", "nick": "vitorbaptista", "message": "Trying with --upgrade... just a sec", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T17:45:39.989693+00:00", "nick": "nramirezuy", "message": "system package?", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T17:56:56.165564+00:00", "nick": "vitorbaptista", "message": "yeah, same problem. That's weird...", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T17:57:05.255491+00:00", "nick": "vitorbaptista", "message": "Anyway, I found a better solution for now: enabling HTTP caching", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T17:58:43.398986+00:00", "nick": "vitorbaptista", "message": "That'll solve my issue somewhat. It'll still take the time to parse/load the data, but at least it won't be redownloading everything.", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T17:59:20.058329+00:00", "nick": "vitorbaptista", "message": "Thanks a LOT for your help, I really appreciate it.", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T18:00:16.469779+00:00", "nick": "nramirezuy", "message": "you're welcome", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T18:00:23.985213+00:00", "nick": "vitorbaptista", "message": "Oh, one last question: is there any way to communicate between spiders? Right now what I'm planning to do is running the Foo spider once and outputting to a known file, and then Bar will read from that file", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T18:00:57.126712+00:00", "nick": "vitorbaptista", "message": "But the filename will need to be hardcoded, which is a bit hacky. Does scrapy provides a more convenient way of doing this?", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T18:02:52.273613+00:00", "nick": "nramirezuy", "message": "no right now, but you can create an extension for Foo that schedule Bar on engine_stopped signal using and argument for the filename", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T18:09:22.532615+00:00", "nick": "vitorbaptista", "message": "hummm", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T18:09:40.229926+00:00", "nick": "vitorbaptista", "message": "Is it possible to do that but instead of passing the filename, passing the crawled items themselves?", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T18:12:36.373714+00:00", "nick": "nramirezuy", "message": "nope", "links": [], "channel": "scrapy"},
{"date": "2014-08-21T18:17:03.879818+00:00", "nick": "vitorbaptista", "message": "OK, I'll do that then. Thanks again :)", "links": [], "channel": "scrapy"},
{"date": "2014-08-22T08:09:16.853970+00:00", "nick": "flyingtriangle", "message": "I'm having a tough time sending a cookie with scrapy. Goal: send a request with a specific cookie ignoring any other cookies. This cookie should be nonsticky and should only persist for that one request. I see that I can set request.meta['cookie jar'] but I'm having trouble create the cookie jar. Here's what I'm doing: http://pastebin.aquilenet.fr/?29bca5af3522f697#...", "links": ["http://pastebin.aquilenet.fr/?29bca5af3522f697=#EvuA4zktmafCsvjq5WfoN5LlI0AkcAdUe4R+r/HTl7k="], "channel": "scrapy"},
{"date": "2014-08-22T09:32:48.847251+00:00", "nick": "flyingtriangle", "message": "Here's a better paste of exactly what I'm doing http://0paste.com/6232", "links": ["http://0paste.com/6232"], "channel": "scrapy"},
{"date": "2014-08-22T09:33:41.588268+00:00", "nick": "flyingtriangle", "message": "I just want a single request with a single, nonsticky cookie sent. I cannot disable cookies middleware and because of this I can't just use the cookies={'foo':'bar'} header since cookies middleware makes that sticky", "links": [], "channel": "scrapy"},
{"date": "2014-08-22T11:18:12.678698+00:00", "nick": "stevezau", "message": "hi guys, i'm looking at porting a custom scraping app i wrote years ago (in java).. to scrapy. I've read the tutorial but i'm still a little confused on how things work.. some of the websites i scrape have a login page that i want scrapy to try scrape with some preset cookies.. if if gets redirected to the login page (cookies expire) then it logs in and goes back to scrape the previous page..", "links": [], "channel": "scrapy"},
{"date": "2014-08-22T11:18:55.013322+00:00", "nick": "stevezau", "message": "is it possible using scrapy? I assume i'd need to write a custom spider and possibly middleware?? can anyone point me in the right direction.. im trying to figure out the right design before i start writing code", "links": [], "channel": "scrapy"},
{"date": "2014-08-22T12:23:57.708856+00:00", "nick": "stevezau", "message": "i guess to detect the login page (and login) i should write a custom middleware that detect /login.. logs in sets the cookies then lets it continue..", "links": [], "channel": "scrapy"},
{"date": "2014-08-22T16:50:25.616972+00:00", "nick": "roundysquares", "message": "hi there! i have question regarding drop-down menus and selecting options from them with a spider. does anyone here have experience with that?", "links": [], "channel": "scrapy"},
{"date": "2014-08-22T20:33:45.688724+00:00", "nick": "yoh", "message": "Hi!  I wondered (just getting familiar with scrapy and Co.) -- why in slybot all the tests data etc uses 'http://http://' prefix -- just to signal that those are for tests only or there is some other special handling for double-protocoled URLs?", "links": [], "channel": "scrapy"},
{"date": "2014-08-22T21:06:05.257517+00:00", "nick": "yoh", "message": "brr -- so was slybot absorbed into portia? they seems have just merged it in (instead of e.g. submodule) and added few changes on top... I guess it was just a \"suboptimal\" move", "links": [], "channel": "scrapy"},
{"date": "2014-08-23T13:29:20.034869+00:00", "nick": "stevezau", "message": "hey guys, any idea how i can run a spider within django?? i've read a whole bunch of posts on it but nothing has worked.. i cant get around the \"exceptions.ValueError: signal only works in main thread\" problem", "links": [], "channel": "scrapy"},
{"date": "2014-08-24T20:06:24.767100+00:00", "nick": "js__", "message": "hi guys, I am very new at this and have been trying to get my head around my first selector", "links": [], "channel": "scrapy"},
{"date": "2014-08-24T20:06:28.806646+00:00", "nick": "js__", "message": "can somebody help?", "links": [], "channel": "scrapy"},
{"date": "2014-08-24T20:07:04.125059+00:00", "nick": "js__", "message": "i am trying to extract data from page http://groceries.asda.com/asda-webstore/landing...", "links": ["http://groceries.asda.com/asda-webstore/landing/home.shtml?cmpid=ahc-_-ghs-d1-_-asdacom-dsk-_-hp#/shelf/1215337195041/1/so_false"], "channel": "scrapy"},
{"date": "2014-08-24T20:07:45.800376+00:00", "nick": "js__", "message": "all the info under div class = listing clearfix shelfListing", "links": [], "channel": "scrapy"},
{"date": "2014-08-24T20:08:18.798624+00:00", "nick": "js__", "message": "but i cant seem to figur out how to format response.xpath()", "links": [], "channel": "scrapy"},
{"date": "2014-08-24T20:09:11.640286+00:00", "nick": "js__", "message": "some starting up helpwould be very much appreaciated fromyou experts :)", "links": [], "channel": "scrapy"},
{"date": "2014-08-24T21:25:01.783790+00:00", "nick": "js__", "message": "hi anybody able to answer a simple question?", "links": [], "channel": "scrapy"},
{"date": "2014-08-24T23:44:22.495826+00:00", "nick": "pig_raft", "message": "js__ couldn", "links": [], "channel": "scrapy"},
{"date": "2014-08-24T23:44:43.365820+00:00", "nick": "pig_raft", "message": "couldn't find that specific div class in the source", "links": [], "channel": "scrapy"},
{"date": "2014-08-25T01:52:44.587348+00:00", "nick": "pause", "message": "Hello", "links": [], "channel": "scrapy"},
{"date": "2014-08-25T01:52:55.692311+00:00", "nick": "pause", "message": "Anyone knows how to call a spider rule from another ?", "links": [], "channel": "scrapy"},
{"date": "2014-08-25T01:53:07.409992+00:00", "nick": "pause", "message": "I'm trying to call directly (self.method) but the method isnt called", "links": [], "channel": "scrapy"},
{"date": "2014-08-25T01:53:11.616566+00:00", "nick": "pause", "message": "Very strange", "links": [], "channel": "scrapy"},
{"date": "2014-08-25T07:45:13.899734+00:00", "nick": "flyingtriangle", "message": "anyone in here? I cannot for the life of me figure out how to monkeypatch the Request clas", "links": [], "channel": "scrapy"},
{"date": "2014-08-25T07:45:39.682404+00:00", "nick": "flyingtriangle", "message": "I've imported the entire http/ folder into my project dir but it seems scrapy.__init__ itself is importing Request", "links": [], "channel": "scrapy"},
{"date": "2014-08-25T07:45:57.115343+00:00", "nick": "flyingtriangle", "message": "I just need to change one line in the class", "links": [], "channel": "scrapy"},
{"date": "2014-08-25T07:46:06.129480+00:00", "nick": "flyingtriangle", "message": "so it won't URL encode certain requests", "links": [], "channel": "scrapy"},
{"date": "2014-08-25T07:46:11.201283+00:00", "nick": "flyingtriangle", "message": "certain request urls*", "links": [], "channel": "scrapy"},
{"date": "2014-08-25T19:30:01.744271+00:00", "nick": "yoLo_", "message": "how do i use the Downloader Middleware ?", "links": [], "channel": "scrapy"},
{"date": "2014-08-25T19:30:10.871812+00:00", "nick": "yoLo_", "message": "nevermind", "links": [], "channel": "scrapy"},
{"date": "2014-08-26T02:42:25.235506+00:00", "nick": "Rhomas", "message": "Hi!", "links": [], "channel": "scrapy"},
{"date": "2014-08-26T02:42:43.479881+00:00", "nick": "Rhomas", "message": "Can anyone point me to a good tutorial to follow after the one in the official docs?", "links": [], "channel": "scrapy"},
{"date": "2014-08-26T06:40:04.719789+00:00", "nick": "Rhomas", "message": "hi! when does this channel get the most active?", "links": [], "channel": "scrapy"},
{"date": "2014-08-26T09:19:26.269911+00:00", "nick": "rtew2", "message": "hmm i'm having a problem with spider_closed signal in scrapy and i can't figure out what's wrong with it", "links": [], "channel": "scrapy"},
{"date": "2014-08-26T09:20:04.914631+00:00", "nick": "rtew2", "message": "i'm running the \"Run Scrapy from a script\" code in http://doc.scrapy.org/en/latest/topics/practice...", "links": ["http://doc.scrapy.org/en/latest/topics/practices.html"], "channel": "scrapy"},
{"date": "2014-08-26T09:20:20.775027+00:00", "nick": "rtew2", "message": "except that I changed reactor.stop to my own function that will print out \"CALLED\"", "links": [], "channel": "scrapy"},
{"date": "2014-08-26T09:21:03.222211+00:00", "nick": "rtew2", "message": "but by some reason it is never called even though I can see in the log data, INFO: Closing spider (finished)", "links": [], "channel": "scrapy"},
{"date": "2014-08-26T09:21:08.980951+00:00", "nick": "rtew2", "message": "running version 0.24.2", "links": [], "channel": "scrapy"},
{"date": "2014-08-26T09:22:27.187340+00:00", "nick": "rtew2", "message": "i'm also running my custom spider but the spider doesn't have to return any special value, since the INFO is printing out that the spider is closing so it has already been taken care of, right?", "links": [], "channel": "scrapy"},
{"date": "2014-08-26T09:39:39.945289+00:00", "nick": "rtew2", "message": "gma", "links": [], "channel": "scrapy"},
{"date": "2014-08-26T10:55:28.562126+00:00", "nick": "nika_", "message": "H", "links": [], "channel": "scrapy"},
{"date": "2014-08-26T10:55:30.357303+00:00", "nick": "nika_", "message": "i", "links": [], "channel": "scrapy"},
{"date": "2014-08-26T10:55:59.497105+00:00", "nick": "nika_", "message": "Help!!!! is it possible that the crawler request page with POST method with header params?", "links": [], "channel": "scrapy"},
{"date": "2014-08-26T12:01:49.187624+00:00", "nick": "Roux_taff", "message": "hey there", "links": [], "channel": "scrapy"},
{"date": "2014-08-26T12:02:01.589433+00:00", "nick": "Roux_taff", "message": "anybody knows what the \"partial\" flag means in Response objects?", "links": [], "channel": "scrapy"},
{"date": "2014-08-26T12:02:36.680170+00:00", "nick": "Roux_taff", "message": "when trying to get this page http://www.freewebs.com/amigosdelviento/ scrapy keeps getting a normal Response instead of a Html one for some mysterious reason", "links": ["http://www.freewebs.com/amigosdelviento/"], "channel": "scrapy"},
{"date": "2014-08-26T12:02:47.532033+00:00", "nick": "Roux_taff", "message": "the only weird thing I can find is this \"partial\" flag in Response.flags", "links": [], "channel": "scrapy"},
{"date": "2014-08-26T12:03:18.216183+00:00", "nick": "Roux_taff", "message": "apparentyly there's  an image encoded in base64 embedeed in the webpage, maybe that's why scrapy thinks it's not a webpage???", "links": [], "channel": "scrapy"},
{"date": "2014-08-26T20:38:04.651131+00:00", "nick": "mechies", "message": "hii", "links": [], "channel": "scrapy"},
{"date": "2014-08-26T20:41:40.385495+00:00", "nick": "nramirezuy", "message": "hello", "links": [], "channel": "scrapy"},
{"date": "2014-08-26T21:18:54.375366+00:00", "nick": "x3000", "message": "exit", "links": [], "channel": "scrapy"},
{"date": "2014-08-26T22:09:52.226951+00:00", "nick": "dk92", "message": "Hi!", "links": [], "channel": "scrapy"},
{"date": "2014-08-26T22:10:35.488979+00:00", "nick": "dk92", "message": "I have a dns/ip scrapy question please if someone can help", "links": [], "channel": "scrapy"},
{"date": "2014-08-27T04:24:45.115892+00:00", "nick": "banana_", "message": "how do i send a POST request when my POST data has no key value pairs, just one long value?", "links": [], "channel": "scrapy"},
{"date": "2014-08-27T13:31:17.804637+00:00", "nick": "Ixio", "message": "Hello!", "links": [], "channel": "scrapy"},
{"date": "2014-08-27T13:31:39.269719+00:00", "nick": "nramirezuy", "message": "hello", "links": [], "channel": "scrapy"},
{"date": "2014-08-27T13:33:03.913267+00:00", "nick": "Ixio", "message": "I have a spider that fills a form on a page A gets redirected to B and then sometimes redirect a second time to C when all I really want is B. I tried setting REDIRECT_MAX_TIMES to 1 but then it just discards page B. Any ideas on how I can simply solve this?", "links": [], "channel": "scrapy"},
{"date": "2014-08-27T13:33:26.102734+00:00", "nick": "Ixio", "message": "I'd rather not code my own custom redirect middleware.", "links": [], "channel": "scrapy"},
{"date": "2014-08-27T13:34:30.633973+00:00", "nick": "nramirezuy", "message": "so A -> B -> C and you want B ?", "links": [], "channel": "scrapy"},
{"date": "2014-08-27T13:34:50.477679+00:00", "nick": "nramirezuy", "message": "but if B is a redirect is probably empty", "links": [], "channel": "scrapy"},
{"date": "2014-08-27T13:35:19.191242+00:00", "nick": "Ixio", "message": "It's not empty, for reasons unknown it only redirects B to C half the time, the other half I get the info I need from B", "links": [], "channel": "scrapy"},
{"date": "2014-08-27T13:35:57.419387+00:00", "nick": "Ixio", "message": "So even if the B trying to redirect to C is empty the url is of interest to me, I can re-open it later and get the info I need", "links": [], "channel": "scrapy"},
{"date": "2014-08-27T13:36:16.346077+00:00", "nick": "nramirezuy", "message": "set the request A with meta dont_redirect = True", "links": [], "channel": "scrapy"},
{"date": "2014-08-27T13:37:14.797841+00:00", "nick": "Ixio", "message": "Ok I should've tried that earlier but I thought it might stop the redirection from A to B (let's see)", "links": [], "channel": "scrapy"},
{"date": "2014-08-27T13:37:35.181867+00:00", "nick": "nramirezuy", "message": "you also want to set handle_httpstatus_list = [302,303] or whatever is the status code", "links": [], "channel": "scrapy"},
{"date": "2014-08-27T13:38:01.180819+00:00", "nick": "nramirezuy", "message": "if you are filling a form A goes to B without redirection", "links": [], "channel": "scrapy"},
{"date": "2014-08-27T13:44:25.118038+00:00", "nick": "Ixio", "message": "I get: exceptions.ValueError: No <form> element found in <302 PAGE A>", "links": [], "channel": "scrapy"},
{"date": "2014-08-27T13:44:43.847561+00:00", "nick": "Ixio", "message": "Does FormRequest.from_response also take in meta arguments?", "links": [], "channel": "scrapy"},
{"date": "2014-08-27T13:45:53.912841+00:00", "nick": "Ixio", "message": "I just added meta={'dont_redirect': True,'handle_httpstatus_list': [302,303]}, and that makes it fail", "links": [], "channel": "scrapy"},
{"date": "2014-08-27T13:46:37.726854+00:00", "nick": "nramirezuy", "message": "A has the form?", "links": [], "channel": "scrapy"},
{"date": "2014-08-27T13:47:49.508799+00:00", "nick": "Ixio", "message": "Yes, I think the form targets A and only after it redirects to B", "links": [], "channel": "scrapy"},
{"date": "2014-08-27T13:48:30.829364+00:00", "nick": "Ixio", "message": "so A=POST=>A=302=>B=302=>C", "links": [], "channel": "scrapy"},
{"date": "2014-08-27T13:50:44.582288+00:00", "nick": "nramirezuy", "message": "then you want a different callback for the to handle 302 A and get the B url", "links": [], "channel": "scrapy"},
{"date": "2014-08-27T13:54:00.958051+00:00", "nick": "Ixio", "message": "That sounds good though I'm not sure what you mean exactly, my callback only gets the end results from the redirections so either B or C depending if I'm lucky or not", "links": [], "channel": "scrapy"},
{"date": "2014-08-27T13:54:23.661659+00:00", "nick": "Ixio", "message": "If I could just record the url redirects then I could get back to B later on but...", "links": [], "channel": "scrapy"},
{"date": "2014-08-27T13:54:57.673643+00:00", "nick": "nramirezuy", "message": "the url to B is on 302 A response", "links": [], "channel": "scrapy"},
{"date": "2014-08-27T13:56:26.075581+00:00", "nick": "Ixio", "message": "Yes but I think the redirect middleware takes care of that so I don't have any direct access to it, could I catch a signal or something?", "links": [], "channel": "scrapy"},
{"date": "2014-08-27T13:57:03.542445+00:00", "nick": "nramirezuy", "message": "but that what the meta does", "links": [], "channel": "scrapy"},
{"date": "2014-08-27T13:57:26.724440+00:00", "nick": "nramirezuy", "message": "dont_redirect = middleware don't make the redirection automatically for that request", "links": [], "channel": "scrapy"},
{"date": "2014-08-27T13:58:05.111662+00:00", "nick": "nramirezuy", "message": "handle_httpstatus_list = spider will handle that response if match the status", "links": [], "channel": "scrapy"},
{"date": "2014-08-27T13:59:49.622551+00:00", "nick": "Ixio", "message": "Huh so that's good only I get the \"exceptions.ValueError: No <form> element found in <302 PAGE A>\" error", "links": [], "channel": "scrapy"},
{"date": "2014-08-27T14:01:03.122229+00:00", "nick": "nramirezuy", "message": "because you are looking for a form on a 302 response, thats why you need a different callback on the POST", "links": [], "channel": "scrapy"},
{"date": "2014-08-27T14:05:43.640788+00:00", "nick": "Ixio", "message": "Yeah I hadn't understood that callback part, thanks for being patient. So I made a new callback that just prints the URL but it prints A, so I guess the 'dont_redirect' did stop A from redirecting to B after processing the post.", "links": [], "channel": "scrapy"},
{"date": "2014-08-27T14:06:17.939763+00:00", "nick": "nramirezuy", "message": "the url to B might be on the headers", "links": [], "channel": "scrapy"},
{"date": "2014-08-27T14:06:34.024749+00:00", "nick": "Ixio", "message": "oh right, let me check that", "links": [], "channel": "scrapy"},
{"date": "2014-08-27T14:07:54.235910+00:00", "nick": "nramirezuy", "message": "then you can do the request to B with the metas but checking that has the right status code 302 = retry, 200 = parse the results", "links": [], "channel": "scrapy"},
{"date": "2014-08-27T14:08:25.082452+00:00", "nick": "Ixio", "message": "Great! 'Location' in header does give me B, thanks a lot for the good help ;)", "links": [], "channel": "scrapy"},
{"date": "2014-08-27T14:08:38.849075+00:00", "nick": "nramirezuy", "message": "np", "links": [], "channel": "scrapy"},
{"date": "2014-08-28T11:21:08.930174+00:00", "nick": "fahrr", "message": "hello, is there someone, who can help me with scraping data from api and saving/processing it into database/csv?", "links": [], "channel": "scrapy"},
{"date": "2014-08-28T22:54:40.610047+00:00", "nick": "trist", "message": "Hey, I'm trying to set up a spider with scrapy and djangoitem. But I keep getting import errors when importing my django model. Code and Traceback here: http://pastebin.com/HcsLKPHw Any help/suggestions appreciated.", "links": ["http://pastebin.com/HcsLKPHw"], "channel": "scrapy"},
{"date": "2014-08-29T13:08:00.051270+00:00", "nick": "Roux_taff", "message": "hey there", "links": [], "channel": "scrapy"},
{"date": "2014-08-29T13:08:21.004622+00:00", "nick": "Roux_taff", "message": "any idea how to always run some piece of code when a spider job is canceled ?", "links": [], "channel": "scrapy"},
{"date": "2014-08-29T13:08:36.659829+00:00", "nick": "Roux_taff", "message": "I have a close() function in my code that runs when a spider job is over", "links": [], "channel": "scrapy"},
{"date": "2014-08-29T13:08:50.196245+00:00", "nick": "Roux_taff", "message": "but apparently it does not when a job is canceled :(", "links": [], "channel": "scrapy"},
{"date": "2014-08-29T13:09:53.284895+00:00", "nick": "Roux_taff", "message": "using scrapy.signals.spider_closed maybe ?", "links": [], "channel": "scrapy"},
{"date": "2014-08-29T13:11:40.562711+00:00", "nick": "traverserdaLapto", "message": "So does djangoItems atually work? I run the scrapy shell, and I can import the python settings module. But I try to import an actual model, and it says that it can't import the settings.", "links": [], "channel": "scrapy"},
{"date": "2014-08-29T13:11:45.741006+00:00", "nick": "traverserdaLapto", "message": "It is a mystery.", "links": [], "channel": "scrapy"},
{"date": "2014-08-29T13:13:37.225187+00:00", "nick": "traverserdaLapto", "message": "and no one cares :(. I will file a bug report.", "links": [], "channel": "scrapy"},
{"date": "2014-08-29T13:33:59.998396+00:00", "nick": "nramirezuy", "message": "@Roux_taff you should use the spider_closed signal", "links": [], "channel": "scrapy"},
{"date": "2014-08-29T13:34:23.657448+00:00", "nick": "Roux_taff", "message": "yep I'm trying with this now, thx nramirezuy", "links": [], "channel": "scrapy"},
{"date": "2014-08-29T13:39:51.594165+00:00", "nick": "nramirezuy", "message": "@traverserdaLapto which settgins module are you missing?", "links": [], "channel": "scrapy"},
{"date": "2014-08-29T13:44:41.832633+00:00", "nick": "traverserdaLapto", "message": "The django settings module. http://pastebin.com/Fr4isYFG", "links": ["http://pastebin.com/Fr4isYFG"], "channel": "scrapy"},
{"date": "2014-08-29T13:45:41.948394+00:00", "nick": "traverserdaLapto", "message": "I can import RiSettings from the scrapy shell", "links": [], "channel": "scrapy"},
{"date": "2014-08-29T13:51:50.088795+00:00", "nick": "traverserdaLapto", "message": "trist, is uploading some sample code to github", "links": [], "channel": "scrapy"},
{"date": "2014-08-29T13:54:52.879353+00:00", "nick": "nramirezuy", "message": "I never used django item but I can tell it will slow down the spider", "links": [], "channel": "scrapy"},
{"date": "2014-08-29T13:55:44.425369+00:00", "nick": "nramirezuy", "message": "you usually yield items faster than you can save them one by one", "links": [], "channel": "scrapy"},
{"date": "2014-08-29T13:55:48.466351+00:00", "nick": "nramirezuy", "message": "on Mysql", "links": [], "channel": "scrapy"},
{"date": "2014-08-29T13:58:39.881630+00:00", "nick": "traverserdaLapto", "message": "We are okay wiht that", "links": [], "channel": "scrapy"},
{"date": "2014-08-29T13:59:44.282032+00:00", "nick": "trist", "message": "Here's my broken code: https://github.com/TristanTrim/DjangoItemImport... though we may be better forgetting the djangoitem thing.", "links": ["https://github.com/TristanTrim/DjangoItemImportError"], "channel": "scrapy"},
{"date": "2014-08-29T14:20:23.785258+00:00", "nick": "nyov", "message": "trist: I don't really get the layout of that github repo. but maybe this little reference helps: https://github.com/nyov/scrapyext/blob/master/e...", "links": ["https://github.com/nyov/scrapyext/blob/master/extras/django_orm.py"], "channel": "scrapy"},
{"date": "2014-08-29T14:21:56.633797+00:00", "nick": "nyov", "message": "your settings seem a bit mixed up. the django settings in the scrapy project or something like that", "links": [], "channel": "scrapy"},
{"date": "2014-08-29T14:59:26.354127+00:00", "nick": "traverserdaLapto", "message": "nyov, thanks. Didn't know the django_orm thing existed", "links": [], "channel": "scrapy"},
{"date": "2014-08-29T15:02:06.605466+00:00", "nick": "nyov", "message": "traverserdaLapto: the file basically only mentions what to put in your files (pipelines, settings). can't use that as-is.", "links": [], "channel": "scrapy"},
{"date": "2014-08-29T15:04:50.707534+00:00", "nick": "nyov", "message": "and it's been a while since I made scrapy+django work for purely testing reasons. but the only issue I remember having, was getting the path to the django project right", "links": [], "channel": "scrapy"},
{"date": "2014-08-29T15:05:13.263361+00:00", "nick": "nyov", "message": "and all the config stuff on django's side. haha", "links": [], "channel": "scrapy"},
{"date": "2014-08-29T15:06:27.257436+00:00", "nick": "trist", "message": "I'm getting `cannot import name setup_environ` from the bots settings.py now.", "links": [], "channel": "scrapy"},
{"date": "2014-08-29T15:09:31.172745+00:00", "nick": "nyov", "message": "oh, maybe django moved on. that code snippet is from around 2012 after all. sorry.", "links": [], "channel": "scrapy"},
{"date": "2014-08-29T15:10:57.023909+00:00", "nick": "nyov", "message": "maybe django docs mention what happened to setup_environ, if it's not in django.core.management anymore", "links": [], "channel": "scrapy"},
{"date": "2014-08-29T15:12:54.601782+00:00", "nick": "nyov", "message": "(and if you find a fix, let me know so I can update that)", "links": [], "channel": "scrapy"},
{"date": "2014-08-29T15:13:51.364159+00:00", "nick": "traverserdaLapto", "message": "nyov, django wants you to run something like \"os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"fooproject.settings\")\"", "links": [], "channel": "scrapy"},
{"date": "2014-08-29T15:13:57.180818+00:00", "nick": "traverserdaLapto", "message": "We are doing that, but it breaks", "links": [], "channel": "scrapy"},
{"date": "2014-08-29T15:14:11.221610+00:00", "nick": "traverserdaLapto", "message": "but yeah. Setup_environ is deprecated", "links": [], "channel": "scrapy"},
{"date": "2014-08-29T17:52:21.698378+00:00", "nick": "imachuchu", "message": "so I'm having a problem (which might be pretty common): in my spider I can get the body of the response just fine (\"response.body\") but any selector I use returns nothing (say like \"response.css('.title').extract()\" where I know there are multiple tags with the title class). What am I doing wrong?", "links": [], "channel": "scrapy"},
{"date": "2014-08-29T17:54:46.837351+00:00", "nick": "nyov", "message": "imachuchu: your scrapy version?", "links": [], "channel": "scrapy"},
{"date": "2014-08-29T17:56:15.520914+00:00", "nick": "imachuchu", "message": "nyov: 0.24.4", "links": [], "channel": "scrapy"},
{"date": "2014-08-29T17:57:53.649722+00:00", "nick": "nyov", "message": "okay, maybe your selectors are off. are you actually looking for a css class named title?", "links": [], "channel": "scrapy"},
{"date": "2014-08-29T17:58:16.981609+00:00", "nick": "nyov", "message": "a title tag would be css('title')", "links": [], "channel": "scrapy"},
{"date": "2014-08-29T18:00:10.883563+00:00", "nick": "imachuchu", "message": "nyov: I'm looking for the text encapsulated in the tag eventually", "links": [], "channel": "scrapy"},
{"date": "2014-08-29T18:00:59.306331+00:00", "nick": "nyov", "message": "then you could do response.css('title').xpath('node()').extract()", "links": [], "channel": "scrapy"},
{"date": "2014-08-29T18:01:41.686277+00:00", "nick": "imachuchu", "message": "nyov: ok, let me give it a try", "links": [], "channel": "scrapy"},
{"date": "2014-08-29T18:02:38.301303+00:00", "nick": "nyov", "message": "or use xpath all the way. don't know css selectors that well", "links": [], "channel": "scrapy"},
{"date": "2014-08-29T18:03:42.857608+00:00", "nick": "nyov", "message": "oops, should be text(), not node()", "links": [], "channel": "scrapy"},
{"date": "2014-08-29T18:03:47.809982+00:00", "nick": "nyov", "message": "response.xpath('//title/text()').extract()", "links": [], "channel": "scrapy"},
{"date": "2014-08-29T18:05:54.722252+00:00", "nick": "imachuchu", "message": "nyov: still nothing. Would it matter that I'm trying to just log the results instead of passing them into a scrapy item?", "links": [], "channel": "scrapy"},
{"date": "2014-08-29T18:09:41.187896+00:00", "nick": "imachuchu", "message": "nyov: oh, and it's not a title tag, it's a title class", "links": [], "channel": "scrapy"},
{"date": "2014-08-29T18:11:54.153257+00:00", "nick": "nyov", "message": "imachuchu: are you testing in the scrapy shell? what you do with the results should not matter of course", "links": [], "channel": "scrapy"},
{"date": "2014-08-29T18:14:25.412873+00:00", "nick": "nyov", "message": "$ scrapy shell 'http://google.com'", "links": [], "channel": "scrapy"},
{"date": "2014-08-29T18:14:26.388528+00:00", "nick": "nyov", "message": "In [1]: response.xpath('//title/text()').extract()", "links": [], "channel": "scrapy"},
{"date": "2014-08-29T18:14:26.519661+00:00", "nick": "nyov", "message": "Out[1]: [u'Google']", "links": [], "channel": "scrapy"},
{"date": "2014-08-29T18:14:42.389101+00:00", "nick": "nyov", "message": "if you get that, then things work ;)", "links": [], "channel": "scrapy"},
{"date": "2014-08-29T18:17:21.785462+00:00", "nick": "imachuchu", "message": "nyov: ok, let me give it a try", "links": [], "channel": "scrapy"},
{"date": "2014-08-29T18:21:45.132917+00:00", "nick": "imachuchu", "message": "well that worked, let me see if I can try to run my spider code (more or less) in the shell", "links": [], "channel": "scrapy"},
{"date": "2014-08-29T19:15:24.945764+00:00", "nick": "traverserdaLapto", "message": "nyov, It was a strange strange bug. An import in the config file was failing. Problem with django-adminactions. Did eventually get it sorted out.", "links": [], "channel": "scrapy"},
{"date": "2014-08-29T19:15:28.433746+00:00", "nick": "traverserdaLapto", "message": "Thanks for your help", "links": [], "channel": "scrapy"},
{"date": "2014-08-29T19:38:26.501441+00:00", "nick": "nyov", "message": "traverserdaLapto: great stuff :)", "links": [], "channel": "scrapy"},
{"date": "2014-08-30T10:43:24.738395+00:00", "nick": "bwe", "message": "Hi, I have built a working scrapy spider. Now, I want melt all files into one script.py file which I just want to execute on the command line afterwards. How can I do it?", "links": [], "channel": "scrapy"},
{"date": "2014-08-30T12:45:16.300996+00:00", "nick": "madvas_", "message": "Hey guys, I have a liitle question, when I run multiple spiders in the same process as in docs in \"Common Practices\", is there a way how to throttle number of spiders running at once? Thank you for any help", "links": [], "channel": "scrapy"},
{"date": "2014-08-30T13:36:48.297887+00:00", "nick": "nyov", "message": "madvas_: good question. so far concurrency was always asked in terms of requests. usually you're scheduling spiders as you need. in short, I don't know (doesn't seem to be a setting like that)", "links": [], "channel": "scrapy"},
{"date": "2014-08-30T13:38:37.999278+00:00", "nick": "nyov", "message": "madvas_: but since in the example code, you're doing the setup yourself (calling setup_crawler), I believe it would be your responsibility to ensure only two running instances of Crawler", "links": [], "channel": "scrapy"},
{"date": "2014-08-30T13:38:55.660065+00:00", "nick": "nyov", "message": "(or as many as you want concurrently)", "links": [], "channel": "scrapy"},
{"date": "2014-08-30T13:39:57.326345+00:00", "nick": "nyov", "message": "madvas_: but feel free to open an issue on github and get a response from a few more devs. I'd be interested in it myself", "links": [], "channel": "scrapy"},
{"date": "2014-08-30T13:42:43.985724+00:00", "nick": "madvas_", "message": "yes, thank you, spider I'm doing is creating few hundreds and logging didn't even started :D But I coded some workaround by myself and it's ok now. .. Yes, I'll try to post it, it would be good feature :)", "links": [], "channel": "scrapy"},
{"date": "2014-08-30T13:46:28.394278+00:00", "nick": "nyov", "message": "if you're instancing hundreds of spiders, I'd say you're doing it wrong. you can't have that many different spiders?", "links": [], "channel": "scrapy"},
{"date": "2014-08-30T13:47:18.621034+00:00", "nick": "nyov", "message": "in that case I'd rather write the spider to handle the concurrency and branching out into multiple domains", "links": [], "channel": "scrapy"},
{"date": "2014-08-30T13:51:02.111170+00:00", "nick": "nyov", "message": "but yes, please post your question and lets see how that goes", "links": [], "channel": "scrapy"},
{"date": "2014-08-30T13:51:11.877228+00:00", "nick": "nyov", "message": "and i'm out. bye", "links": [], "channel": "scrapy"},
{"date": "2014-08-30T19:49:32.839369+00:00", "nick": "madvas_", "message": "Hey guys, Has anybody experience with distributing scrapy spider to a Windows machine with no python installed. I mean I've got this client, who wants to be able to run it on his win machine, so I'd like to simplify it for him. I tried py2exe, pyinstaller, cx_freeze, portablepython you name it. No success with scrapy. Does anybody have experience with this?", "links": [], "channel": "scrapy"},
{"date": "2014-08-31T09:28:29.352349+00:00", "nick": "gcfhvjbkn", "message": "hello guys", "links": [], "channel": "scrapy"},
{"date": "2014-08-31T09:28:42.868870+00:00", "nick": "gcfhvjbkn", "message": "so i have run into a problem with w3lib and scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-08-31T09:28:58.742579+00:00", "nick": "gcfhvjbkn", "message": "for some reason here", "links": [], "channel": "scrapy"},
{"date": "2014-08-31T09:28:59.628269+00:00", "nick": "gcfhvjbkn", "message": "https://github.com/scrapy/w3lib/blob/master/w3l...", "links": ["https://github.com/scrapy/w3lib/blob/master/w3lib/http.py#L65"], "channel": "scrapy"},
{"date": "2014-08-31T09:29:53.892589+00:00", "nick": "gcfhvjbkn", "message": "there's no type check for members of that sequence, so if an integer was anywhere in that sequence, the w3lib would fail with an error", "links": [], "channel": "scrapy"},
{"date": "2014-08-31T09:30:21.646010+00:00", "nick": "gcfhvjbkn", "message": "which it does when scrapy passes an integer for Content-Type header (like so: [386])", "links": [], "channel": "scrapy"},
{"date": "2014-08-31T09:33:00.232723+00:00", "nick": "gcfhvjbkn", "message": "so is it scrapy which is at fault for not coercing integer to bytes at some point?", "links": [], "channel": "scrapy"},
{"date": "2014-08-31T09:33:56.361343+00:00", "nick": "gcfhvjbkn", "message": "i'm sorry, not Conent-Type, but Content-Length", "links": [], "channel": "scrapy"},
{"date": "2014-08-31T10:22:51.847227+00:00", "nick": "gcfhvjbkn", "message": "btw i'm noticing this problem has gone away in master", "links": [], "channel": "scrapy"},
{"date": "2014-08-31T10:23:00.783617+00:00", "nick": "gcfhvjbkn", "message": "when is the next release due?", "links": [], "channel": "scrapy"},
{"date": "2014-08-31T19:52:06.678566+00:00", "nick": "Blonga", "message": "hello", "links": [], "channel": "scrapy"},
{"date": "2014-08-31T20:38:54.007719+00:00", "nick": "EWB", "message": "hey, so I'm using the code on the official docs to run scrapy from a script, but I don't know how to get the list of found items out of the crawler. The code I'm looking at: http://doc.scrapy.org/en/latest/topics/practice...", "links": ["http://doc.scrapy.org/en/latest/topics/practices.html"], "channel": "scrapy"},
{"date": "2014-08-31T21:11:48.276406+00:00", "nick": "a1g", "message": "hello", "links": [], "channel": "scrapy"},
{"date": "2014-08-31T21:11:56.214992+00:00", "nick": "Blonga", "message": "heello", "links": [], "channel": "scrapy"},
{"date": "2014-08-31T21:12:34.804578+00:00", "nick": "a1g", "message": "I am working on a project and when i try to run it I keep getting error with zope.interface", "links": [], "channel": "scrapy"},
{"date": "2014-08-31T21:12:35.836857+00:00", "nick": "a1g", "message": "ImportError: Twisted requires zope.interface 3.6.0 or later: no module named zope.interface.", "links": [], "channel": "scrapy"},
{"date": "2014-08-31T21:12:58.909837+00:00", "nick": "Blonga", "message": "can't help you, scapy doesn't even install for me", "links": [], "channel": "scrapy"},
{"date": "2014-08-31T21:13:00.497133+00:00", "nick": "a1g", "message": "but when i go to python console I can import the package just fine", "links": [], "channel": "scrapy"},
{"date": "2014-08-31T21:13:07.976578+00:00", "nick": "Blonga", "message": "issue with Crytography package", "links": [], "channel": "scrapy"},
{"date": "2014-08-31T21:13:14.249431+00:00", "nick": "a1g", "message": "ok", "links": [], "channel": "scrapy"},
{"date": "2014-08-31T21:13:56.910888+00:00", "nick": "a1g", "message": "are you on ubuntu?", "links": [], "channel": "scrapy"},
{"date": "2014-08-31T21:14:13.632724+00:00", "nick": "Blonga", "message": "nope", "links": [], "channel": "scrapy"},
{"date": "2014-08-31T21:14:17.025794+00:00", "nick": "Blonga", "message": "windows 7", "links": [], "channel": "scrapy"},
{"date": "2014-08-31T21:14:47.021602+00:00", "nick": "a1g", "message": "lol, good luck, maybe you should run it in a vagrant box/virtualmachine", "links": [], "channel": "scrapy"},
{"date": "2014-08-31T21:14:59.528518+00:00", "nick": "Blonga", "message": "maybe", "links": [], "channel": "scrapy"},
{"date": "2014-08-31T21:15:07.925134+00:00", "nick": "a1g", "message": "thats what I'm doing", "links": [], "channel": "scrapy"},
{"date": "2014-08-31T21:15:09.669200+00:00", "nick": "Blonga", "message": "scrapy is supposed to work with windows without issue", "links": [], "channel": "scrapy"},
{"date": "2014-08-31T21:15:35.296338+00:00", "nick": "a1g", "message": "windows is not for true developers, In my opinion", "links": [], "channel": "scrapy"},
{"date": "2014-08-31T21:15:52.982898+00:00", "nick": "Blonga", "message": "yeah i'm just messing around", "links": [], "channel": "scrapy"},
{"date": "2014-08-31T21:15:59.074386+00:00", "nick": "Blonga", "message": "i plan on going to gentoo once i get a laptop", "links": [], "channel": "scrapy"},
{"date": "2014-08-31T21:16:02.828679+00:00", "nick": "Blonga", "message": "or arch", "links": [], "channel": "scrapy"},
{"date": "2014-08-31T21:16:12.645386+00:00", "nick": "a1g", "message": "yeah gentoo is the best", "links": [], "channel": "scrapy"},
{"date": "2014-08-31T21:16:19.316757+00:00", "nick": "a1g", "message": "even better than ubuntu", "links": [], "channel": "scrapy"},
{"date": "2014-09-01T08:29:06.526406+00:00", "nick": "rohitt", "message": "Hi all!", "links": [], "channel": "scrapy"},
{"date": "2014-09-01T08:29:25.108500+00:00", "nick": "rohitt", "message": "I was wondering if there is any limit on the length of start_urls list!", "links": [], "channel": "scrapy"},
{"date": "2014-09-01T08:30:20.107520+00:00", "nick": "nikolaosk", "message": "only python imposed ones", "links": [], "channel": "scrapy"},
{"date": "2014-09-01T08:31:18.753803+00:00", "nick": "rohitt", "message": "nikolaosk, thanks for the input! :)", "links": [], "channel": "scrapy"},
{"date": "2014-09-02T22:02:39.886845+00:00", "nick": "asifzuba", "message": "hey", "links": [], "channel": "scrapy"},
{"date": "2014-09-03T01:16:08.138300+00:00", "nick": "nyov", "message": "pablohof: would this project care for a more colorful github/travis irc notification bot?", "links": [], "channel": "scrapy"},
{"date": "2014-09-03T01:18:38.776134+00:00", "nick": "nyov", "message": "it's been slightly bugging me for a while now. the channel notices these standard services send aren't all that readable, IMO", "links": [], "channel": "scrapy"},
{"date": "2014-09-03T07:10:05.018905+00:00", "nick": "Logicgate", "message": "Hi!", "links": [], "channel": "scrapy"},
{"date": "2014-09-03T22:58:53.320438+00:00", "nick": "Jake232", "message": "Hey, How can I override settings on a per spider basis? (My use case is I want some spiders to be breadth first, but other to be depth first)", "links": [], "channel": "scrapy"},
{"date": "2014-09-04T08:16:57.656056+00:00", "nick": "dasher", "message": "Is anyone here?", "links": [], "channel": "scrapy"},
{"date": "2014-09-04T08:17:30.198976+00:00", "nick": "Jake232", "message": "Hey, How can I override settings on a per spider basis? (My use case is I want some spiders to be breadth first, but other to be depth first)", "links": [], "channel": "scrapy"},
{"date": "2014-09-04T08:18:50.770562+00:00", "nick": "dasher", "message": "I found that i can freely use scrapy to run my spiders, but when i use scrapyd to run my spiders, it didnt work. the job just hang up, and there is no log, just a sign that spider has started", "links": [], "channel": "scrapy"},
{"date": "2014-09-04T13:41:46.170920+00:00", "nick": "Jake232", "message": "Hey, How can I override settings on a per spider basis? (My use case is I want some spiders to be breadth first, but other to be depth first)", "links": [], "channel": "scrapy"},
{"date": "2014-09-04T13:47:22.608160+00:00", "nick": "nramirezuy", "message": "We merged something to solve this problem 2 days ago, it not super tested yet and may change. https://github.com/scrapy/scrapy/blob/master/sc...", "links": ["https://github.com/scrapy/scrapy/blob/master/scrapy/spider.py#L70"], "channel": "scrapy"},
{"date": "2014-09-06T05:17:57.358761+00:00", "nick": "wirehead", "message": "Hi, can someone please help with using scrapy to scrape video links from reddit", "links": [], "channel": "scrapy"},
{"date": "2014-09-06T05:18:28.885120+00:00", "nick": "wirehead", "message": "I don't really know where to start even", "links": [], "channel": "scrapy"},
{"date": "2014-09-06T12:22:57.995800+00:00", "nick": "nyov", "message": "wirehead: probably best to start with reading http://doc.scrapy.org/en/latest/", "links": ["http://doc.scrapy.org/en/latest/"], "channel": "scrapy"},
{"date": "2014-09-06T15:01:55.271920+00:00", "nick": "scrapyhire", "message": "hey guys whats up", "links": [], "channel": "scrapy"},
{"date": "2014-09-06T19:56:55.484329+00:00", "nick": "wirehead", "message": "ok, thanks i guess", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T04:13:23.036432+00:00", "nick": "snapsnail", "message": "I'm sure this is well known, however I seem to have a difference in my xpath query selection in Scrapy.", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T04:14:28.523887+00:00", "nick": "snapsnail", "message": "For example, if I try and scrape the http://scrapy.org/download", "links": ["http://scrapy.org/download"], "channel": "scrapy"},
{"date": "2014-09-07T04:15:44.311518+00:00", "nick": "snapsnail", "message": "and I want to scrape \"Scrapy on PyPI\".", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T04:16:31.740447+00:00", "nick": "snapsnail", "message": "The xpath query that works in firebug etc: \"//ul[2]/li[1]/a/text()\"", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T04:17:44.540648+00:00", "nick": "snapsnail", "message": "However when I try and select it in scrapy, I need to use: //ul[3]/li[1]/a/text()", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T04:20:55.978848+00:00", "nick": "snapsnail", "message": "It seems this has changed in a recent version, as a prior version worked fine.", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T04:21:13.197263+00:00", "nick": "snapsnail", "message": "I am currently using 0.24.4", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T05:08:09.831669+00:00", "nick": "nyov", "message": "snapsnail: can't reproduce. the first xpath works, same in firebug as in scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T05:09:04.063430+00:00", "nick": "nyov", "message": "possibly lxml's fault, what version do you have there?", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T05:11:09.924623+00:00", "nick": "nyov", "message": ">> from lxml import etree", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T05:11:12.895357+00:00", "nick": "nyov", "message": ">> etree.LXML_VERSION", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T05:12:40.661087+00:00", "nick": "nyov", "message": "(mine says: \"(3, 3, 1, 0)\" -> python-lxml 3.3.1)", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T05:13:33.394505+00:00", "nick": "nyov", "message": "using libxml 2.9.1", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T14:54:49.635194+00:00", "nick": "snapsnail", "message": "nyov: Hmm, interesting", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T14:54:51.413448+00:00", "nick": "snapsnail", "message": "In [5]: etree.LXML_VERSION", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T14:54:53.879017+00:00", "nick": "snapsnail", "message": "Out[5]: (3, 3, 6, 0)", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T15:40:13.101021+00:00", "nick": "snapsnail", "message": "nyov: Is there anything special that you need to do when installing scrapy on OSX?", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T15:58:34.230856+00:00", "nick": "nyov", "message": "snapsnail: not that I'm aware of. do you get anything returned in scrapy useing a \"0' as index? as in \"//ul[0]\"?", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T16:01:12.540127+00:00", "nick": "nyov", "message": "though I don't think lxml changed API in a minor version", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T16:02:17.055501+00:00", "nick": "snapsnail", "message": "In [1]: response.xpath('''//ul[0]/li[1]/a/text()''')", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T16:02:18.636430+00:00", "nick": "snapsnail", "message": "Out[1]: []", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T16:05:47.444723+00:00", "nick": "nyov", "message": "can you figure out what libxml2 version it's linked against? If it's not much different from mine I would infer it's a scrapy issue", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T16:05:47.801779+00:00", "nick": "snapsnail", "message": "In [4]: response.xpath('''//ul[0]''')", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T16:05:50.160937+00:00", "nick": "snapsnail", "message": "Out[4]: []", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T16:06:21.465662+00:00", "nick": "nyov", "message": "i don't know how, but I know mine is using '/usr/lib/x86_64-linux-gnu/libxml2.so.2.9.1' (on linux)", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T16:06:35.534637+00:00", "nick": "nyov", "message": "libxml 2.9.1", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T16:10:36.541477+00:00", "nick": "snapsnail", "message": "Hmm, not sure how to check that...", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T16:11:17.232459+00:00", "nick": "nyov", "message": "ah I found it in the sources...", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T16:11:23.528885+00:00", "nick": "nyov", "message": "from lxml import etree", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T16:11:25.558882+00:00", "nick": "nyov", "message": "etree.LIBXML_VERSION", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T16:12:41.925505+00:00", "nick": "snapsnail", "message": "In [2]: etree.LIBXML_VERSION", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T16:12:43.622503+00:00", "nick": "snapsnail", "message": "Out[2]: (2, 9, 0)", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T16:14:53.914395+00:00", "nick": "nyov", "message": "hm, okay. something strange then, I'm sure. I'll go search the net just in case there was a bugged lxml/libxml2 version", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T16:17:56.569961+00:00", "nick": "nyov", "message": "in any case, are you on github? then you could please open an issue about this at https://github.com/scrapy/scrapy/issues", "links": ["https://github.com/scrapy/scrapy/issues"], "channel": "scrapy"},
{"date": "2014-09-07T16:20:42.753257+00:00", "nick": "snapsnail", "message": "Yeah, I can do that", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T16:38:11.426010+00:00", "nick": "nyov", "message": "okay, could you test this code piece please?", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T16:38:18.967312+00:00", "nick": "nyov", "message": "import urllib2", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T16:38:19.084268+00:00", "nick": "nyov", "message": "import libxml2", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T16:38:19.084335+00:00", "nick": "nyov", "message": "page = urllib2.urlopen('http://scrapy.org/download...", "links": ["http://urllib2.urlopen('http://scrapy.org/download').read()"], "channel": "scrapy"},
{"date": "2014-09-07T16:38:19.085904+00:00", "nick": "nyov", "message": "doc = lxml.etree.HTML(page)", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T16:38:20.769027+00:00", "nick": "nyov", "message": "print doc.xpath(\"//ul[2]/li[1]/a/text()\")", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T16:38:38.525270+00:00", "nick": "nyov", "message": "# should return ['Scrapy on PyPI']", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T16:38:48.929267+00:00", "nick": "nyov", "message": "this is not using scrapy at all", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T16:39:00.224157+00:00", "nick": "snapsnail", "message": "K, one sec", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T16:39:42.040977+00:00", "nick": "nyov", "message": "oops", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T16:39:49.865663+00:00", "nick": "snapsnail", "message": "no libxml2", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T16:40:02.260071+00:00", "nick": "nyov", "message": "import lxml", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T16:40:06.641409+00:00", "nick": "nyov", "message": "my fault", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T16:40:14.538385+00:00", "nick": "snapsnail", "message": "yeah, I was going to say that... :-)", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T16:40:17.133975+00:00", "nick": "snapsnail", "message": "No prob", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T16:40:57.256191+00:00", "nick": "snapsnail", "message": "AttributeError: 'module' object has no attribute 'etree'", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T16:41:37.189407+00:00", "nick": "nyov", "message": "really?", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T16:41:42.939000+00:00", "nick": "snapsnail", "message": "happens on doc = lxml.etree.HTML(page)", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T16:42:43.818338+00:00", "nick": "snapsnail", "message": "I switched it around", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T16:42:50.391481+00:00", "nick": "nyov", "message": "oh again, my fault. strange what did I import", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T16:42:56.405607+00:00", "nick": "snapsnail", "message": "from lxml import etree", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T16:43:06.490030+00:00", "nick": "snapsnail", "message": "doc = etree.HTML(page)", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T16:43:17.928207+00:00", "nick": "nyov", "message": "aha", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T16:43:43.366477+00:00", "nick": "snapsnail", "message": "In [11]: print doc.xpath(\"//ul[2]/li[1]/a/text()\")", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T16:43:45.634449+00:00", "nick": "snapsnail", "message": "['Release Notes']", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T16:43:53.142403+00:00", "nick": "nyov", "message": "huh", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T16:44:58.398988+00:00", "nick": "nyov", "message": "well, looks like a lxml or libxml2 bug", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T16:45:57.831654+00:00", "nick": "nyov", "message": "maybe try it on libxml2 directly, though I'd have to look if it has a html parser too. it probably barfs on html", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T16:49:47.840287+00:00", "nick": "nyov", "message": "yeah, libxml2.htmlParseDoc(page, 'utf8') fails at '&'", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T16:51:04.749684+00:00", "nick": "nyov", "message": "I'm too lazy to try and fix the html for libxml now", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T16:51:41.328639+00:00", "nick": "nyov", "message": "snapsnail: easiest solution, try a newer libxml2 or lxml build", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T16:53:36.137370+00:00", "nick": "snapsnail", "message": "Is there newer for python 2?", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T16:57:41.240280+00:00", "nick": "nyov", "message": "well you said yours was 2.9.0, maybe 2.9.1 is the fix", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T16:57:56.607749+00:00", "nick": "nyov", "message": "otherwise it might be a bug in the newer lxml version you have", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T16:59:29.552342+00:00", "nick": "nyov", "message": "i'm looking through libxml bugtracker right now. but bugzilla is so ugly", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T17:02:10.764988+00:00", "nick": "nyov", "message": "snapsnail: yeah looks like this is your bug: https://bugzilla.gnome.org/show_bug.cgi?id=695699", "links": ["https://bugzilla.gnome.org/show_bug.cgi?id=695699"], "channel": "scrapy"},
{"date": "2014-09-07T17:02:36.157417+00:00", "nick": "nyov", "message": "or maybe not. dang", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T17:03:59.642165+00:00", "nick": "nyov", "message": "or it was. https://git.gnome.org/browse/libxml2/commit/?id...", "links": ["https://git.gnome.org/browse/libxml2/commit/?id=b4bcba23f64b71105514875f165a63d4cc720609"], "channel": "scrapy"},
{"date": "2014-09-07T17:04:37.770065+00:00", "nick": "nyov", "message": "according to the patch, if you don't use '//' as predicate, but the full path, you should get the right result?", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T17:33:42.026790+00:00", "nick": "snapsnail", "message": "Hmm, I haven't tried using the full path, I avoid doing that for fragility reasons", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T18:38:20.370985+00:00", "nick": "brotherBox", "message": "Hey, using scrapy from git and FormRequest.from_response, I get the error message that the Response object has no attribute \"encoding\". I have researched the error but only found information about xpath not working on python2.7 strings, not from_response causing this error. Is this error known and how can I resolve this?", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T18:38:22.698007+00:00", "nick": "nyov", "message": "which is quite correct. anyway, we might have had that bug crop before, the patch is that old after all, but I can't remember those things", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T18:39:28.948835+00:00", "nick": "nyov", "message": "sorry, taht was for snapsnail", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T18:40:16.381433+00:00", "nick": "brotherBox", "message": "I figured you were a snarky bot ;)", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T18:40:39.147778+00:00", "nick": "nyov", "message": "brotherBox: IIRC if the response has no encoding, you got a raw response which couldn't be decoded as xml", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T18:41:22.353760+00:00", "nick": "nyov", "message": "check that your response is actually what you think it is", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T18:42:59.400477+00:00", "nick": "nyov", "message": "for example with $scrapy shell <url>", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T18:43:19.548111+00:00", "nick": "nyov", "message": "you should get an ipython shell, where you can type \"response?\"", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T18:43:25.728660+00:00", "nick": "nyov", "message": "In [1]: response?", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T18:43:25.836686+00:00", "nick": "nyov", "message": "Type:       HtmlResponse", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T18:43:35.223301+00:00", "nick": "nyov", "message": "In [2]: response.encoding", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T18:43:35.331900+00:00", "nick": "nyov", "message": "Out[2]: 'utf-8'", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T18:45:01.594844+00:00", "nick": "brotherBox", "message": "Indeed, it has no .encoding attribute.", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T18:45:21.673630+00:00", "nick": "nyov", "message": "but it is a HtmlResponse?", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T18:45:50.781169+00:00", "nick": "nyov", "message": "(it shouldn't be then)", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T18:45:55.803653+00:00", "nick": "brotherBox", "message": "Its a <class 'scrapy.http.response.Response'>; the HTTP header reads Content-Type: text/html; charset=UTF-8", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T18:46:51.764113+00:00", "nick": "nyov", "message": "yep, that's basically a \"binary\" response", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T18:47:10.337759+00:00", "nick": "brotherBox", "message": "I parsed it with xpath though", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T18:49:03.832190+00:00", "nick": "brotherBox", "message": "I'll try just using a FormRequest with the raw url and manually include the hidden attributes", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T18:49:33.314430+00:00", "nick": "nyov", "message": "this sometimes happens when lxml detects null strings or other strangeness in the response. it'll fail to decode it", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T18:51:09.375479+00:00", "nick": "brotherBox", "message": "Could I manually decode it? I learned Python3 only to avoid that issue exactly to be honest", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T18:51:51.389253+00:00", "nick": "nyov", "message": "ually it's okay to drop these characters, so you can use something like this to fix the response:", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T18:51:56.685672+00:00", "nick": "nyov", "message": "https://gist.github.com/nyov/8746440", "links": ["https://gist.github.com/nyov/8746440"], "channel": "scrapy"},
{"date": "2014-09-07T18:52:00.949976+00:00", "nick": "nyov", "message": "*usually", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T19:06:34.752348+00:00", "nick": "brotherBox", "message": "Hm, I have it now. Thanks a bunch!", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T21:21:46.246247+00:00", "nick": "snapsnail", "message": "nyov: Still have the same issue with libxml2 2.9.1", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T21:26:13.490319+00:00", "nick": "nyov", "message": "snapsnail: crap. but that is strange", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T21:27:22.689229+00:00", "nick": "nyov", "message": "well, we have established it is not a scrapy bug, so far, since it's reproducible in lxml. So it must be either in the lxml python bindings or in libxml2", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T21:29:14.363083+00:00", "nick": "nyov", "message": "now I don't own a mac, so I can't try to reproduce it. But if you haven't yet, I would still suggest to post an issue on scrapy's tracker, to get more intel from other users", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T21:30:15.186238+00:00", "nick": "nyov", "message": "maybe someone else knows what the issue is, and at least we'll have the topic covered if it comes up in future again", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T21:31:02.291798+00:00", "nick": "nyov", "message": "(also, the core devs are more active on GH than in irc)", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T21:32:09.390338+00:00", "nick": "snapsnail", "message": "In [7]: etree.LIBXML_COMPILED_VERSION", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T21:32:11.635953+00:00", "nick": "snapsnail", "message": "Out[7]: (2, 9, 0)", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T21:32:26.382207+00:00", "nick": "snapsnail", "message": "hmm, that hasn't updated to 2.9.1 yet", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T21:32:27.376201+00:00", "nick": "nyov", "message": "ah, I was just about to ask if maybe you still used the old library", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T21:32:43.597975+00:00", "nick": "snapsnail", "message": "In [8]: etree.LIBXML_VERSION", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T21:32:45.635328+00:00", "nick": "snapsnail", "message": "Out[8]: (2, 9, 1)", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T21:32:46.978814+00:00", "nick": "nyov", "message": "is it maybe pip installed somewhere in /usr/local ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T21:32:52.123656+00:00", "nick": "nyov", "message": "huh", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T21:32:57.045796+00:00", "nick": "snapsnail", "message": "Odd that they are different...", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T21:33:00.695765+00:00", "nick": "nyov", "message": "indeed", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T21:33:11.505854+00:00", "nick": "nyov", "message": "how did you upgrade?", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T21:34:08.648082+00:00", "nick": "nyov", "message": "well, lxml is a python binding to the lxml c code. so maybe if you pip installed, it didn't compile or copy the new library into place", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T21:34:32.635482+00:00", "nick": "snapsnail", "message": "LDFLAGS=\"-L/usr/local/opt/libxml2/lib\" CPPFLAGS=\"-I/usr/local/opt/libxml2/include\" pip install lxml -I", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T21:35:24.076567+00:00", "nick": "nyov", "message": "are you sure \"/usr/local/opt/\" is in your $PATH? that looks convoluted", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T21:35:52.805803+00:00", "nick": "nyov", "message": "is that a mac os thing? usually it's /usr/local or /opt in my linux world", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T21:37:45.621092+00:00", "nick": "nyov", "message": "or rather, are those include files actually in that path? it's not necessarily the same as where it was copied after install, I guess.", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T21:37:51.772489+00:00", "nick": "nyov", "message": "i'm not really a 'pip' user", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T21:38:43.998881+00:00", "nick": "snapsnail", "message": "Yeah, it's a bit of a mac thing.", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T21:38:59.873664+00:00", "nick": "nyov", "message": "okay, learn something new every day :)", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T21:39:00.624927+00:00", "nick": "snapsnail", "message": "the newer version of libxml2 was installed via homebrew", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T21:39:26.369926+00:00", "nick": "snapsnail", "message": "however to avoid clobbering the default versions, it puts it in it's own folder", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T21:40:12.727498+00:00", "nick": "nyov", "message": "hm, don't know that at all. well, make sure the new .so lib, wherever it's placed, can be found by python and isn't superceded by the older", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T21:41:19.356704+00:00", "nick": "nyov", "message": "for testing, you could copy it into your current project maybe", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T22:02:23.939896+00:00", "nick": "snapsnail", "message": "Just in the root of the project directory?", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T22:11:18.498405+00:00", "nick": "nyov", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T22:12:02.838176+00:00", "nick": "nyov", "message": "I think that should work, as the library should be looked up from your $PATH, and the current directory is always in there", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T22:12:39.911264+00:00", "nick": "nyov", "message": "(so to clarify, put it in the directory from where you run the thing)", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T22:41:22.357427+00:00", "nick": "snapsnail", "message": "Yeah, I can't get that compiled version to change in lxml.", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T22:41:31.759273+00:00", "nick": "snapsnail", "message": "I'm about to throw in the towel", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T22:48:49.101091+00:00", "nick": "nyov", "message": "I'm sorry :(", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T22:49:11.488715+00:00", "nick": "nyov", "message": "snapsnail: can't you find the old version and just remove it?", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T22:49:33.773032+00:00", "nick": "nyov", "message": "at least then it might work without the optimized compiled code.", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T22:51:49.997488+00:00", "nick": "snapsnail", "message": "The optimized one that it is referencing is OS default. I don't think it's a good idea to remove it as it could be used by something.", "links": [], "channel": "scrapy"},
{"date": "2014-09-07T23:22:50.086972+00:00", "nick": "snapsnail", "message": "I'll see if I can find a way to link lxml properly. That must be the problem.", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T02:29:42.577423+00:00", "nick": "nyov", "message": "GAH. when building a debian package for a tiny 16kb executable in a clean build env takes 30 minutes (for a single ARCH), and then fails because it couldn't install a manpage, I can't help but think this is a total waste of ressources.", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T02:29:57.358639+00:00", "nick": "nyov", "message": "oh my time!!", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T03:29:24.983395+00:00", "nick": "larryxiao", "message": "howdy", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T03:31:48.879336+00:00", "nick": "larryxiao", "message": "I want to save responses and re-parse if needed (so don't crawl again), any suggestions?", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T03:34:05.366968+00:00", "nick": "larryxiao", "message": "Because it's massive API calls, and the response will not change much, but the info I want to extract might change.", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T03:35:51.858785+00:00", "nick": "nyov", "message": "larryxiao: use HTTPCACHE", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T03:36:20.571603+00:00", "nick": "nyov", "message": "with the dumb cache backend", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T03:37:16.378361+00:00", "nick": "larryxiao", "message": "thank nyov! let me check", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T03:37:41.641146+00:00", "nick": "nyov", "message": "http://doc.scrapy.org/en/latest/topics/download...", "links": ["http://doc.scrapy.org/en/latest/topics/downloader-middleware.html#std:setting-HTTPCACHE_ENABLED"], "channel": "scrapy"},
{"date": "2014-09-08T03:38:20.629904+00:00", "nick": "larryxiao", "message": "thank you very much!", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T03:38:31.682371+00:00", "nick": "nyov", "message": "what you want is something like this:", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T03:38:40.718238+00:00", "nick": "nyov", "message": "HTTPCACHE_ENABLED = True", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T03:38:42.923156+00:00", "nick": "nyov", "message": "HTTPCACHE_STORAGE = 'scrapy.contrib.httpcache.FilesystemCacheStorage'", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T03:38:50.991397+00:00", "nick": "nyov", "message": "HTTPCACHE_DIR = '/var/tmp/scrapy-httpcache'", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T03:39:11.704456+00:00", "nick": "nyov", "message": "HTTPCACHE_POLICY = 'scrapy.contrib.httpcache.DummyPolicy'", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T03:39:41.740378+00:00", "nick": "nyov", "message": "oh and maybe change what gets cached and what not, like never cache redirects (for example)", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T03:39:45.580127+00:00", "nick": "nyov", "message": "HTTPCACHE_IGNORE_HTTP_CODES = ['301', '302', '404', '405', '501', '502']", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T03:40:23.202678+00:00", "nick": "nyov", "message": "that should do it.", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T03:41:14.627784+00:00", "nick": "nyov", "message": "or if you use mongodb, there is a cache backend for that (and probably others like redis)", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T03:41:59.899522+00:00", "nick": "larryxiao", "message": "thank you! exactly what I wanted", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T03:42:08.305468+00:00", "nick": "nyov", "message": "you're welcome", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T03:44:35.675560+00:00", "nick": "larryxiao", "message": "kind people =)", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T03:45:23.899658+00:00", "nick": "nyov", "message": "depends on the tim. i had a good day ;)", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T05:12:01.071705+00:00", "nick": "snapsnail", "message": "nyov: Got libxml 2.9.1 fully installed. Still doesn't work... :-(", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T05:12:09.004897+00:00", "nick": "snapsnail", "message": "In [14]: etree.LIBXML_VERSION", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T05:12:10.623140+00:00", "nick": "snapsnail", "message": "Out[14]: (2, 9, 1)", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T05:12:12.627461+00:00", "nick": "snapsnail", "message": "In [15]: etree.LIBXML_COMPILED_VERSION", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T05:12:14.665052+00:00", "nick": "snapsnail", "message": "Out[15]: (2, 9, 1)", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T12:18:11.385113+00:00", "nick": "brotherBox", "message": "Hi, is there a way to display what links a LinkExtractor finds? I have a problem with a link that does not appear to be recognized and I would like to resolve that issue.", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T14:33:22.838850+00:00", "nick": "nyov", "message": "snapsnail: Well that really sucks. then I'm totally out of ideas.", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T14:33:52.121478+00:00", "nick": "nyov", "message": "or, maybe it has to do with increased solar flare activity", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T14:33:54.625003+00:00", "nick": "nyov", "message": "j/k", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T14:37:44.308617+00:00", "nick": "nyov", "message": "brotherBox: MyLinkExtractor.extract_links(response) will return a list. (unless you overrode _process_links() in your subclass)", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T14:38:38.779779+00:00", "nick": "nyov", "message": "so just print() it and see what it has", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T18:01:23.914741+00:00", "nick": "Digenis", "message": "lol at #878", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T18:01:41.447305+00:00", "nick": "Digenis", "message": "yes, there is a test", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T18:01:51.748677+00:00", "nick": "Digenis", "message": "it's called static typing", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T18:12:52.021166+00:00", "nick": "snapsnail", "message": "nyov: I got it working, I had to roll back to libxml2 2.8.0. Brutal.", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T18:13:05.581545+00:00", "nick": "snapsnail", "message": "I'm shocked that more people are not having issues.", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T18:28:31.210885+00:00", "nick": "nyov", "message": "snapsnail: congratulations", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T18:30:40.003309+00:00", "nick": "nyov", "message": "no idea why this is an issue in your libxml2 builds and not for others. maybe debian packages ship with a patch, and mac os does not. really no idea though.", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T18:38:53.714869+00:00", "nick": "snapsnail", "message": "I was even building lxml statically with the libxml2-2.9.1 beind downloaded as a tar.gz", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T18:39:51.625971+00:00", "nick": "snapsnail", "message": "and still no-go", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T18:40:12.959817+00:00", "nick": "snapsnail", "message": "2.9.1 works perfect on my linux box as well, so who knows.", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T18:40:24.619101+00:00", "nick": "nyov", "message": ":/", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T18:44:08.824516+00:00", "nick": "snapsnail", "message": "I do have another question more specifically around scrapy... ;-)", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T18:45:32.988763+00:00", "nick": "snapsnail", "message": "I'd like to scrape a site that is structured with three nested items: grandparent - parent - child", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T18:46:02.717353+00:00", "nick": "snapsnail", "message": "And child - parent, and parent-grandparent is one to many.", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T18:46:42.636159+00:00", "nick": "nyov", "message": "meaning one child in many parents?", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T18:47:05.546689+00:00", "nick": "snapsnail", "message": "I can either try and have one big nested grandparent object, or return them all seperately.", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T18:47:14.600765+00:00", "nick": "snapsnail", "message": "one parent, many child", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T18:47:23.872514+00:00", "nick": "snapsnail", "message": "one grandparent, many parent", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T18:47:56.426188+00:00", "nick": "nyov", "message": "what's your exporting format?", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T18:48:59.276593+00:00", "nick": "nyov", "message": "for XML exporting it makes sense to use multiple Items and nest them", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T18:49:54.270653+00:00", "nick": "snapsnail", "message": "I'd like to export them to a db.", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T18:51:06.009269+00:00", "nick": "snapsnail", "message": "My preference is to do multiple items and nest.", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T18:51:58.185181+00:00", "nick": "nyov", "message": "okay, I'm not sure in a DB case, since it really is your choice how to write to that", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T18:52:24.094380+00:00", "nick": "snapsnail", "message": "However, ideally in that scenario scrapy would only pass on one grandparent item to the item pipelines", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T18:53:24.926582+00:00", "nick": "nyov", "message": "I'd say it depends on you needs. If an item should be atomically written (as a whole, with all children) use a single Item, if you want to do updates to children only, use multiple items", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T18:53:26.874619+00:00", "nick": "snapsnail", "message": "I'm not concerned on the db interface side of things, I got that covered.", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T18:54:11.881473+00:00", "nick": "snapsnail", "message": "My pref is returning grandparents with all descendants nested", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T18:54:44.227900+00:00", "nick": "nyov", "message": "for the case of passing on only a single grandparent/the same grandparent once - either handle that in your spider (only crawl it once) or use a filter (item-has-been-seen-already-now-drop-it)", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T18:54:44.823263+00:00", "nick": "snapsnail", "message": "However, when I do it that way, I am getting a returned item for each child returned.", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T18:55:34.507907+00:00", "nick": "snapsnail", "message": "because I am passing grandparent -> parent, and parent -> child in the meta[item]", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T18:55:43.150011+00:00", "nick": "nyov", "message": "right", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T18:57:15.456257+00:00", "nick": "snapsnail", "message": "The filter method seems very hokie as it will be filtering out (child_instances-1)", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T18:57:22.754168+00:00", "nick": "nyov", "message": "well, consider that if those children come from many different pages and get compiled together first, the Item hangs around longer in your memory. I usually go with a push-to-pipeline-asap mentality", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T18:57:25.460273+00:00", "nick": "snapsnail", "message": "Which would be quite a lot.", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T18:58:04.148672+00:00", "nick": "snapsnail", "message": "So then return grandparent, parent, and child all separately?", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T18:58:04.273791+00:00", "nick": "nyov", "message": "if those child instances are all defined Items in their own right, fingerprinting should work okay on that", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T18:58:24.931770+00:00", "nick": "nyov", "message": "I'd think it's more extendable", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T18:59:06.104972+00:00", "nick": "snapsnail", "message": "It just means that I need to add a couple keys on the parent and child items so they can tie back properly.", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T19:01:07.933342+00:00", "nick": "snapsnail", "message": "My concern/risk with that approach is that I then need to worry about concurrency. Eg. The grandparent needs to be processed before the parent, and the parent before the child. Otherwise I'll run into issues when inserting/joining it up.", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T19:07:02.649609+00:00", "nick": "nyov", "message": "that would suggest your child doesn't know it's parent, correct?", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T19:08:32.099280+00:00", "nick": "nyov", "message": "if the child can identify it's parent, I'd either handle it in the DB or use a Queue to write the child into, until the parent exists in the database and the pop it from the queue", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T19:09:24.367907+00:00", "nick": "nyov", "message": "though that's theoretical, haven't done that yet", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T19:10:18.503649+00:00", "nick": "nyov", "message": "that's the queue scrapy uses internally: https://github.com/scrapy/scrapy/blob/master/sc...", "links": ["https://github.com/scrapy/scrapy/blob/master/scrapy/squeue.py"], "channel": "scrapy"},
{"date": "2014-09-08T19:10:40.201499+00:00", "nick": "nyov", "message": "and queuelib supports disk and memory queues", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T19:13:00.466666+00:00", "nick": "nyov", "message": "for duplicate filtering see https://github.com/nyov/scrapyext/blob/master/s...", "links": ["https://github.com/nyov/scrapyext/blob/master/scrapyext/duplicate.py"], "channel": "scrapy"},
{"date": "2014-09-08T19:14:06.928103+00:00", "nick": "nyov", "message": "(doh, gotta update that stuff. moar timeeee)", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T19:19:49.196008+00:00", "nick": "snapsnail", "message": "lol, that's helpful", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T19:20:26.666180+00:00", "nick": "snapsnail", "message": "I was hoping that I could rely on scrapy to do that queueing to assemble the nested item, however I can't find an approach that works to do that.", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T19:21:42.786148+00:00", "nick": "nyov", "message": "hm, from the top of my head I can't say if I've seen code related to doing that", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T19:21:57.857951+00:00", "nick": "nyov", "message": "what's your database?", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T19:22:08.510758+00:00", "nick": "snapsnail", "message": "mysql", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T19:22:47.402971+00:00", "nick": "snapsnail", "message": "But I need to write the insert logic in the pipeline, however I haven't got to that yet as I haven't found an approach that works yet.", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T19:23:52.896228+00:00", "nick": "nyov", "message": "hm, so you want to assemble items and write them as a single entry? not normalized?", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T19:24:28.082651+00:00", "nick": "snapsnail", "message": "My thought was to normalize it in the pipeline.", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T19:25:24.106584+00:00", "nick": "snapsnail", "message": "I like the one nested grandparent item because it is a cohesive whole. It can be either written out json or spit out into insert statement. Both ways make sense.", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T19:26:00.224335+00:00", "nick": "nyov", "message": "well then it should be possible to try an insert statement, if the parent is missing, push it onto an item<class> queue instead", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T19:26:17.428240+00:00", "nick": "snapsnail", "message": "Versus if I split them up and write it out, I will have a whole mess of things.", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T19:27:02.470781+00:00", "nick": "nyov", "message": "but with one nested grandparent item, how do you do updates of (subsets) of items, without going through your whole source-pages again?", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T19:27:35.195124+00:00", "nick": "nyov", "message": "certainly possible, but normalized would be easier for that", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T19:27:58.927350+00:00", "nick": "snapsnail", "message": "I am totally fine with grandparent level of atomicity. All or nothing.", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T19:28:08.068263+00:00", "nick": "nyov", "message": "ah okay then", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T19:28:12.158653+00:00", "nick": "snapsnail", "message": "Whole granparent and all children or nothing.", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T19:28:23.306941+00:00", "nick": "nyov", "message": "then I'd consider it a single item", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T19:28:55.302162+00:00", "nick": "nyov", "message": "yeah, that might be easier then to just compile all together in the spider", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T19:28:58.338592+00:00", "nick": "snapsnail", "message": "Not nested items?", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T19:29:46.330619+00:00", "nick": "snapsnail", "message": "There is a page for the grandparent with links to all the parent/child pages.", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T19:29:50.053770+00:00", "nick": "nyov", "message": "well, why instance multiple item types if you don't need/use them except for having another named dict", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T19:30:12.626031+00:00", "nick": "snapsnail", "message": "That's fair.", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T19:34:01.995734+00:00", "nick": "nyov", "message": "if you have an entry point at the top of the tree/grandparent like that, it should be possible to crawl top-to-bottom? then you wouldn't have issues with missing parents", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T19:35:02.379041+00:00", "nick": "snapsnail", "message": "How do I return just one grandparent item with many parent/child scrapes necessary?", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T19:35:37.712177+00:00", "nick": "snapsnail", "message": "Because I'll need to make requests to get their pages as well.", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T19:36:05.023077+00:00", "nick": "nyov", "message": "hm, that's the tricky part I guess. you'll need a finish condition to know when all the children are done", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T19:36:17.462035+00:00", "nick": "nyov", "message": "before inserting the item", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T19:38:04.721783+00:00", "nick": "snapsnail", "message": "Or have parent items return to the grandparent instead of to the item pipeline.", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T19:38:27.478663+00:00", "nick": "nyov", "message": "i remember hashing that out with someone here in the past. can't quite remember, yet", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T19:40:19.213546+00:00", "nick": "snapsnail", "message": "I can't find anything in the documentation relating to doing that.", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T19:41:05.256345+00:00", "nick": "nyov", "message": "no, you can't ususally go back to your parent yielding method. it's a waterfall, no going back up", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T19:42:01.181794+00:00", "nick": "nyov", "message": "guess you'll have to use a global hash in the class where you store and append values", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T19:44:51.587568+00:00", "nick": "nyov", "message": "I wonder. somehow it should be possible to keep a reference to your parent from all the child method calls, and exiting the initial method when no more children exist", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T19:45:05.677794+00:00", "nick": "nyov", "message": "that'd be the stop condition then to yield the 'grandparent'", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T19:48:58.043202+00:00", "nick": "nyov", "message": "pablohof: will we see a new scrapyd debian package in the future? i'm still a bit waiting for my sysv initscript patch, so I don't have to build my own patched packages anymore :D", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T19:53:24.068595+00:00", "nick": "nyov", "message": "the latest one I see in the repository is 1.0~r111", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T19:53:35.121677+00:00", "nick": "nyov", "message": "and still depends on upstart-job for me", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T20:06:16.120400+00:00", "nick": "snapsnail", "message": "Is there a way to have the parent/child not pass an item or request to the pipeline?", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T20:12:00.280641+00:00", "nick": "nyov", "message": "if you don't yield/return it, it won't go to the pipeline", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T20:24:23.474489+00:00", "nick": "snapsnail", "message": "Oh, so no return or yield at all", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T20:26:02.668028+00:00", "nick": "nyov", "message": "yeah, just pass (don't need to write that). and possibly return/yield a new request", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T20:26:33.901788+00:00", "nick": "nyov", "message": "s/and/or/", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T20:28:48.166336+00:00", "nick": "snapsnail", "message": "Then have an indicator or index to know when all the decendant requests have finished, then return.", "links": [], "channel": "scrapy"},
{"date": "2014-09-08T20:31:03.608727+00:00", "nick": "nyov", "message": "no \"return\" back to the original calling method. but, yeah, \"return\" the compiled Item (return self.items['#572']). exactly", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T00:10:24.896670+00:00", "nick": "snapsnail", "message": "nyov, I came across the following article that had a pretty good strategy for dealing with what we were talking about earlier:", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T00:10:27.119945+00:00", "nick": "snapsnail", "message": "http://oliverguenther.de/2014/08/almost-asynchr...", "links": ["http://oliverguenther.de/2014/08/almost-asynchronous-requests-for-single-item-processing-in-scrapy/"], "channel": "scrapy"},
{"date": "2014-09-09T00:11:10.088829+00:00", "nick": "snapsnail", "message": "I don't think I will do it as it is probably a little more than I need, but it shows an interesting approach on the matter.", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T00:38:17.645751+00:00", "nick": "nyov", "message": "snapsnail: using a stack is the right idea, but the way described on the website seems overly complicated", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T00:38:31.763001+00:00", "nick": "nyov", "message": "also, passing on ItemLoader in meta is bad", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T00:40:44.035693+00:00", "nick": "nyov", "message": "i'll have to dig through some irc history to remember why.", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T00:56:12.521024+00:00", "nick": "snapsnail", "message": "yeah, I wouldn't pass ItemLoader as I don't even use the loaders currently. I parse directly to the objects themselves. I don't fully get the benefit (at least not in my case).", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T00:58:32.258621+00:00", "nick": "nyov", "message": "hm, actually I guess I remembered that wrong, it was about handing items along to a new loader instance", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T01:00:53.772593+00:00", "nick": "nyov", "message": "but there is still the issue of keeping references to potentially very old requests around, and thus keeping them in memory", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T01:02:49.665298+00:00", "nick": "nyov", "message": "anyway, about the blog post again, I don't see why you would need to pass this whole callstack along in item meta", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T08:27:45.260750+00:00", "nick": "anvnguyen", "message": "Hi there", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T08:28:06.528987+00:00", "nick": "anvnguyen", "message": "I try to crawl html from this domain http://tiki.vn/", "links": ["http://tiki.vn/"], "channel": "scrapy"},
{"date": "2014-09-09T08:28:38.203831+00:00", "nick": "anvnguyen", "message": "but scrapy always return 404 http status code", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T08:29:09.015527+00:00", "nick": "anvnguyen", "message": "if I use browser or php code (curl), I still get html from this domain", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T08:29:14.952483+00:00", "nick": "anvnguyen", "message": "I am not sure why", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T08:29:27.206997+00:00", "nick": "nkuttler", "message": "anvnguyen: just a guess, but user agent sniffing?", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T08:30:17.501657+00:00", "nick": "anvnguyen", "message": "nkuttler: what do you mean by user agent sniffing?", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T08:30:56.928697+00:00", "nick": "nkuttler", "message": "anvnguyen: they could be trying to block crawlers. set a more common user agent string", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T08:31:26.159354+00:00", "nick": "anvnguyen", "message": "how to set this string nkuttler", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T08:31:32.464258+00:00", "nick": "anvnguyen", "message": "sorry, I am new to this lib", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T08:32:12.746897+00:00", "nick": "nkuttler", "message": "anvnguyen: it's in the documentation", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T08:32:41.299022+00:00", "nick": "anvnguyen", "message": "ok nkuttler, thank you a lot ;)", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T08:33:04.767188+00:00", "nick": "nkuttler", "message": "anvnguyen: make sure your urls are correct though. wget works just fine, which is often blocked as a crawler", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T08:34:10.634946+00:00", "nick": "anvnguyen", "message": "yes, I am sure that the url is valid", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T15:51:03.287877+00:00", "nick": "larryxiao", "message": "hello!", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T15:51:57.692701+00:00", "nick": "larryxiao", "message": "I'm now using scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware as nyov suggested, and it works great", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T15:53:12.354449+00:00", "nick": "larryxiao", "message": "the problem now is that I want to discard and redo the request, based on the response I parse", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T15:53:24.093369+00:00", "nick": "larryxiao", "message": "is there a way to do this?", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T15:53:26.220088+00:00", "nick": "larryxiao", "message": "thanks", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T15:56:52.285502+00:00", "nick": "nramirezuy", "message": "Don't think so, good hacking :D", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T15:57:23.884916+00:00", "nick": "larryxiao", "message": "=P", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T16:01:50.626621+00:00", "nick": "larryxiao", "message": "or to paraphrase it: can I explicitly cache requests and responses?", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T17:11:10.588756+00:00", "nick": "SirSkitzo", "message": "I am trying to scrape a table but the table data changes locations from profile to profile. If I have <td>Name:</td><td>Bob</td> how can I use xpath to check and see if Name: exists and pull Bob if it does", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T17:11:22.391858+00:00", "nick": "SirSkitzo", "message": "?", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T17:53:29.601453+00:00", "nick": "nramirezuy", "message": "try with '/td[text()=\"Name:\"]/following-sibling::td/text()'", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T18:29:44.553310+00:00", "nick": "localhot", "message": "what is the default amount of links scrapy will follow before it stops crawling?", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T18:30:03.332309+00:00", "nick": "localhot", "message": "also, does it ignore duplicated urls?", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T18:52:18.763911+00:00", "nick": "nramirezuy", "message": "it ignore duplicated urls by default", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T18:52:32.564034+00:00", "nick": "nramirezuy", "message": "there is no default to stop", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T22:10:33.796819+00:00", "nick": "larryxiao", "message": "can i get request in parse()?", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T22:24:30.051651+00:00", "nick": "nyov", "message": "larryxiao: yes, use response.request", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T22:24:39.724225+00:00", "nick": "nyov", "message": "that is the request which generated your response", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T22:25:33.401516+00:00", "nick": "nyov", "message": "or, if you want a new request, use scrapy.Request", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T22:32:22.609579+00:00", "nick": "larryxiao", "message": "thanks again nyov!", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T22:33:16.059616+00:00", "nick": "larryxiao", "message": "I was trying to write a middleware to do this, but it affects other middleware ..", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T22:36:46.358611+00:00", "nick": "nyov", "message": "middleware to do what?", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T22:54:43.232069+00:00", "nick": "larryxiao", "message": "to add a pass to get the response, to see if need to redo. (so construct a new request, and add value to meta)", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T22:55:53.411149+00:00", "nick": "larryxiao", "message": "and some exception occur, handled by httpproxy middleware. and there's no request.meta['proxy']", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T22:57:04.287543+00:00", "nick": "larryxiao", "message": "oops.. it's not caused by my middleware", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T22:59:16.310761+00:00", "nick": "larryxiao", "message": "Traceback (most recent call last):", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T22:59:16.430795+00:00", "nick": "larryxiao", "message": "File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/base.py\", line 1201, in mainLoop", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T22:59:16.430865+00:00", "nick": "larryxiao", "message": "self.runUntilCurrent()", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T22:59:16.432070+00:00", "nick": "larryxiao", "message": "File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/base.py\", line 824, in runUntilCurrent", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T22:59:16.434111+00:00", "nick": "larryxiao", "message": "call.func(*call.args, **call.kw)", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T22:59:16.669584+00:00", "nick": "larryxiao", "message": "File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py\", line 423, in errback", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T22:59:16.671175+00:00", "nick": "larryxiao", "message": "self._startRunCallbacks(fail)", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T22:59:17.615534+00:00", "nick": "larryxiao", "message": "File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py\", line 490, in _startRunCallbacks", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T22:59:17.736005+00:00", "nick": "larryxiao", "message": "self._runCallbacks()", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T22:59:18.610493+00:00", "nick": "larryxiao", "message": "--- <exception caught here> ---", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T22:59:18.730573+00:00", "nick": "larryxiao", "message": "File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py\", line 577, in _runCallbacks", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T22:59:19.653215+00:00", "nick": "larryxiao", "message": "current.result = callback(current.result, *args, **kw)", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T23:02:00.906905+00:00", "nick": "larryxiao", "message": "nvm the middleware shouldn't catch this exception", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T23:08:47.463320+00:00", "nick": "nyov", "message": "not sure I understand what you had in mind there. but RetryMiddleware already does retry on likely temporary download failures", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T23:30:40.760070+00:00", "nick": "larryxiao", "message": "sorry I didn't explain clear. It's just that I want to retry based on the response, and I don't want the cached response for the retry. So now I construct request in parse(), add a key \"_force_update\" in meta. And change HttpCache to check this flag.", "links": [], "channel": "scrapy"},
{"date": "2014-09-09T23:30:53.304235+00:00", "nick": "larryxiao", "message": "I think it's ok now", "links": [], "channel": "scrapy"},
{"date": "2014-09-10T08:43:19.926948+00:00", "nick": "richard_ma", "message": "hi, I use yield in parse of a spider to give a lint to crawl, but it doesn't work", "links": [], "channel": "scrapy"},
{"date": "2014-09-10T08:43:28.355280+00:00", "nick": "richard_ma", "message": "link...", "links": [], "channel": "scrapy"},
{"date": "2014-09-10T12:01:39.321088+00:00", "nick": "localhost_", "message": "anyone know how to add scrapyd to supervisord?", "links": [], "channel": "scrapy"},
{"date": "2014-09-10T13:31:48.787782+00:00", "nick": "localhost_", "message": "can scrapy save the pages it crawls, separated from the scraped data?", "links": [], "channel": "scrapy"},
{"date": "2014-09-10T14:46:54.690199+00:00", "nick": "scrapy082", "message": "test", "links": [], "channel": "scrapy"},
{"date": "2014-09-10T14:47:21.821505+00:00", "nick": "scrapy082", "message": "anyone know the best way of crawling ajax sites?", "links": [], "channel": "scrapy"},
{"date": "2014-09-10T14:51:20.290382+00:00", "nick": "scrapy082", "message": "maybe just too broad, i should've asked how do you crawl ajax sites? do you integrate selenium in scrapy?", "links": [], "channel": "scrapy"},
{"date": "2014-09-10T14:51:30.122326+00:00", "nick": "scrapy082", "message": "or any other way?", "links": [], "channel": "scrapy"},
{"date": "2014-09-10T14:51:31.576641+00:00", "nick": "scrapy082", "message": "thanks", "links": [], "channel": "scrapy"},
{"date": "2014-09-10T15:05:09.233983+00:00", "nick": "nramirezuy", "message": "localhost_ try httpcache", "links": [], "channel": "scrapy"},
{"date": "2014-09-10T15:05:51.507238+00:00", "nick": "nramirezuy", "message": "scrapy082 monitor the traffic and reproduce the requests? ajax moves information is json or something", "links": [], "channel": "scrapy"},
{"date": "2014-09-10T21:27:09.758057+00:00", "nick": "nevodka", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2014-09-10T21:27:20.856322+00:00", "nick": "nevodka", "message": "is there an inverse of restrict_xpaths?", "links": [], "channel": "scrapy"},
{"date": "2014-09-10T21:27:25.541487+00:00", "nick": "nevodka", "message": "in crawlspider rules", "links": [], "channel": "scrapy"},
{"date": "2014-09-10T21:27:43.239966+00:00", "nick": "nevodka", "message": "as in i want to follow all links except the ones in a certain xpath", "links": [], "channel": "scrapy"},
{"date": "2014-09-12T23:24:32.524941+00:00", "nick": "nevodka", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2014-09-13T06:55:48.165384+00:00", "nick": "helpmepls", "message": "how can i do `scrapy deploy` unto multiple targets?", "links": [], "channel": "scrapy"},
{"date": "2014-09-13T06:56:06.112146+00:00", "nick": "helpmepls", "message": "`scrapy deploy target1 target2` returns an error", "links": [], "channel": "scrapy"},
{"date": "2014-09-13T06:56:18.392011+00:00", "nick": "helpmepls", "message": "deploy: error: Too many arguments: target1 target2", "links": [], "channel": "scrapy"},
{"date": "2014-09-13T14:01:54.371464+00:00", "nick": "asd_", "message": "anyone freelanced web scraping project before?", "links": [], "channel": "scrapy"},
{"date": "2014-09-13T14:29:16.454758+00:00", "nick": "Llamageddon", "message": "Hey everyone", "links": [], "channel": "scrapy"},
{"date": "2014-09-13T14:29:28.154973+00:00", "nick": "Llamageddon", "message": "Could someone tell me, how do you exactly use this thing? I'm following the tutorial, but...", "links": [], "channel": "scrapy"},
{"date": "2014-09-13T14:29:40.010234+00:00", "nick": "Llamageddon", "message": "Well, I do the \"scrapy crawl\" thing, I have my rules and whatnot", "links": [], "channel": "scrapy"},
{"date": "2014-09-13T14:29:47.974596+00:00", "nick": "Llamageddon", "message": "But what do I even do with the data I obtain?", "links": [], "channel": "scrapy"},
{"date": "2014-09-13T14:30:10.186491+00:00", "nick": "Llamageddon", "message": "I have rules for two different URLs that contain parts of the same dataset, so to say, how can I combine that?", "links": [], "channel": "scrapy"},
{"date": "2014-09-13T14:30:42.969136+00:00", "nick": "Llamageddon", "message": "Do I need to write the results to external files and parse them with some separate script?", "links": [], "channel": "scrapy"},
{"date": "2014-09-13T14:31:25.900585+00:00", "nick": "Llamageddon", "message": "The entire things seems extremely and unnecessarily convoluted", "links": [], "channel": "scrapy"},
{"date": "2014-09-15T01:31:07.028698+00:00", "nick": "bluesteal", "message": "I'm using scrapy and I would like to be able to manually handle this error", "links": [], "channel": "scrapy"},
{"date": "2014-09-15T01:31:11.535939+00:00", "nick": "bluesteal", "message": "Error downloading <GET http://mydomain.com&gt;: DNS lookup failed: address 'mydomain.com' not found: [Errno 8] nodename nor servname provided, or not known.", "links": ["http://mydomain.com&gt"], "channel": "scrapy"},
{"date": "2014-09-15T01:31:41.914356+00:00", "nick": "bluesteal", "message": "I know why the dns lookup is failing but I can't figure out how to let scrapy allow me to test for these failures and handle it myself, how can I do that?", "links": [], "channel": "scrapy"},
{"date": "2014-09-15T02:04:32.913881+00:00", "nick": "nyov", "message": "bluesteal: I think you'll have to add an errback to scrapy's CachingThreadedResolver", "links": [], "channel": "scrapy"},
{"date": "2014-09-15T02:13:19.334403+00:00", "nick": "bluesteal", "message": "nyov thanks let me check that out", "links": [], "channel": "scrapy"},
{"date": "2014-09-15T02:26:54.479499+00:00", "nick": "nyov", "message": "I think I heard this question before. will put up a gist for future references :)", "links": [], "channel": "scrapy"},
{"date": "2014-09-15T02:39:15.089729+00:00", "nick": "nyov", "message": "bluesteal: try this on for size. https://gist.github.com/nyov/11b228bde3cfe7dbfe80", "links": ["https://gist.github.com/nyov/11b228bde3cfe7dbfe80"], "channel": "scrapy"},
{"date": "2014-09-15T02:42:59.822047+00:00", "nick": "nyov", "message": "hmm actually that's a stupid way to go about it. just noticed", "links": [], "channel": "scrapy"},
{"date": "2014-09-15T02:44:38.478058+00:00", "nick": "nyov", "message": "the middlewares get a process_exception() callback, where you can check and handle exceptions", "links": [], "channel": "scrapy"},
{"date": "2014-09-15T02:45:08.377244+00:00", "nick": "nyov", "message": "so just write a little middleware which catches your exception", "links": [], "channel": "scrapy"},
{"date": "2014-09-15T03:33:56.031071+00:00", "nick": "nyov", "message": "bluesteal: did you manage?", "links": [], "channel": "scrapy"},
{"date": "2014-09-15T03:35:05.937741+00:00", "nick": "nyov", "message": "bluesteal: otherwise, would you maybe test this for me: https://github.com/nyov/scrapyext/blob/master/s...", "links": ["https://github.com/nyov/scrapyext/blob/master/scrapyext/downloadermiddleware/nodnsfail.py"], "channel": "scrapy"},
{"date": "2014-09-15T04:03:05.677021+00:00", "nick": "bluesteal", "message": "nyov I was reading about", "links": [], "channel": "scrapy"},
{"date": "2014-09-15T04:03:27.785913+00:00", "nick": "bluesteal", "message": "Let me take a look at the github post you just put up", "links": [], "channel": "scrapy"},
{"date": "2014-09-15T04:17:06.005361+00:00", "nick": "scaper", "message": "i am confused with sitemap spider", "links": [], "channel": "scrapy"},
{"date": "2014-09-15T04:17:15.215662+00:00", "nick": "scaper", "message": "need some help", "links": [], "channel": "scrapy"},
{"date": "2014-09-15T04:21:38.426590+00:00", "nick": "nyov", "message": "that's alright. everyone needs help some time", "links": [], "channel": "scrapy"},
{"date": "2014-09-15T06:59:12.636897+00:00", "nick": "nyov", "message": "bluesteal: so what's the verdict? does it work?", "links": [], "channel": "scrapy"},
{"date": "2014-09-15T10:54:56.494058+00:00", "nick": "exarkun_", "message": "Can the parse callback return a Deferred?", "links": [], "channel": "scrapy"},
{"date": "2014-09-15T10:59:20.847207+00:00", "nick": "nyov", "message": "exarkun_: have you tried?", "links": [], "channel": "scrapy"},
{"date": "2014-09-15T11:00:56.167310+00:00", "nick": "nyov", "message": "because I would say yes", "links": [], "channel": "scrapy"},
{"date": "2014-09-15T11:04:06.845584+00:00", "nick": "exarkun_", "message": "The documentation says otherwise, http://doc.scrapy.org/en/latest/topics/spiders....", "links": ["http://doc.scrapy.org/en/latest/topics/spiders.html#scrapy.spider.Spider.parse"], "channel": "scrapy"},
{"date": "2014-09-15T11:04:12.357429+00:00", "nick": "exarkun_", "message": "But that seems ridiculous to me.", "links": [], "channel": "scrapy"},
{"date": "2014-09-15T11:23:57.548184+00:00", "nick": "nyov", "message": "I guess then the docs are right.", "links": [], "channel": "scrapy"},
{"date": "2014-09-15T22:16:30.035297+00:00", "nick": "meunierd_", "message": "Does anyone know how I can start a spider from a FormRequest?", "links": [], "channel": "scrapy"},
{"date": "2014-09-15T22:17:29.308916+00:00", "nick": "meunierd_", "message": "I'm iterating through a search form's filters. Should I just start with the base page?", "links": [], "channel": "scrapy"},
{"date": "2014-09-15T22:19:40.545211+00:00", "nick": "nyov", "message": "meunierd_: if you don't want to use FormRequest.from_response, of course you can start out with a FormRequest", "links": [], "channel": "scrapy"},
{"date": "2014-09-15T22:21:54.619013+00:00", "nick": "nyov", "message": "you would use start_requests() to create your custom Request and return that", "links": [], "channel": "scrapy"},
{"date": "2014-09-15T22:22:02.008136+00:00", "nick": "meunierd_", "message": "gotcha", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T02:59:29.542841+00:00", "nick": "o_o_", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T02:59:56.682151+00:00", "nick": "o_o_", "message": "I posted this question on stackoverflow http://stackoverflow.com/questions/25829733/get...", "links": ["http://stackoverflow.com/questions/25829733/get-links-from-two-level-of-sitemap-xml-using-scrapy"], "channel": "scrapy"},
{"date": "2014-09-16T03:01:27.136751+00:00", "nick": "o_o_", "message": "please suggest me the solution", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T03:06:58.668597+00:00", "nick": "nyov", "message": "o_o_: http://doc.scrapy.org/en/latest/topics/spiders....", "links": ["http://doc.scrapy.org/en/latest/topics/spiders.html#scrapy.contrib.spiders.SitemapSpider.sitemap_alternate_links"], "channel": "scrapy"},
{"date": "2014-09-16T03:09:04.736252+00:00", "nick": "nyov", "message": "basically, sitemapspider will already follow index sitemaps to the leaf nodes. but if your sitemap defines 'alternate' url schemes, this will crawl them too", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T03:13:02.237253+00:00", "nick": "o_o_", "message": "@nyov the spider is not following in my case", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T03:16:25.205355+00:00", "nick": "nyov", "message": "maybe you declared sitemap_follow wrongly?", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T03:17:04.295284+00:00", "nick": "o_o_", "message": "nyov: http://www.salon.com/sitemap.xml", "links": ["http://www.salon.com/sitemap.xml"], "channel": "scrapy"},
{"date": "2014-09-16T03:17:24.155486+00:00", "nick": "o_o_", "message": "i am working on this sitemap", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T03:19:14.808733+00:00", "nick": "o_o_", "message": "sitemap_urls = [\"http://www.salon.com/sitemap.xml\"]", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T03:19:28.712813+00:00", "nick": "o_o_", "message": "sitemap_follow = ['http://sitemaps.salon.com/\\w+']", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T03:19:47.323359+00:00", "nick": "o_o_", "message": "sitemap_rules = [('\\d{4}/\\d{2}/\\d{2}/\\w+', 'parse_post')]", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T03:20:36.182451+00:00", "nick": "nyov", "message": "your sitemap_follow regex is likely wrong. I don't think you need to specify it at all", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T03:20:47.368273+00:00", "nick": "nyov", "message": "http://doc.scrapy.org/en/latest/topics/spiders....", "links": ["http://doc.scrapy.org/en/latest/topics/spiders.html#scrapy.contrib.spiders.SitemapSpider.sitemap_follow"], "channel": "scrapy"},
{"date": "2014-09-16T03:20:58.869247+00:00", "nick": "nyov", "message": "\"By default, all sitemaps are followed.\"", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T03:21:49.528685+00:00", "nick": "nyov", "message": "but if you want to declare it, I think removing the host part would suffice (the spider is already bounded to the domain)", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T03:22:07.538270+00:00", "nick": "nyov", "message": "such as sitemap_follow = ['/sitemap_shops']", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T03:22:34.276632+00:00", "nick": "o_o_", "message": "what about sitemap_rules", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T03:22:58.927734+00:00", "nick": "o_o_", "message": "does it imply to all the urls", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T03:24:26.222243+00:00", "nick": "nyov", "message": "i'll have to look", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T03:25:32.397338+00:00", "nick": "nyov", "message": "nope, that doesn't apply to other sitemap urls from sitemapindex pages", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T03:25:56.573096+00:00", "nick": "nyov", "message": "only to \"final\" urls", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T03:28:00.707287+00:00", "nick": "nyov", "message": "so if you leave sitemap_follow empty, and don't define sitemap_rules, all the URLs in those sitemaps will hit your parse() method", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T03:28:48.753835+00:00", "nick": "o_o_", "message": "the spider gets stuck in first sitemap if i do so", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T03:30:18.197672+00:00", "nick": "o_o_", "message": "2014-09-16 09:14:31+0545 [salon] DEBUG: Filtered offsite request to 'sitemaps.sa lon.com': <GET http://sitemaps.salon.com/sitemap_recent.xml&gt; 2014-09-16 09:14:31+0545 [salon] INFO: Closing spider (finished)", "links": ["http://sitemaps.salon.com/sitemap_recent.xml&amp;gt"], "channel": "scrapy"},
{"date": "2014-09-16T03:34:59.523164+00:00", "nick": "o_o_", "message": "nyov: found the solution", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T03:35:05.461970+00:00", "nick": "o_o_", "message": "its was due to allowed domain", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T03:35:07.438154+00:00", "nick": "nyov", "message": "o_o_: add sitemaps.salon.com to 'allowed_domains'", "links": ["http://sitemaps.salon.com"], "channel": "scrapy"},
{"date": "2014-09-16T03:35:12.094439+00:00", "nick": "nyov", "message": "right", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T03:35:14.586898+00:00", "nick": "o_o_", "message": "yeah", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T03:35:29.398464+00:00", "nick": "o_o_", "message": "thank you for your help", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T03:35:40.921619+00:00", "nick": "nyov", "message": "no problem", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T10:02:42.961059+00:00", "nick": "brotherBox", "message": "Hi. Scrapy has a downloader middleware which limits the download time of a resource to 180 seconds. I could deactivate that via the DOWNLOADER_MIDDLEWARES setting. However, ScrapyAgent has a connectTimeout setting, giving a resource only 10 seconds to arrive. Is there any way to deactivate that?", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T10:16:57.649387+00:00", "nick": "nyov", "message": "brotherBox: timeout = request.meta.get('download_timeout') or self._connectTimeout", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T10:17:21.910241+00:00", "nick": "nyov", "message": "so, yes you can change the timeout, if your connections really take that long to establish?", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T10:22:05.124599+00:00", "nick": "brotherBox", "message": "nyov: its not that the connections take that long to establish, but I download files of around 100MB from a server with 100kb/s download speed. After deactivating the downloader middleware, either the ScrapyAgent or ScrapyHTTPPageGetter (as I found out by grepping through the module folder for \"took longer than\") cancels the connection after 10 seconds. I verified with wireshark that data is transferred to", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T10:22:08.064752+00:00", "nick": "brotherBox", "message": "me, so whatever is interfering is not interfering with establishing a connection but keeping it open.", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T10:24:38.381235+00:00", "nick": "nyov", "message": "hum. right, normally I guess 10 seconds should be enough to fetch a normal page", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T10:25:05.534961+00:00", "nick": "nyov", "message": "but with the downloader middleware it stays open 180 seconds?", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T10:25:36.442778+00:00", "nick": "nyov", "message": "which middleware is that?", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T10:26:52.916179+00:00", "nick": "brotherBox", "message": "Thats entirely right. Its the scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware. I might be doing something wrong/non-standard when it comes to fetching, I fetch content in my program and then put it in an item.", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T10:27:43.025435+00:00", "nick": "nyov", "message": "yeah, I just found it. the downloader middleware set the request.meta download_timeout, with it absent, it defaults to 10 seconds", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T10:28:13.969739+00:00", "nick": "nyov", "message": "have you tried changing DOWNLOAD_TIMEOUT setting to zero?", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T10:28:44.543855+00:00", "nick": "nyov", "message": "or any sufficiently high value should work", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T10:31:20.719990+00:00", "nick": "brotherBox", "message": "I have not as there was no mention of the special value 0. I'll try it out", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T10:31:57.167281+00:00", "nick": "nyov", "message": "that just popped into my head. zero might not work, but usually it's synonymous with 'infinite' in such settings", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T10:32:08.349305+00:00", "nick": "brotherBox", "message": "Or -1", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T10:32:11.877458+00:00", "nick": "nyov", "message": "or that", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T10:32:42.006333+00:00", "nick": "nyov", "message": "or None, if that works. hm, try these then ;)", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T10:33:01.675382+00:00", "nick": "nyov", "message": "though it might not be a bad idea to have an upper bounding, say 3 hours or so anyway", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T10:33:27.476841+00:00", "nick": "nyov", "message": "so your spider doesn't hang forever if it got tarpitted by the remote or something", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T10:37:02.714593+00:00", "nick": "brotherBox", "message": "Nope, 0 doesnt solve the 180s problem, trying -1", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T10:38:00.200556+00:00", "nick": "nyov", "message": "why not read the source, might be faster :D", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T10:40:00.071060+00:00", "nick": "brotherBox", "message": "Because I am stupid and might make mistakes", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T10:46:00.439684+00:00", "nick": "nyov", "message": "hah. okay, i checked. did -1 work? None and 0 won't as I see", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T10:51:18.464287+00:00", "nick": "nyov", "message": "hmm, and anyway reactor.callLater doesn't mean 'never' but 'asap'. guess this functionality would need a change in code", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T10:51:51.459981+00:00", "nick": "nyov", "message": "* reactor.callLater(0, ...) i meant", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T10:53:52.089114+00:00", "nick": "brotherBox", "message": "I just set it to 7200 and see how it works for me lol", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T10:54:37.447805+00:00", "nick": "nyov", "message": "right on! :)", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T10:56:13.139643+00:00", "nick": "nyov", "message": "but I'll throw an issue on the tracker for discussion of this", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T10:56:50.061460+00:00", "nick": "brotherBox", "message": "Nice, thanks!", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T15:55:01.030911+00:00", "nick": "nyov", "message": "Ouch, I ffck'ed up :(", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T15:56:24.215038+00:00", "nick": "nyov", "message": "There I went, happily coding a httpcache storage class around a from_crawler classmethod. Until I FINALLY tested it. damn, but it *looked* nice.", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T15:57:10.262728+00:00", "nick": "nyov", "message": "oh what a waste. time for some shuteye", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T17:23:55.575482+00:00", "nick": "wolever", "message": "Hey! I'm looking to build a scraper that finds links, checks their domain against a database (a check which might involve accessing an HTTP API), then follows those links if they match. What would be the right way to think about that problem in the context of Scrapy?", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T17:45:05.189081+00:00", "nick": "bluesteal", "message": "quick question about passing data in a crawlspider, quick code snippet", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T17:45:05.847485+00:00", "nick": "bluesteal", "message": "http://pastebin.com/Gp1DGERY", "links": ["http://pastebin.com/Gp1DGERY"], "channel": "scrapy"},
{"date": "2014-09-16T17:45:14.474507+00:00", "nick": "bluesteal", "message": "my second function never actually gets called", "links": [], "channel": "scrapy"},
{"date": "2014-09-16T17:50:15.107347+00:00", "nick": "bluesteal", "message": "ok that was a bit, I seem to have figured it out, not sure why yielding or returning the request wasn't working", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T01:40:54.954842+00:00", "nick": "bluesteal", "message": "Running the crawl spider how would I be able to access the crawling url queue?", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T01:42:10.810462+00:00", "nick": "bluesteal", "message": "I'd like to say if a url meets a certain criteria save all the links from that url to a list", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T02:49:37.743094+00:00", "nick": "nyov", "message": "bluesteal: crawling url queue? when a url meets your criteria, send it through a LinkExtractor, bang there is your link list", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T03:45:49.386832+00:00", "nick": "bluesteal", "message": "nyov I am using a crawlspider I crawl a domain just parsing all the url's coming through which works well so far, I am just a little confused on the logic to do this next part.", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T03:46:24.691009+00:00", "nick": "bluesteal", "message": "In the parse_urls function I screen out most of the urls and only a few make the list of urls that I would like to extract all the links from", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T03:46:56.278267+00:00", "nick": "bluesteal", "message": "my thinking was, when I screen out the url's save that to a database that I can then go back to and pull items out of the database to crawl", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T03:47:42.759217+00:00", "nick": "bluesteal", "message": "but that would require two spiders just to crawl and get the list of url's that I'm interested in, do you have any opinion on a better solution as opposed to the one that i'm having?", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T03:53:34.744723+00:00", "nick": "nyov", "message": "bluesteal: what you're trying to do is basic functionality of crawlspider -- you're just trying to do it in a totally obscure and overly complex fashion :D", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T03:54:59.071416+00:00", "nick": "nyov", "message": "1. CrawlSpider has Rule()s to screen out urls. with a regex it only fetches all the links that match a path, or with some callback it matches on content (imho).", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T03:56:03.529464+00:00", "nick": "nyov", "message": "2. Scrapy's Scheduler class is the part that manages urls/requests. It handles all the queueing up and prioritizing of which url to crawl next or whenever.", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T03:57:21.320606+00:00", "nick": "nyov", "message": "3. Scheduler has a Queue to do so. This is a MemoryQueue by default (kept in RAM), but can be changed to be a DiskQueue. Now if you set this to a diskqueue, you'll get all the URL's youwant to crawl 'later' on disk.", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T03:58:44.254720+00:00", "nick": "nyov", "message": "If you write another backend for the Queue, you can also store it in a database. see RedisQueue for an example.", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T03:59:29.610901+00:00", "nick": "nyov", "message": "https://github.com/darkrho/scrapy-redis", "links": ["https://github.com/darkrho/scrapy-redis"], "channel": "scrapy"},
{"date": "2014-09-17T04:04:51.276617+00:00", "nick": "nyov", "message": "bluesteal: but of course there are also a million ways to plug things together with scrapy. so if you want to use a middleware instead for logging urls for later, do that. Want it in the spider code? sure thing, just need to write it.", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T04:05:59.062037+00:00", "nick": "nyov", "message": "see this piece of crap...err code: https://github.com/nyov/scrapyext/tree/master/s...", "links": ["https://github.com/nyov/scrapyext/tree/master/scrapyext/visited"], "channel": "scrapy"},
{"date": "2014-09-17T04:06:44.610384+00:00", "nick": "nyov", "message": "one middleware logs all the visited urls, and the other will ignore those on the next run. you could whip something up from that code", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T04:11:29.757892+00:00", "nick": "bluesteal", "message": "so many ways to go about this wow, I am leaning towards using the spider because I can pull in a lot of other files there to do the heavy work. When the spider is running if in the parse_urls method I call a function that sleeps for a few seconds that slows down the spider a bit but doesn't affect the url queue, is that correct?", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T04:12:15.133620+00:00", "nick": "bluesteal", "message": "I don't mind a slower crawl the links that i'm looking for just needs to be from pages with certain metrics", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T04:12:55.954408+00:00", "nick": "bluesteal", "message": "for example I'm using alexa api to find sites w/ certain daily traffic", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T04:14:00.739445+00:00", "nick": "bluesteal", "message": "I'll crawl a site, grab a url, check it against the alexa numbers, if it meets the requirements, I'd then like to crawl and get all urls from that page", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T04:15:11.471778+00:00", "nick": "bluesteal", "message": "that's why I was saying crawl the web->check stats against alexa api [it's limited so download delay is ever 15-30 seconds]->if site meets the criteria->save that url to crawl later and get all the links from that page", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T04:19:17.629727+00:00", "nick": "nyov", "message": "bluesteal: if you sleep in the spider like you would in a synchronous python program, it'll block the whole reactor. that means requests could time out or other nasties. You could just tell scrapy to only fetch a certain number of links by setting DOWNLOAD_DELAY", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T04:19:50.059910+00:00", "nick": "nyov", "message": "http://doc.scrapy.org/en/latest/topics/settings...", "links": ["http://doc.scrapy.org/en/latest/topics/settings.html#std:setting-DOWNLOAD_DELAY"], "channel": "scrapy"},
{"date": "2014-09-17T04:20:53.263487+00:00", "nick": "nyov", "message": "and if you want some randomness in the time it waits, http://doc.scrapy.org/en/latest/topics/autothro...", "links": ["http://doc.scrapy.org/en/latest/topics/autothrottle.html"], "channel": "scrapy"},
{"date": "2014-09-17T04:21:57.787269+00:00", "nick": "bluesteal", "message": "the sleep is actually to limit the alexa stats api", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T04:22:29.424128+00:00", "nick": "nyov", "message": "anyway, sure you can do your grab&check in the spider and save it for a different spider or later run", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T04:23:24.687982+00:00", "nick": "nyov", "message": "and how is the alexa stats api different than any other site you crawl? the spider doesn't care. download delay is download delay", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T04:23:30.438234+00:00", "nick": "nyov", "message": ":P", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T04:24:05.137774+00:00", "nick": "bluesteal", "message": "I might just scrape all the urls to a database, pull out the data from the db and run the ranking test on the data, drop the data from the db that doesn't meet the requirements, create a separate spider to recrawl those links", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T04:24:30.809556+00:00", "nick": "bluesteal", "message": "I'm using the api not crawling the alexa site", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T04:25:09.899639+00:00", "nick": "bluesteal", "message": "say i pull down domain.com/test.html pass that url to alexa's api to get the stats. There's a rate limit plus I don't want to crash any sites, a slow crawl will be ok", "links": ["http://domain.com/test.html"], "channel": "scrapy"},
{"date": "2014-09-17T04:25:42.273867+00:00", "nick": "bluesteal", "message": "when the stats come back from alexa I check to see if it meets certain requirements, if it does I save that url, if not just ditch it", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T04:26:30.573644+00:00", "nick": "bluesteal", "message": "once the url has been saved then I need to go back and crawl the url ex: domain.com/test.html and get all those links from that particular page, then do some more testing with those links", "links": ["http://domain.com/test.html"], "channel": "scrapy"},
{"date": "2014-09-17T04:26:36.529562+00:00", "nick": "nyov", "message": "what kind of api is that then, on the alexa site? soap,xmlrpc?", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T04:27:34.016990+00:00", "nick": "bluesteal", "message": "so the criteria that I'm looking for can't really be done w/ a rule per se", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T04:27:47.040557+00:00", "nick": "bluesteal", "message": "I'm using these python package to get alexa api stuff: python-awis", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T04:29:00.222252+00:00", "nick": "bluesteal", "message": "historic web traffic and sites linking in data mostly", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T04:30:38.617049+00:00", "nick": "nyov", "message": "ah okay. well, good code would be writing deferred() calls instead of blocking the crawler thread, in the asynchronous way of twisted. but I get the picture", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T08:12:43.896835+00:00", "nick": "scmp", "message": "Hi, is it possible to schedule Requests for the next run of the spider? I need to visit some pages again until a condition is met.", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T08:14:17.918422+00:00", "nick": "scmp", "message": "(First run still needs to produce an Item.)", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T08:24:39.931900+00:00", "nick": "nyov", "message": "scmp: you mean persistend disk storage?", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T08:26:58.351362+00:00", "nick": "nyov", "message": "you can also re-visit pages indefinitely in the same run if you'd like. you just have disable the duplicate detection conditionally", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T09:09:45.919905+00:00", "nick": "scmp", "message": "what is a good way to get persistence across jobs? The DotScrapy module is already on, just dump my own file there?", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T09:19:04.150053+00:00", "nick": "nyov", "message": "sorry, what is DotScrapy?", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T09:20:31.634786+00:00", "nick": "scmp", "message": "http://doc.scrapinghub.com/addons.html#dotscrap...", "links": ["http://doc.scrapinghub.com/addons.html#dotscrapy-persistence"], "channel": "scrapy"},
{"date": "2014-09-17T09:20:36.068979+00:00", "nick": "nyov", "message": "persistence across jobs depends, maybe jobs are good enough? http://doc.scrapy.org/en/latest/topics/jobs.html", "links": ["http://doc.scrapy.org/en/latest/topics/jobs.html"], "channel": "scrapy"},
{"date": "2014-09-17T09:21:39.425370+00:00", "nick": "nyov", "message": "ooh nice and shiny. haven't read about this one before", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T09:22:29.354188+00:00", "nick": "scmp", "message": "it's scrapinghub related", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T09:24:36.273925+00:00", "nick": "nyov", "message": "well, for persistence of Items.. I think that's obvious. For persistence of URLs, you could either just save to a file and write a loader which re-loads some file on startup", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T09:24:54.588209+00:00", "nick": "scmp", "message": "yeah, i was thinking of doing the later...", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T09:25:23.005898+00:00", "nick": "nyov", "message": "or use something like https://github.com/scrapy/queuelib, which is what scrapy's using internally to manage URL queues in memory. Though you can switch it to a disk-based queue, and voila, you have persistence", "links": ["https://github.com/scrapy/queuelib"], "channel": "scrapy"},
{"date": "2014-09-17T09:26:14.883729+00:00", "nick": "nyov", "message": "(of scheduler state)", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T09:26:17.779701+00:00", "nick": "scmp", "message": "oh very nice", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T09:27:40.494814+00:00", "nick": "scmp", "message": "perfect, that will do. thanks!", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T09:28:49.522823+00:00", "nick": "nyov", "message": "I can't seem to find the docs again", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T09:34:14.377251+00:00", "nick": "nyov", "message": "well, I seem to have misplaced my memory.", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T09:38:38.345549+00:00", "nick": "nyov", "message": "anyway, I think the magic is magic enough, to not need to know it. using a JOBDIR and spider.state attribute will do all the things: http://doc.scrapy.org/en/latest/topics/jobs.htm...", "links": ["http://doc.scrapy.org/en/latest/topics/jobs.html#keeping-persistent-state-between-batches"], "channel": "scrapy"},
{"date": "2014-09-17T11:47:14.293738+00:00", "nick": "SthNotTaken", "message": "Is there a convenient way to scrape articles from sites such as economist.com", "links": ["http://economist.com"], "channel": "scrapy"},
{"date": "2014-09-17T12:05:57.496085+00:00", "nick": "nyov", "message": "SthNotTaken: define convenient? super convenient: ask someone to make it happen for you :P", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T12:07:24.924953+00:00", "nick": "SthNotTaken", "message": "I mean going around paywalls", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T12:08:03.914414+00:00", "nick": "nyov", "message": "sorry, I don't know what that is", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T12:08:25.036109+00:00", "nick": "SthNotTaken", "message": "\"you can only view 5 articles/month, then you must subscribe and pay us money\"", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T12:09:03.859922+00:00", "nick": "nyov", "message": "ah. that depends on how they figure out what 5 visits are. do you need to log in?", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T13:42:44.485174+00:00", "nick": "Jake232", "message": "SthNotTaken: They ussually don't block you if you set your referrer as Google", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T13:42:53.822133+00:00", "nick": "Jake232", "message": "that works with 90% of paywall news sites", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T13:43:00.642159+00:00", "nick": "SthNotTaken", "message": "Jake232: that can be faked in a header, yeah?", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T13:43:06.850019+00:00", "nick": "Jake232", "message": "Yeah", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T13:43:12.852068+00:00", "nick": "Jake232", "message": "referrer header", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T13:43:59.517117+00:00", "nick": "Jake232", "message": "Referer: https://www.google.com", "links": ["https://www.google.com"], "channel": "scrapy"},
{"date": "2014-09-17T13:44:08.424535+00:00", "nick": "Jake232", "message": "Or something along those lines usually works", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T13:55:13.952443+00:00", "nick": "nyov", "message": "does that still work? I didn't think people were still stupid enough to accept a Googlebot UserAgent without verifying the bot's netblock", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T13:55:32.336280+00:00", "nick": "nyov", "message": "but if it works, use it", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T13:56:10.583772+00:00", "nick": "Jake232", "message": "It's not pretending the be googlebot", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T13:56:17.332466+00:00", "nick": "Jake232", "message": "It's just pretending you came from a google search", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T13:56:27.634622+00:00", "nick": "Jake232", "message": "They only paywall direct links to the articles", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T13:56:39.369404+00:00", "nick": "Jake232", "message": "if you find an article via a google search, they just let you view it without the paywall", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T13:56:44.035017+00:00", "nick": "nyov", "message": "oh really. how funny :D", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T13:56:45.180991+00:00", "nick": "Jake232", "message": "works for almost every news site", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T13:56:54.277305+00:00", "nick": "nyov", "message": "haha awesome trick", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T13:58:24.951060+00:00", "nick": "nyov", "message": "otherwise, best bet is a big ip address space. if the target site speaks IPv6, I'd try that", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T13:59:10.628904+00:00", "nick": "Jake232", "message": "I should add the paywall trick to my scraping article actually", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T13:59:20.098069+00:00", "nick": "Jake232", "message": "I wrote a huge article on python scraping earlier this year, forgot to add that to it", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T13:59:27.277818+00:00", "nick": "Jake232", "message": "Thanks for the reminder :P", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T14:00:05.580786+00:00", "nick": "nyov", "message": "yeah, I never scraped news sites before. learn something new every day ;)", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T14:00:36.808378+00:00", "nick": "Jake232", "message": "This article isn't related to scrapy, just scraping in general", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T14:00:50.146845+00:00", "nick": "Jake232", "message": "but it's well worth a read (shameless self promotion)", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T14:00:50.847869+00:00", "nick": "Jake232", "message": "http://jakeaustwick.me/python-web-scraping-reso...", "links": ["http://jakeaustwick.me/python-web-scraping-resource/"], "channel": "scrapy"},
{"date": "2014-09-17T14:02:10.506646+00:00", "nick": "Jake232", "message": "I'll add the paywall trick to it later too :)", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T14:06:07.805038+00:00", "nick": "nyov", "message": "the code blocks look bad for me. grey text on white background with yellow selection text. huh, can't read that at all :(", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T14:07:36.928441+00:00", "nick": "nyov", "message": "ah, ajax scraping. phantomjs is nice. and there was a python version in older releases. but now you could also give https://github.com/scrapinghub/splash a try", "links": ["https://github.com/scrapinghub/splash"], "channel": "scrapy"},
{"date": "2014-09-17T14:08:14.611375+00:00", "nick": "nyov", "message": "oh, actually the very last codeblock on the page looks okay", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T14:25:39.503247+00:00", "nick": "Jake232", "message": "the codeblocks should be syntax highlighted", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T14:25:41.398191+00:00", "nick": "Jake232", "message": "if you have JS on", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T14:26:00.832694+00:00", "nick": "Jake232", "message": "nyov: Are they not for you?", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T14:31:25.417118+00:00", "nick": "nyov", "message": "ah, that'll be it. usually running around with noscript", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T14:33:52.179757+00:00", "nick": "nyov", "message": "nothing against javascript, but it's way too easy to shoot a browser with it. and I'd rather be safe than sorry. heard about the \"new\" canvas-style tracking? doesn't happen with noscript :)", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T14:34:27.556417+00:00", "nick": "Jake232", "message": "I've come to terms with everything I do being tracked", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T14:34:40.412983+00:00", "nick": "Jake232", "message": "I try not to do anything too shady, and I don't think I'll have any issues :)", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T14:39:46.737479+00:00", "nick": "nyov", "message": "tracking is the least problem. it's more about computer security. ads and stuff already get blocked by blackholing most adserver netblocks so that's not it either. but most browser security flaws involve scripting", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T14:46:44.732844+00:00", "nick": "nyov", "message": "and did you know openx (the big free php adserver software) had a code execution backdoor in it's releases for around 5 years (Ithinkitwas)? any insider could try jacking your browser remotely through 'reputable' adserver networks. and those ads all run on javascript :D", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T14:47:20.664460+00:00", "nick": "nyov", "message": "but I'm getting very OT. back to work", "links": [], "channel": "scrapy"},
{"date": "2014-09-17T19:18:34.107114+00:00", "nick": "localhost_", "message": "scrapyd-deploy won't include my middlewares.py file after deployment, how do I add the middleware to the deployed .egg?", "links": [], "channel": "scrapy"},
{"date": "2014-09-18T11:50:32.752574+00:00", "nick": "localhost_", "message": "http://stackoverflow.com/questions/25898851/imp...", "links": ["http://stackoverflow.com/questions/25898851/importerror-error-loading-object-scrap-middlewares-randomuseragentmiddleware"], "channel": "scrapy"},
{"date": "2014-09-19T06:15:26.938755+00:00", "nick": "Guest51268", "message": "If I wouldn't like to export a specified field of item, is there any setting in item exporter?", "links": [], "channel": "scrapy"},
{"date": "2014-09-19T06:16:42.723063+00:00", "nick": "Guest51268", "message": "default item exporter's always export all of fields, it is not what I want", "links": [], "channel": "scrapy"},
{"date": "2014-09-19T06:31:36.017494+00:00", "nick": "nyov", "message": "Guest51268: why would you set the field value if you don't want to export it?", "links": [], "channel": "scrapy"},
{"date": "2014-09-19T06:33:23.503184+00:00", "nick": "nyov", "message": "if you need more fine-grained control, write a item pipeline using the exporter, or subclass the exporter and modify it to your needs", "links": [], "channel": "scrapy"},
{"date": "2014-09-19T06:34:12.570990+00:00", "nick": "Guest51268", "message": "I need condition to decide how to process the itme", "links": [], "channel": "scrapy"},
{"date": "2014-09-19T06:34:33.688781+00:00", "nick": "Guest51268", "message": "I need some conditions to decide how to process the item", "links": [], "channel": "scrapy"},
{"date": "2014-09-19T06:35:38.668284+00:00", "nick": "Guest51268", "message": "as you says, I have to  subclass the exporter", "links": [], "channel": "scrapy"},
{"date": "2014-09-19T06:36:11.073155+00:00", "nick": "nyov", "message": "Guest51268: actually there is an option", "links": [], "channel": "scrapy"},
{"date": "2014-09-19T06:36:25.071191+00:00", "nick": "Guest51268", "message": "how?", "links": [], "channel": "scrapy"},
{"date": "2014-09-19T06:36:48.524794+00:00", "nick": "nyov", "message": "fields_to_export, but find it in the docs on how to use it, I only found it in the source", "links": [], "channel": "scrapy"},
{"date": "2014-09-19T06:38:14.646267+00:00", "nick": "Guest51268", "message": "ok, I view it. thanks very much", "links": [], "channel": "scrapy"},
{"date": "2014-09-19T12:03:05.471706+00:00", "nick": "Zladivliba", "message": "hello !", "links": [], "channel": "scrapy"},
{"date": "2014-09-19T12:03:41.914598+00:00", "nick": "Zladivliba", "message": "I just installed scrapyd on a freebsd server but I don't know where's the configuration file, any ideas ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-19T12:03:51.402263+00:00", "nick": "Zladivliba", "message": "It's not /etc/scrapyd", "links": [], "channel": "scrapy"},
{"date": "2014-09-19T12:04:07.886779+00:00", "nick": "Zladivliba", "message": "i've tried scrapyd --help but there's nothing about configuration file", "links": [], "channel": "scrapy"},
{"date": "2014-09-19T12:15:05.971490+00:00", "nick": "nyov", "message": "depends what you installed, how you installed it... i only know about the ubuntu package", "links": [], "channel": "scrapy"},
{"date": "2014-09-19T12:15:34.947846+00:00", "nick": "nyov", "message": "if you downloaded source from github you should kow where that landed, or where pip installed it for you", "links": [], "channel": "scrapy"},
{"date": "2014-09-19T16:21:51.545721+00:00", "nick": "wirher", "message": "hello", "links": [], "channel": "scrapy"},
{"date": "2014-09-19T16:22:36.158485+00:00", "nick": "wirher", "message": "anybody alive here? ^^", "links": [], "channel": "scrapy"},
{"date": "2014-09-19T17:46:25.826029+00:00", "nick": "nramirezuy", "message": "nope, we are all here because it's free", "links": [], "channel": "scrapy"},
{"date": "2014-09-19T20:12:37.209834+00:00", "nick": "Zladivliba", "message": "hzllo", "links": [], "channel": "scrapy"},
{"date": "2014-09-20T06:10:39.570815+00:00", "nick": "bluesteal", "message": "I'm having a bit of trouble calling a function in a base spider, the function runs fine once but then after update_url the new request doesn't start parsing the url again: http://pastebin.com/ibxnR5ez", "links": ["http://pastebin.com/ibxnR5ez"], "channel": "scrapy"},
{"date": "2014-09-20T06:10:41.942726+00:00", "nick": "bluesteal", "message": "any tips?", "links": [], "channel": "scrapy"},
{"date": "2014-09-20T13:22:28.066642+00:00", "nick": "bluesteal", "message": "hello guys earlier I posted a code snippet on pastebin for a spider that would call a function at the end to update some variables to scrape again, that code wasn't working but when I break out the code from the function and just slap it right in line everything works fine", "links": [], "channel": "scrapy"},
{"date": "2014-09-20T13:23:08.431030+00:00", "nick": "bluesteal", "message": "I would like to break up the code so that it's easily maintainable and modular without have huge chunks of code that I have to read through to spider stuff, I was wondering  if anyone knew why that was happening", "links": [], "channel": "scrapy"},
{"date": "2014-09-20T13:32:05.220892+00:00", "nick": "nyov", "message": "on a weekend? hmm, lets see... that'll cost extra", "links": [], "channel": "scrapy"},
{"date": "2014-09-20T13:33:16.646151+00:00", "nick": "nyov", "message": "could be that you need to say \"return self.update_url()\"", "links": [], "channel": "scrapy"},
{"date": "2014-09-20T13:33:34.993494+00:00", "nick": "nyov", "message": "since you need to return a deferred from parse_category, not None", "links": [], "channel": "scrapy"},
{"date": "2014-09-20T13:34:06.188448+00:00", "nick": "nyov", "message": "or is it getting called?", "links": [], "channel": "scrapy"},
{"date": "2014-09-20T13:37:36.095881+00:00", "nick": "bluesteal", "message": "nyov your so right and I just came across this stackoverflow kinda explaining why also: http://stackoverflow.com/questions/22648475/und...", "links": ["http://stackoverflow.com/questions/22648475/understanding-callback-in-scrapy-python"], "channel": "scrapy"},
{"date": "2014-09-20T13:38:06.814242+00:00", "nick": "bluesteal", "message": "python/ scrapy is something to wrap my head around for sure", "links": [], "channel": "scrapy"},
{"date": "2014-09-20T13:41:13.559089+00:00", "nick": "bluesteal", "message": "the functions should return a request or items", "links": [], "channel": "scrapy"},
{"date": "2014-09-20T13:45:06.150336+00:00", "nick": "nyov", "message": "it's a bit like being in a labyrinth. [twisted reactor] being the starting point, to not get lost you need a thread to run with, always return'ing back to the reactor. you can run through lots of rooms (deferreds), but all need a RETURN to the reactor loop", "links": [], "channel": "scrapy"},
{"date": "2014-09-20T13:45:30.622093+00:00", "nick": "nyov", "message": "cutting the thread will get you lost. oh well, that's a bad analogy. better to understand it for real", "links": [], "channel": "scrapy"},
{"date": "2014-09-20T13:46:56.427071+00:00", "nick": "nyov", "message": "I had my issues with twisted's way of doing async, still do. javascript's model and style of writing is so much simpler to grasp", "links": [], "channel": "scrapy"},
{"date": "2014-09-20T13:59:31.118443+00:00", "nick": "bluesteal", "message": "yes it is quite a lot", "links": [], "channel": "scrapy"},
{"date": "2014-09-20T13:59:38.710520+00:00", "nick": "bluesteal", "message": "I looked at twisted docs and said umm later", "links": [], "channel": "scrapy"},
{"date": "2014-09-20T13:59:57.070154+00:00", "nick": "nyov", "message": ";)", "links": [], "channel": "scrapy"},
{"date": "2014-09-20T14:00:25.506826+00:00", "nick": "bluesteal", "message": "I'll spend more time within scrapy for now I like it and quite powerful, i'll deep dive into twisted when I have nothing else to do", "links": [], "channel": "scrapy"},
{"date": "2014-09-20T14:01:14.757055+00:00", "nick": "bluesteal", "message": "scrapy is really awesome for so many things, browsing the web w/o a browser is so much more powerful", "links": [], "channel": "scrapy"},
{"date": "2014-09-20T14:01:33.394343+00:00", "nick": "bluesteal", "message": "if you enjoy eating the beef and not the plate", "links": [], "channel": "scrapy"},
{"date": "2014-09-20T14:46:49.183057+00:00", "nick": "nyov", "message": "haha", "links": [], "channel": "scrapy"},
{"date": "2014-09-20T16:23:47.782648+00:00", "nick": "nyov", "message": "oh this is hot. finally added on-the-fly de/compression to my httpcache and about 1gb of cache deflates down to about 150mb stored.", "links": [], "channel": "scrapy"},
{"date": "2014-09-21T07:54:32.181358+00:00", "nick": "Zladivliba", "message": "hello", "links": [], "channel": "scrapy"},
{"date": "2014-09-21T07:54:42.011300+00:00", "nick": "Zladivliba", "message": "I'm trying to use scrapyd", "links": [], "channel": "scrapy"},
{"date": "2014-09-21T07:55:04.427153+00:00", "nick": "Zladivliba", "message": "and when I do this : scrapyd-deploy -l", "links": [], "channel": "scrapy"},
{"date": "2014-09-21T07:55:08.385876+00:00", "nick": "Zladivliba", "message": "I get nothing...", "links": [], "channel": "scrapy"},
{"date": "2014-09-21T07:55:43.038049+00:00", "nick": "Zladivliba", "message": "although scrapyd is running (i can connect through the web interface)", "links": [], "channel": "scrapy"},
{"date": "2014-09-21T07:56:55.738073+00:00", "nick": "afreak_", "message": "hi, im trying to follow the scrapy tutorial http://doc.scrapy.org/en/latest/intro/tutorial.... , but im getting error on my spider class DmozSpider(scrapy.Spider): AttributeError: 'module' object has no attribute 'Spider'", "links": ["http://doc.scrapy.org/en/latest/intro/tutorial.html"], "channel": "scrapy"},
{"date": "2014-09-21T07:58:37.380885+00:00", "nick": "afreak_", "message": "has there been some changes ? I tried spider with small letters but then i got this error     module.__init__() takes at most 2 arguments (3 given)", "links": [], "channel": "scrapy"},
{"date": "2014-09-21T08:10:42.775149+00:00", "nick": "afreak_", "message": "aha now its scrapy.spider.BaseSpider :p", "links": [], "channel": "scrapy"},
{"date": "2014-09-21T08:29:57.414023+00:00", "nick": "afreak_", "message": "hmm why do i have version 0.14.4 :p thats really old right ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-21T09:01:37.680434+00:00", "nick": "bluesteal", "message": "hey afreak_ go to pastebin.com and copy your spider class", "links": ["http://pastebin.com"], "channel": "scrapy"},
{"date": "2014-09-21T09:01:46.901882+00:00", "nick": "bluesteal", "message": "paste it in here let us see", "links": [], "channel": "scrapy"},
{"date": "2014-09-21T09:02:40.965701+00:00", "nick": "bluesteal", "message": "afreak_ the latest version is 0.24", "links": [], "channel": "scrapy"},
{"date": "2014-09-21T09:11:03.263414+00:00", "nick": "afreak_", "message": "bluesteal: yeah my problem was that i was using 0.14.4 and following tutorial for 0.24 :p", "links": [], "channel": "scrapy"},
{"date": "2014-09-21T09:11:11.872508+00:00", "nick": "afreak_", "message": "had installed for package manager", "links": [], "channel": "scrapy"},
{"date": "2014-09-21T09:12:01.243710+00:00", "nick": "afreak_", "message": "but now im using the newest and evrything is going smooth :)", "links": [], "channel": "scrapy"},
{"date": "2014-09-21T09:15:49.599310+00:00", "nick": "bluesteal", "message": "afreak_ glad you got it solved", "links": [], "channel": "scrapy"},
{"date": "2014-09-21T13:18:26.359985+00:00", "nick": "bluesteal", "message": "this is strange, in my items pipeline i'm getting a synthax error on my semicolon but without it i'm getting a depreciation error, any ideas?", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T00:40:47.634991+00:00", "nick": "bluesteal", "message": "man that last question was quite silly of me.... :/", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T07:43:08.953226+00:00", "nick": "rohitt", "message": "Hello everyone!", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T07:44:38.290263+00:00", "nick": "Zladivliba", "message": "hello !", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T07:44:47.080553+00:00", "nick": "rohitt", "message": "Is it true that, while scraping web-pages, if I am downloading images, the image field will be populated, and the images will be downloaded before we move on to the next pipeline code?", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T07:44:51.190862+00:00", "nick": "rohitt", "message": "Hey Zladivliba!", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T07:45:22.706139+00:00", "nick": "Zladivliba", "message": "I have a quick question : how do I get the current URL inside parse_item ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T07:45:26.035139+00:00", "nick": "Zladivliba", "message": "rohitt: !!!!", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T07:45:42.600399+00:00", "nick": "rohitt", "message": "Zladivliba, response.url", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T07:45:57.111744+00:00", "nick": "Zladivliba", "message": "ok thanks I'm going to investigate that", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T07:51:31.637689+00:00", "nick": "Newbie0086", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T07:52:14.765222+00:00", "nick": "Newbie0086", "message": "hi there", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T07:52:27.033777+00:00", "nick": "Newbie0086", "message": "anyone can help me?", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T07:55:30.116859+00:00", "nick": "Newbie0086", "message": "oops its ok", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T08:09:55.597566+00:00", "nick": "nyov", "message": "rohitt: you would use the images pipeline for that", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T08:12:56.317447+00:00", "nick": "rohitt", "message": "nyov, Hey! Thanks for responding. Yes, I am using images pipeline for that. But I want to process those images AFTER they're downloaded in pipelines.py itself. So the question is, whether they'd be downloaded and image field would be populated BEFORE scrapy runs code in pipelines.py.", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T08:14:43.029846+00:00", "nick": "nyov", "message": "let me check that code, it's been awhile", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T08:15:00.438169+00:00", "nick": "Zladivliba", "message": "hey !", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T08:15:17.529765+00:00", "nick": "Zladivliba", "message": "is there a simple way for me to fetch a page once the crawl is done ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T08:15:51.293334+00:00", "nick": "Zladivliba", "message": "I use pipelines and I'd like to be able to do that in def spider_closed(self, spider):", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T08:16:29.261262+00:00", "nick": "Zladivliba", "message": "rohitt: any magical idea ? as usual ;)", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T08:17:15.085083+00:00", "nick": "nyov", "message": "rohitt: do I get that right and you don't want to have the image pipeline doing any processing like the thumbs stuff?", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T08:19:18.146106+00:00", "nick": "rohitt", "message": "nyov, no, it will be a 'default-download', 'store-info-in-images' thing. AFTER it is done, I want to process downloaded image (generating another hash, for other purposes)", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T08:21:49.491515+00:00", "nick": "nyov", "message": "image pipeline runs at the stage which is defined in ITEM_PIPELINES. so before it runs, you have defined the image_urls field in your item, and after it runs (the next pipeline) it might have processed/stored the image", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T08:22:22.148616+00:00", "nick": "nyov", "message": "I'm not quite sure if the other pipelines run, before it downloaded and processed the image(s)", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T08:22:28.955329+00:00", "nick": "nyov", "message": "or if it waits", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T08:23:18.585486+00:00", "nick": "nyov", "message": "Zladivliba: simple? no. possible? yes", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T08:23:52.983342+00:00", "nick": "nyov", "message": "Zladivliba: see here: https://gist.github.com/nyov/8720340", "links": ["https://gist.github.com/nyov/8720340"], "channel": "scrapy"},
{"date": "2014-09-22T08:23:54.033705+00:00", "nick": "Zladivliba", "message": "nyov: ah... hummm should I use another solution inside my function ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T08:24:42.961707+00:00", "nick": "Zladivliba", "message": "thanks, reading...", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T08:24:45.169295+00:00", "nick": "nyov", "message": "the code there does something, once spider_idle was called (last thing before shutting down) and injects another Request at that time", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T08:26:00.978429+00:00", "nick": "Zladivliba", "message": "hummm, very interesting !", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T08:26:06.254472+00:00", "nick": "Zladivliba", "message": "thanks a lot nyov", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T08:26:25.017435+00:00", "nick": "nyov", "message": "np. and spider_closed is too late for that", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T08:26:50.907614+00:00", "nick": "Zladivliba", "message": "i'm going to use spider_closed", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T08:26:59.341178+00:00", "nick": "Zladivliba", "message": "ah no, sorry...", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T08:27:00.632224+00:00", "nick": "Zladivliba", "message": "humm", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T08:27:03.039268+00:00", "nick": "Zladivliba", "message": "can't ???", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T08:27:24.201448+00:00", "nick": "nyov", "message": "once it closed down, no it can't fetch stuff", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T08:27:32.150927+00:00", "nick": "Zladivliba", "message": "humm....", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T08:27:48.920727+00:00", "nick": "Zladivliba", "message": "so what would be the right place to do it (after parse_item", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T08:28:05.484236+00:00", "nick": "Zladivliba", "message": "sorry process_item", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T08:29:09.964533+00:00", "nick": "Zladivliba", "message": "nyov: basically I need to process some stuff once the last item has been processed I need to fetch a page and analyse it", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T08:29:39.674040+00:00", "nick": "nyov", "message": "I just showd you :/ you'll just have to modify the code for a pipeline", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T08:30:14.225213+00:00", "nick": "Zladivliba", "message": "ok i'll use that (it's going to take me some time to understand your code I'm not that a good coder unfortunately ;)", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T08:31:26.301488+00:00", "nick": "nyov", "message": "1st: connect a callback to signals.spider_idle, 2nd engine.schedule another callback and raise DontCloseSpider, 3rd return some Request in the scheduled callback", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T08:32:00.377552+00:00", "nick": "rohitt", "message": "nyov, I am not sure about the same thing. Thanks for the help, anyway. I will write the code and test it. :)", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T08:33:37.521711+00:00", "nick": "nyov", "message": "rohitt: yeah, sorry. never really used it. maybe you don't want ImagesPipeline there? there is also FilesPipeline now which is doing a bit less", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T08:34:14.205325+00:00", "nick": "rohitt", "message": "nyov, np, I will look into it. Thanks. :)", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T09:08:39.270696+00:00", "nick": "Newbie0086", "message": "hello anyone can help my", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T09:08:42.569156+00:00", "nick": "Newbie0086", "message": "me", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T09:10:08.338615+00:00", "nick": "Newbie0086", "message": "i pasted my code here https://gist.github.com/newbiethetest/472c1d536... ,when i execute \"scrapy crawl domz\",it show errro: self.cursor.execute(\"\"\"INSERT INTO niaoyunTable (title, href)", "links": ["https://gist.github.com/newbiethetest/472c1d5362a97687474d"], "channel": "scrapy"},
{"date": "2014-09-22T09:10:08.564630+00:00", "nick": "Newbie0086", "message": "exceptions.AttributeError: 'MysqlStordPipeline' object has no attribute 'cursor'", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T09:12:18.326461+00:00", "nick": "Newbie0086", "message": "help", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T09:13:46.500492+00:00", "nick": "nyov", "message": "scrapy runs on twisted. you need to code the twisted way", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T09:14:23.289106+00:00", "nick": "Newbie0086", "message": "u means ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T09:15:41.742492+00:00", "nick": "nyov", "message": "you can use this as a base: https://github.com/nyov/scrapyext/blob/master/s...", "links": ["https://github.com/nyov/scrapyext/blob/master/scrapyext/mysql.py"], "channel": "scrapy"},
{"date": "2014-09-22T09:15:50.692753+00:00", "nick": "nyov", "message": "or this one https://github.com/nyov/scrapyext/blob/master/s...", "links": ["https://github.com/nyov/scrapyext/blob/master/scrapyext/adbapi.py"], "channel": "scrapy"},
{"date": "2014-09-22T09:16:33.021349+00:00", "nick": "nyov", "message": "I should clean and merge them, but for now you have two examples how to write an asynchronous mysql pipeline", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T09:19:08.272391+00:00", "nick": "nyov", "message": "...or, if you can grok it, you can try the totally over the top https://github.com/nyov/scrapyext/blob/master/s...", "links": ["https://github.com/nyov/scrapyext/blob/master/scrapyext/sqlmagic.py"], "channel": "scrapy"},
{"date": "2014-09-22T09:20:55.472238+00:00", "nick": "Newbie0086", "message": "if my mysqlPipelines have any errors?", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T09:49:49.145951+00:00", "nick": "chinaguy", "message": "anyone know the error ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T09:59:47.463760+00:00", "nick": "Gue______", "message": "a", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T11:25:21.143431+00:00", "nick": "skillachie", "message": "Hi Guys", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T11:25:30.365410+00:00", "nick": "skillachie", "message": "Can someone tell me what i am doing wrong", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T11:25:49.135534+00:00", "nick": "skillachie", "message": "I have been using scrapy for html content but just started using it for xml content", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T11:25:49.742535+00:00", "nick": "skillachie", "message": "http://pastebin.com/tKgq6tgp", "links": ["http://pastebin.com/tKgq6tgp"], "channel": "scrapy"},
{"date": "2014-09-22T11:26:30.289383+00:00", "nick": "skillachie", "message": "The problem is i am having some difficulty extracting a specific link for a item in the xml.", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T11:26:46.751016+00:00", "nick": "skillachie", "message": "Can you please look at the pastebin above and give me some advice", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T11:46:27.326047+00:00", "nick": "nyov", "message": "skillachie: your xpath is wrong", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T11:46:38.602336+00:00", "nick": "nyov", "message": "I wonder how that'd work with html and not xml pages", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T11:47:15.789821+00:00", "nick": "nyov", "message": "try this: .xpath('//link[@rel=\"standout\"]/@hr...", "links": ["mailto:.xpath('//link[@rel=\"standout\"]/@href').extract()"], "channel": "scrapy"},
{"date": "2014-09-22T11:48:16.045539+00:00", "nick": "skillachie", "message": "@nyov", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T11:48:30.391914+00:00", "nick": "skillachie", "message": "nyov: I tried that also", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T11:48:36.772871+00:00", "nick": "skillachie", "message": "It returns empty array", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T11:48:50.500870+00:00", "nick": "nyov", "message": "it does? huh", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T11:49:29.403035+00:00", "nick": "skillachie", "message": "nyov: yup, forgot to put it in the pastebin. So i was wondering why and if it is because the document is xml", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T11:49:56.100193+00:00", "nick": "nyov", "message": "what's the url to that xml?", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T11:50:38.255260+00:00", "nick": "skillachie", "message": "nyov: been scratching my head on this for a embarrassing 2 to 3 hours", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T11:50:42.699098+00:00", "nick": "skillachie", "message": "http://rss.nytimes.com/services/xml/rss/nyt/Art...", "links": ["http://rss.nytimes.com/services/xml/rss/nyt/Arts.xml"], "channel": "scrapy"},
{"date": "2014-09-22T11:52:06.904529+00:00", "nick": "skillachie", "message": "I want to iterate over each item node in the xml and obtain the link for the article in each item", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T11:52:46.883947+00:00", "nick": "skillachie", "message": "I am able to extract the title and so on but extracting the link that points to the article has been a pain", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T11:55:29.535848+00:00", "nick": "nyov", "message": "ah, i'm rusty with xml namespaces in scrapy. let me dig a bit", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T11:55:46.059813+00:00", "nick": "skillachie", "message": "nyov: Thanks for the help", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T11:59:08.354611+00:00", "nick": "nyov", "message": "looks like they are not link's but atom:link's which have that attribute", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T12:01:22.173678+00:00", "nick": "skillachie", "message": "nyov: ok taking a look", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T12:01:38.743101+00:00", "nick": "nyov", "message": "now I have to dig what happened to xmlfeedspider since these were merged I guess", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T12:02:52.844329+00:00", "nick": "nyov", "message": "actually not, i looked elsewhere :P", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T12:03:41.549477+00:00", "nick": "skillachie", "message": "omg!", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T12:03:47.616836+00:00", "nick": "skillachie", "message": "that was it", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T12:03:54.956861+00:00", "nick": "skillachie", "message": "i had to remove the names spaces", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T12:04:06.387985+00:00", "nick": "skillachie", "message": "node.remove_namespaces()", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T12:04:06.979884+00:00", "nick": "skillachie", "message": "print node.xpath('//link[@rel=\"standout\"]...", "links": ["mailto:node.xpath('//link[@rel=\"standout\"]/@href').extract()"], "channel": "scrapy"},
{"date": "2014-09-22T12:04:24.384815+00:00", "nick": "skillachie", "message": "nyov: Thanks alot for the tipsss", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T12:04:25.131318+00:00", "nick": "nyov", "message": "oh, that works? didn't know that", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T12:04:44.138395+00:00", "nick": "skillachie", "message": "nyov: http://doc.scrapy.org/en/latest/topics/selector...", "links": ["http://doc.scrapy.org/en/latest/topics/selectors.html#removing-namespaces"], "channel": "scrapy"},
{"date": "2014-09-22T12:04:58.077278+00:00", "nick": "nyov", "message": "oh great, I was just about to link https://scrapy.readthedocs.org/en/latest/topics...", "links": ["https://scrapy.readthedocs.org/en/latest/topics/spiders.html#xmlfeedspider"], "channel": "scrapy"},
{"date": "2014-09-22T12:05:07.737909+00:00", "nick": "nyov", "message": "thanks, too", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T12:05:22.696895+00:00", "nick": "skillachie", "message": "Selectors were good all along", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T12:05:25.178852+00:00", "nick": "skillachie", "message": "Once in the shell we can try selecting all <link> objects and see that it doesn\u2019t work (because the Atom XML namespace is obfuscating those nodes)", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T12:05:31.720571+00:00", "nick": "skillachie", "message": "No problem. Thanks again", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T12:06:21.898732+00:00", "nick": "nyov", "message": "yeah, I worried I had to try figure out how to run a shell with that XmlFeedSpider", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T12:06:36.859455+00:00", "nick": "skillachie", "message": "nyov: hehe cool", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T12:08:06.575622+00:00", "nick": "nyov", "message": "though it looks like most of the functionality was moved to Spider. nice, this is just a little wrapper", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T12:56:30.138906+00:00", "nick": "nyov", "message": "meh, I guess nobody likes me anymore. my pull request getting rejected or ignored, my questions on irc, too, paypal doesn't have any pals paying me... ah, life is hard", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T12:57:36.487155+00:00", "nick": "nyov", "message": "and i shouldn't have said that. now i'm a crybaby too", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T13:13:11.761543+00:00", "nick": "nyov", "message": "ProTip! Follow the drama with comments:>50 for heavy discussions.", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T13:13:23.813444+00:00", "nick": "nyov", "message": "lol. let's find me some bikeshedding", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T16:04:45.496463+00:00", "nick": "coder46_", "message": "Hi !", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T16:05:17.608269+00:00", "nick": "coder46_", "message": "i get this error when running my spider on ubuntu ImportError: Error loading object 'scrapy.telnet.TelnetConsole': No module named conch", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T16:06:39.096550+00:00", "nick": "coder46_", "message": "pablohof: ^ can you help me with the error ? (I have an assignment to complete :P :D )", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T16:48:59.311804+00:00", "nick": "skillachie", "message": "nyov: lol whats your github", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T16:50:20.231589+00:00", "nick": "skillachie", "message": "coder46_: Do you get that after trying a spider ? You need to adjust your import path", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T17:54:28.233131+00:00", "nick": "coder46_", "message": "Can scrapy be run on locally saved/archived web pages ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T17:54:32.750424+00:00", "nick": "coder46_", "message": "skillachie: ^ ??", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T17:56:03.949476+00:00", "nick": "skillachie", "message": "coder46_: have not tried before, but i assume you can. Try reading it as a string and then passing it to the html selector in scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T18:05:19.197810+00:00", "nick": "coder46_", "message": "skillachie: its not working .. i guess i am doing something wrong .. i am passing the local path of the html file on my machine to the start_urls field in my spider .. and its not working .. can you elaborate your suggestion ? What is the html selector ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T18:06:21.673439+00:00", "nick": "skillachie", "message": "coder46_:  Oh no, I doubt it will work with start url. I thought you wanted to just use the parse. But i think i understand what you are trying to do", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T18:06:46.800106+00:00", "nick": "skillachie", "message": "coder46_:  You would like to use the spider in scrapy to traverse all the html you already have saved", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T18:06:49.206091+00:00", "nick": "nyov", "message": "coder46_: have you used the 'file:///' scheme?", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T18:07:19.513610+00:00", "nick": "nyov", "message": "skillachie: why? so you can hate me on gh? :D", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T18:07:53.203211+00:00", "nick": "skillachie", "message": "coder46_:  I have not tried that with scrapy before but  it sounds like it should work. Probably one of the other more senior users will be able to help", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T18:07:54.383418+00:00", "nick": "coder46_", "message": "yup .. like this file:///home/faisal/Desktop/sample.html", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T18:08:09.742148+00:00", "nick": "nyov", "message": "and what happens?", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T18:08:22.143924+00:00", "nick": "skillachie", "message": "nyov:  haha not at all. So i can follow you", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T18:08:42.190765+00:00", "nick": "coder46_", "message": "and i get this error SyntaxError: invalid syntax ..", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T18:10:50.186191+00:00", "nick": "nyov", "message": "coder46_: works for me", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T18:10:57.753790+00:00", "nick": "nyov", "message": "$ scrapy shell 'file:///tmp/index.html'", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T18:11:09.257479+00:00", "nick": "nyov", "message": "In [1]: response.url", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T18:11:10.302256+00:00", "nick": "nyov", "message": "Out[1]: 'file:///tmp/index.html'", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T18:11:50.146283+00:00", "nick": "coder46_", "message": "nyov: are you keeping your allowed_domains empty ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T18:12:47.301322+00:00", "nick": "coder46_", "message": "oh .. ok you have used scrapy shell ..", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T18:14:15.980456+00:00", "nick": "coder46_", "message": "nyov: any idea why i am getting syntax error ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T18:14:44.760022+00:00", "nick": "nyov", "message": "probably becuase you have a syntax error somewhere", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T18:16:05.790537+00:00", "nick": "coder46_", "message": "nyov: ok ... :P .. i forgot to place the url within quotes .. :P", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T18:16:19.553184+00:00", "nick": "coder46_", "message": "nyov: its working now :D", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T18:20:48.445201+00:00", "nick": "nyov", "message": "skillachie: ah, a stalker.", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T18:21:01.171013+00:00", "nick": "skillachie", "message": "nyov: lol!!", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T18:22:01.195185+00:00", "nick": "nyov", "message": "ah how great that no-one knows, my name like rumpelstiltskin goes? or what was it again", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T18:32:23.380128+00:00", "nick": "coder46_", "message": "nyov: skillachie: i need some advice from you guys .. this is my problem statement .. i have been given 2000 yelp business pages .. and i have to obtain structured data for the business:  business name, business phone number,", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T18:32:23.380197+00:00", "nick": "coder46_", "message": "business home page URL, contact\u2010us URL for the business, email id for the business ... i can easily get the first 3 things from the yelp pages itself using scrapy .. but for contact url and contact email , i will have to parse the business website and apply some heuristics ... my question is will scrapy be useful or am i better off using beatiful soup ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T18:32:35.293821+00:00", "nick": "coder46_", "message": "sorry for such a long message :P", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T18:33:14.214295+00:00", "nick": "coder46_", "message": "*beautifulsoup", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T18:33:46.816488+00:00", "nick": "coder46_", "message": "especially for the heuristics part .. can scrapy help in any way ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T18:34:29.974115+00:00", "nick": "skillachie", "message": "coder46_:  honestly you can make the request in scrapy and then use the selectors as well. What exactly do you mean by heuristics . nyov is a pro compared to me, so see what he says", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T18:38:25.347359+00:00", "nick": "coder46_", "message": "i mean  you are given a business's url .. and now you have to find it's contact-us page and their contact-us email address .. this is quite tough i guess .. and a naive method will be to read through the entire html page and find a \"contact-us\" inside a <a href ></a> ..", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T18:40:30.350829+00:00", "nick": "coder46_", "message": "oh wait .. i think i got something .. scrapy can easily give me all instances of href's inside a webpage containing contact in its text .. :D :D :D .. let me try this !!!", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T18:41:32.041347+00:00", "nick": "coder46_", "message": "nyov: any insights from you ??", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T18:41:39.921871+00:00", "nick": "nyov", "message": "that's pretty tough. and i'm not so big a fan of collecting that kind of data. can't give much help there. but at worst case, your spider will hunt all the business website's hundreds pages for something resembling what you want", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T18:42:05.875927+00:00", "nick": "coder46_", "message": "its my assignment :P", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T18:42:11.296504+00:00", "nick": "coder46_", "message": "school", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T18:42:17.857910+00:00", "nick": "nyov", "message": "yeah right?", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T18:43:50.293125+00:00", "nick": "nyov", "message": "well, that's something for the NLTK guys, heuristics and algorithms about what might be where on a website", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T18:44:10.692482+00:00", "nick": "coder46_", "message": "yup :)", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T18:45:26.922040+00:00", "nick": "blusteal", "message": "quick xpath question, is it possible to get a title attirbute via xpath? say a <h3 title=\"some data\" id=\"specific id\" test=\"some id value\" _text=\"some more data\">Header</h3> can i use xpath to say grab the title? I know I can do response.xpath('//h3[@id=\"specific id\"]/text()).extract() to grab the H3 tag but what about grabbing the data from title", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T18:46:20.731220+00:00", "nick": "nyov", "message": "blusteal: //h3/@title", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T18:46:49.421298+00:00", "nick": "blusteal", "message": "without the bracket...", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T18:46:49.926656+00:00", "nick": "blusteal", "message": "hmmm", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T18:47:07.734294+00:00", "nick": "coder46_", "message": "^^^nice :)", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T18:47:22.953998+00:00", "nick": "nyov", "message": "yes, or //h3[@id=\"specific id\"]/@title", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T18:49:51.401619+00:00", "nick": "coder46_", "message": "on similar lines, is it possible to do a  contains search based text() field", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T18:50:18.873791+00:00", "nick": "coder46_", "message": "nyov: something like /text()[contains(....)]", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T18:51:05.025750+00:00", "nick": "nyov", "message": "I think it's *[contains(text(), \"\")]", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T18:52:18.032813+00:00", "nick": "coder46_", "message": "nyov:  is this valid response.xpath('//h3[@id=\"specific id\"]/*[contains(text(), \"\")]).extract()  ??", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T18:52:58.933447+00:00", "nick": "nyov", "message": "not sure", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T18:53:01.045476+00:00", "nick": "blusteal", "message": "nyov that's perfect tanks", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T18:53:41.044831+00:00", "nick": "blusteal", "message": "coder46 I've been trying to get contains to work but so far I've been out of luck, maybe my quotation marks or something", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T18:54:04.178521+00:00", "nick": "blusteal", "message": "I can say it's pretty acurate up to the /*[contains..... part", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T18:54:58.404031+00:00", "nick": "blusteal", "message": "right click on your page and inspect the element, google chrome and firefox can both give you direct xpath's code", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T19:00:07.612252+00:00", "nick": "nyov", "message": "yeah xpath is pretty awesome. I'm sad that people invent css preprocessors for logic like axis, where xpath already shines.", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T19:01:29.961407+00:00", "nick": "nyov", "message": "you can use xpath with javascript as selectors, but it'd be nice to use it in css itself", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T19:02:26.273388+00:00", "nick": "nyov", "message": "anyway, I'll be going for the day. until next time", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T19:02:35.495232+00:00", "nick": "coder46_", "message": "nyov: Thanks !", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T19:02:41.459222+00:00", "nick": "nyov", "message": "no problem", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T19:03:03.785562+00:00", "nick": "coder46_", "message": "nyov: whats your time zone ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T22:39:24.985053+00:00", "nick": "hardy", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2014-09-22T22:39:44.472727+00:00", "nick": "hardy", "message": "anyone?", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T01:10:27.981055+00:00", "nick": "Newbie0086", "message": "morning", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T01:28:56.451823+00:00", "nick": "Newbie0086", "message": "h", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T01:28:57.629892+00:00", "nick": "Newbie0086", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T01:30:00.933728+00:00", "nick": "Newbie0086", "message": "anyone can give me the document of adbapi.py", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T05:25:29.185871+00:00", "nick": "Zladivliba", "message": "nyov", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T05:25:34.577310+00:00", "nick": "Zladivliba", "message": "u there ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T05:41:03.736542+00:00", "nick": "Zladivliba", "message": "hello !", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T05:41:37.693788+00:00", "nick": "Zladivliba", "message": "", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T05:41:37.827009+00:00", "nick": "Zladivliba", "message": "dispatcher.connect(self.spider_closed, signals.spider_closed)", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T05:41:37.827102+00:00", "nick": "Zladivliba", "message": "dispatcher.connect(self.spider_idle, signals.spider_idle)", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T05:41:39.666720+00:00", "nick": "Zladivliba", "message": "anyone know why this is not executed (the second functino is never started) :", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T05:46:11.970022+00:00", "nick": "Zladivliba", "message": "ok seems the signals.spider_idle is unknown, or never started", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T06:00:45.078332+00:00", "nick": "chinaboy_rubyfan", "message": "hello", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T09:00:29.765027+00:00", "nick": "Newbie0086", "message": "hello", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T09:00:40.001925+00:00", "nick": "Newbie0086", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T09:01:53.066354+00:00", "nick": "Newbie0086", "message": "anyone can tell me the number of \"BOT_NAME+'.pipelines.TutorialPipeline':5,    BOT_NAME+'.pipelines.TutorialPipeline2':300\"", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T09:02:08.626676+00:00", "nick": "Newbie0086", "message": "what the effect of 5 or 300", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T09:03:13.604268+00:00", "nick": "Newbie0086", "message": "help", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T09:03:21.724814+00:00", "nick": "Newbie0086", "message": "anyone can tell me please", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T09:14:58.033173+00:00", "nick": "nikolaosk", "message": "look up base pipeline dict or something", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T09:16:13.228638+00:00", "nick": "nikolaosk", "message": "ITEM_PIPELINES_BASE", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T09:30:39.143463+00:00", "nick": "Newbie0086", "message": "it say its order", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T09:31:31.210904+00:00", "nick": "Newbie0086", "message": "i want to know which one will be  execute firstly", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T10:05:48.185924+00:00", "nick": "rohitt", "message": "Hi all!", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T10:06:27.236624+00:00", "nick": "rohitt", "message": "Is there any way to read settings from Pipelines.py (Settings which have been changed through spider using scrapy.settings.Settings object) ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T10:25:06.478611+00:00", "nick": "nikolaosk", "message": "rohitt: I don't understand", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T10:25:38.633703+00:00", "nick": "nikolaosk", "message": "do you mean to write a pipeline class that can read settings as overriden by a spider?", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T10:26:56.691701+00:00", "nick": "rohitt", "message": "nikolaosk, exactly", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T10:28:30.792116+00:00", "nick": "nikolaosk", "message": "I don't know about this, I think it's a feature of the new release", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T10:28:38.804675+00:00", "nick": "nikolaosk", "message": "are you on .24?", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T10:29:57.957968+00:00", "nick": "nikolaosk", "message": "if you are actually looking for pipeline-spider interaction then it's something else", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T10:30:18.226054+00:00", "nick": "rohitt", "message": "yes, I am on .24", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T10:30:25.867320+00:00", "nick": "rohitt", "message": "hmm", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T10:31:24.212829+00:00", "nick": "Zladivli_", "message": "hello", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T10:31:27.934274+00:00", "nick": "Zladivli_", "message": "no one here ???", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:10:16.369662+00:00", "nick": "Zladivliba", "message": "hello !", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:11:19.052520+00:00", "nick": "Zladivliba", "message": "anyone and idea why this scrapy is not recursive ? https://gist.github.com/anonymous/c8152957c890e...", "links": ["https://gist.github.com/anonymous/c8152957c890efaa8a5b"], "channel": "scrapy"},
{"date": "2014-09-23T11:24:29.185655+00:00", "nick": "nikolaosk", "message": "it is, see the rules attr", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:24:42.461512+00:00", "nick": "nikolaosk", "message": "the crawl spider wraps your parse methods", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:24:59.931637+00:00", "nick": "nikolaosk", "message": "to first extract the links from the respone", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:25:16.722455+00:00", "nick": "nikolaosk", "message": "the links defined in the rules", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:25:30.037941+00:00", "nick": "nikolaosk", "message": "and traverse them too", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:25:38.956494+00:00", "nick": "nikolaosk", "message": "while you are only busy parsing items", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:25:42.204947+00:00", "nick": "nyov", "message": "Zladivliba: Rules are crawlspider stuff, and not in spider", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:26:05.233412+00:00", "nick": "nikolaosk", "message": "oups, indeed", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:26:22.877082+00:00", "nick": "nikolaosk", "message": "Zladivliba: you subclass a spider that doesn't need the rules attr", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:26:26.081694+00:00", "nick": "Zladivliba", "message": "nyov: !!", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:26:35.944900+00:00", "nick": "Zladivliba", "message": "humm I'm not sure I understand...", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:27:32.563138+00:00", "nick": "Zladivliba", "message": "ok got it", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:29:03.832561+00:00", "nick": "Zladivliba", "message": "ok I changed it but still not recursive crawling : https://gist.github.com/anonymous/94ee8f17b9476...", "links": ["https://gist.github.com/anonymous/94ee8f17b9476a506937"], "channel": "scrapy"},
{"date": "2014-09-23T11:29:17.641906+00:00", "nick": "Zladivliba", "message": "nikolaosk: did I replaced it right ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:31:37.288570+00:00", "nick": "Zladivliba", "message": "nyov: could it be realted by the fact that the urls are relative ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:34:11.828642+00:00", "nick": "nyov", "message": "no, I think that looks okay. linkextractor should be making them absolute", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:34:39.151600+00:00", "nick": "nyov", "message": "except you're overriding parse in a crawlspider", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:36:30.085046+00:00", "nick": "nyov", "message": "that -still- doesn't work. it won't ever I guess, since nobody liked my pull request in that regard", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:38:44.993204+00:00", "nick": "nikolaosk", "message": "Zladivliba: rename to parse_item", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:39:02.283542+00:00", "nick": "nikolaosk", "message": "nyov: about hidding parse?", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:39:10.584714+00:00", "nick": "nikolaosk", "message": "__parse or something", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:39:29.781958+00:00", "nick": "Zladivliba", "message": "nikolaosk: nope still not crawling", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:40:08.973866+00:00", "nick": "nikolaosk", "message": "not at all? oh, I thought not parsing, sorry", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:40:20.058691+00:00", "nick": "Zladivliba", "message": "not it's not crawling recursively", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:40:27.607225+00:00", "nick": "Zladivliba", "message": "try it yourself if you want", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:40:31.295177+00:00", "nick": "nikolaosk", "message": "this was the problem that didn't arise yet", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:40:34.966979+00:00", "nick": "nyov", "message": "yeah, that was my proposal. https://github.com/scrapy/scrapy/pull/732", "links": ["https://github.com/scrapy/scrapy/pull/732"], "channel": "scrapy"},
{"date": "2014-09-23T11:40:36.341619+00:00", "nick": "Zladivliba", "message": "seems really wierd", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:40:50.845671+00:00", "nick": "nikolaosk", "message": "I don't know if allow=() actually extracts nothing", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:41:02.363566+00:00", "nick": "Zladivliba", "message": "ah...", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:41:05.447937+00:00", "nick": "Zladivliba", "message": "what should I put here", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:41:18.495843+00:00", "nick": "Zladivliba", "message": "it's wierd because I have allow=() on another crawler and it works fine...", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:41:27.947898+00:00", "nick": "nikolaosk", "message": "I see", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:41:34.357249+00:00", "nick": "nikolaosk", "message": "do you yied requests manually", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:41:35.177979+00:00", "nick": "nikolaosk", "message": "?", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:42:08.357887+00:00", "nick": "Zladivliba", "message": "here's my current code : https://gist.github.com/anonymous/f0d69ed7259a7...", "links": ["https://gist.github.com/anonymous/f0d69ed7259a71b392fe"], "channel": "scrapy"},
{"date": "2014-09-23T11:42:17.073233+00:00", "nick": "Zladivliba", "message": "yied requests ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:42:19.862431+00:00", "nick": "Zladivliba", "message": "what do you mean ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:42:41.911509+00:00", "nick": "Zladivliba", "message": "I see only a couple of requests made", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:42:57.238048+00:00", "nick": "nyov", "message": "wiht follow=true, the crawlspider should run \"recursive\"", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:43:04.650864+00:00", "nick": "Zladivliba", "message": "or not even... ere's my log : https://gist.github.com/anonymous/bda36127acf45...", "links": ["https://gist.github.com/anonymous/bda36127acf452b0b4ce"], "channel": "scrapy"},
{"date": "2014-09-23T11:43:16.814376+00:00", "nick": "Zladivliba", "message": "nyov: yearh I know and true is by default", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:43:31.617356+00:00", "nick": "nyov", "message": "only if you have no callback", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:43:51.589177+00:00", "nick": "Zladivliba", "message": "what ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:44:18.779282+00:00", "nick": "Zladivliba", "message": "ok I added follow=True, but makes no difference", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:44:27.289207+00:00", "nick": "Zladivliba", "message": "still not recursive... ahhh", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:44:33.188693+00:00", "nick": "Zladivliba", "message": "this is a headache...", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:45:21.025304+00:00", "nick": "nikolaosk", "message": "nyov: I think name mangling is more appropriate for hidding from subclasses", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:45:26.281134+00:00", "nick": "nikolaosk", "message": "__parse", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:45:48.742848+00:00", "nick": "nyov", "message": "allowed_domains = ['dmoz.fr']", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:45:50.810414+00:00", "nick": "nyov", "message": "start_urls = ['http://www.dmoz.org/']", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:45:56.725350+00:00", "nick": "nyov", "message": "offsite rquests?", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:46:12.856856+00:00", "nick": "nikolaosk", "message": "althought here it's just to embrace good practices, why would someone implement \"private\" methods in spiders", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:46:16.687384+00:00", "nick": "Zladivliba", "message": "ok sorry corrected that point", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:46:20.795556+00:00", "nick": "Zladivliba", "message": "still does'nt work", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:46:30.307354+00:00", "nick": "Zladivliba", "message": "I changed to dmoz.com", "links": ["http://dmoz.com"], "channel": "scrapy"},
{"date": "2014-09-23T11:46:58.977615+00:00", "nick": "Zladivliba", "message": "nyov: YEAHHHHAAAA", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:47:02.590315+00:00", "nick": "nyov", "message": "nikolaosk: I don't think it should be _really_ hidden. after all crawlspider still needs to use it in a subclass, and an advanced user might want to override it with a super call or such", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:47:09.092825+00:00", "nick": "Zladivliba", "message": "yearh you were right it was a offsite request", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:47:19.244059+00:00", "nick": "Zladivliba", "message": "I didn't see I was using .fr instead of .org...", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:47:25.329530+00:00", "nick": "Zladivliba", "message": "ahhhh BIG THANKS !!!", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:47:56.455565+00:00", "nick": "nikolaosk", "message": "oh, does it need to? I din't know", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:48:01.437188+00:00", "nick": "nikolaosk", "message": "where?", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:48:22.099751+00:00", "nick": "nyov", "message": "nikolaosk: that's where the rules are attached. it's the default callback after all", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:48:54.022844+00:00", "nick": "nyov", "message": "but I also proposed just renaming it something different instead, like init instead of parse, while parse is for the user", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:49:08.516978+00:00", "nick": "nikolaosk", "message": "oh, sorry, didn't see callback=parse in the rules", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:49:15.313075+00:00", "nick": "nikolaosk", "message": "I thought it was simpler", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:49:26.011042+00:00", "nick": "nikolaosk", "message": "I am not really into the crawlspider", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:50:20.313507+00:00", "nick": "Zladivliba", "message": "hummmmm", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T11:50:40.902833+00:00", "nick": "Zladivliba", "message": "Now my spider runs a few pages and stops... any idea why ? https://gist.github.com/anonymous/316e4c8a48f3c...", "links": ["https://gist.github.com/anonymous/316e4c8a48f3cf59415c"], "channel": "scrapy"},
{"date": "2014-09-23T12:03:55.708846+00:00", "nick": "Zladivliba", "message": "ok, solved", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T12:03:58.910625+00:00", "nick": "Zladivliba", "message": "that was a rule problem", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T12:04:00.878946+00:00", "nick": "Zladivliba", "message": "ahhh...", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T12:04:05.550167+00:00", "nick": "Zladivliba", "message": "I'm going to make it after all ;)))", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T12:05:50.394238+00:00", "nick": "Zladivliba", "message": "nyov: is there a simple way to extract every link from each page crawled ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T12:06:01.158370+00:00", "nick": "Zladivliba", "message": "I want to save them in a file...", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T12:07:20.563573+00:00", "nick": "nikolaosk", "message": "crawled or parsed?", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T12:07:48.177716+00:00", "nick": "nikolaosk", "message": "for parsed it's easy, you can do it in parse_item", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T12:07:51.982634+00:00", "nick": "nyov", "message": "add a callback to the rule, before it processes the links", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T12:08:49.785621+00:00", "nick": "nyov", "message": "https://scrapy.readthedocs.org/en/latest/topics...", "links": ["https://scrapy.readthedocs.org/en/latest/topics/link-extractors.html#basesgmllinkextractor"], "channel": "scrapy"},
{"date": "2014-09-23T12:09:15.881062+00:00", "nick": "nyov", "message": "you want process_value()", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T12:09:30.192901+00:00", "nick": "nyov", "message": "err process_value=yourcallback", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T12:09:48.241105+00:00", "nick": "nyov", "message": "or actually...", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T12:10:42.333320+00:00", "nick": "Zladivliba", "message": "nikolaosk: I just need to save every link I find in a file", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T12:11:10.009641+00:00", "nick": "nyov", "message": "yep. try https://scrapy.readthedocs.org/en/latest/topics...", "links": ["https://scrapy.readthedocs.org/en/latest/topics/spiders.html?highlight=process_links#scrapy.contrib.spiders.Rule"], "channel": "scrapy"},
{"date": "2014-09-23T12:12:00.113188+00:00", "nick": "Zladivliba", "message": "nyov: but aren't the link extracted going to feed the crawler ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T12:12:00.589933+00:00", "nick": "nyov", "message": "you'll have a callback which get's every link seen. there you can save them", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T12:12:51.550947+00:00", "nick": "nyov", "message": "why would you need to \"feed\" the crawler, if you save them at this stage?. oh sure, if you don't actually filter them out there, they'll get to the crawler afterwards", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T12:13:26.637449+00:00", "nick": "Zladivliba", "message": "ok, sorry : I'm trying to extract every domain name in .fr", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T12:13:57.353544+00:00", "nick": "Zladivliba", "message": "so I thought I'd go to dmoz and crawl it entirely and each time I have a url that has a .fr, I'm saving it in a file", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T12:14:30.504054+00:00", "nick": "Zladivliba", "message": "now another thing that would be usefull would be to crawl each .fr I found after that (but that would take a while...)", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T12:16:25.881333+00:00", "nick": "nyov", "message": "then better get to it, wouldn't be nice to have a grey beard when it finally finishes crawling all of .fr :>", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T12:16:39.437782+00:00", "nick": "Zladivliba", "message": "hahaha ;)", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T12:17:17.577651+00:00", "nick": "Zladivliba", "message": "I'm reading what u sent but I'm having hard time understanding...", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T12:20:35.591185+00:00", "nick": "nyov", "message": "Zladivliba: it's simple. just as you have a parse_item callback, you add a filter_links callback to process_links. then you define that method and print all the stuff it gets", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T12:20:46.276625+00:00", "nick": "nyov", "message": "you'll see a lot of links passing your screen", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T12:25:29.554076+00:00", "nick": "Zladivliba", "message": "nyov: I'm trying this but with no results: https://gist.github.com/anonymous/32835c856337f...", "links": ["https://gist.github.com/anonymous/32835c856337f0719fe8"], "channel": "scrapy"},
{"date": "2014-09-23T12:38:30.853662+00:00", "nick": "nyov", "message": "you'll figure it out. here's a hint. Rule(SgmlLinkExtractor(allow=()), callback='parse_item', follow=True, process_links='filter_links')", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T12:51:01.675698+00:00", "nick": "Zladivliba", "message": "nyov: ok thanks I'll try that ;)", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T13:32:36.741148+00:00", "nick": "blusteal", "message": "hey Zladivliba here's a code sample, hope it helps: http://pastebin.com/sH5rucvM", "links": ["http://pastebin.com/sH5rucvM"], "channel": "scrapy"},
{"date": "2014-09-23T13:34:59.141321+00:00", "nick": "freedog", "message": "Hi , Newb here, I am trying to follow http://doc.scrapy.org/en/latest/intro/tutorial.... but see this error AttributeError: 'module' object has no attribute 'Spider'", "links": ["http://doc.scrapy.org/en/latest/intro/tutorial.html"], "channel": "scrapy"},
{"date": "2014-09-23T13:35:27.942369+00:00", "nick": "blusteal", "message": "code sample?", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T13:35:58.317901+00:00", "nick": "blusteal", "message": "freedog did you import scrapy at the top of your spider python class?", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T13:36:39.580694+00:00", "nick": "freedog", "message": "[scrapy] INFO: Scrapy 0.14.4 started (bot: tutorial)", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T13:36:46.733005+00:00", "nick": "freedog", "message": "this is my version, does it look old ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T13:36:53.757526+00:00", "nick": "blusteal", "message": "yes your version is too old", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T13:36:59.550010+00:00", "nick": "freedog", "message": "okay", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T13:37:01.069428+00:00", "nick": "blusteal", "message": "the tutorial is for version .24", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T13:37:13.492851+00:00", "nick": "freedog", "message": "debian repos :)", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T13:38:02.302159+00:00", "nick": "nyov", "message": "freedog: there ar better repos. debians version is currently not yet updated again", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T13:38:32.684511+00:00", "nick": "nyov", "message": "deb http://archive.scrapy.org/ubuntu scrapy main", "links": ["http://archive.scrapy.org/ubuntu"], "channel": "scrapy"},
{"date": "2014-09-23T13:38:43.652941+00:00", "nick": "blusteal", "message": "http://stackoverflow.com/questions/22556965/how...", "links": ["http://stackoverflow.com/questions/22556965/how-to-install-scrapy-on-ubuntu"], "channel": "scrapy"},
{"date": "2014-09-23T13:38:52.256990+00:00", "nick": "blusteal", "message": "this is how I had to install it", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T13:39:14.385438+00:00", "nick": "nyov", "message": "works for debian, except for scrapyd, unless you're using upstart on ubuntu", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T13:39:26.565350+00:00", "nick": "blusteal", "message": "nyov that looks like a lot easier way to install this stuff", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T13:40:32.655241+00:00", "nick": "freedog", "message": "ok thanks, let me try this repo.", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T13:40:40.888983+00:00", "nick": "nyov", "message": "http://scrapy.readthedocs.org/en/latest/topics/...", "links": ["http://scrapy.readthedocs.org/en/latest/topics/ubuntu.html"], "channel": "scrapy"},
{"date": "2014-09-23T13:40:51.950998+00:00", "nick": "nyov", "message": "for the archive key", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T13:46:45.002305+00:00", "nick": "nyov", "message": "anyone got a script to quickly git remote add all github forks of a project, that have actual changes?", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T13:54:59.894804+00:00", "nick": "blusteal", "message": "i'm not good with git, i used mercurial git is kinda overkill for me", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T13:57:03.816883+00:00", "nick": "nyov", "message": ":) okay", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T14:10:44.286790+00:00", "nick": "Digenis", "message": "nyov I think the confusion with the CrawlSpider.parse lies on the tutorial", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T14:12:23.411438+00:00", "nick": "Digenis", "message": "ahm, the tutorial in the overview I mean", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T14:12:51.560599+00:00", "nick": "Digenis", "message": "there was some ticket about this", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T14:13:18.050661+00:00", "nick": "Digenis", "message": "to move the crawlspider to an \"advanced\" tutorial, I 'd rather call it \"specific\"", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T14:18:35.953022+00:00", "nick": "nyov", "message": "I think the problem lies in handling Spiders differently (to the users viewpoint). with Spider, parse() is a required step. with CrawlSpider, suddenly it's a 'must avoid'", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T14:19:27.093080+00:00", "nick": "nyov", "message": "from a programmers viewpoint, it makes perfect sense, looking at the code. though apparently lots of people don't look at what they're subclassing", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T20:26:24.793085+00:00", "nick": "jasony", "message": "hi community", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T20:26:32.624706+00:00", "nick": "jasony", "message": "i was wondering if someone could assist me", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T20:26:36.408778+00:00", "nick": "jasony", "message": "i'm having the issue found here", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T20:26:37.025433+00:00", "nick": "jasony", "message": "https://github.com/scrapy/scrapy/issues/838", "links": ["https://github.com/scrapy/scrapy/issues/838"], "channel": "scrapy"},
{"date": "2014-09-23T20:26:59.699013+00:00", "nick": "jasony", "message": "where i have a space before and after the URL", "links": [], "channel": "scrapy"},
{"date": "2014-09-23T20:48:20.217051+00:00", "nick": "nyov", "message": "I suppose there was a question in there somewhere", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T00:47:43.417734+00:00", "nick": "Newbie0086", "message": "about this \" \"BOT_NAME+'.pipelines.TutorialPipeline':5,    BOT_NAME+'.pipelines.TutorialPipeline2':300\"\" ,it means that \"\"BOT_NAME+'.pipelines.TutorialPipeline\" will be execute firstly?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T00:48:43.613054+00:00", "nick": "Newbie0086", "message": "i cannot find the difference", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T00:50:27.429836+00:00", "nick": "Newbie0086", "message": "how to set the pipeline value", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T00:51:52.989912+00:00", "nick": "moskiteau", "message": "hello", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T00:52:13.275724+00:00", "nick": "moskiteau", "message": "is there a chrome extension to select the content to be scraped in a page?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T00:52:42.857315+00:00", "nick": "moskiteau", "message": "i.e. select the div's, etc. and generate a scraper for a page?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T03:14:10.053988+00:00", "nick": "SthNotTaken", "message": "Is there an easy way to get YouTube videos?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T03:34:49.348435+00:00", "nick": "Newbie0086", "message": "some website offer the video download service", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T06:01:50.681510+00:00", "nick": "Zladivliba", "message": "hello !!!!!!!", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T06:04:14.019181+00:00", "nick": "Zladivliba", "message": "nyov: hey !!!!!", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T06:04:48.253596+00:00", "nick": "Zladivliba", "message": "I'm trying to implement the code you gave me to make a request once the spider kind of stopped processing all the items", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T06:05:04.735134+00:00", "nick": "Zladivliba", "message": "but this is never launched : dispatcher.connect(self.spider_idle, signals.spider_idle)", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T06:05:14.540914+00:00", "nick": "Zladivliba", "message": "i don't know why...", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T07:36:54.425228+00:00", "nick": "Zladivliba", "message": "is there a way for me to process a url last ? Like once all the other have finished ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:08:33.551724+00:00", "nick": "Zladivliba", "message": "gosh, no one here today ???", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:16:54.237656+00:00", "nick": "nikolaosk", "message": "Zladivliba: do you mean to process the url list when the spider finished crawling and is about to close?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:17:02.949711+00:00", "nick": "Zladivliba", "message": "nikolaosk: yes !", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:17:09.236709+00:00", "nick": "Zladivliba", "message": "sorry", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:17:22.178375+00:00", "nick": "Zladivliba", "message": "I mean : once all the urls are processed I need to analyse a specific page", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:17:27.224577+00:00", "nick": "nikolaosk", "message": "and would you yield any new requests then?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:17:37.365753+00:00", "nick": "nikolaosk", "message": "aha", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:17:38.784423+00:00", "nick": "nikolaosk", "message": "I see", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:17:40.849645+00:00", "nick": "nikolaosk", "message": "you mean", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:17:41.581201+00:00", "nick": "Zladivliba", "message": "actyally", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:17:48.878866+00:00", "nick": "Zladivliba", "message": "I'm doing a website analysus", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:17:49.011051+00:00", "nick": "nikolaosk", "message": "once everything got crawled", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:17:53.073865+00:00", "nick": "Zladivliba", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:18:05.029797+00:00", "nick": "Zladivliba", "message": "and once everything is cralwed I need to hit on one of the page I analysed", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:18:11.004629+00:00", "nick": "nikolaosk", "message": "that's a common issue", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:18:24.731359+00:00", "nick": "Zladivliba", "message": "because it's once I've done all the crawling that I know which page to hit on", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:18:40.943376+00:00", "nick": "nikolaosk", "message": "how many websites will yous spider analyse?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:18:49.462498+00:00", "nick": "Zladivliba", "message": "one by one", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:18:49.952964+00:00", "nick": "nikolaosk", "message": "at the same time", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:18:52.608694+00:00", "nick": "nikolaosk", "message": "ok", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:18:55.798634+00:00", "nick": "Zladivliba", "message": "humm", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:18:59.156044+00:00", "nick": "Zladivliba", "message": "hold on", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:19:03.317988+00:00", "nick": "nikolaosk", "message": "and will you yield any requests then?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:19:05.459072+00:00", "nick": "nikolaosk", "message": "if not", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:19:08.169973+00:00", "nick": "Zladivliba", "message": "I mean I'm going to use scrapyd at some point", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:19:20.226022+00:00", "nick": "Zladivliba", "message": "what do you mean \"yield any request\" ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:19:25.879542+00:00", "nick": "Zladivliba", "message": "I don't know \"yield\"", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:19:27.874244+00:00", "nick": "nikolaosk", "message": "define the closed(self, reason) method", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:19:33.168663+00:00", "nick": "nikolaosk", "message": "in your spider", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:19:45.109571+00:00", "nick": "Zladivliba", "message": "I have the closed method", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:19:45.698690+00:00", "nick": "nikolaosk", "message": "but wait", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:19:48.115427+00:00", "nick": "Zladivliba", "message": "like...", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:19:50.070923+00:00", "nick": "nikolaosk", "message": "you said analyze", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:20:03.211171+00:00", "nick": "Zladivliba", "message": "dispatcher.connect(self.spider_closed, signals.spider_closed)", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:20:06.401371+00:00", "nick": "nikolaosk", "message": "based on what you scraped or what you traversed?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:20:07.383176+00:00", "nick": "Zladivliba", "message": "in my pipleline", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:20:19.789299+00:00", "nick": "Zladivliba", "message": "scraped", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:20:30.173728+00:00", "nick": "Zladivliba", "message": "I pass some data to items", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:20:41.067255+00:00", "nick": "Zladivliba", "message": "and then from this data I analyse it", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:20:53.884908+00:00", "nick": "Zladivliba", "message": "and then on my last function (dispatcher.connect(self.spider_closed, signals.spider_closed))", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:21:00.368062+00:00", "nick": "Zladivliba", "message": "I want to hit on a page", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:21:06.468483+00:00", "nick": "Zladivliba", "message": "and make the analysis of that page", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:21:10.419310+00:00", "nick": "nikolaosk", "message": "do the data take up too much space?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:21:21.105660+00:00", "nick": "Zladivliba", "message": "humm no", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:21:24.874522+00:00", "nick": "Zladivliba", "message": "not at that point", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:21:29.727460+00:00", "nick": "Zladivliba", "message": "I'm analysing 1000 pages", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:21:32.905937+00:00", "nick": "Zladivliba", "message": "per website", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:21:34.954412+00:00", "nick": "Zladivliba", "message": "more or less", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:21:43.393778+00:00", "nick": "nikolaosk", "message": "what about storing them and then performing the analysis?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:21:57.680606+00:00", "nick": "Zladivliba", "message": "nope, not practical for me at all :(", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:22:05.465817+00:00", "nick": "nikolaosk", "message": "why?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:22:08.397638+00:00", "nick": "Zladivliba", "message": "it needs to be done within the spider", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:22:13.036124+00:00", "nick": "nikolaosk", "message": "why?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:22:26.416457+00:00", "nick": "Zladivliba", "message": "because i'm updating the data on the last function spider_closed", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:22:34.635603+00:00", "nick": "Zladivliba", "message": "and I'm not saving the rest", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:22:44.187561+00:00", "nick": "nikolaosk", "message": "what if something goes wrong then?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:22:51.899812+00:00", "nick": "nikolaosk", "message": "you will have to crawl again", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:22:57.839305+00:00", "nick": "Zladivliba", "message": "I don't care about each url I've analysed I just need the summary of everything", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:23:07.074175+00:00", "nick": "nikolaosk", "message": "every time you want to update the analysis part", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:23:14.521166+00:00", "nick": "nikolaosk", "message": "you will have to test by crawling again", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:23:15.691791+00:00", "nick": "Zladivliba", "message": "oh, that's not a problem", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:23:20.235870+00:00", "nick": "Zladivliba", "message": "people pay for that", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:23:27.141709+00:00", "nick": "Zladivliba", "message": "so that's ok", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:23:31.431169+00:00", "nick": "nikolaosk", "message": "to be crawled?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:23:32.745466+00:00", "nick": "nikolaosk", "message": "ok", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:23:34.154666+00:00", "nick": "Zladivliba", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:23:35.801525+00:00", "nick": "nikolaosk", "message": "weird", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:24:05.590736+00:00", "nick": "Zladivliba", "message": "well people pay to get many wierd things done ;))))", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:24:14.465408+00:00", "nick": "nikolaosk", "message": "so, then you can skip pipelines and everything", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:24:28.713631+00:00", "nick": "nikolaosk", "message": "and simplify it as much as possible", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:24:40.369494+00:00", "nick": "Zladivliba", "message": "humm currently I use piplelines", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:24:41.177991+00:00", "nick": "nikolaosk", "message": "like having the storage in the spider it self", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:24:48.014783+00:00", "nick": "nikolaosk", "message": "how many?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:25:00.418189+00:00", "nick": "Zladivliba", "message": "humm what ??", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:27:08.513461+00:00", "nick": "nikolaosk", "message": "pipelines?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:27:40.647986+00:00", "nick": "Zladivliba", "message": "I use one", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:27:47.672701+00:00", "nick": "Zladivliba", "message": "only", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:28:11.603953+00:00", "nick": "Zladivliba", "message": "it's very simple stuff", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:28:23.884241+00:00", "nick": "nikolaosk", "message": "just caching everything right?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:28:27.023567+00:00", "nick": "Zladivliba", "message": "and mostly I use that so I can pass data to my last function", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:28:29.729565+00:00", "nick": "Zladivliba", "message": "yearh pretty much", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:28:34.319630+00:00", "nick": "Zladivliba", "message": "+ some basic calculation", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:28:46.281555+00:00", "nick": "Zladivliba", "message": "Like how many pages have X or Z", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:29:10.060725+00:00", "nick": "nikolaosk", "message": "while processing items or when it closes?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:29:28.914233+00:00", "nick": "Zladivliba", "message": "like the spider looks every pages and says : this one has X problems, then I save this in the pipleine", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:29:58.448368+00:00", "nick": "Zladivliba", "message": "and once it's ended, the function gets all the data, and sends it to a mysqld db which collects all the information for each website", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:30:46.830220+00:00", "nick": "nikolaosk", "message": "I see, so the pipeline just accumulates some sums of so-and-so", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:31:14.238157+00:00", "nick": "nikolaosk", "message": "you can do this in the spider then", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:31:23.349740+00:00", "nick": "Zladivliba", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:31:23.481911+00:00", "nick": "nikolaosk", "message": "instead of yield item", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:31:24.686821+00:00", "nick": "Zladivliba", "message": "ah ok", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:31:29.437531+00:00", "nick": "Zladivliba", "message": "I didn't know that", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:31:43.840361+00:00", "nick": "Zladivliba", "message": "but how do I get the results ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:31:47.029852+00:00", "nick": "nikolaosk", "message": "you can have a self.accumulators_of_so_and_so", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:31:54.053583+00:00", "nick": "Zladivliba", "message": "and fetch my last page ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:32:01.413643+00:00", "nick": "Zladivliba", "message": "ahhh ok, I'll look it up !", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:32:03.778242+00:00", "nick": "nikolaosk", "message": "and when parsing pages", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:32:16.909930+00:00", "nick": "nikolaosk", "message": "you can increase specific accumulators", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:32:23.544360+00:00", "nick": "Zladivliba", "message": "ok", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:32:26.213317+00:00", "nick": "Zladivliba", "message": "nice !", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:32:37.581653+00:00", "nick": "nikolaosk", "message": "actually scrapy has some stats collector extension built in", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:32:53.209972+00:00", "nick": "nikolaosk", "message": "the you can define a closed(self, reason) method", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:33:01.753817+00:00", "nick": "nikolaosk", "message": "in the spider", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:33:05.707847+00:00", "nick": "Zladivliba", "message": "ok", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:33:09.349776+00:00", "nick": "nikolaosk", "message": "access thos stat and save away", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:33:12.761759+00:00", "nick": "Zladivliba", "message": "and hit my last page ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:33:27.078100+00:00", "nick": "nikolaosk", "message": "what do you mean hit your last page?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:33:49.291322+00:00", "nick": "Zladivliba", "message": "once i've scraped all the pages, I need to hit on one last page and analyse it", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:34:07.294740+00:00", "nick": "Zladivliba", "message": "and then I can upload the stuff in my db and end the spider", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:35:07.485668+00:00", "nick": "nikolaosk", "message": "aha", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:35:27.132412+00:00", "nick": "nikolaosk", "message": "so you have to make one request last", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:35:30.780479+00:00", "nick": "nikolaosk", "message": "but why?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:35:47.018549+00:00", "nick": "nikolaosk", "message": "does't it require any IO?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:36:05.976507+00:00", "nick": "nikolaosk", "message": "do you need to construct the request to the last page according to the data you 've been collecting?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:36:13.261630+00:00", "nick": "Zladivliba", "message": "yes I have to make a last request", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:36:23.853686+00:00", "nick": "Zladivliba", "message": "yes exactly !", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:36:36.151777+00:00", "nick": "Zladivliba", "message": "the scraping tells me which page is the one I'm looking for", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:36:43.972362+00:00", "nick": "Zladivliba", "message": "then once this is done I need to analyse it", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:36:56.663868+00:00", "nick": "nikolaosk", "message": "well that's a mess", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:37:01.970343+00:00", "nick": "Zladivliba", "message": "ahhh...", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:37:24.083112+00:00", "nick": "nikolaosk", "message": "by scrapping do you mean 100% of the scrapping, not a page less?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:37:37.268797+00:00", "nick": "Zladivliba", "message": "I was trying to look at this road : http://stackoverflow.com/questions/16970112/scr...", "links": ["http://stackoverflow.com/questions/16970112/scrapy-how-to-manually-insert-a-request-from-a-spider-idle-event-callback"], "channel": "scrapy"},
{"date": "2014-09-24T11:37:50.173415+00:00", "nick": "nikolaosk", "message": "or will you suddently bump on page and say \"aha, this is the last page\"", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:37:57.666590+00:00", "nick": "nikolaosk", "message": "or the link to eat", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:38:00.194185+00:00", "nick": "nikolaosk", "message": "it*", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:38:16.333777+00:00", "nick": "Zladivliba", "message": "it doesn't have to be 100% of the pages scraped but when I'm around 90% I know which one is the one I'm looking for", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:38:17.105027+00:00", "nick": "nikolaosk", "message": "lol, not that hungry", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:38:25.078910+00:00", "nick": "Zladivliba", "message": ";)", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:38:42.496934+00:00", "nick": "Zladivliba", "message": "No it's not like : I accidently can bump on that page", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:38:54.324259+00:00", "nick": "nikolaosk", "message": "aha", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:38:55.049768+00:00", "nick": "Zladivliba", "message": "it's the scraping that tells me which page it is", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:39:04.634782+00:00", "nick": "nikolaosk", "message": "but the final request", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:39:06.120183+00:00", "nick": "Zladivliba", "message": "because i give points to each page", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:39:10.607263+00:00", "nick": "nikolaosk", "message": "is revealed by a single page", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:39:21.789940+00:00", "nick": "Zladivliba", "message": "so when the scraping is done I know whoch page has the mosts points", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:39:31.710859+00:00", "nick": "Zladivliba", "message": "so I know wich one to hit", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:39:36.101389+00:00", "nick": "nikolaosk", "message": "ok, that's different then", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:39:52.192554+00:00", "nick": "Zladivliba", "message": "ah", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:45:23.523720+00:00", "nick": "nikolaosk", "message": "let me guess, you want to make a report like, I found 100 so-and-so in the website, the page with the most so-and-so was whosiewhatsit.html", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:47:18.889511+00:00", "nick": "Zladivliba", "message": "YES !!!", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:47:20.434042+00:00", "nick": "Zladivliba", "message": "exactly", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:47:55.376675+00:00", "nick": "Zladivliba", "message": "and I want to do this - if possible - from the piplelines", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:48:11.066071+00:00", "nick": "Zladivliba", "message": "because it's where I'm doing all my stuff", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:52:43.133570+00:00", "nick": "nikolaosk", "message": "what do you store in the database? a single report?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:53:01.553908+00:00", "nick": "nikolaosk", "message": "or all pages with the count of so-and-so", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:53:26.347267+00:00", "nick": "Zladivliba", "message": "I'm just storing a report of the data scraped", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:53:46.774172+00:00", "nick": "nikolaosk", "message": "how many rows per crawling session?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:53:54.065808+00:00", "nick": "nikolaosk", "message": "1 or as many as the pages?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:54:00.818395+00:00", "nick": "Zladivliba", "message": "one row for each website", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:54:08.717031+00:00", "nick": "Zladivliba", "message": "I'm not storing the pages for now", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:54:15.913604+00:00", "nick": "Zladivliba", "message": "(the results of the page)", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:56:51.479861+00:00", "nick": "Zladivliba", "message": "if it's too complex I'll just use urllib and move on", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:57:03.303394+00:00", "nick": "Zladivliba", "message": "nikolaosk:", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:57:33.641497+00:00", "nick": "nikolaosk", "message": "I can't know", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:57:56.923827+00:00", "nick": "nikolaosk", "message": "one last thing, why do you need the whole whosiewhatsit.html page?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:58:33.062386+00:00", "nick": "nikolaosk", "message": "I mean the whole body, does it need to go in the report?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:58:42.376467+00:00", "nick": "nikolaosk", "message": "because if it doesn't", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:59:04.999156+00:00", "nick": "Zladivliba", "message": "I need to fetch the entire page actually", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:59:13.189940+00:00", "nick": "Zladivliba", "message": "and analyze it too", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T11:59:33.761692+00:00", "nick": "nikolaosk", "message": "with different logic than the rest?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:00:33.842655+00:00", "nick": "Zladivliba", "message": "it's the analysis I'm making", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:00:56.414009+00:00", "nick": "Zladivliba", "message": "if you don't see a solution it doesn't matter I'll just fetch the page with urllib", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:01:35.150447+00:00", "nick": "nikolaosk", "message": "is it different analysis for this last page?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:02:27.750261+00:00", "nick": "Zladivliba", "message": "it's a different anlaysis then the one done in the items yes", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:02:37.254961+00:00", "nick": "Zladivliba", "message": "I have a specific class for that", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:02:54.154220+00:00", "nick": "nikolaosk", "message": "so, you will have to go through the dispatcher for the spider idle signal", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:03:05.534726+00:00", "nick": "nikolaosk", "message": "to fetch it", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:03:13.426646+00:00", "nick": "nikolaosk", "message": "or fetch this last one with url lib", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:03:41.819825+00:00", "nick": "nikolaosk", "message": "but you will have to do some extra reading to use it with the Selector class", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:04:07.636220+00:00", "nick": "Zladivliba", "message": "ok", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:04:11.261086+00:00", "nick": "Zladivliba", "message": "i'll take a look", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:04:14.610083+00:00", "nick": "Zladivliba", "message": "seems complex though", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:04:38.569494+00:00", "nick": "Zladivliba", "message": "I've tried that actually but was not able to make it work", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:04:59.694464+00:00", "nick": "Zladivliba", "message": "is the idle_signal only supposed to be launched through the spider ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:05:07.623900+00:00", "nick": "Zladivliba", "message": "Or can it be reached through the pipleine ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:05:25.982483+00:00", "nick": "Zladivliba", "message": "I wasn't able to intercept that signal inside the pipeline", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:05:45.801510+00:00", "nick": "nikolaosk", "message": "but to find the page with the maximum so-and-so, I don't know anything else than a dict", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:05:56.483996+00:00", "nick": "nikolaosk", "message": "I mean, anything thread safe", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:06:58.131276+00:00", "nick": "nikolaosk", "message": "usually this hints me that it's time to move to another lang", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:07:18.613037+00:00", "nick": "Zladivliba", "message": "ah...", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:07:20.481816+00:00", "nick": "Zladivliba", "message": "hummm", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:07:36.403300+00:00", "nick": "nikolaosk", "message": "whose strategy for concurrency is not a GIL", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:07:43.821918+00:00", "nick": "Zladivliba", "message": "GIL ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:07:51.495882+00:00", "nick": "nikolaosk", "message": "global interpreter lock", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:08:29.126087+00:00", "nick": "nikolaosk", "message": "but we went too far already", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:08:58.158400+00:00", "nick": "nikolaosk", "message": "there is something new for the signal", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:09:01.861822+00:00", "nick": "nikolaosk", "message": "not the dispatcher", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:09:44.941660+00:00", "nick": "Zladivliba", "message": "???", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:10:07.874084+00:00", "nick": "nyov", "message": "Zladivliba: dispatcher.connect(self.spider_idle, signals.spider_idle) only works if you have the signals imported: from scrapy import signals", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:10:23.976372+00:00", "nick": "nikolaosk", "message": "http://doc.scrapy.org/en/latest/topics/api.html...", "links": ["http://doc.scrapy.org/en/latest/topics/api.html#topics-api-signals"], "channel": "scrapy"},
{"date": "2014-09-24T12:10:56.925722+00:00", "nick": "nikolaosk", "message": "both: which scrapy version?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:11:52.617743+00:00", "nick": "Zladivliba", "message": "nyov: I've done that but no results", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:12:01.460822+00:00", "nick": "Zladivliba", "message": "the function is never launched", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:12:13.341665+00:00", "nick": "Zladivliba", "message": "I've tried to use your code", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:12:47.378021+00:00", "nick": "nyov", "message": "it only launches when the spider has no more URLs to crawl. or you connected it wrong", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:13:55.030747+00:00", "nick": "nikolaosk", "message": "import signals?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:14:11.570737+00:00", "nick": "nikolaosk", "message": "like a singleton or something?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:14:11.702948+00:00", "nick": "nyov", "message": "actually which one was that.. it's been too long ago for my backlog", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:14:28.474226+00:00", "nick": "Zladivliba", "message": "This is my code https://gist.github.com/anonymous/2c2442a4144d4...", "links": ["https://gist.github.com/anonymous/2c2442a4144d49e7a883"], "channel": "scrapy"},
{"date": "2014-09-24T12:15:00.370769+00:00", "nick": "nikolaosk", "message": "nonono", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:15:02.163196+00:00", "nick": "nikolaosk", "message": "this is old", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:15:04.373538+00:00", "nick": "nikolaosk", "message": "way old", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:15:23.219621+00:00", "nick": "nyov", "message": "? what is old", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:15:36.014063+00:00", "nick": "nyov", "message": "using dispatcher?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:15:46.824519+00:00", "nick": "nikolaosk", "message": "yes, to connect to the signals", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:16:03.853593+00:00", "nick": "Zladivliba", "message": "ok I'm using for the close signal", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:16:05.440944+00:00", "nick": "Zladivliba", "message": "and it works", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:16:05.977564+00:00", "nick": "nyov", "message": "so what's the new way?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:16:11.938867+00:00", "nick": "nikolaosk", "message": "nodays you define a class method in your class", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:16:19.909084+00:00", "nick": "nikolaosk", "message": "called from_crawler", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:16:22.846189+00:00", "nick": "nikolaosk", "message": "@classmethod", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:16:28.937854+00:00", "nick": "Zladivliba", "message": "i'm using this : dispatcher.connect(self.spider_closed, signals.spider_closed)", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:16:30.859991+00:00", "nick": "Zladivliba", "message": "and this works", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:16:31.943161+00:00", "nick": "nikolaosk", "message": "def from_crawler(cls, crawler):", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:16:41.961746+00:00", "nick": "Zladivliba", "message": "but this doesn't : dispatcher.connect(self.spider_idle, signals.spider_idle)", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:16:48.273558+00:00", "nick": "Zladivliba", "message": "I'm like WTF...", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:16:49.418758+00:00", "nick": "nikolaosk", "message": "this class method received the instatiated crawler", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:16:50.919502+00:00", "nick": "nyov", "message": "yeah, but only if you have a from_crawler", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:16:57.965981+00:00", "nick": "Zladivliba", "message": "yes exactly", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:17:07.849466+00:00", "nick": "Zladivliba", "message": "it's not working from the pipleines", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:17:12.658785+00:00", "nick": "nikolaosk", "message": "you then use it to connect to the signal", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:17:15.274386+00:00", "nick": "Zladivliba", "message": "and I need that from the pipleines", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:17:24.878198+00:00", "nick": "nyov", "message": "it IS working in the pipelines", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:17:34.656421+00:00", "nick": "nikolaosk", "message": "and return an instatiated class, here: the pipeline", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:17:41.100552+00:00", "nick": "Zladivliba", "message": "hummm... nyov have u seen my code ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:17:50.519245+00:00", "nick": "Zladivliba", "message": "maybe I'm doing somehting wrong", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:17:52.823622+00:00", "nick": "nikolaosk", "message": "no, it does", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:18:14.193010+00:00", "nick": "nikolaosk", "message": "I think in works on every extension-like class", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:18:17.764791+00:00", "nick": "nikolaosk", "message": "it*", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:18:22.413630+00:00", "nick": "Zladivliba", "message": "nikolaosk: would u have a piece of code as an example ? I'm kind of a very bad coder...", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:18:54.964716+00:00", "nick": "Zladivliba", "message": "unless I see an example I have really hard time making things happen", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:19:07.625742+00:00", "nick": "nikolaosk", "message": "write another method", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:19:17.043157+00:00", "nick": "nikolaosk", "message": "wait a sec", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:19:21.404340+00:00", "nick": "Zladivliba", "message": "ah", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:21:06.737864+00:00", "nick": "nikolaosk", "message": "http://dpaste.com/3MSRN62", "links": ["http://dpaste.com/3MSRN62"], "channel": "scrapy"},
{"date": "2014-09-24T12:21:36.751154+00:00", "nick": "nikolaosk", "message": "instead of init, from_crawler will be called", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:21:45.215540+00:00", "nick": "nikolaosk", "message": "with the crawler argument", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:21:55.871419+00:00", "nick": "nikolaosk", "message": "the you can use it to connect to the crawler", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:22:02.733844+00:00", "nick": "nikolaosk", "message": "if it doesn't work try this", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:22:12.049288+00:00", "nick": "nikolaosk", "message": "drop in a pdb shell in init", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:22:19.666544+00:00", "nick": "nikolaosk", "message": "and expirement connecting to the crawler", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:22:40.237732+00:00", "nick": "nikolaosk", "message": "mistek, to the signal", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:22:44.604160+00:00", "nick": "nikolaosk", "message": "mistake*", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:22:49.333586+00:00", "nick": "Zladivliba", "message": "ok sorry far too complex for me", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:22:52.376933+00:00", "nick": "Zladivliba", "message": ":(", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:23:01.099633+00:00", "nick": "nikolaosk", "message": "no, just another method", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:23:02.903027+00:00", "nick": "nikolaosk", "message": "that's all", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:23:26.112489+00:00", "nick": "nyov", "message": "lol", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:25:42.018145+00:00", "nick": "nikolaosk", "message": "why a separate class method? backward comp. with the old __init__ prototype?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:26:02.962693+00:00", "nick": "nikolaosk", "message": "or is there a reason to init some classes later", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:26:32.921489+00:00", "nick": "nikolaosk", "message": "and the abense of from_crawler hints that", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:26:34.737663+00:00", "nick": "nikolaosk", "message": "no idea", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:27:27.175428+00:00", "nick": "nyov", "message": "Zladivliba: all you need to add to your Pipeline is a method from_crawler like this:", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:27:34.593961+00:00", "nick": "nyov", "message": "@classmethod", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:27:34.726290+00:00", "nick": "nyov", "message": "def from_crawler(cls, crawler):", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:27:34.726356+00:00", "nick": "nyov", "message": "o = cls(crawler)", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:27:34.727240+00:00", "nick": "nyov", "message": "crawler.signals.connect(o.spider_idle signal=crawler.spider_idle)", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:27:35.638317+00:00", "nick": "nyov", "message": "return o", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:27:49.539323+00:00", "nick": "Zladivliba", "message": "ok thanks that's clear I'll try", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:27:58.032693+00:00", "nick": "nyov", "message": "forget about __init__ unless you need to change something there", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:28:49.073674+00:00", "nick": "nyov", "message": "then define a method named spider_idle(self, spider) which does your stuff", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:30:19.625926+00:00", "nick": "nikolaosk", "message": "o is the pipeline instance, name it that if you want", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:31:33.615003+00:00", "nick": "Zladivliba", "message": "nyov: there's a syntax error in your code I can't see where... https://gist.github.com/anonymous/77d1155bd200b...", "links": ["https://gist.github.com/anonymous/77d1155bd200b65aae9c"], "channel": "scrapy"},
{"date": "2014-09-24T12:32:44.589635+00:00", "nick": "nyov", "message": "okay, a missing comma", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:32:52.953534+00:00", "nick": "nyov", "message": "crawler.signals.connect(o.spider_idle, signal=crawler.spider_idle)", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:33:43.626074+00:00", "nick": "Zladivliba", "message": "'Crawler' object has no attribute 'spider_idle'", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:33:51.045801+00:00", "nick": "nyov", "message": "not? wait", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:34:27.879886+00:00", "nick": "nyov", "message": "now where did I get that from", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:36:03.367512+00:00", "nick": "nyov", "message": "oh well. add this:", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:36:05.355448+00:00", "nick": "nyov", "message": "from scrapy import signals", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:36:12.194100+00:00", "nick": "nyov", "message": "and change the line to", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:36:27.508756+00:00", "nick": "nyov", "message": "crawler.signals.connect(o.spider_idle, signal=signals.spider_idle)", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T12:39:34.478903+00:00", "nick": "nyov", "message": "and once you have that, make sure spider_idle looks like the `dont_close_me(self, spider)` at https://stackoverflow.com/questions/16970112/sc...", "links": ["https://stackoverflow.com/questions/16970112/scrapy-how-to-manually-insert-a-request-from-a-spider-idle-event-callback#answer-16971379"], "channel": "scrapy"},
{"date": "2014-09-24T12:58:08.110969+00:00", "nick": "nikolaosk", "message": "weren't signals just strings?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T13:04:54.214343+00:00", "nick": "nikolaosk", "message": "nop, primitives", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T13:04:56.099459+00:00", "nick": "nikolaosk", "message": "just named", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T13:05:15.174215+00:00", "nick": "nikolaosk", "message": "but there's a PR and a plan to turn them into classes", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T13:05:21.220279+00:00", "nick": "nikolaosk", "message": "to complicate things further", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T14:22:51.259971+00:00", "nick": "Zladivliba", "message": "hey !", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T14:23:31.529819+00:00", "nick": "Zladivliba", "message": "is there a way to invoke scrapy with specific command line arguments (so I can pass a few options specific to my crawler) ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T14:23:37.554229+00:00", "nick": "Zladivliba", "message": "what would be the right way to do that ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T14:31:19.438651+00:00", "nick": "kfb4", "message": "hey. has anyone had luck putting scrapy cralwers in the non-main thread? Ideally, I'd like to start a crawler thread in the background that I could progressively feed URLs into as part of a realtime system. Then I'd get the items back via a callback, something like how making async db calls works (e.g. the python cassandra driver).", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T14:31:36.217343+00:00", "nick": "kfb4", "message": "I keep getting a Twisted error about \"exceptions.ValueError: signal only works in main thread\"", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T14:33:49.593045+00:00", "nick": "kfb4", "message": "Actually, I found this, but I have a larger question if anyone is around to help. http://twistedmatrix.com/trac/wiki/FrequentlyAs...", "links": ["http://twistedmatrix.com/trac/wiki/FrequentlyAskedQuestions#Igetexceptions.ValueError:signalonlyworksinmainthreadwhenItrytorunmyTwistedprogramWhatswrong"], "channel": "scrapy"},
{"date": "2014-09-24T14:34:07.598134+00:00", "nick": "kfb4", "message": "Basically, if I have a crawler running in a background thread, is there any way to feed it URLs to crawl after it's started.", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T14:34:55.578966+00:00", "nick": "blusteal", "message": "kfb4 I was thinking about something like this as well", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T14:35:36.752120+00:00", "nick": "blusteal", "message": "I had an idea to have a database that your spider pings every so often", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T14:36:06.400341+00:00", "nick": "kfb4", "message": "Yeah, I'm hoping to embed it as part of a larger system. In this case Apache Storm.", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T14:36:27.958019+00:00", "nick": "blusteal", "message": "just start with a random url that leads to a site that you control", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T14:36:36.187712+00:00", "nick": "blusteal", "message": "actually there's a simpler way", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T14:36:57.489278+00:00", "nick": "kfb4", "message": "blusteal, I haven't worked with it recently, but we had scrapyd doing that via redis at one point. But it runs as its own thing, instead of being part of a larger pipeline.", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T14:37:26.806293+00:00", "nick": "blusteal", "message": "you control scrapyd w/ http requests", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T14:37:53.802567+00:00", "nick": "kfb4", "message": "yeah, we wrote our own queue for it with redis. this was an old version, tho.", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T14:38:09.565003+00:00", "nick": "blusteal", "message": "lets say you add some data to a database, send a http request to scrapyd telling it to launch a spider with some data from the db", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T14:38:32.494554+00:00", "nick": "blusteal", "message": "that way the jobs go off based on what's in the db or some system that you setup", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T14:38:44.426211+00:00", "nick": "kfb4", "message": "ah, i see", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T14:39:55.146349+00:00", "nick": "blusteal", "message": "with spiders if you try to set delays and timers as nyov told me it can cause issues w/ the spiders making them do strange things, just have a way to send http requests to scrapyd letting it fire off spiders when some condition is met with", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T14:40:29.821116+00:00", "nick": "blusteal", "message": "you can pass the spiders variables from scrapyd or you can have the spiders connect to the db and grab some data in their init functions before they start crawling", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T14:41:45.739963+00:00", "nick": "kfb4", "message": "yeah, it's not quite as flexible as i was hoping. i really want to use scrapy as a library within a larger system. with something like Storm, instead of running scrapyd servers, it would be components within a Storm topology. I just need scrapy playing nicer with other parts of the same python process. Twisted kind of takes over the whole thing when it starts running.", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T14:46:19.326626+00:00", "nick": "nyov", "message": "kfb4: it's happening. scrapy will soon be much more useable as a library", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T14:46:36.377415+00:00", "nick": "kfb4", "message": "hm. no way to do it in 0.24 then?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T14:46:59.015567+00:00", "nick": "blusteal", "message": "nyov mind expanding on that", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T14:46:59.147698+00:00", "nick": "nyov", "message": "though it'll still run on twisted, you just control the reactor in your app, yourself", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T14:48:00.388989+00:00", "nick": "kfb4", "message": "yeah, i've been wondering if i'll have to make a component for Storm that works with Twisted. It's a bit of pain I was hoping to avoid.", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T14:48:54.989384+00:00", "nick": "nyov", "message": "blusteal: mind expanding yourself? https://github.com/scrapy/scrapy/blob/master/se...", "links": ["https://github.com/scrapy/scrapy/blob/master/sep/sep-019.rst"], "channel": "scrapy"},
{"date": "2014-09-24T14:51:26.638092+00:00", "nick": "nyov", "message": "actually this might be a nicer ressource to read, http://gsocjumedina.blogspot.de/2014_08_01_arch...", "links": ["http://gsocjumedina.blogspot.de/2014_08_01_archive.html"], "channel": "scrapy"},
{"date": "2014-09-24T14:51:31.800270+00:00", "nick": "nyov", "message": "see the embedded code samples", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T14:51:53.460338+00:00", "nick": "kfb4", "message": "cool, i'll take a look", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T14:51:54.754072+00:00", "nick": "kfb4", "message": "thanks", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T14:52:47.606459+00:00", "nick": "nyov", "message": "this has been merged, if you want to play with this check out scrapy's master branch", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T14:52:48.247471+00:00", "nick": "Zladivliba", "message": "how come I get nothing when I do scrapyd-deploy -l", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T14:53:00.214268+00:00", "nick": "Zladivliba", "message": "I have scrapyd running on my machine", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T14:57:20.318596+00:00", "nick": "Zladivliba", "message": "ok cleared the problem... I needed to execute that from the root path of the priject", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:02:20.428637+00:00", "nick": "blusteal", "message": "nyov that's sweet", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:02:42.026579+00:00", "nick": "Zladivliba", "message": "is there a scrapyd expert here ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:03:37.789718+00:00", "nick": "nyov", "message": "Zladivliba: i guess not", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:03:46.976005+00:00", "nick": "Zladivliba", "message": "ok :(", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:03:47.514050+00:00", "nick": "nyov", "message": "blusteal: yeah, don't tell me. that's pretty awesome", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:07:18.755207+00:00", "nick": "blusteal", "message": "wow", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:07:27.577831+00:00", "nick": "blusteal", "message": "those new changes are looking really sweet omg", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:09:14.673835+00:00", "nick": "Zladivliba", "message": "hey !", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:09:43.894728+00:00", "nick": "Zladivliba", "message": "I have a project that is set up now. I need to run different websites based on that project", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:09:49.147782+00:00", "nick": "Zladivliba", "message": "what's the best way to do it ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:09:59.974686+00:00", "nick": "Zladivliba", "message": "should I copy my project and replace the urls ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:10:30.107297+00:00", "nick": "Zladivliba", "message": "or is there a way to start one website, and then another, etc.", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:11:07.566390+00:00", "nick": "blusteal", "message": "zladivliba you can add multiple start urls", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:11:51.402733+00:00", "nick": "Zladivliba", "message": "isn't three an option to start scrapy with command line arguments (the url...)", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:12:23.198015+00:00", "nick": "Zladivliba", "message": "I'll need to start mutiple process with different urls each time", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:12:30.386515+00:00", "nick": "Digenis", "message": "Zladivliba: yes", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:12:36.176699+00:00", "nick": "Zladivliba", "message": "ah !", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:12:42.609405+00:00", "nick": "Zladivliba", "message": "Digenis: how ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:12:58.495501+00:00", "nick": "Digenis", "message": "how do you start scrapy? with scrapyd or the scrapy command?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:13:08.470096+00:00", "nick": "Digenis", "message": "the scrapy \"crawl\" command*", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:13:49.746133+00:00", "nick": "Zladivliba", "message": "well currently with the crawl command but I'm trying to setup scrapyd", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:13:51.064755+00:00", "nick": "Zladivliba", "message": "also", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:13:58.081970+00:00", "nick": "Zladivliba", "message": "I don't know whioch is more suited", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:14:21.678061+00:00", "nick": "Digenis", "message": "scrapy crawl spidername -aArgument=string", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:14:32.273023+00:00", "nick": "Zladivliba", "message": "I have a project set up and now I'd like to run 500 different websites with it", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:14:38.820557+00:00", "nick": "Digenis", "message": "and in your spiders __init__", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:14:39.925830+00:00", "nick": "Zladivliba", "message": "or 50.000 websites with it", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:14:54.571200+00:00", "nick": "Digenis", "message": "add a keyword argument for \"Argument\"", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:15:00.597520+00:00", "nick": "Zladivliba", "message": "ok", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:15:08.106933+00:00", "nick": "Digenis", "message": "then set start_urls", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:15:23.322693+00:00", "nick": "Digenis", "message": "maybe there is something already for start urls but I am not sure", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:15:48.018137+00:00", "nick": "Digenis", "message": "I don't know what is going on in scrapyd", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:15:49.553855+00:00", "nick": "Zladivliba", "message": "ok so : scrapy crawl spidername -a urls=http://www.dmoz.org", "links": ["http://urls=http://www.dmoz.org"], "channel": "scrapy"},
{"date": "2014-09-24T15:16:08.605810+00:00", "nick": "Digenis", "message": "I think the core team is not willing to maintain it", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:16:22.697693+00:00", "nick": "Zladivliba", "message": "ah :(", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:16:30.204736+00:00", "nick": "Zladivliba", "message": "any way to reach the core team ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:16:35.210534+00:00", "nick": "Digenis", "message": "they assigned the repo to someone", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:16:40.591537+00:00", "nick": "Zladivliba", "message": "ah...", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:16:46.430428+00:00", "nick": "Zladivliba", "message": "ok this is bad... :(", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:16:47.529849+00:00", "nick": "Digenis", "message": "but they are propably busy on scraping hub", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:18:30.114743+00:00", "nick": "Digenis", "message": "there are discussions every once in a while on using scrapy outside the commandline and scrapyd", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:19:17.190365+00:00", "nick": "Digenis", "message": "but something as simple as coding scrapy projects as modules to be imported elsewhere", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:19:30.918399+00:00", "nick": "Zladivliba", "message": "would u recommend using scrapyd or only scrapy projects ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:19:32.587455+00:00", "nick": "Digenis", "message": "is not something I see coming soon if ever", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:20:04.054309+00:00", "nick": "Digenis", "message": "I use scrapyd but I really have trouble choosing too", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:20:21.256789+00:00", "nick": "Zladivliba", "message": "well I'd be willing to use scrapyd wich seems more suited to run mutiple crawls at the same time but it's a bit of a pain to setup", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:20:50.353189+00:00", "nick": "Zladivliba", "message": "by the way : I deployed a project in scrapyd but there's no news about it", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:20:59.241974+00:00", "nick": "Zladivliba", "message": "how do we know it's been executed... ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:23:44.541622+00:00", "nick": "nyov", "message": "< Digenis> they assigned the repo to someone", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:23:55.950261+00:00", "nick": "nyov", "message": "what do you mean? who is assigned for that?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:25:21.374312+00:00", "nick": "Zladivliba", "message": "Digenis: ok AWESOME, the url argument works, thanks a lot for the tip :)", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:25:30.541057+00:00", "nick": "blusteal", "message": "you know something that tripped me up a lil bit, when you apply a rule if you don't set follow=True you'll kinda run out of links I mean, it's a crawl spider but applying a callback switches the follow attribute to false", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:27:35.508114+00:00", "nick": "nyov", "message": "that's because once you put a custom callback method in there, you'll likely want to parse the _content_ of that page and not follow all it's links ad infinitum", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:28:19.210867+00:00", "nick": "nyov", "message": "or at least, only follow them selectively by returning requests from that method", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:30:12.334203+00:00", "nick": "nyov", "message": "so it's the safer default, not like you can't override it", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:31:03.603118+00:00", "nick": "blusteal", "message": "your right that it's safer but it kinda caught me off guard took me a few hours to figure out what was going on", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:31:57.968765+00:00", "nick": "blusteal", "message": "but i still love this framework, after about a week it've been getting it to do some amazing things. I still want to deep dive into the settings and pipelines but so far i haven't really needed to", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:36:28.538873+00:00", "nick": "Digenis", "message": "I don't remember, I watch every software I use at work and one day I saw an issue about it", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:37:32.989296+00:00", "nick": "Digenis", "message": "nyov: https://github.com/jayzeng", "links": ["https://github.com/jayzeng"], "channel": "scrapy"},
{"date": "2014-09-24T15:38:25.697972+00:00", "nick": "Digenis", "message": "he was supposed to review some old PRs and maybe merge", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:38:50.790787+00:00", "nick": "Digenis", "message": "there is some other guy who was working on a new gui", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:39:21.535256+00:00", "nick": "Digenis", "message": "but this other guy, he had not scrapyd fork in github", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:41:29.375139+00:00", "nick": "nyov", "message": "well. he's a member of the scrapy team, I don't see why it'd be wrong to assign people to tasks like that", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:41:43.209907+00:00", "nick": "Digenis", "message": "and I think it was going to be full of clutter, I think bootstrap was referenced somewhere", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:41:49.305922+00:00", "nick": "Digenis", "message": "but don't take my word for it", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:42:05.112969+00:00", "nick": "nyov", "message": "yeah that was a PR, but obviously it didn't get merged", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:42:18.369952+00:00", "nick": "Digenis", "message": "phew", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:42:20.885117+00:00", "nick": "nyov", "message": "haha", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:42:24.637399+00:00", "nick": "nyov", "message": "here, too", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:43:11.520986+00:00", "nick": "nyov", "message": "that'd be a shame. scrapyd should be a lightweight, web frontends are for sissies anyway. IF, then it should be built on top of the json-rpc api", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:43:40.994077+00:00", "nick": "Digenis", "message": "blusteal: go on, the code is readable", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:43:58.985623+00:00", "nick": "Digenis", "message": "nowdays I don't even read the doc", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:44:21.222648+00:00", "nick": "Digenis", "message": "just the code", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:44:41.533578+00:00", "nick": "Digenis", "message": "nyov: agreed", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:45:36.773148+00:00", "nick": "Digenis", "message": "but something happened to the json-rpc I think, stuff moved to other repos", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:45:42.746280+00:00", "nick": "Digenis", "message": "or this was something else", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:49:17.283740+00:00", "nick": "nyov", "message": "ah, that was just an include. I PR'd that bugfix. Some code got dropped from scrapy, which scrapyd still needed", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:50:21.536353+00:00", "nick": "nyov", "message": "it was previously imported from scrapy. It's actually better that way I think, if scrapyd doesn't depend so much on scrapy?", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:59:30.662872+00:00", "nick": "Digenis", "message": "well, I guess", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T15:59:40.873725+00:00", "nick": "Digenis", "message": "scrapy moves faster and may break things", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T17:14:56.109971+00:00", "nick": "Zladivliba", "message": "hey", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T17:15:25.065946+00:00", "nick": "Zladivliba", "message": "anyone know why i'm having this error : https://gist.github.com/anonymous/39f104e437c88...", "links": ["https://gist.github.com/anonymous/39f104e437c88f3fca7a"], "channel": "scrapy"},
{"date": "2014-09-24T17:15:35.462138+00:00", "nick": "Zladivliba", "message": "I've added an argument in the command line", "links": [], "channel": "scrapy"},
{"date": "2014-09-24T17:15:46.950292+00:00", "nick": "Zladivliba", "message": "and defined a init function in my crawler", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T05:33:45.756243+00:00", "nick": "Zladivliba", "message": "helloooo scrapy !!!", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T05:50:17.302068+00:00", "nick": "Digenis", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T05:50:32.439948+00:00", "nick": "Digenis", "message": "did you solve the _rules thingy?", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T05:53:46.944813+00:00", "nick": "Zladivliba", "message": "Digenis: hello !!!", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T05:53:57.202768+00:00", "nick": "Zladivliba", "message": "hummm what was that problem again ? ;)))", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T05:54:06.908829+00:00", "nick": "Zladivliba", "message": "Well I guess I probably did ;)", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T05:54:13.769797+00:00", "nick": "Zladivliba", "message": "I have a new one...", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T05:54:53.042375+00:00", "nick": "Zladivliba", "message": "I'm trying to make this work :  https://gist.github.com/anonymous/0fe0d04fcea2267", "links": ["https://gist.github.com/anonymous/0fe0d04fcea2267"], "channel": "scrapy"},
{"date": "2014-09-25T05:54:59.060984+00:00", "nick": "Zladivliba", "message": "passing arguments to scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T05:55:09.975804+00:00", "nick": "Zladivliba", "message": "using the URL of the website i'm crawling...", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T05:55:25.401222+00:00", "nick": "Zladivliba", "message": "I'm getting errors and I don't understand why...", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T06:07:52.354249+00:00", "nick": "Digenis", "message": "gist gone", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T06:08:25.878769+00:00", "nick": "Digenis", "message": "anyway, can't help you right now, ping nikolaosk after 2 hours", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T06:11:27.094746+00:00", "nick": "Zladivliba", "message": "ok", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T06:18:21.705440+00:00", "nick": "Digenis", "message": "http://www.freepen.gr/2014/09/blog-post_3078.html", "links": ["http://www.freepen.gr/2014/09/blog-post_3078.html"], "channel": "scrapy"},
{"date": "2014-09-25T06:18:31.650554+00:00", "nick": "Digenis", "message": "crap, wrong window", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T08:36:11.811411+00:00", "nick": "cp1024", "message": "Hi!", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T08:37:29.674501+00:00", "nick": "cp1024", "message": "Can anyone tell me is scrapy capable of getting email addresses from links that are encrypted up until they are clicked on, that run a javascript and display the correct data", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T08:46:33.217517+00:00", "nick": "nikolaosk", "message": "lol", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T08:46:33.679636+00:00", "nick": "Zladivliba", "message": "cp1024: you'll need to execute javascript then ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T08:46:34.269131+00:00", "nick": "nikolaosk", "message": "no", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T08:46:46.198693+00:00", "nick": "nikolaosk", "message": "maybe selenium or something", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T08:46:48.816930+00:00", "nick": "Zladivliba", "message": "If yes, scrapy is not your best friend", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T08:46:54.719898+00:00", "nick": "Zladivliba", "message": "hey nikolaosk !!!", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T08:47:04.229143+00:00", "nick": "nikolaosk", "message": "alo", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T08:47:47.945902+00:00", "nick": "Zladivliba", "message": "ok I have a big question for you ;)", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T08:48:06.184445+00:00", "nick": "Zladivliba", "message": "I've tried to implement the -a url=http...", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T08:48:13.581517+00:00", "nick": "Zladivliba", "message": "it half works", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T08:48:43.245683+00:00", "nick": "Zladivliba", "message": "when I do this, the url is never updated to self.start_url", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T08:48:51.354938+00:00", "nick": "Zladivliba", "message": "https://gist.github.com/anonymous/f74a266abba6a...", "links": ["https://gist.github.com/anonymous/f74a266abba6ab6745f0"], "channel": "scrapy"},
{"date": "2014-09-25T08:48:54.273381+00:00", "nick": "Zladivliba", "message": "any idea why ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T08:49:12.148242+00:00", "nick": "nikolaosk", "message": "I see", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T08:49:18.210450+00:00", "nick": "nikolaosk", "message": "I said keyword arguments", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T08:49:31.501242+00:00", "nick": "nikolaosk", "message": "replace url with **kwargs", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T08:49:37.745949+00:00", "nick": "nikolaosk", "message": "(double star included)", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T08:50:03.038067+00:00", "nick": "nikolaosk", "message": "and then you can say", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T08:50:14.087854+00:00", "nick": "nikolaosk", "message": "url = kwargs['url']", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T08:50:28.045895+00:00", "nick": "nikolaosk", "message": "or, you can even say url=None", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T08:50:33.202575+00:00", "nick": "nikolaosk", "message": "in place of the url", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T08:50:46.167537+00:00", "nick": "nikolaosk", "message": "def __init__(self, url=None):", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T08:51:25.882563+00:00", "nick": "nikolaosk", "message": "without a url arg it will be None", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T08:52:02.111604+00:00", "nick": "cp1024", "message": "ok and having only just found out about scrapy, is it possible to, erm for instance, a result from a site contains many pages, each with 30 results on it, and the difference between clicking, 1,2,3,4  in the url is oa30, oa60, oa90, etc", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T08:52:16.204488+00:00", "nick": "cp1024", "message": "is it possible to make scrapy goto each of these, and then run my spider", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T08:53:23.300162+00:00", "nick": "Zladivliba", "message": "nikolaosk: ok I'll try def __init__(self, url=None):", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T08:55:38.097481+00:00", "nick": "Zladivliba", "message": "nikolaosk: ok same problem, here's my code : https://gist.github.com/anonymous/789b83f852d01...", "links": ["https://gist.github.com/anonymous/789b83f852d01cc6a296"], "channel": "scrapy"},
{"date": "2014-09-25T08:55:51.861242+00:00", "nick": "Zladivliba", "message": "nikolaosk: and here's the error I get : NameError: name 'self' is not defined", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T08:56:14.111128+00:00", "nick": "Zladivliba", "message": "i've been working on this for 2 hours this morning... trying to solve it", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T08:59:24.533416+00:00", "nick": "Zladivliba", "message": "and exact same problem when I use the code from the documentation", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:04:11.953051+00:00", "nick": "nikolaosk", "message": "the self is only available on instance methods", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:04:28.877633+00:00", "nick": "nikolaosk", "message": "you are using it in a class attribute", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:04:52.292981+00:00", "nick": "Zladivliba", "message": "ok so what should I do ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:04:55.757707+00:00", "nick": "Zladivliba", "message": "I don't understand", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:05:00.798596+00:00", "nick": "nikolaosk", "message": "those last 2 lines", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:05:06.108083+00:00", "nick": "nikolaosk", "message": "move them in __init__", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:05:10.887627+00:00", "nick": "nikolaosk", "message": "and s/^/self./", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:10:03.340597+00:00", "nick": "nikolaosk", "message": "lol at #904", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:10:12.766804+00:00", "nick": "nikolaosk", "message": "somebody noticed", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:10:25.459300+00:00", "nick": "nikolaosk", "message": "yesterday's discussion", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:23:08.551125+00:00", "nick": "cp1024", "message": "so if an email link contains this, im screwed? :D", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:23:11.378484+00:00", "nick": "cp1024", "message": "<!-- function escramble_640(){ var a,b,c a='E' b='i' a+='-m' b+='l' c='a' document.write(a+c+b) } escramble_640() //--> E-mail", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:30:50.561852+00:00", "nick": "nikolaosk", "message": "no, you can use selenium or something", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:35:03.642650+00:00", "nick": "Zladivliba", "message": "nikolaosk: nope still not working : here's the code I changed : https://gist.github.com/anonymous/d79df6986e571...", "links": ["https://gist.github.com/anonymous/d79df6986e57147f9886"], "channel": "scrapy"},
{"date": "2014-09-25T09:35:27.644344+00:00", "nick": "Zladivliba", "message": "and here's the errors : https://gist.github.com/anonymous/84ca280599aa9...", "links": ["https://gist.github.com/anonymous/84ca280599aa9112d8a7"], "channel": "scrapy"},
{"date": "2014-09-25T09:35:38.981905+00:00", "nick": "Zladivliba", "message": "\"exceptions.AttributeError: 'MySpider' object has no attribute 'start_urls'\"", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:39:06.419049+00:00", "nick": "blusteal", "message": "Zladivliba when you see \"MySpider' has no attribute 'stat_urls' it means you didn't define the object start_urls or if your calling it you should put self.stat_urls", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:39:35.038307+00:00", "nick": "Zladivliba", "message": "blusteal: nikolaosk advised me to remove self.", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:39:39.029132+00:00", "nick": "blusteal", "message": "Zladivliba at line 8 add start_urls = [ ]", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:40:41.197322+00:00", "nick": "blusteal", "message": "a bit difficult to tell you how to fix the error at line 21 since i didn't see how you passed in your start urls", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:41:15.740822+00:00", "nick": "blusteal", "message": "http://doc.scrapy.org/en/latest/topics/spiders....", "links": ["http://doc.scrapy.org/en/latest/topics/spiders.html"], "channel": "scrapy"},
{"date": "2014-09-25T09:41:29.077660+00:00", "nick": "Zladivliba", "message": "ok it seems better", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:41:30.976815+00:00", "nick": "blusteal", "message": "press control F to find scrapy crawl myspider -a category=electronics", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:41:46.338915+00:00", "nick": "blusteal", "message": "see how they pass the arguments and how they handle it", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:41:59.726231+00:00", "nick": "Zladivliba", "message": "I still have one probleme I can't understand : https://gist.github.com/anonymous/8c435e6072e7f...", "links": ["https://gist.github.com/anonymous/8c435e6072e7feb0c887"], "channel": "scrapy"},
{"date": "2014-09-25T09:42:10.089636+00:00", "nick": "blusteal", "message": "if your just copy/pasting and not understanding whats  happening your going to keep on running into issues", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:42:14.079243+00:00", "nick": "Zladivliba", "message": "Now in my code I define a dictionnary mentionsLegalesdict = {}", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:42:24.662675+00:00", "nick": "Zladivliba", "message": "I don't know why it's not set up...", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:42:36.055148+00:00", "nick": "Zladivliba", "message": "when I remove the init function everything works blusteal", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:42:44.885218+00:00", "nick": "Zladivliba", "message": "any idea what's messing up?", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:43:12.964527+00:00", "nick": "blusteal", "message": "let's stay", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:43:24.484767+00:00", "nick": "blusteal", "message": "you get in your car, put the key in the engine and turn", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:43:37.621350+00:00", "nick": "blusteal", "message": "lots of things happen behind the turning of the key but the designers built the car so it just works", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:43:38.946151+00:00", "nick": "Zladivliba", "message": "hahah ok", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:43:47.981423+00:00", "nick": "Zladivliba", "message": "ok", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:44:04.832862+00:00", "nick": "blusteal", "message": "now lets say you want to change it so when you turn your key, something happens before the engine starts", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:44:28.961555+00:00", "nick": "blusteal", "message": "you have to figure out how to make the engine send a signal to do whatever you want, then whatever you did should also send a signal to start the engine", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:44:50.109405+00:00", "nick": "blusteal", "message": "when you mess with an __init___ function your doing basically that", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:45:09.425156+00:00", "nick": "blusteal", "message": "but you don't quite understand how to make your function send the proper signal to start the engine", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:45:16.554717+00:00", "nick": "Zladivliba", "message": "ah ok", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:45:19.457452+00:00", "nick": "Zladivliba", "message": "yes I get it", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:45:21.858042+00:00", "nick": "blusteal", "message": "for now", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:45:35.839202+00:00", "nick": "blusteal", "message": "it's best just to hard code the values that you want into scrapy spider.py files", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:45:47.794984+00:00", "nick": "blusteal", "message": "until you have a better understanding of how to do more dynamic stuff", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:45:54.440810+00:00", "nick": "Zladivliba", "message": "ok", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:45:54.768957+00:00", "nick": "blusteal", "message": "with scrapy and python you see a funky message", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:46:13.409386+00:00", "nick": "blusteal", "message": "imagine if you were doing this to a car and if you didn't do it properly the car would explode and you died....", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:46:22.206879+00:00", "nick": "Zladivliba", "message": "ok one way to do it would be to fetch it from a database", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:46:30.720947+00:00", "nick": "blusteal", "message": "while not that serious and  just a metaphor", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:46:36.623579+00:00", "nick": "Zladivliba", "message": "ok yearh well hopefully the app won't blow into my face ;)", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:46:42.351604+00:00", "nick": "Zladivliba", "message": "but I get you", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:46:43.182380+00:00", "nick": "nyov", "message": "if you re-defined __init__ in a subclassed class, you must call the parent's class __init__ as well. that's what super() is for", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:46:57.459077+00:00", "nick": "blusteal", "message": "Zladivliba that's true, do you know how to connect to a database?", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:47:05.249838+00:00", "nick": "Zladivliba", "message": "blusteal: sure !", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:47:17.201047+00:00", "nick": "blusteal", "message": "what are you trying to do?", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:48:09.677759+00:00", "nick": "Zladivliba", "message": "Ok here's the thing : I make an analysis of websites. So I have the same program for every website I'm analysing. So I wanted to send parameters to start scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:48:47.813287+00:00", "nick": "Zladivliba", "message": "I have 3 options :", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:48:54.871510+00:00", "nick": "Zladivliba", "message": "1) Use parameters", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:49:03.413520+00:00", "nick": "Zladivliba", "message": "2) Fetch the website through a db", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:49:36.229627+00:00", "nick": "Zladivliba", "message": "3) Make a copy of the program each time I need it and replace the main variables (mostly the urls)", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:49:54.763384+00:00", "nick": "blusteal", "message": "for now do #3", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:50:38.141313+00:00", "nick": "Zladivliba", "message": "ok", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:50:39.459075+00:00", "nick": "blusteal", "message": "watch these", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:50:40.303112+00:00", "nick": "blusteal", "message": "https://www.youtube.com/watch?v=ffWcSDzI-o8", "links": ["https://www.youtube.com/watch?v=ffWcSDzI-o8"], "channel": "scrapy"},
{"date": "2014-09-25T09:51:00.705197+00:00", "nick": "blusteal", "message": "take some time and learn these things, it'll help you more than you realize now", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:51:04.394907+00:00", "nick": "blusteal", "message": "good luck", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:51:06.683134+00:00", "nick": "blusteal", "message": "i gotta run", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T09:52:57.159970+00:00", "nick": "Zladivliba", "message": "ok, thanks !!", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T10:04:56.038276+00:00", "nick": "cp1024", "message": "Could someone maybe help me with a regex? i need to get the email address, basically return the content between the start and end text, like this this,'email@email.com',&#10;", "links": ["mailto:this,'email@email.com',&#10"], "channel": "scrapy"},
{"date": "2014-09-25T10:06:43.144311+00:00", "nick": "cp1024", "message": "basically i want to start of the text \"this,\" end of the string would be \"'&#10;\"", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T10:06:52.853004+00:00", "nick": "cp1024", "message": "and just return the email address", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T10:10:15.562619+00:00", "nick": "nikolaosk", "message": "Zladivliba: I told you to prepend self", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T10:10:29.285378+00:00", "nick": "Zladivliba", "message": "prepend ???", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T10:10:31.626964+00:00", "nick": "Zladivliba", "message": "I thought remove", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T10:10:35.604362+00:00", "nick": "nikolaosk", "message": "yes, on the lvalue", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T10:10:43.326971+00:00", "nick": "Zladivliba", "message": "lvalue ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T10:10:51.568346+00:00", "nick": "nikolaosk", "message": "well, I should have been more exmplicit", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T10:10:54.808851+00:00", "nick": "Zladivliba", "message": "sorry don't understand", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T10:10:55.725966+00:00", "nick": "nikolaosk", "message": "yes, the assignment", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T10:11:26.188907+00:00", "nick": "Zladivliba", "message": "well I'm going for the copy/replace solution now", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T10:11:30.288342+00:00", "nick": "Zladivliba", "message": "instead of the arguments", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T10:11:44.885586+00:00", "nick": "nikolaosk", "message": "https://gist.github.com/anonymous/789b83f852d01...", "links": ["https://gist.github.com/anonymous/789b83f852d01cc6a296"], "channel": "scrapy"},
{"date": "2014-09-25T10:11:48.927527+00:00", "nick": "Zladivliba", "message": "blusteal advised me to drop the first one", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T10:11:49.969374+00:00", "nick": "nikolaosk", "message": "here", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T10:12:06.166454+00:00", "nick": "nikolaosk", "message": "when you move the last two lines inside __init__", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T10:12:29.897923+00:00", "nick": "nikolaosk", "message": "you have to use the asignees as attributes of self", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T10:12:55.605651+00:00", "nick": "nikolaosk", "message": "because you are not defining class attributes any more", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T10:13:09.843007+00:00", "nick": "nikolaosk", "message": "but initializing instance attributes", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T10:13:15.934369+00:00", "nick": "nikolaosk", "message": "so, self.allowed_domains = self.ad", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T10:13:30.110712+00:00", "nick": "nikolaosk", "message": "or not at all just [url]", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T13:53:48.214784+00:00", "nick": "jsjc", "message": "Hi! I wonder if any of you knows if is possible and how I could add Fields to my Items from the spider", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T15:02:21.534635+00:00", "nick": "Digenis", "message": "jsjc: do you want a more flexible item class?", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T15:02:36.488747+00:00", "nick": "Digenis", "message": "assigning fields as you would in a dict", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T15:02:42.741065+00:00", "nick": "Digenis", "message": "with no restrictions", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T15:15:40.953420+00:00", "nick": "jsjc", "message": "digenis that is the option\u2026 at the moment I am doing some sort of stuffing everything into a field and after i do a post-processing of the result.", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T15:19:37.010182+00:00", "nick": "Digenis", "message": "that's an option", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T15:20:00.010752+00:00", "nick": "Digenis", "message": "you can also try defining the item class on runtime", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T15:20:29.520671+00:00", "nick": "Digenis", "message": "I 'd check for possible python3 pitfalls if I did this though", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T15:21:28.465763+00:00", "nick": "Digenis", "message": "haven't tried it but you could have a class factory", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T15:22:13.849118+00:00", "nick": "Digenis", "message": "that turns a dict to an instatiated item class with its attributes made from the keys", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T15:25:29.376107+00:00", "nick": "nyov", "message": "I believe there is something in the docs about dynamic items", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T17:55:39.075186+00:00", "nick": "brotherBox", "message": "Hi, is there a way to repeat a request in scrapy after x seconds?", "links": [], "channel": "scrapy"},
{"date": "2014-09-25T18:25:39.980626+00:00", "nick": "AndroidLoverInSF", "message": "does anyone know if rotating proxies is effective in avoid getting robot blocks? how do you prevent from getting blocked?", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T07:44:23.521093+00:00", "nick": "kakashi__", "message": "Hi, all", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T07:44:44.302702+00:00", "nick": "kakashi__", "message": "I have a question about crawl https site by proxy", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T07:45:09.727798+00:00", "nick": "kakashi__", "message": "I have set request.meta['proxy'] = \"https://\" + proxy_ip", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T07:45:57.294908+00:00", "nick": "kakashi__", "message": "but when I try to crawl google site, I always get \"exception:Could not open CONNECT tunnel\"", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T07:50:43.821295+00:00", "nick": "nikolaosk", "message": "kakashi__: have you tried this from curl, or a browser", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T07:50:53.438864+00:00", "nick": "kakashi__", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T07:50:59.298970+00:00", "nick": "kakashi__", "message": "curl is ok!", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T07:51:19.771031+00:00", "nick": "nikolaosk", "message": "what about the proxy port?", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T07:51:35.017794+00:00", "nick": "nikolaosk", "message": "do you include it in proxy_ip?", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T07:52:01.646192+00:00", "nick": "kakashi__", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T07:52:34.770148+00:00", "nick": "nikolaosk", "message": "hmm", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T07:52:36.393595+00:00", "nick": "kakashi__", "message": "actually, when I use request.meta['proxy'] = \"http://\" + proxy_ip, it can perfectly crawl non-https site", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T07:52:55.502879+00:00", "nick": "nikolaosk", "message": "yes, I was about to ask about this", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T07:54:06.520399+00:00", "nick": "nikolaosk", "message": "which version of twisted do you have?", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T07:56:09.623925+00:00", "nick": "nikolaosk", "message": "this is a blind shot but try installing service-identity", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T07:56:19.093134+00:00", "nick": "nikolaosk", "message": "if you don't have it already", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T07:56:21.036336+00:00", "nick": "nyov", "message": "I'd rather ask what your proxy is. probably you cannot connect to it at 443 (https) and need to use \"http://\" + proxy_ip aswell", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T07:57:47.473076+00:00", "nick": "kakashi__", "message": "Twisted==14.0.2", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T07:57:58.778268+00:00", "nick": "nyov", "message": "if this a *nix like environment, you can always export https_proxy=\"http://myproxy:8080\" and scrapy will pick it up", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T07:58:28.482870+00:00", "nick": "nyov", "message": "s/always/also/", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T07:58:47.695880+00:00", "nick": "kakashi__", "message": "okay, let me try export https_proxy=\"http://myproxy:8080\"", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T07:59:12.429295+00:00", "nick": "nyov", "message": "notice how it's using http, to go to the proxy and do a CONNECT.", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T07:59:59.589647+00:00", "nick": "Zladivliba", "message": "hello scrapyyyyyyy !!!!!", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T08:00:09.696344+00:00", "nick": "Zladivliba", "message": "I have a quick question !", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T08:00:37.694583+00:00", "nick": "Zladivliba", "message": "Is it possible to personalize the behavior of scrapy based on robots.txt", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T08:00:41.433543+00:00", "nick": "nikolaosk", "message": "kakashi__: when you tried with curl, did you use it as an https proxy?", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T08:00:49.918869+00:00", "nick": "kakashi__", "message": "sorry, how can I export https_proxy with auth, like  https_proxy=\"http://user:passwd@myproxy:8080\"?", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T08:01:19.908955+00:00", "nick": "nyov", "message": "that I don't know", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T08:01:21.217709+00:00", "nick": "Zladivliba", "message": "I want webistes to be able to refuse my scrapy robot (like deny: mybot)", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T08:01:28.707757+00:00", "nick": "Zladivliba", "message": "is this possible ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T08:01:47.597805+00:00", "nick": "nikolaosk", "message": "Zladivliba: scrapy already handles robots.txt", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T08:02:07.909331+00:00", "nick": "Zladivliba", "message": "nikolaosk: yes but for all bots, not for my specific bot", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T08:03:06.798565+00:00", "nick": "kakashi__", "message": "@nikolaosk yes, I use it like curl --proxy https://ip:port --proxy-user user:passwd https://www.google.co.jp/", "links": ["https://ip:port", "https://www.google.co.jp/"], "channel": "scrapy"},
{"date": "2014-09-26T08:03:09.463524+00:00", "nick": "Zladivliba", "message": "User-agent: googlebot", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T08:03:09.661504+00:00", "nick": "Zladivliba", "message": "Disallow:", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T08:03:10.985537+00:00", "nick": "Zladivliba", "message": "I mean if I say", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T08:03:15.527595+00:00", "nick": "Zladivliba", "message": "this it will worj ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T08:03:28.650551+00:00", "nick": "Zladivliba", "message": "shit bad paste...", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T08:03:39.629780+00:00", "nick": "Zladivliba", "message": "ok let me say this clearly...", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T08:04:13.186337+00:00", "nick": "nikolaosk", "message": "Zladivliba: wait, just noticed", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T08:04:18.945999+00:00", "nick": "Zladivliba", "message": "if I put \"bigBot\" in the user agent, and someone puts \"User-agent: bigBot, Deny\", will scrapy scrape the website", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T08:04:30.106674+00:00", "nick": "Zladivliba", "message": "?", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T08:04:48.985769+00:00", "nick": "nikolaosk", "message": "you need a setting", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T08:04:50.419472+00:00", "nick": "nikolaosk", "message": "if not crawler.settings.getbool('ROBOTSTXT_OBEY'):", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T08:04:50.551569+00:00", "nick": "nikolaosk", "message": "raise NotConfigured", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T08:04:58.817012+00:00", "nick": "Zladivliba", "message": "or, will scrapy only work for \"User-agent: * Deny\"", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T08:05:14.837474+00:00", "nick": "nikolaosk", "message": "duh, ignore the rest, jut the string inside getbool", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T08:05:16.277201+00:00", "nick": "Zladivliba", "message": "nikolaosk: I've setup ROBOTSTST_OBEY", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T08:05:35.149500+00:00", "nick": "Zladivliba", "message": "yearh but that's the whole problem !", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T08:06:02.985261+00:00", "nick": "Zladivliba", "message": "my feeling is that scrapy probably just analyses the robot.txt if there's like User-agent: *", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T08:06:05.498987+00:00", "nick": "nikolaosk", "message": "kakashi__: so, the https proxy works only with curl, not with scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T08:06:08.473627+00:00", "nick": "Zladivliba", "message": "and not specifically", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T08:06:22.533643+00:00", "nick": "nikolaosk", "message": "Zladivliba: do you want to enable it for a specific spider?", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T08:06:46.943851+00:00", "nick": "Zladivliba", "message": "nikolaosk: I want someone to be able to deny my specific bot, based on the user agent i'm going to set up", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T08:06:58.305177+00:00", "nick": "Zladivliba", "message": "if possible", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T08:07:02.601945+00:00", "nick": "nikolaosk", "message": "I think wildcards are not supported, there was some PR recently", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T08:07:08.773115+00:00", "nick": "nikolaosk", "message": "just a sec to look the source", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T08:08:09.292269+00:00", "nick": "nikolaosk", "message": "dunno, it's an external module", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T08:08:27.543461+00:00", "nick": "kakashi__", "message": "@nikolaosk yes...", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T08:08:38.128763+00:00", "nick": "Zladivliba", "message": "ah, the python robot thing, yearh its what I thought", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T08:08:41.649335+00:00", "nick": "Zladivliba", "message": "then it's not supported", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T08:09:21.184802+00:00", "nick": "nikolaosk", "message": "kakashi__: you can try filling a bug report then, or asking in #twisted first about https proxies", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T08:09:26.517037+00:00", "nick": "nikolaosk", "message": "I think the webclient is theirs", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T08:09:44.961272+00:00", "nick": "nikolaosk", "message": "Zladivliba: you can override settings per run", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T08:10:02.002158+00:00", "nick": "Zladivliba", "message": "hummm", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T08:10:07.430836+00:00", "nick": "nikolaosk", "message": "scrapy crawl spidername -sROBOTS_OBEY=1", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T08:10:16.489763+00:00", "nick": "Zladivliba", "message": "oh it's already configured", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T08:10:22.722910+00:00", "nick": "Zladivliba", "message": "that's not a problem", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T08:10:44.133594+00:00", "nick": "nikolaosk", "message": "but you want to do this for a specific spider, I think there were some recent developments on this", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T08:11:14.401289+00:00", "nick": "Zladivliba", "message": "the problem was : if I scrape a website with a user-agent bigBot, and the guys puts \"bigBot deny\" in it will scrapy scrape it or not", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T08:11:16.336189+00:00", "nick": "nikolaosk", "message": "maybe on one of the gsoc projects, look up per spider settings in scrapy's github", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T08:11:18.362782+00:00", "nick": "Zladivliba", "message": "and the anwser is : it will", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T08:11:30.773079+00:00", "nick": "Zladivliba", "message": "ok", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T08:11:36.111406+00:00", "nick": "nikolaosk", "message": "but I doubt it's on the current stable release", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T08:12:24.893347+00:00", "nick": "nikolaosk", "message": "it will? how do you know?", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T09:40:54.314981+00:00", "nick": "nikolaosk", "message": "a question to scrapy developers", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T09:41:40.274367+00:00", "nick": "nikolaosk", "message": "why does the use of %s string formatting prevails so much?", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T09:42:10.442114+00:00", "nick": "nikolaosk", "message": "is it just because of the early days of scrapy?", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T09:47:31.632524+00:00", "nick": "nyov", "message": "because there's nothing wrong with it ;)", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T09:48:43.162572+00:00", "nick": "nyov", "message": "actually the scrapy dev's you're questioning aren't here I believe", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T09:50:40.261922+00:00", "nick": "nyov", "message": "but IMO, %s formatting is great because it's the common style used in several other languages. the others are ugly python specifics, and I use them only where necessary", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T09:58:41.643000+00:00", "nick": "nikolaosk", "message": "I despise it for multiple reasons", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T10:00:20.561769+00:00", "nick": "nikolaosk", "message": "but mostly because of the loose typing", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T10:01:05.893692+00:00", "nick": "nikolaosk", "message": "formats can be used in many ways", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T10:03:05.383956+00:00", "nick": "nikolaosk", "message": "you can construct a format string and pass its bound format method to a high order function", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T10:07:52.799851+00:00", "nick": "nikolaosk", "message": "of course you can always '%s'.__mod__", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T10:52:33.235114+00:00", "nick": "nyov", "message": "interesting that users of dynamic typed python would have such grief with this format, where as users of static typed C don't seem to have these issues with it?", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T10:57:47.239570+00:00", "nick": "nikolaosk", "message": "nyov: do you mean issues with variadic functions in C?", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T10:58:34.623118+00:00", "nick": "nikolaosk", "message": "I do mind, or maybe not", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T10:58:50.077630+00:00", "nick": "nikolaosk", "message": "it's a love&hate relationship", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T10:59:21.383728+00:00", "nick": "nikolaosk", "message": "but guessing over a collection or a single argument", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T10:59:39.405219+00:00", "nick": "nikolaosk", "message": "in '%s' % (s,) and '%s' % s", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T10:59:51.665373+00:00", "nick": "nyov", "message": "okay there is that", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T10:59:54.365629+00:00", "nick": "nikolaosk", "message": "makes me nuts", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T11:00:58.631889+00:00", "nick": "nikolaosk", "message": "but at least not as nuts as accidently treating strings as a collections", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T11:01:10.979701+00:00", "nick": "nikolaosk", "message": "(some_str)", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T11:01:15.951871+00:00", "nick": "nikolaosk", "message": "oups, not a tuple!", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T11:01:33.188101+00:00", "nick": "nyov", "message": "that would be erlang style...", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T11:05:49.848257+00:00", "nick": "nikolaosk", "message": "it's a tuple! it's a string! No, def quack(self):", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T11:06:20.658811+00:00", "nick": "nyov", "message": "?", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T11:06:27.567222+00:00", "nick": "nikolaosk", "message": "duck typing joke", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T11:06:41.202767+00:00", "nick": "nikolaosk", "message": "(if it quacks)", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T17:49:57.293654+00:00", "nick": "brotherBox", "message": "Hi, is there any way to repeat a request with a delay of x seconds?", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T18:00:19.869869+00:00", "nick": "nyov", "message": "not really. how? why?", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T18:01:18.003585+00:00", "nick": "brotherBox", "message": "Since there are pages which sometimes ask you to watch some advertisement for 10 seconds", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T18:02:03.254630+00:00", "nick": "nyov", "message": "won't they just pop back in the next time you go there?", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T18:02:37.594410+00:00", "nick": "nyov", "message": "and how does that even happen without javascript...", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T18:03:23.267149+00:00", "nick": "brotherBox", "message": "I'm not sure. I was just wondering if that is possible - I know that the twisted reactor can do callLater", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T18:03:50.784241+00:00", "nick": "nyov", "message": "anyway, maybe you could implement different scheduler queues for that, and a timing function that executes those requests delayed on another queue", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T18:04:36.764165+00:00", "nick": "brotherBox", "message": "I have no idea how I would even approach that lol. I'll research scrapy queues and schedulers.", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T18:04:45.391654+00:00", "nick": "nyov", "message": "sure, you can run requests later, though probably not exactly 10 seconds since then and then", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T18:04:55.910038+00:00", "nick": "nyov", "message": "actually I just remembered.. this should work nicely with the priority setting", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T18:05:53.874497+00:00", "nick": "nyov", "message": "you could delay the request 10 seconds, then inject it with a very high priority so it gets executed instantly, before the other stuff gets resumed", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T18:06:35.077006+00:00", "nick": "brotherBox", "message": "How could I delay just this one request? As it happens in one thread I can't just do time.sleep in my mind", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T18:07:50.712717+00:00", "nick": "nyov", "message": "with calllater, as you mentioned", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T18:08:41.924304+00:00", "nick": "brotherBox", "message": "Yes, but scrapy is built on top of twisted; I'd consider it a bit \"low-level\" to mess with the reactor from within twisted", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T18:08:51.431480+00:00", "nick": "brotherBox", "message": "Or rather from within scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T18:09:40.277829+00:00", "nick": "nyov", "message": "haha", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T18:11:07.842037+00:00", "nick": "nyov", "message": "look at scrapy/utils/defer.py", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T18:11:32.390138+00:00", "nick": "nyov", "message": "you can just do like in defer_succeed", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T18:12:14.723443+00:00", "nick": "nyov", "message": "(with an added timeout). maybe name it defer_ten_seconds(result)", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T18:12:48.245117+00:00", "nick": "brotherBox", "message": "Ah, thats pretty much what I wanted. Thanks!", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T18:16:35.852606+00:00", "nick": "nyov", "message": "and for priority of the new request, you could do Request(url, ..., priority=2)", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T18:16:57.196747+00:00", "nick": "nyov", "message": "default is 0, so 2 should be high enough", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T18:17:39.620421+00:00", "nick": "nyov", "message": "e.g. REDIRECT_PRIORITY_ADJUST = +2 and RETRY_PRIORITY_ADJUST = -1", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T18:18:28.470371+00:00", "nick": "nyov", "message": "this means redirects happen pretty much instantly, and retrys (server timeouts and such) happen some time later, when there's nothing much else going on", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T18:18:48.172159+00:00", "nick": "brotherBox", "message": "I get it ;)", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T18:21:11.389737+00:00", "nick": "nyov", "message": "just thinking about that... I think different priorities are actually different scheduler queues", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T18:21:24.153220+00:00", "nick": "nyov", "message": "need to investigate that some time", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T18:23:03.723765+00:00", "nick": "nyov", "message": "also. sqlalchemy orm makes me go nuts. grr. and I hate EAV table schemes", "links": [], "channel": "scrapy"},
{"date": "2014-09-26T18:27:58.306813+00:00", "nick": "nyov", "message": "has anyone else mapped a scrapy item to an EAV database, by chance?", "links": [], "channel": "scrapy"},
{"date": "2014-09-27T05:03:36.522049+00:00", "nick": "dhruvagga", "message": "Hello I am new to this organisation and i want to contribute . I am fairly good in python. Please suggest me from where to start.", "links": [], "channel": "scrapy"},
{"date": "2014-09-27T07:28:08.513438+00:00", "nick": "rohitt", "message": "Hi all!", "links": [], "channel": "scrapy"},
{"date": "2014-09-27T07:31:19.076813+00:00", "nick": "rohitt", "message": "Is there any way, to return special error codes while running scrapy, to indicate something went wrong? I am invoking raising CloseSpider exception if scraped items are missing critical fields. At the same time, I want to return some integer (like sys.exit(1)). Is that possible? I couldn't find any docs on this in Scrapy's documentation.", "links": [], "channel": "scrapy"},
{"date": "2014-09-27T07:31:37.261144+00:00", "nick": "rohitt", "message": "(like UNIX programs, which returns 0, on success and others on errors)", "links": [], "channel": "scrapy"},
{"date": "2014-09-27T07:42:05.923080+00:00", "nick": "dpn`", "message": "rohitt, there's hooks for it", "links": [], "channel": "scrapy"},
{"date": "2014-09-27T07:42:12.332447+00:00", "nick": "dpn`", "message": "I can't figure them out at a cursory look though", "links": [], "channel": "scrapy"},
{"date": "2014-09-27T07:43:21.040825+00:00", "nick": "rohitt", "message": "I think I can raise CloseSpider in item pipelines with reason set to something, have close_spider in pipeline, and check everytime spider closes for \"reason\", and exit with sys.exit(SOME_INT) on errors.", "links": [], "channel": "scrapy"},
{"date": "2014-09-27T07:43:41.041958+00:00", "nick": "rohitt", "message": "sounds good?", "links": [], "channel": "scrapy"},
{"date": "2014-09-27T07:43:50.677593+00:00", "nick": "dpn`", "message": "https://github.com/scrapy/scrapy/blob/master/sc...", "links": ["https://github.com/scrapy/scrapy/blob/master/scrapy/cmdline.py#L144"], "channel": "scrapy"},
{"date": "2014-09-27T07:43:56.185401+00:00", "nick": "dpn`", "message": "that's where the exist code is read", "links": [], "channel": "scrapy"},
{"date": "2014-09-27T07:45:29.347123+00:00", "nick": "dpn`", "message": "it's a hack - but you could do:  from scrapy.commands.crawl import Command; Command.exitcode = 123", "links": [], "channel": "scrapy"},
{"date": "2014-09-27T07:45:34.086641+00:00", "nick": "dpn`", "message": "right before you raise CloseSpider", "links": [], "channel": "scrapy"},
{"date": "2014-09-27T07:47:13.289621+00:00", "nick": "rohitt", "message": "Cool! I'll give it a shot! Thanks. :)", "links": [], "channel": "scrapy"},
{"date": "2014-09-27T07:47:19.346615+00:00", "nick": "dpn`", "message": "hope it helps", "links": [], "channel": "scrapy"},
{"date": "2014-09-27T09:22:31.549054+00:00", "nick": "rohitt", "message": "raise CloseSpider('foo') only shows thrown exception in logs, but doesn't actually close/stop the running Spider, and it keeps on scraping next elements. Why is that?", "links": [], "channel": "scrapy"},
{"date": "2014-09-27T09:23:08.449402+00:00", "nick": "dpn`", "message": "rohitt, at a guess I'd say you may have multiple spiders running in the one process", "links": [], "channel": "scrapy"},
{"date": "2014-09-27T09:23:39.136896+00:00", "nick": "rohitt", "message": "I've just one.", "links": [], "channel": "scrapy"},
{"date": "2014-09-27T09:24:40.730365+00:00", "nick": "rohitt", "message": "Even sys.exit(1) gives -- exceptions.SystemExit -- only in the logs, and won't shut the spider.", "links": [], "channel": "scrapy"},
{"date": "2014-09-27T09:25:10.461367+00:00", "nick": "rohitt", "message": "by the way, I am calling these things from pipeline.", "links": [], "channel": "scrapy"},
{"date": "2014-09-27T09:26:05.441930+00:00", "nick": "dpn`", "message": "you could call crawler.engine.close_spider - though I suspect that might do a similar thing", "links": [], "channel": "scrapy"},
{"date": "2014-09-27T09:26:26.769973+00:00", "nick": "dpn`", "message": "I'm not sure beyond that.. sorry I'm a bit rusty with scrapy at the moment", "links": [], "channel": "scrapy"},
{"date": "2014-09-27T09:26:34.364521+00:00", "nick": "dpn`", "message": "most of the guys would be waking up in a couple of hours", "links": [], "channel": "scrapy"},
{"date": "2014-09-27T09:26:42.124106+00:00", "nick": "dpn`", "message": "so if you hang around for a bit you may get better help", "links": [], "channel": "scrapy"},
{"date": "2014-09-27T09:26:54.996757+00:00", "nick": "rohitt", "message": "hehe thanks. np. I'll keep looking for the solution. :)", "links": [], "channel": "scrapy"},
{"date": "2014-09-27T09:27:01.180656+00:00", "nick": "rohitt", "message": "Thanks. :)", "links": [], "channel": "scrapy"},
{"date": "2014-09-27T09:27:04.233804+00:00", "nick": "dpn`", "message": "np", "links": [], "channel": "scrapy"},
{"date": "2014-09-27T09:27:07.771208+00:00", "nick": "dpn`", "message": "have a read of the source too", "links": [], "channel": "scrapy"},
{"date": "2014-09-27T09:27:12.204043+00:00", "nick": "rohitt", "message": "yup", "links": [], "channel": "scrapy"},
{"date": "2014-09-27T09:27:12.818221+00:00", "nick": "dpn`", "message": "if you haven't already", "links": [], "channel": "scrapy"},
{"date": "2014-09-27T15:08:04.737520+00:00", "nick": "Chez", "message": "Hi all, having a problem with my crawler, and would appreciate any advice. I'm crawling a UTF-8 website, but it's getting converted to ascii somewhere. e.g. \u00e9 is being serialised as \\u00c9. I've tried calling .encode('utf-8') in my callback function, but it's not making any difference. Thanks!", "links": [], "channel": "scrapy"},
{"date": "2014-09-27T15:37:42.389743+00:00", "nick": "nyov", "message": "Chez: maybe you just don't print it right? what do you mean by serialised?", "links": [], "channel": "scrapy"},
{"date": "2014-09-27T15:41:20.883253+00:00", "nick": "Chez", "message": "nyov: if you view the website in a browser, it shows \u00e9 quite happily, but my crawler is saving this character as \\u00c9. I'm not doing anything with the strings, just puliing them from the response object with an HtmlXPathSelector", "links": [], "channel": "scrapy"},
{"date": "2014-09-27T15:42:14.359287+00:00", "nick": "nyov", "message": "and are you print()ing the string?", "links": [], "channel": "scrapy"},
{"date": "2014-09-27T15:42:43.781607+00:00", "nick": "Chez", "message": "No, this is the only line of code touching it: item['body'] = hxs.select('//*[@id=\"mddocument\"]/d...", "links": ["mailto:hxs.select('//*[@id=\"mddocument\"]/div').extract()[0]"], "channel": "scrapy"},
{"date": "2014-09-27T15:42:49.035078+00:00", "nick": "nyov", "message": "...", "links": [], "channel": "scrapy"},
{"date": "2014-09-27T15:43:53.032303+00:00", "nick": "nyov", "message": "so what are you doing with item['body'] what brings you to the conclusion it isn't encoded correctly", "links": [], "channel": "scrapy"},
{"date": "2014-09-27T15:45:18.429689+00:00", "nick": "Chez", "message": "the item is being exported using json_lines and when I view the output file I get the ascii \\u00c9 instead of \u00e9.", "links": [], "channel": "scrapy"},
{"date": "2014-09-27T15:48:32.457725+00:00", "nick": "nyov", "message": "okay, that gets us somewhere. Try to print(item['body']) from your spider, and print response.encoding", "links": [], "channel": "scrapy"},
{"date": "2014-09-27T15:49:38.751938+00:00", "nick": "nyov", "message": "oh, but first please get the latest scrapy version, because encoding issues have been fixed since 0.14", "links": [], "channel": "scrapy"},
{"date": "2014-09-27T15:50:41.385393+00:00", "nick": "Chez", "message": "scrapy was only installed yesterday :) v 0.24.4", "links": [], "channel": "scrapy"},
{"date": "2014-09-27T15:51:04.770581+00:00", "nick": "nyov", "message": "why would you be using HtmlXPathSelector and .select() then?", "links": [], "channel": "scrapy"},
{"date": "2014-09-27T15:51:49.034366+00:00", "nick": "nyov", "message": "nevermind, test what the spider detects as encoding in response.encoding, that would be the first place to look", "links": [], "channel": "scrapy"},
{"date": "2014-09-27T15:53:32.153763+00:00", "nick": "Chez", "message": "Because the spider was originally written over a year ago, I'm reusing it now.", "links": [], "channel": "scrapy"},
{"date": "2014-09-27T15:53:50.336635+00:00", "nick": "Chez", "message": "OK, printing the body shows everything is encoded properly, and response.encoding is utf-8", "links": [], "channel": "scrapy"},
{"date": "2014-09-27T15:54:26.347501+00:00", "nick": "nyov", "message": "actually, from checking the documentation of json.JSONEncoder(), what you're seeing in your output is perfectly correct output", "links": [], "channel": "scrapy"},
{"date": "2014-09-27T15:54:49.897533+00:00", "nick": "nyov", "message": "\"If *ensure_ascii* is true (the default), all non-ASCII characters in the output are escaped with \\uXXXX sequences, [...]\"", "links": [], "channel": "scrapy"},
{"date": "2014-09-27T15:56:18.229156+00:00", "nick": "Chez", "message": "Hmm, ok then, that would do it...", "links": [], "channel": "scrapy"},
{"date": "2014-09-27T15:57:58.044020+00:00", "nick": "Chez", "message": "Any idea how I can change that setting? (I've not seen anything in the scrapy docs for specifying the output encoding for json_lines) Or would I be better using a different serialisation format?", "links": [], "channel": "scrapy"},
{"date": "2014-09-27T16:00:05.278403+00:00", "nick": "nyov", "message": "you would only need to pass ensure_ascii=False to the JsonLinesItemExporter() instance. unfortunately I don't know how to pass additional arguments to an exporter", "links": [], "channel": "scrapy"},
{"date": "2014-09-27T16:05:11.670938+00:00", "nick": "Chez", "message": "Me neither :(", "links": [], "channel": "scrapy"},
{"date": "2014-09-27T16:06:29.514374+00:00", "nick": "nyov", "message": "well, quickest way then would be to subclass the Exporter and add your kwarg in the super() call to the parent's __init__", "links": [], "channel": "scrapy"},
{"date": "2014-09-27T16:06:40.212570+00:00", "nick": "nyov", "message": "then run it with your customized exporter", "links": [], "channel": "scrapy"},
{"date": "2014-09-27T16:48:48.069657+00:00", "nick": "Chez", "message": "Took some effort, but I have got that to work, thankyou nyov!", "links": [], "channel": "scrapy"},
{"date": "2014-09-27T16:54:45.502124+00:00", "nick": "nyov", "message": "Chez: great!", "links": [], "channel": "scrapy"},
{"date": "2014-09-27T17:03:22.439282+00:00", "nick": "Chez", "message": "Cheers for your help, adios!", "links": [], "channel": "scrapy"},
{"date": "2014-09-28T19:15:22.556296+00:00", "nick": "brotherBox", "message": "Hi. The other day I asked about making scrapy do a request after a delay and I was told to look into scrapy.utils.defer to see an example of callLater. Calling callLater on twisted.internet.reactor with the right arguments does not issue a new request. It is not apparent to me how to exactly deal with a Request like as a Defered (since Request doesnt inherit from twisted.defer.Defered), which confuses me. Any", "links": [], "channel": "scrapy"},
{"date": "2014-09-28T19:15:24.697319+00:00", "nick": "brotherBox", "message": "advice?", "links": [], "channel": "scrapy"},
{"date": "2014-09-28T23:27:09.539121+00:00", "nick": "eVRiAL", "message": "Please point me string = '<body><a href=\"#\">Click here to go to the <strong>Next Page</strong></a></body>'", "links": [], "channel": "scrapy"},
{"date": "2014-09-28T23:27:09.673147+00:00", "nick": "eVRiAL", "message": "whats the xpath to get result? string = 'Click here to go to the <strong>Next Page</strong>'", "links": [], "channel": "scrapy"},
{"date": "2014-09-28T23:27:58.874854+00:00", "nick": "eVRiAL", "message": "Been playing a while with text() and . but no luck", "links": [], "channel": "scrapy"},
{"date": "2014-09-28T23:32:06.919520+00:00", "nick": "nyov", "message": "a/text() ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-28T23:36:42.784259+00:00", "nick": "eVRiAL", "message": "nop, lurking the answer on SO..", "links": [], "channel": "scrapy"},
{"date": "2014-09-28T23:37:52.696616+00:00", "nick": "nyov", "message": "then ask what you need instead. the nodes or only strings?", "links": [], "channel": "scrapy"},
{"date": "2014-09-28T23:38:00.301550+00:00", "nick": "nyov", "message": "a//text() ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-28T23:38:43.909965+00:00", "nick": "eVRiAL", "message": "I want 'Click here to go to the <strong>Next Page</strong>'", "links": [], "channel": "scrapy"},
{"date": "2014-09-28T23:39:02.488753+00:00", "nick": "eVRiAL", "message": "i.e. anything inside node", "links": [], "channel": "scrapy"},
{"date": "2014-09-28T23:39:18.769826+00:00", "nick": "nyov", "message": "sel.xpath('a').xpath('text()|node()') ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-28T23:42:13.656771+00:00", "nick": "eVRiAL", "message": "In [1]: from scrapy import Selector", "links": [], "channel": "scrapy"},
{"date": "2014-09-28T23:42:13.791123+00:00", "nick": "eVRiAL", "message": "In [2]: sel = sel = Selector(text='<a href=\"#\">Click here to go to the <strong>Next Page</strong></a>')", "links": [], "channel": "scrapy"},
{"date": "2014-09-28T23:42:13.791178+00:00", "nick": "eVRiAL", "message": "In [28]: sel.xpath('a').xpath('text()|node()')", "links": [], "channel": "scrapy"},
{"date": "2014-09-28T23:42:13.792308+00:00", "nick": "eVRiAL", "message": "Out[28]: []", "links": [], "channel": "scrapy"},
{"date": "2014-09-28T23:42:15.526983+00:00", "nick": "eVRiAL", "message": ":(", "links": [], "channel": "scrapy"},
{"date": "2014-09-28T23:43:07.474380+00:00", "nick": "nyov", "message": "well", "links": [], "channel": "scrapy"},
{"date": "2014-09-28T23:43:12.033817+00:00", "nick": "nyov", "message": "sel.xpath('//a').xpath('text()|node()') ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-28T23:43:37.342273+00:00", "nick": "nyov", "message": "or sel.xpath('//a').xpath('.//text()|.//node()')", "links": [], "channel": "scrapy"},
{"date": "2014-09-28T23:43:42.912310+00:00", "nick": "nyov", "message": "ah well", "links": [], "channel": "scrapy"},
{"date": "2014-09-28T23:47:08.916262+00:00", "nick": "eVRiAL", "message": "yeah close", "links": [], "channel": "scrapy"},
{"date": "2014-09-28T23:48:53.353847+00:00", "nick": "eVRiAL", "message": "funny but the answer should be simple or I'm stupid :)", "links": [], "channel": "scrapy"},
{"date": "2014-09-28T23:50:34.927667+00:00", "nick": "nyov", "message": "still not what you want? okay I give up", "links": [], "channel": "scrapy"},
{"date": "2014-09-28T23:51:45.988389+00:00", "nick": "eVRiAL", "message": "thanks though", "links": [], "channel": "scrapy"},
{"date": "2014-09-28T23:59:52.002530+00:00", "nick": "eVRiAL", "message": "ok the answer it can't be done using xpath", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T00:00:51.098926+00:00", "nick": "nyov", "message": "i don't think so. everything can be done with xpath", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T00:00:55.228273+00:00", "nick": "nyov", "message": ":)", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T00:09:41.602267+00:00", "nick": "eVRiAL", "message": "In [52]: \"\".join(sel.xpath('//a').xpath('./text()|node()').extract())", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T00:09:41.736357+00:00", "nick": "eVRiAL", "message": "Out[52]: u'Click here to go to the <strong>Next Page</strong>'", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T00:09:41.736416+00:00", "nick": "eVRiAL", "message": "not pretty but works", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T00:10:38.264536+00:00", "nick": "eVRiAL", "message": "https://stackoverflow.com/questions/10898035/ho...", "links": ["https://stackoverflow.com/questions/10898035/how-to-get-node-value-innerhtml-with-xpath"], "channel": "scrapy"},
{"date": "2014-09-29T00:14:22.271032+00:00", "nick": "nyov", "message": "yeah, so isn't that what you wanted? ...that's the xpath I posted", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T00:17:52.567386+00:00", "nick": "eVRiAL", "message": "yep join makes what I wanted", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T00:21:19.662023+00:00", "nick": "eVRiAL", "message": "3 AM -_-", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T01:41:05.412652+00:00", "nick": "blusteal", "message": "nyov we were talking a while back about the dns middleware, is there a way for me to have those domains that throw a dns error just continue to the spider and be able to catch them in the spider as I need to make note of them.", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T01:43:53.135294+00:00", "nick": "nyov", "message": "blusteal: uh. sorry, remind me what the middleware did?", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T01:44:17.796562+00:00", "nick": "blusteal", "message": "it basically logged when a url returned any types of dns error", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T01:44:46.098447+00:00", "nick": "blusteal", "message": "the thing is that I don't want the dns middleware to throw out the domain, just continue to pass it to the spider", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T01:45:24.738341+00:00", "nick": "blusteal", "message": "if I could find somehow tag the domain with some meta info so that I can return an item from the broken domains", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T01:46:46.021206+00:00", "nick": "nyov", "message": "yeah, if I knew how that thing looked... been a while", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T01:47:27.952822+00:00", "nick": "nyov", "message": "oh, wait. was it that code I wrote?", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T01:47:35.926303+00:00", "nick": "blusteal", "message": "yea you wrote it on your gist", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T01:48:14.352625+00:00", "nick": "nyov", "message": "really? I can't seem to find it", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T01:49:10.385315+00:00", "nick": "nyov", "message": "ah right, there it is. Did that work btw?", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T01:49:23.505998+00:00", "nick": "blusteal", "message": "it allowed me to see the domains yes", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T01:49:39.239285+00:00", "nick": "blusteal", "message": "but getting them to the spider to return an item", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T01:50:07.238934+00:00", "nick": "blusteal", "message": "maybe I can add some meta data to it somehow and check for it in the spider, response and return an item", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T01:51:49.832366+00:00", "nick": "nyov", "message": "well if you're using that code as I wrote it, you could replace the raise IgnoreRequest with returning a new Response object, or the old response with added metadata", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T01:55:54.644203+00:00", "nick": "nyov", "message": "actually I see there is no old response... I just manufactured one. heh. well then, replace the new TextResponse with either an actual HtmlResponse, or just add a meta object to it, and comment out the whole process_response block", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T02:04:06.629581+00:00", "nick": "nyov", "message": "...silence :P Does that make sense to you?", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T02:12:48.067471+00:00", "nick": "nyov", "message": "maybe to elaborate, this is a DNS error. there never was a response, there never actually was a request sent or a http connection. So at this point you're just completely making something up with a ficticious response object", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T02:13:58.929462+00:00", "nick": "nyov", "message": "then it's basically turning around, putting that response with the others going to the spider, and trying to be inconspicious about it", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T02:14:44.428139+00:00", "nick": "nyov", "message": "err, anyway, what you make up in that response, and how you handle that in your spider then is totally up to you", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T02:16:04.840786+00:00", "nick": "blusteal", "message": "hmm", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T02:16:07.973292+00:00", "nick": "blusteal", "message": "i'll keep plugging away at it", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T02:17:33.146793+00:00", "nick": "nyov", "message": "the code not working for you?", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T02:19:34.446375+00:00", "nick": "nyov", "message": "well. if you have any questions about it, just ask", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T02:24:47.558183+00:00", "nick": "nyov", "message": "ah, how I wish I had 500TB storage! the internet in my basement. wouldn't that be something :D", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T02:26:08.612752+00:00", "nick": "blusteal", "message": "500tb what are you the nsa or something?", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T02:26:09.386663+00:00", "nick": "blusteal", "message": "haha", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T02:27:34.304937+00:00", "nick": "nyov", "message": "I could download the whole common crawl dataset without the whole seeking thing", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T02:29:34.824496+00:00", "nick": "nyov", "message": "nggaaah. oh well. throwing money at some map reduce jobs in an aws cluster is too big for me.", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T02:31:59.259769+00:00", "nick": "nyov", "message": "but why crawl the internetz when someone else already did it for you? haha. that is so bad", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T03:37:13.186128+00:00", "nick": "blusteal", "message": "i've been thinking about just crawling as much of the internet as possible", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T03:37:30.922130+00:00", "nick": "blusteal", "message": "but then I run into that storage thing", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T06:33:30.823806+00:00", "nick": "blusteal", "message": "nyov don't know if your around but using the code i'm getting an error: process_response must return Response or Request, got <type 'NoneType'>", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T07:19:19.233031+00:00", "nick": "blusteal", "message": "i'm still having a bit of trouble actually returning the url back to the spider :s", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T07:29:39.142148+00:00", "nick": "blusteal", "message": "i'm getting DNS lookup failed: address 'www.domain.com' not found: [Errno 8] nodename nor servname provided, or not known. no matter what I do inside the process_response or process_exception functions, what function is actually responsible for that message above", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T07:30:14.457260+00:00", "nick": "blusteal", "message": "i'd just like to say it's okay, send it over to the spider anyways", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T08:24:23.393410+00:00", "nick": "blusteal", "message": "nyov are you around?", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T08:46:43.504193+00:00", "nick": "blusteal", "message": "some feedback, i created a @classmethod in the dns middleware to call a function in the spider class, when I run into the process_exception function I just pass the url back to the spider and left the code mostly as you designed it nyov, do you see that causing any issues with async functions?", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T10:19:55.928262+00:00", "nick": "brotherBox", "message": "nyov: is your offer for help from yesterday (or not yesterday, depending on your timezone) still valid?", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T12:44:46.921566+00:00", "nick": "nyov", "message": "blusteal: that might not be the nice way, but it should work I think... if you check for all the eventualities", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T12:45:06.003171+00:00", "nick": "blusteal", "message": "nyov actually it didn't work so well", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T12:45:22.622372+00:00", "nick": "nyov", "message": "brotherBox: yeah I was really disappointed when I had this done and you had left without a say", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T12:46:15.865131+00:00", "nick": "blusteal", "message": "making static calls back to a spider wasn't working at all. I kinda hacked it sending a new request to a domain with some meta, check the meta in the parse function and grab the urls that I required BUT this causes me sending requests to a domain [mine] but I would prefer not to slam my host like that", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T12:48:56.280586+00:00", "nick": "nyov", "message": "haha, others are okay but not yourself? :P anyway, why send a request. constructing a response alone should be okay.", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T12:49:56.762942+00:00", "nick": "blusteal", "message": "I couldn't get the response to work", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T12:50:37.143478+00:00", "nick": "blusteal", "message": "even though we disable the dns there's still something catching the dns error, looking at the code base I saw in retry.py that it's doing a dns retry", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T12:50:55.177509+00:00", "nick": "blusteal", "message": "dns retry loop", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T12:51:31.035283+00:00", "nick": "blusteal", "message": "that's pretty close to twisted and I have no real idea how to tackle that thing atm, i tried just returning a request but then I get stuck in a loop", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T12:53:08.123547+00:00", "nick": "nyov", "message": "the error condition should be caught by the process_exception, and go away by not bubbling it, up but returning a response instead - which should go into the next callback instead of an errback.", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T12:54:51.934251+00:00", "nick": "nyov", "message": "I think.", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T12:55:14.335939+00:00", "nick": "nyov", "message": "can see that from the example actually. once I return a TextResponse from the process_exception(), it comes right back into the process_response() of that same pipeline (running the callback after the errback)", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T12:55:37.564980+00:00", "nick": "nyov", "message": "brotherBox: pastebin is still loading.. haha", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T12:56:44.578298+00:00", "nick": "blusteal", "message": "I created a response with return Response(url=request.url)", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T12:57:24.854675+00:00", "nick": "blusteal", "message": "in the process response I return the response but it never makes it back to the spider's parse_item", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T12:59:27.152728+00:00", "nick": "nyov", "message": "no, you should return that in process_exception", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T13:00:18.149168+00:00", "nick": "blusteal", "message": "let me try that again", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T13:00:52.573743+00:00", "nick": "nyov", "message": "you don't need process_response from what I can see", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T13:01:27.325906+00:00", "nick": "nyov", "message": "hmm, okay actually i think it shouldn't matter. uhm", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T13:01:37.087563+00:00", "nick": "nyov", "message": "...need something to test with", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T13:04:00.881442+00:00", "nick": "nyov", "message": "brotherBox: http://lpaste.net/111803", "links": ["http://lpaste.net/111803"], "channel": "scrapy"},
{"date": "2014-09-29T13:05:38.357769+00:00", "nick": "nyov", "message": "you can run it with `python spider.py`", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T13:07:54.268851+00:00", "nick": "blusteal", "message": "getting this", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T13:07:57.504879+00:00", "nick": "blusteal", "message": "raise AttributeError(\"Response.meta not available, this response \" \\", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T13:07:57.603936+00:00", "nick": "blusteal", "message": "exceptions.AttributeError: Response.meta not available, this response is not tied to any request", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T13:09:07.479368+00:00", "nick": "nyov", "message": "blusteal: right. there is no request ;) so that makes sense.", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T13:09:32.503452+00:00", "nick": "blusteal", "message": "see that's why i sent a request with one of my domains....", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T13:09:41.025924+00:00", "nick": "blusteal", "message": ":(", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T13:10:13.666988+00:00", "nick": "nyov", "message": "what do you need meta{} for? can't you just use the url for the information you put into it?", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T13:10:50.730013+00:00", "nick": "nyov", "message": "well, you do have a request at the point when you create the response. maybe bind them together", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T13:13:04.143090+00:00", "nick": "blusteal", "message": "bind them together?", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T13:17:02.890303+00:00", "nick": "nyov", "message": "well, a normal response typically carries the request which generated it. So you do that in manual style", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T13:17:47.895923+00:00", "nick": "blusteal", "message": "hmm", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T13:17:49.818404+00:00", "nick": "nyov", "message": "blusteal: response = TextResponse(url=whatever, request=request)", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T13:18:21.054804+00:00", "nick": "nyov", "message": "response.meta.update({})", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T13:18:24.530000+00:00", "nick": "nyov", "message": "return response", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T13:19:02.817170+00:00", "nick": "blusteal", "message": "that's still back to the hackish way that I had it setup firing off requests, it's not insane i have a download delay in the spider between 10-20 seconds", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T13:19:13.434099+00:00", "nick": "blusteal", "message": "but this way seems so", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T13:19:29.364226+00:00", "nick": "nyov", "message": "actually I'm not sure you can update the meta from the response", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T13:19:36.396968+00:00", "nick": "blusteal", "message": "nope", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T13:19:47.414001+00:00", "nick": "blusteal", "message": "the response will throw errors when u parse it", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T13:21:25.444974+00:00", "nick": "nyov", "message": "how is that hackish. and what do you mean by parsing?", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T13:22:55.125136+00:00", "nick": "nyov", "message": "it's just a \"flag\". there is nothing response-ish about it. I would just put the failing domain in the url and check for that. there isn't possibly more information a dnserror could carry", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T13:23:37.635758+00:00", "nick": "nyov", "message": "besides the request bound to it, which gives more info on what failed", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T13:26:32.560566+00:00", "nick": "nyov", "message": "otoh you could fake a json response by putting json in the body.. should give all the possibly space to put meta information", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T13:42:04.407945+00:00", "nick": "blusteal", "message": "nyov pastebin: http://pastebin.com/xTg9xbPT", "links": ["http://pastebin.com/xTg9xbPT"], "channel": "scrapy"},
{"date": "2014-09-29T14:09:17.588709+00:00", "nick": "blusteal", "message": "man this thing goes all the way down to the twisted metal, how.... gah", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T14:20:33.556591+00:00", "nick": "kamikaz3", "message": "hello", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T14:27:12.297272+00:00", "nick": "kamikaz3", "message": "I just started using scrapy and selenium and wanted some advice on how to integrate it together to run it quicker ? What I am trying to do is, I am going to Page#1 and fetching links with scrapy. I navigate to link-1 which displays Page#2 with selenium by webdriver.click() method . Once I get everything from Page#2 I have to navigate again to Page#1 to click on link-2. And then fetch information from link-2 and go back to link-3. Is there an easier way to", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T14:28:57.734768+00:00", "nick": "nyov", "message": "blusteal: is the paste what you use now?", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T14:29:29.002385+00:00", "nick": "blusteal", "message": "nyov yea it's what's \"working\"", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T14:46:50.649933+00:00", "nick": "blusteal", "message": "nyov are you around, i wanted to show something to you", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T14:49:33.117157+00:00", "nick": "blusteal", "message": "I basically set the default retry middleware to none, copy/paste it into a new retry updated and did the error checking right inside the retry class. I got to find the dns error domains which is cool, how can I just send this back to the spider? http://pastebin.com/Gtjj8sSu", "links": ["http://pastebin.com/Gtjj8sSu"], "channel": "scrapy"},
{"date": "2014-09-29T14:50:26.334429+00:00", "nick": "nyov", "message": "blusteal: http://pastebin.com/iCzwzgYX", "links": ["http://pastebin.com/iCzwzgYX"], "channel": "scrapy"},
{"date": "2014-09-29T14:51:50.294665+00:00", "nick": "blusteal", "message": "I was close but your an expert, this is nice", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T14:52:05.646406+00:00", "nick": "nyov", "message": "this is just how I said to do it ;)", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T14:52:26.531178+00:00", "nick": "nyov", "message": "well, if you want to hack up the retry class, I think that is okay as well", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T14:53:00.155445+00:00", "nick": "blusteal", "message": "i'd prefer to stay away from that", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T14:53:06.454519+00:00", "nick": "blusteal", "message": "it'll break as soon as there's an update", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T14:53:35.129938+00:00", "nick": "nyov", "message": "it's just another middleware, works with the same methods and the same layout.", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T14:54:03.067618+00:00", "nick": "blusteal", "message": "yea but this one comes baked in, i'll stay out unless i have to", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T14:54:07.844124+00:00", "nick": "blusteal", "message": "thanks for this though", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T14:54:36.337804+00:00", "nick": "nyov", "message": "well I didn't test it, maybe there are syntax errors. but in principle it should do the trick", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T14:55:20.602362+00:00", "nick": "blusteal", "message": "i'm knee deep in it now thanks", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T15:12:07.521443+00:00", "nick": "nyov", "message": "ah kamikaze3 left?", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T15:13:51.112954+00:00", "nick": "nyov", "message": "well I have better stuff to do anyway. heh.", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T15:58:28.110737+00:00", "nick": "brotherBox", "message": "Not sure if my message came before or after my disconnect, so I'll repeat it. nyov: the code you pasted appears to have several errors. I am completely unfamiliar with starting a spider from python, that is, not via \"scrapy crawl\". The crawler instantiated in \"if __name__ ...\" appears to have no \"configure\" and no \"start\" method", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T15:58:47.849767+00:00", "nick": "brotherBox", "message": "I checked spiders.crawl.Crawler, it indeed has no configure method", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T16:05:50.151792+00:00", "nick": "brotherBox", "message": "Also, I had to add the spider to the instantiation parameters. All the information I can find passes only one parameters to Crawler, whereas the code I have requires two. I'm quite confused, can you shed some light on what the error might be? Updating via pip didn't resolve the issue.", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T16:51:05.610777+00:00", "nick": "blusteal", "message": "how do you format strings when doing xpath in python? ex: typical response.xpath('//ul/li')", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T16:51:51.236462+00:00", "nick": "blusteal", "message": "but when I try to use response.xpath('//ul/li[contains(text(), 'ex')]')", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T16:52:03.877756+00:00", "nick": "blusteal", "message": "i get a error and crash when testing with scrapy shell", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T16:53:02.570260+00:00", "nick": "brotherBox", "message": "Whats the error you get?", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T16:55:50.129387+00:00", "nick": "tomwardill", "message": "blusteal: try response.xpath('//ul/li[contains(text(), \"ex\")]')", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T16:56:47.492510+00:00", "nick": "tomwardill", "message": "you can't have a string that includes the same string delimiter as you used to start the string, without escaping it.", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T16:58:54.107319+00:00", "nick": "blusteal", "message": "hi tomwardill and brotherBox still no go", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T16:59:43.954518+00:00", "nick": "brotherBox", "message": "You didn't link to a bin with your traceback so helping you is quite hard", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T17:00:01.941431+00:00", "nick": "blusteal", "message": "-bash: syntax error near unexpected token  `''//ul/li[contains(text(), \"ex\")]''", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T17:00:24.975349+00:00", "nick": "blusteal", "message": "it's in scrapy shell for testing, that's the only error i get, then it just crashes", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T17:00:35.636047+00:00", "nick": "blusteal", "message": "[1]+ Stopped \\ [2] Done", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T17:00:50.893802+00:00", "nick": "brotherBox", "message": "Did you put your url into quotes?", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T17:01:15.004569+00:00", "nick": "brotherBox", "message": "It sounds like you have & characters in the url you put into scrapy shell, which are interpreted as shell information.", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T17:04:45.793247+00:00", "nick": "blusteal", "message": "hmm, let me try that", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T17:06:45.757807+00:00", "nick": "blusteal", "message": "oh you guys are correct, put the url in quotes", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T17:06:53.031964+00:00", "nick": "blusteal", "message": "thanks, that was it", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T17:34:13.857815+00:00", "nick": "nyov", "message": "brotherBox: sorry I didn't get a hilight. the code runs perfectly fine with scrapy 0.24 as a standalone python script.", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T17:34:50.172762+00:00", "nick": "nyov", "message": "it's a MWE as I said i'd write. https://en.wikipedia.org/wiki/Minimal_Working_E...", "links": ["https://en.wikipedia.org/wiki/Minimal_Working_Example"], "channel": "scrapy"},
{"date": "2014-09-29T17:36:51.366953+00:00", "nick": "nyov", "message": "I didn't say I'd code your project for you. how you take the concepts from that to your own project should be relatively obvious though", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T17:37:38.609358+00:00", "nick": "nyov", "message": "as in how to start it: save the paste as a python file and run \"python yourfile.py\"", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T17:40:13.188857+00:00", "nick": "brotherBox", "message": "nyov: thats entirely true and I did not want to give the impression as though that was my intention. I got it to work, but not via \"python myfile.py\", as the mentioned errors occur.", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T17:47:08.317690+00:00", "nick": "nyov", "message": "which errors?", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T17:51:57.590230+00:00", "nick": "brotherBox", "message": "Let me reproduce it, there was one thing with respect to it not finding settings.py, I resolved it but reverted the fix because I got stuck later on anyway.", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T17:52:26.061555+00:00", "nick": "nyov", "message": "< brotherBox> I checked spiders.crawl.Crawler, it indeed has no configure method", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T17:52:29.570325+00:00", "nick": "nyov", "message": "https://github.com/scrapy/scrapy/blob/0.24/scra...", "links": ["https://github.com/scrapy/scrapy/blob/0.24/scrapy/crawler.py#L39"], "channel": "scrapy"},
{"date": "2014-09-29T17:52:37.265509+00:00", "nick": "nyov", "message": "https://github.com/scrapy/scrapy/blob/0.24/scra...", "links": ["https://github.com/scrapy/scrapy/blob/0.24/scrapy/crawler.py#L63"], "channel": "scrapy"},
{"date": "2014-09-29T17:52:44.457773+00:00", "nick": "nyov", "message": "?", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T17:55:55.459520+00:00", "nick": "nyov", "message": "oh never mind reproducing it, if it worked for you in the end that's okay", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T17:58:05.868552+00:00", "nick": "brotherBox", "message": "I got it now, I needed to call it from completely outside the project directory because it wouldn't find settings.py otherwise for some reason.", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T17:59:38.636906+00:00", "nick": "brotherBox", "message": "Also, I can't explain why it would work now and not right after I updated. I should get my mind clear, immensely sorry for wasting your time.", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T18:00:29.121247+00:00", "nick": "nyov", "message": "ah, okay that sucks about pulling wrong settings or something", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T18:03:39.309766+00:00", "nick": "brotherBox", "message": "Essentially, I have a folder structure like $HOME/python/scrapy/delay/delay/spiders/delay.py, with scrapy.cfg in $HOME/python/scrapy/delay. From any directory \"closer\" to spiders/ than $HOME/python/scrapy it wouldn't find the settings.", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T18:04:32.919270+00:00", "nick": "brotherBox", "message": "That is, from $HOME/python/scrapy, $HOME/python and $HOME it would find the settings", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T19:10:00.037510+00:00", "nick": "localhost_", "message": "how does scrapyd manage the urls already scraped? If I run the spider 3 times a day, will it duplicate scraped urls or just get the neew ones?", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T21:44:56.950535+00:00", "nick": "DanielW", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T21:54:01.530549+00:00", "nick": "nyov", "message": "hello", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T21:58:18.010945+00:00", "nick": "DanielW", "message": "i am a little unsure how to handle errors or problems from nightly runs of scrapy crawl.    i want to write a bash (or python) script which makes a backup of some data, runs scrapy, and then sends a report (optional only when there were \"errors\") with the scrapy stats", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T21:58:41.741106+00:00", "nick": "DanielW", "message": "scrapy will crawl some website and update a database", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T21:59:45.634780+00:00", "nick": "DanielW", "message": "i could parse the logfile afterwards (search for Exceptions and the stats block at the end...), but what is the best practice?", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T22:02:41.516654+00:00", "nick": "nyov", "message": "if you would write python scripts, you can write one to run scrapy in and handle errors \"inline\"", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T22:03:06.336790+00:00", "nick": "brotherBox", "message": "Maybe you could monkeypatch the scrapy.log module, call scrapy within your script and have the messages used in your program.", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T22:03:24.397218+00:00", "nick": "brotherBox", "message": "Or am I understanding you horribly wrong?", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T22:05:48.711278+00:00", "nick": "nyov", "message": "https://scrapy.readthedocs.org/en/latest/topics...", "links": ["https://scrapy.readthedocs.org/en/latest/topics/practices.html#run-scrapy-from-a-script"], "channel": "scrapy"},
{"date": "2014-09-29T22:07:45.761399+00:00", "nick": "nyov", "message": "you can of course modify the logging behaviour of the logformatter, if you're only looking for logs to parse after the fact", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T22:10:01.316943+00:00", "nick": "DanielW", "message": "if i run scrapy from a script,  what will happen when some parse callback method raises and exception?", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T22:10:39.375562+00:00", "nick": "DanielW", "message": "with scrapy crawl it will just continue with the next request", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T22:12:04.941599+00:00", "nick": "nyov", "message": "that depends on if you catch it. but it'll behave the same as if you ran it from the commandline (given the same settings and all)", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T22:13:40.182055+00:00", "nick": "nyov", "message": "afaik at least. haven't seen any cmdline module code to suggest otherwise", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T22:13:51.162185+00:00", "nick": "DanielW", "message": "one of my problems is: normaly i program runs, and returns 0 when everything was fine or !=0 when there was a problem.  in some nightly batch job, with multiple steps, i can break when one step failed..   i find it hard in my spiders to define \"ok now this is a final failure state\"  and even if, i don't know the way to give that info the the calling script (for exampe a batch script)", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T22:14:15.707590+00:00", "nick": "DanielW", "message": "a", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T22:14:17.833647+00:00", "nick": "DanielW", "message": "not i", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T22:17:20.443536+00:00", "nick": "DanielW", "message": "stats.get_stats() returns a dict?", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T22:17:23.156591+00:00", "nick": "nyov", "message": "hm, what's the question here?", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T22:17:39.888271+00:00", "nick": "nyov", "message": "I believe it does", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T22:20:06.624729+00:00", "nick": "DanielW", "message": "well you see i am confused...  maybe i should just play arround with runing scrapy inside a python script  and get a feeling for it (how to handle exceptions there, how to get the stats and how then decide if everything was ok)", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T22:22:48.360159+00:00", "nick": "nyov", "message": "scripting scrapy is a bit advanced, especially if you don't know twisted. you might have to learn a bit about callbacks and errbacks and the chaining of those", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T22:24:55.192799+00:00", "nick": "nyov", "message": "you could also put a normal scrapy project somewhere, and then remotely control it from the telnet or webconsole", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T22:26:01.825052+00:00", "nick": "nyov", "message": "but would have to start it first the usual way. and it doesn't give any flow control really. just the option to read stats and terminate the run", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T22:33:33.858331+00:00", "nick": "nyov", "message": "/sbin/xfsdump: Dump Status: SUCCESS", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T22:33:37.725613+00:00", "nick": "nyov", "message": "oops", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T22:33:55.068422+00:00", "nick": "nyov", "message": "but. yarrrr. FINALLY", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T22:34:06.920558+00:00", "nick": "nyov", "message": "actually i meant: https://scrapy.readthedocs.org/en/latest/topics...", "links": ["https://scrapy.readthedocs.org/en/latest/topics/webservice.html#web-service-resources"], "channel": "scrapy"},
{"date": "2014-09-29T22:52:10.899491+00:00", "nick": "DanielW", "message": "thanks btw.  the example code from \"run-scrapy-from-script\" works with my own spider now.. i think i can get all infos from signals (though twisted is new for me) and the stats", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T22:54:46.810053+00:00", "nick": "nyov", "message": "great! that was quick", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T23:01:56.511288+00:00", "nick": "DanielW", "message": "an exception in start_requests of sspider doesn't trigger the signal \"spider_error\"?", "links": [], "channel": "scrapy"},
{"date": "2014-09-29T23:02:31.569989+00:00", "nick": "nyov", "message": "no idea", "links": [], "channel": "scrapy"},
{"date": "2014-09-30T07:20:54.394646+00:00", "nick": "kakashi__", "message": "hello, I have a html like <span>text1<span> text2 <span>text3<span>, how can I parse it by xpath to text1 text2 text3", "links": [], "channel": "scrapy"},
{"date": "2014-09-30T08:04:57.457812+00:00", "nick": "blusteal_", "message": "kakashi__ you can do response.xpath('//span/text()').extract()", "links": [], "channel": "scrapy"},
{"date": "2014-09-30T08:05:06.876450+00:00", "nick": "blusteal_", "message": "that should work give it a shot", "links": [], "channel": "scrapy"},
{"date": "2014-09-30T08:05:35.959998+00:00", "nick": "kakashi__", "message": "I know that, but the result would be text1 text3", "links": [], "channel": "scrapy"},
{"date": "2014-09-30T08:06:26.235706+00:00", "nick": "blusteal_", "message": "x = response.xpath('//span/text()')", "links": [], "channel": "scrapy"},
{"date": "2014-09-30T08:06:35.910586+00:00", "nick": "blusteal_", "message": "for items in x:", "links": [], "channel": "scrapy"},
{"date": "2014-09-30T08:06:38.924057+00:00", "nick": "blusteal_", "message": "print item", "links": [], "channel": "scrapy"},
{"date": "2014-09-30T08:07:44.339548+00:00", "nick": "kakashi__", "message": "sorry... I found out that html is wrong, it should be <span>text1</span> text2 <span>text3</span>", "links": [], "channel": "scrapy"},
{"date": "2014-09-30T08:08:07.363987+00:00", "nick": "kakashi__", "message": "so that I can't extract text2", "links": [], "channel": "scrapy"},
{"date": "2014-09-30T08:09:11.909695+00:00", "nick": "blusteal_", "message": "if it's outside of a tag, you can grab the parent tag", "links": [], "channel": "scrapy"},
{"date": "2014-09-30T08:09:38.499511+00:00", "nick": "blusteal_", "message": "are those span tags wrapped in a div or something, grab that and wrangle it out", "links": [], "channel": "scrapy"},
{"date": "2014-09-30T08:14:21.428405+00:00", "nick": "kakashi__", "message": "okay~ let me try it", "links": [], "channel": "scrapy"},
{"date": "2014-09-30T08:14:24.857777+00:00", "nick": "kakashi__", "message": "thanks!", "links": [], "channel": "scrapy"},
{"date": "2014-09-30T08:22:19.193774+00:00", "nick": "nyov", "message": "kakashi__: try sel.xpath('//span').xpath('text()|node()')", "links": [], "channel": "scrapy"},
{"date": "2014-09-30T08:22:30.758530+00:00", "nick": "nyov", "message": "if you don't need nested nodes", "links": [], "channel": "scrapy"},
{"date": "2014-09-30T08:22:50.732257+00:00", "nick": "nyov", "message": "otherwise, sel.xpath('//a').xpath('.//text()|.//node()')", "links": [], "channel": "scrapy"},
{"date": "2014-09-30T08:23:56.290794+00:00", "nick": "nyov", "message": "eh. span, not a", "links": [], "channel": "scrapy"},
{"date": "2014-09-30T08:25:32.993666+00:00", "nick": "nyov", "message": "or actually just //text()", "links": [], "channel": "scrapy"},
{"date": "2014-09-30T08:25:40.632817+00:00", "nick": "nyov", "message": "lol", "links": [], "channel": "scrapy"},
{"date": "2014-09-30T13:27:13.104430+00:00", "nick": "brotherBox", "message": "Hi. In my program I access a number of urls with a number in a range of, say, 1 to 2,000,000 appended to it. Is there a way to only yield, say, 50 requests from these urls before issuing the next batch of 50?", "links": [], "channel": "scrapy"},
{"date": "2014-09-30T13:36:02.565456+00:00", "nick": "tomwardill", "message": "brotherBox: I have no sample code I can share for it, but the spider_idle signal (http://doc.scrapy.org/en/latest/topics/signals.... ) will do what you want.", "links": ["http://doc.scrapy.org/en/latest/topics/signals.html#spider-idle"], "channel": "scrapy"},
{"date": "2014-09-30T13:36:24.239539+00:00", "nick": "tomwardill", "message": "keep a list of the urls, and use the handler for that signal to yield the next batch", "links": [], "channel": "scrapy"},
{"date": "2014-09-30T13:41:18.229678+00:00", "nick": "brotherBox", "message": "tomwardill: That would work with a generator returning the urls, right? A list of the urls alone would be quite large, and so far they regularly managed to completely trash my ram", "links": [], "channel": "scrapy"},
{"date": "2014-09-30T13:41:44.653230+00:00", "nick": "tomwardill", "message": "as far as I know, I've only worked with the end result, not implemented it myself", "links": [], "channel": "scrapy"},
{"date": "2014-09-30T13:42:05.670395+00:00", "nick": "tomwardill", "message": "I can't think of a reason why not though", "links": [], "channel": "scrapy"},
{"date": "2014-09-30T13:50:42.907637+00:00", "nick": "brotherBox", "message": "tomwardill: I'll look into it, thanks so far", "links": [], "channel": "scrapy"},
{"date": "2014-09-30T14:49:58.711986+00:00", "nick": "blusteal", "message": "brotherBox you can have the list in a file, database or somewhere else", "links": [], "channel": "scrapy"},
{"date": "2014-09-30T14:50:29.885043+00:00", "nick": "blusteal", "message": "hook into the close_spider function, when all requests are finished grab another 50", "links": [], "channel": "scrapy"},
{"date": "2014-09-30T16:48:29.060462+00:00", "nick": "wowowow", "message": "Hi!", "links": [], "channel": "scrapy"},
{"date": "2014-09-30T20:33:47.895618+00:00", "nick": "Nyx", "message": "Hello", "links": [], "channel": "scrapy"},
{"date": "2014-09-30T20:34:53.465138+00:00", "nick": "Nyx22", "message": "Anybody here ?", "links": [], "channel": "scrapy"},
{"date": "2014-09-30T23:13:47.858431+00:00", "nick": "SirSkitzo", "message": "With HTMLXPathSelector, is it possible to choose a HTML node only if it has \"display: inline;\" applied to it in the <head> section? There are identical, and randomly placed, nodes in a page I'm trying to select and the only difference between the nodes is the one I want has \"display: inline; applied to it and the others have display: none; applied to it.", "links": [], "channel": "scrapy"},
{"date": "2014-09-30T23:24:46.036149+00:00", "nick": "SirSkitzo", "message": "Sorry, I disconnected. Here's my question again", "links": [], "channel": "scrapy"},
{"date": "2014-09-30T23:24:47.977037+00:00", "nick": "SirSkitzo", "message": "With HTMLXPathSelector, is it possible to choose a HTML node only if it has \"display: inline;\" applied to it in the <head> section? There are identical, and randomly placed, nodes in a page I'm trying to select and the only difference between the nodes is the one I want has \"display: inline; applied to it and the others have display: none; applied to it.", "links": [], "channel": "scrapy"},
{"date": "2014-09-30T23:35:02.538236+00:00", "nick": "nyov", "message": "SirSkitzo: if that's really what you want, yes it's possible", "links": [], "channel": "scrapy"},
{"date": "2014-09-30T23:36:09.718588+00:00", "nick": "nyov", "message": "I'd use the contains() xpath function", "links": [], "channel": "scrapy"},
{"date": "2014-10-01T01:01:07.574918+00:00", "nick": "SirSkitzo", "message": "Thanks nyov. Would contains() know if the node has display: inline; applied to it if it's not done with inline styling? (For instance, if it the css was applied to the node through an external style sheet, or in my case a style tag in the header)", "links": [], "channel": "scrapy"},
{"date": "2014-10-01T01:29:46.799091+00:00", "nick": "nyov", "message": "SirSkitzo: no, that's why I'd be skeptic with such an approach", "links": [], "channel": "scrapy"},
{"date": "2014-10-01T01:30:53.580313+00:00", "nick": "nyov", "message": "also contains() is just a string check, like \"if 'x' is in 'y'\" and could fail", "links": [], "channel": "scrapy"},
{"date": "2014-10-01T01:32:05.519486+00:00", "nick": "nyov", "message": "e.g. if you have '//*[contains(@style, \"display:inline\")]' but the html says style=\"display: inline;\" it'd fail because of the space, newline or whatever there could be in it", "links": [], "channel": "scrapy"},
{"date": "2014-10-01T01:34:04.101448+00:00", "nick": "nyov", "message": "so I think you might have to do two checks, for keyword 'display' and keyword 'inline' in the same attribute. a bit ugly", "links": [], "channel": "scrapy"},
{"date": "2014-10-01T01:34:50.246062+00:00", "nick": "nyov", "message": "or you also use the function normalize-whitespace() to trim that stuff first", "links": [], "channel": "scrapy"},
{"date": "2014-10-01T01:39:33.815910+00:00", "nick": "nyov", "message": "ahahaha. \"windows 9\" is now \"windows 10\". microsoft's trying hard to get distance from win 8?", "links": [], "channel": "scrapy"},
{"date": "2014-10-01T01:40:39.143257+00:00", "nick": "nyov", "message": "or are they just going for the apple-style of \"OS 10\" and renaming it to Windows X later? doh", "links": [], "channel": "scrapy"},
{"date": "2014-10-01T01:41:04.915871+00:00", "nick": "SirSkitzo", "message": "Yeah, that doesn't sound reliable. It's also not using style=\"\" so I don't think it would work at all", "links": [], "channel": "scrapy"},
{"date": "2014-10-01T01:42:58.070374+00:00", "nick": "nyov", "message": "SirSkitzo: if you're using scrapy.. maybe just use some of python's power and do some parsing", "links": [], "channel": "scrapy"},
{"date": "2014-10-01T01:44:47.068768+00:00", "nick": "SirSkitzo", "message": "Yeah, I think I will do that. I was just seeing if scrapy had a solution for this.", "links": [], "channel": "scrapy"},
{"date": "2014-10-01T01:44:50.338707+00:00", "nick": "SirSkitzo", "message": "Thanks for the help", "links": [], "channel": "scrapy"},
{"date": "2014-10-01T01:48:04.130021+00:00", "nick": "nyov", "message": "SirSkitzo: you could for example loop through a more generic SelectorList. that's pretty handy scrapy code", "links": [], "channel": "scrapy"},
{"date": "2014-10-01T04:01:26.565415+00:00", "nick": "kakashi__", "message": "@nyov thanks!", "links": [], "channel": "scrapy"},
{"date": "2014-10-01T13:17:06.857083+00:00", "nick": "asd_", "message": "are there any books about Scrapy?", "links": [], "channel": "scrapy"},
{"date": "2014-10-01T16:09:30.349809+00:00", "nick": "eVRiAL", "message": "what is the best serializer format for 5 gigs of data?", "links": [], "channel": "scrapy"},
{"date": "2014-10-01T16:10:50.023074+00:00", "nick": "nramirezuy", "message": "json lines or xml should work", "links": [], "channel": "scrapy"},
{"date": "2014-10-01T16:12:08.907653+00:00", "nick": "nramirezuy", "message": "json lines will generate a bunch of verbosity, but is more friendly with bash tools", "links": [], "channel": "scrapy"},
{"date": "2014-10-01T16:12:13.518441+00:00", "nick": "eVRiAL", "message": "thx, what about cpickle or sqlite3?", "links": [], "channel": "scrapy"},
{"date": "2014-10-01T16:12:45.168029+00:00", "nick": "nramirezuy", "message": "xml will be more compact but its a pain to split or analyze without a parser", "links": [], "channel": "scrapy"},
{"date": "2014-10-01T16:13:03.857195+00:00", "nick": "nramirezuy", "message": "I don't use them", "links": [], "channel": "scrapy"},
{"date": "2014-10-01T16:13:41.789028+00:00", "nick": "nramirezuy", "message": "both will work", "links": [], "channel": "scrapy"},
{"date": "2014-10-01T16:13:46.330345+00:00", "nick": "nramirezuy", "message": "I believe", "links": [], "channel": "scrapy"},
{"date": "2014-10-01T16:14:06.119185+00:00", "nick": "nramirezuy", "message": "but sqlite can get corrupted", "links": [], "channel": "scrapy"},
{"date": "2014-10-01T16:14:09.139184+00:00", "nick": "eVRiAL", "message": "ok cool", "links": [], "channel": "scrapy"},
{"date": "2014-10-01T16:14:30.262678+00:00", "nick": "nramirezuy", "message": "my preference is jsonlines", "links": [], "channel": "scrapy"},
{"date": "2014-10-01T20:01:03.952255+00:00", "nick": "seni", "message": "Hi", "links": [], "channel": "scrapy"},
{"date": "2014-10-01T20:03:16.558473+00:00", "nick": "seni", "message": "I want to execute a routine after the spider finishes. This routine will be executed in many spiders, so I can't just put it in spider_closed in the spider's class.", "links": [], "channel": "scrapy"},
{"date": "2014-10-01T20:03:28.916481+00:00", "nick": "seni", "message": "What is the best way to achieve that?", "links": [], "channel": "scrapy"},
{"date": "2014-10-01T20:38:52.780066+00:00", "nick": "nyov", "message": "seni...", "links": [], "channel": "scrapy"},
{"date": "2014-10-01T22:46:21.117698+00:00", "nick": "DanielW", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2014-10-01T22:47:08.031959+00:00", "nick": "DanielW", "message": "i have a question again:  is there a simple way, to disable the use of the http cache for single requests?", "links": [], "channel": "scrapy"},
{"date": "2014-10-01T22:47:42.533687+00:00", "nick": "DanielW", "message": "or is the only way to override the dummycache policy ?", "links": [], "channel": "scrapy"},
{"date": "2014-10-01T22:48:27.019555+00:00", "nick": "DanielW", "message": "(so that is doesn't cache requests when some key is in \"meta\")", "links": [], "channel": "scrapy"},
{"date": "2014-10-01T23:06:34.058180+00:00", "nick": "nyov", "message": "oh man, whyever did they decide on a non-indexed container format for WARC? this is so lame for an archival filestructure. sequential seeking through gigabytes? possibly remotely? dangit", "links": [], "channel": "scrapy"},
{"date": "2014-10-01T23:09:10.679050+00:00", "nick": "nyov", "message": "DanielW: no idea, but if not why don't you patch yours to look for that meta key?", "links": [], "channel": "scrapy"},
{"date": "2014-10-01T23:10:20.526164+00:00", "nick": "DanielW", "message": "yes, i am just asking, if i am missing some standard feature of scrapy which can do it for me", "links": [], "channel": "scrapy"},
{"date": "2014-10-01T23:12:34.178608+00:00", "nick": "nyov", "message": "use the source, luke ;)", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T00:26:08.628681+00:00", "nick": "scraystarter", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T00:26:36.857076+00:00", "nick": "scraystarter", "message": "awfully quiet here", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T14:04:49.403408+00:00", "nick": "seni", "message": "Hi!", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T14:31:09.227552+00:00", "nick": "Zladivliba", "message": "hello everyone!", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T14:31:22.671971+00:00", "nick": "Zladivliba", "message": "I've just had scrapy stalling like forever, any idea how I could debug that ?", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T14:39:30.779401+00:00", "nick": "seni", "message": "<Zladivliba> Did you try changing user agent?", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T14:39:53.041295+00:00", "nick": "Zladivliba", "message": "yes my user agent is \"robot\" somehting...", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T14:40:04.776507+00:00", "nick": "Zladivliba", "message": "but the website is scraped all right untill after a while", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T14:44:19.682087+00:00", "nick": "seni", "message": "Maybe you running out of memory?", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T14:47:55.020967+00:00", "nick": "Zladivliba", "message": "seni: unlikely I think", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T14:48:18.832945+00:00", "nick": "Zladivliba", "message": "I've just seen the \"rest\" of a process that I killed, finished running 5 minutes after... hummm wierd !", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T14:52:10.255801+00:00", "nick": "seni", "message": "yes, this is weird", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T15:37:43.702080+00:00", "nick": "elio_rubens", "message": "hey peoples... im having trouble with Beautiful soup here is my code: http://pastie.org/9614106", "links": ["http://pastie.org/9614106"], "channel": "scrapy"},
{"date": "2014-10-02T15:38:00.839736+00:00", "nick": "elio_rubens", "message": "basically i have a list of urls in the csv file. and for each url i call a find_all function once for an image and once for the division", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T15:38:30.366617+00:00", "nick": "elio_rubens", "message": "the div search works but the img search returns the img from the first url even when on we are moving on to the second and third url", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T16:01:16.032252+00:00", "nick": "Roux_taff", "message": "hey there", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T16:01:39.096508+00:00", "nick": "Roux_taff", "message": "is there an easy way to access my spider's settings from within a pipeline class", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T16:01:40.296852+00:00", "nick": "Roux_taff", "message": "?", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T16:02:40.270921+00:00", "nick": "Roux_taff", "message": "within open_spider maybe?", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T16:04:29.052800+00:00", "nick": "Roux_taff", "message": "forget I asked anything, I forgot spider was also an argument of process_item.....", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T16:16:16.142531+00:00", "nick": "elio_rubens", "message": "does this ish work", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T16:16:36.461795+00:00", "nick": "elio_rubens", "message": "im new to irc, wanted ot ask a quesiton but it seems im not getting any messages back?", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T16:28:27.458279+00:00", "nick": "Zladivliba", "message": "hello guys !", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T16:28:48.727183+00:00", "nick": "Zladivliba", "message": "I found a big scrapy BUG !", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T16:28:53.253714+00:00", "nick": "Zladivliba", "message": "I use allowed_domains", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T16:29:11.119992+00:00", "nick": "Zladivliba", "message": "and scrapy hits a domain outside what's allowed", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T16:29:25.755327+00:00", "nick": "Zladivliba", "message": "how do I debug this ? or submit a bug ?", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T17:43:05.553905+00:00", "nick": "agntdrake", "message": "hello peeps", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T17:48:17.956262+00:00", "nick": "SoFLy", "message": "I've got a spider with two seperate functions to return items, and I have two requests being yielded, one with a callback for each one of the \"processing\" functions", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T17:48:34.435924+00:00", "nick": "SoFLy", "message": "the first function gets called fine, but it never makes it to the second function even though it does yield the request... what is going on there?", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T17:50:30.933792+00:00", "nick": "nyov", "message": "SoFLy: not sure I understand it correctly, but if you yield a Request with another callback function, the request must first return a response, to get to the callback", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T17:50:52.977360+00:00", "nick": "SoFLy", "message": "nyov we meet again :) this is the same crawler i had going last time!", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T17:51:11.998684+00:00", "nick": "SoFLy", "message": "can i just paste the code with the 2 requests and the 2 functions to patebin", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T17:51:18.321529+00:00", "nick": "SoFLy", "message": "i think it's a simple issue, i agree", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T17:51:21.997823+00:00", "nick": "nyov", "message": "sure, that'd be helpful", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T17:51:24.534435+00:00", "nick": "SoFLy", "message": "okay :)", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T17:52:29.237336+00:00", "nick": "nyov", "message": "and i was swamped with code lately. I don't exactly remember your case I'm afraid ;)", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T17:52:57.789988+00:00", "nick": "SoFLy", "message": "haha yeah, you just helped me once last week", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T17:53:01.438662+00:00", "nick": "SoFLy", "message": "answered some questions :)", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T17:53:09.417449+00:00", "nick": "SoFLy", "message": "i'm working on pasting it now, noticed an error", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T17:53:57.979949+00:00", "nick": "SoFLy", "message": "yeah, still doesn't make it to the second func", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T17:53:58.912840+00:00", "nick": "SoFLy", "message": "ok", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T17:54:36.717429+00:00", "nick": "SoFLy", "message": "http://pastebin.com/uaVFJUZW nyov", "links": ["http://pastebin.com/uaVFJUZW"], "channel": "scrapy"},
{"date": "2014-10-02T17:54:50.754572+00:00", "nick": "SoFLy", "message": "so, it is never making it to the function def parseMoreImg(self, response):", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T17:56:53.701125+00:00", "nick": "nyov", "message": "parseProduct is working fine, though?", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T17:57:39.088750+00:00", "nick": "SoFLy", "message": "yeah", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T17:57:44.679562+00:00", "nick": "nyov", "message": "I guess the problem could be that you're sending a Request to the same url twice, there. the second will be considered a duplicate and filtered out", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T17:57:56.041900+00:00", "nick": "SoFLy", "message": "it gets called and the console outputs the yielding request 1", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T17:58:03.489364+00:00", "nick": "SoFLy", "message": "hmm okay, is there any way to handle that?", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T17:58:10.490836+00:00", "nick": "SoFLy", "message": "write another spider? haha", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T17:58:48.715775+00:00", "nick": "nyov", "message": "you already have the response with a single request, no need to hit the website twice. instead you should add both callbacks to the same request", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T17:59:00.092799+00:00", "nick": "SoFLy", "message": "ohh... you can do that? -_-", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T17:59:20.966016+00:00", "nick": "SoFLy", "message": "how would the line look", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:00:06.609574+00:00", "nick": "SoFLy", "message": "should i just call the parseProduct and then inside parseProduct call parseMoreImg?", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:01:15.387413+00:00", "nick": "Zladivliba", "message": "nyov: have u seen the problem i posted ?", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:03:08.553607+00:00", "nick": "nyov", "message": "SoFLy: https://bpaste.net/show/dbd73e043662", "links": ["https://bpaste.net/show/dbd73e043662"], "channel": "scrapy"},
{"date": "2014-10-02T18:04:21.808263+00:00", "nick": "nyov", "message": "Zladivliba: yeah, not sure how I can help with that", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:05:33.260989+00:00", "nick": "nyov", "message": "for submitting a bug, go here https://github.com/scrapy/scrapy/issues", "links": ["https://github.com/scrapy/scrapy/issues"], "channel": "scrapy"},
{"date": "2014-10-02T18:05:44.983766+00:00", "nick": "SoFLy", "message": "thank you nyov , that makes much more sense. just made the change, and it still doesnt' seem to be making it to the parseMoreImg", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:05:45.676777+00:00", "nick": "SoFLy", "message": ":(", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:07:47.654578+00:00", "nick": "nyov", "message": "are you sure the code is correct?", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:08:10.624819+00:00", "nick": "nyov", "message": "run it with DEBUG loglevel, that should show any issues", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:08:13.659564+00:00", "nick": "SoFLy", "message": "which, the code inside parseMoreImg?", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:08:14.337308+00:00", "nick": "SoFLy", "message": "okay", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:08:34.974744+00:00", "nick": "nyov", "message": "for example item = response.meta['item2']... is there really an item2 ? doesn't look like it  in the code snippet", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:09:27.549762+00:00", "nick": "SoFLy", "message": "another good catch - artifact from when i was trying it as a different item", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:09:29.630461+00:00", "nick": "SoFLy", "message": "fixing that now", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:09:40.136639+00:00", "nick": "SoFLy", "message": "just re-ran, nothing. still very odd that it's never making it inside the function at all", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:09:47.131213+00:00", "nick": "SoFLy", "message": "since i put the \"debug print\" statement at the top of the function", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:10:22.144589+00:00", "nick": "nyov", "message": "also, using print, for logging isn't really great with twisted. it may get eaten. use log.msg(), or self.log() in a spider, instead", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:10:33.835324+00:00", "nick": "SoFLy", "message": "okay :)", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:11:21.654762+00:00", "nick": "SoFLy", "message": "ah, i see the error now :)", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:11:36.982665+00:00", "nick": "SoFLy", "message": "just commented out the part where it ran parseProduct, so it ONLY called the \"broken\" one", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:11:45.419860+00:00", "nick": "SoFLy", "message": "ERROR: Spider must return Request, BaseItem or None, got 'generator' in", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:11:47.163893+00:00", "nick": "SoFLy", "message": "much more helpful", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:11:47.455486+00:00", "nick": "nyov", "message": "but try to disable the call to self.parseProduct.. eh, yes", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:11:50.565698+00:00", "nick": "SoFLy", "message": "haha yeah", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:12:31.974809+00:00", "nick": "SoFLy", "message": "is it okay to return items with only 2/x fields populated?", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:12:46.601692+00:00", "nick": "nyov", "message": "sure", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:12:58.680763+00:00", "nick": "SoFLy", "message": "okay, just covering my basic bases here making sure im not doing anything horribly wrong", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:13:17.027493+00:00", "nick": "SoFLy", "message": "this is the shopify thing again where i wanted just the URL and handle on a new line, doesn't matter where in the csv", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:13:23.851088+00:00", "nick": "SoFLy", "message": "going to be importing products to a shopify website", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:13:27.575026+00:00", "nick": "nyov", "message": "ah, i remember now", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:14:55.420401+00:00", "nick": "SoFLy", "message": "yeah -_-", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:14:55.897246+00:00", "nick": "nyov", "message": "well, i'm not entirely sure... i think it's wrong to yield in my parsePage. it should return() but then we can't have both calls", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:15:26.881554+00:00", "nick": "SoFLy", "message": "yeah, that makes sense. and if it was wrong to yield in parsePage, why was parseProduct running happily", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:15:28.204806+00:00", "nick": "nyov", "message": "try return self.parseMoreImg", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:15:33.023789+00:00", "nick": "SoFLy", "message": "okay", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:15:38.399666+00:00", "nick": "nyov", "message": "with the other still disabled", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:15:56.311955+00:00", "nick": "SoFLy", "message": "hey, that seemed to work okay :)", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:16:00.200001+00:00", "nick": "nyov", "message": "dang", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:16:04.342622+00:00", "nick": "SoFLy", "message": "haha", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:16:08.294895+00:00", "nick": "SoFLy", "message": "good debugging skillz", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:16:12.143732+00:00", "nick": "SoFLy", "message": "(you)", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:16:13.516444+00:00", "nick": "SoFLy", "message": "from afar", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:16:27.016329+00:00", "nick": "nyov", "message": ";)", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:17:04.312520+00:00", "nick": "nyov", "message": "okay, try \"return [self.parseProduct(response), self.parseMoreImg(response)]\"", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:17:22.058516+00:00", "nick": "nyov", "message": "maybe it can work with a list", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:18:06.231370+00:00", "nick": "SoFLy", "message": "jsut ran it, only did parseProduct this time", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:18:24.124517+00:00", "nick": "SoFLy", "message": "i'm thinking - if i run a spider and dump to csv, it'll append the stdout", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:18:39.326896+00:00", "nick": "SoFLy", "message": "so, could i just run it once with parseProduct and then comment it and run it again with parseMoreImg", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:18:54.441896+00:00", "nick": "nyov", "message": "nah that sucks", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:18:54.441970+00:00", "nick": "SoFLy", "message": "that's like a manual cheating method :)", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:18:57.679045+00:00", "nick": "SoFLy", "message": "yeah haha", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:19:10.176134+00:00", "nick": "SoFLy", "message": "not optimal, but somewhat working", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:19:33.857529+00:00", "nick": "nyov", "message": "okay, how about you just kill one of the methods, and place them together? it'd be the logical thing", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:19:52.149311+00:00", "nick": "SoFLy", "message": "okay", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:19:53.081706+00:00", "nick": "nyov", "message": "just append parseMoreImg to parseProduct", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:19:55.127558+00:00", "nick": "SoFLy", "message": "working on that now", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:20:10.507672+00:00", "nick": "SoFLy", "message": "yeah, makes sense. had it like that at one point, saw some code examples suggesting to break it up so i did and never went back", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:21:27.349412+00:00", "nick": "nyov", "message": "a more advance option I use, is adding some code to allow passing multiple callback methods in meta.", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:21:47.735113+00:00", "nick": "SoFLy", "message": "errored out with the same error about must return Request, Base...", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:21:53.148495+00:00", "nick": "nyov", "message": "but it would just uselessly complicate your code I think.", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:22:04.578375+00:00", "nick": "SoFLy", "message": "yeah", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:22:31.890657+00:00", "nick": "SoFLy", "message": "this crawler has some real weird requirements, probably not the best one to write as my first real crawler project", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:22:58.669898+00:00", "nick": "SoFLy", "message": "the reason it's erroring out is because i have multiple yield items and now that i added that code back to parseProduct it's all workin with the same item", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:23:29.435479+00:00", "nick": "SoFLy", "message": "i needed it to parse the product, then after parse the additional images and make each url a new item with the handle", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:23:51.800400+00:00", "nick": "SoFLy", "message": "i can fix that i believe", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:24:02.192488+00:00", "nick": "nyov", "message": "that is weird, I can't see a problem with the code", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:24:30.655908+00:00", "nick": "nyov", "message": "returning multiple items should not be a problem. what is the exact error again?", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:24:39.801276+00:00", "nick": "nyov", "message": "returning generator?", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:25:11.389788+00:00", "nick": "SoFLy", "message": "oh whoops", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:25:17.962243+00:00", "nick": "SoFLy", "message": "shit, i forgot to change yield to return up in the parsePage", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:25:51.900268+00:00", "nick": "nyov", "message": "yeah, you can remove all that. put your original method as the callback", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:26:14.506986+00:00", "nick": "SoFLy", "message": "fixing & running now", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:27:44.844484+00:00", "nick": "SoFLy", "message": "yay it worked!!", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:27:56.044812+00:00", "nick": "nyov", "message": "finally", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:33:05.061919+00:00", "nick": "SoFLy", "message": "yeah, had to also change the code in parseMoreImg to be a different item (back to item2), since i wanted it to return item2 with just the handle and imgSrc", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:33:06.584164+00:00", "nick": "SoFLy", "message": "looks good :)", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:37:17.566918+00:00", "nick": "SoFLy", "message": "obviously took a lot of coaxing, thank you very much nyov :)", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:37:44.912583+00:00", "nick": "nyov", "message": "no problem, that was simple enough", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:38:56.445231+00:00", "nick": "nyov", "message": "and sure you can split those things up ...somehow. if you get it right", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:39:11.686715+00:00", "nick": "nyov", "message": "I obviously didn't just now", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:40:59.870655+00:00", "nick": "nyov", "message": "but just fyi, this is what I use when I need \"more callbacks\" in a crawlspider: https://github.com/nyov/scrapyext/blob/master/s...", "links": ["https://github.com/nyov/scrapyext/blob/master/scrapyext/spiders/mixin/mcbmixin.py"], "channel": "scrapy"},
{"date": "2014-10-02T18:41:09.278642+00:00", "nick": "nyov", "message": "the description could probably be better", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:52:15.255570+00:00", "nick": "SoFLy", "message": "haha i split them up okay and cleaned up the code", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:52:21.413311+00:00", "nick": "SoFLy", "message": "reading through this callback mixin", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:52:58.530452+00:00", "nick": "SoFLy", "message": "this looks very nice", "links": [], "channel": "scrapy"},
{"date": "2014-10-02T18:53:16.644550+00:00", "nick": "SoFLy", "message": "also reminds me of proper \"production\" practices like actually handling errors", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T04:51:47.202870+00:00", "nick": "agntdrake", "message": "Where do most people do their scraping?  On different cloud service providers?", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T05:54:42.150314+00:00", "nick": "nyov", "message": "well, me, I am scraping by from home. sitting in an office chair, in front of a computer screen. but i'm not most people, so I took a look. but the clouds are so thick today, I can't see anyone scraping up there. though I wouldn't know how it is elsewhere ;)", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T05:55:16.449741+00:00", "nick": "nyov", "message": "joking aside, is there a real question there?", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T06:01:59.257477+00:00", "nick": "nyov", "message": "of course I could just tell you most people are doing their scraping on http://scrapinghub.com/ - and since I don't know better it might actually be true.", "links": ["http://scrapinghub.com/"], "channel": "scrapy"},
{"date": "2014-10-03T11:42:03.917324+00:00", "nick": "Zladivliba", "message": "hey", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T11:42:09.011685+00:00", "nick": "Zladivliba", "message": "any developer from scrapy around ?", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T12:09:29.585912+00:00", "nick": "eVRiAL", "message": "Hi, what's good choise for cache backend for 200k requests? file or dbm?", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T12:25:19.645416+00:00", "nick": "Zladivliba", "message": "db", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T16:18:39.834448+00:00", "nick": "fpghost84", "message": "Hi is there any way to declare an Item in Scrapy that could have a variable number of a certain fields (or have a list as a field that can be appended to). Maybe something like a Foreign key in django might work?", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T16:19:40.057848+00:00", "nick": "fpghost84", "message": "To give some context: my item is EventItem() that has simple fields like name, data, sport, bookmaker, and then numeric fields odd1, odd2, odd3,odd4....it is these numeric odds fields that can be of variable number in my scraped data", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T16:33:18.115134+00:00", "nick": "nyov", "message": "fpghost84: https://scrapy.readthedocs.org/en/latest/topics...", "links": ["https://scrapy.readthedocs.org/en/latest/topics/practices.html#dynamic-creation-of-item-classes"], "channel": "scrapy"},
{"date": "2014-10-03T16:34:11.972393+00:00", "nick": "fpghost84", "message": "nyov: thanks, just having a read", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T16:34:21.552686+00:00", "nick": "nyov", "message": "or if you only want dynamic fields..", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T16:34:46.724363+00:00", "nick": "nyov", "message": "https://stackoverflow.com/questions/5069416/scr...", "links": ["https://stackoverflow.com/questions/5069416/scraping-data-without-having-to-explicitly-define-each-field-to-be-scraped"], "channel": "scrapy"},
{"date": "2014-10-03T16:35:43.470512+00:00", "nick": "fpghost84", "message": "great, looks promising", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T16:36:03.672265+00:00", "nick": "nyov", "message": "if you try this older FlexItem(Item):", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T16:36:18.231084+00:00", "nick": "nyov", "message": "without the itemloader, let me know if it works?", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T16:36:30.728073+00:00", "nick": "nyov", "message": "the one from 'Old solution'", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T16:37:07.389353+00:00", "nick": "fpghost84", "message": "ok just taking a look", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T16:45:25.409017+00:00", "nick": "fpghost84", "message": "nyov: the old solution seems to work in the scrapy shell at least....I don't use loaders so should be good for me", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T16:52:04.933179+00:00", "nick": "nyov", "message": "okay, great", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T16:52:06.849790+00:00", "nick": "nyov", "message": "thanks", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T16:52:34.956419+00:00", "nick": "nyov", "message": "there was a comment that it wouldn't work with exporters or something?", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T16:52:49.195605+00:00", "nick": "nyov", "message": "may the poster was just wrong", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T16:56:29.719037+00:00", "nick": "fpghost84", "message": "i'll give it a go with json feed export one sec", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T16:58:51.177789+00:00", "nick": "danielw__", "message": "fpghost84: you can use every field as a list.  just do item[\"somefield\"]=[]  and then you can use it like any other field.  i believe a field can be of any type (dict or, what i do, list of others items)", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T16:59:47.825438+00:00", "nick": "nyov", "message": "that works as well, though some prefer \"well-defined\" item attributes", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T17:01:57.227645+00:00", "nick": "fpghost84", "message": "nyov: I did ...-o 'JBookie.bjson' -t json", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T17:02:01.684073+00:00", "nick": "fpghost84", "message": "and no probs", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T17:02:33.774534+00:00", "nick": "fpghost84", "message": "DanielW: oh that's interesting, I didn't know I thought it had to be Field(). I guess either approach would be fine for me", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T17:04:22.451395+00:00", "nick": "DanielW", "message": "you have to define it as somefield = Field() in the Item class  and then you can use it as item[\"somefield\"] = []  and item[\"somefield\"].append()", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T17:04:38.253439+00:00", "nick": "fpghost84", "message": "DanielW: oh I see, thanks", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T17:04:39.522322+00:00", "nick": "DanielW", "message": "and i believe some example in the documentation does just that (indirect)", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T17:05:02.523725+00:00", "nick": "DanielW", "message": "it writes the results of an xpath query  which returns a list into a field", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T17:05:35.144954+00:00", "nick": "fpghost84", "message": "thanks", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T17:05:46.257253+00:00", "nick": "DanielW", "message": "in the json export you will see a normal json list for that field", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T17:07:09.239351+00:00", "nick": "fpghost84", "message": "One other question whilst I'm here. At the moment I just store all my output as JSON, but I'm getting quite a large amount of data now and thinking perhaps a \"proper database\" would be the right way to implement things. Given my possible dynamical number of odds fields (or even `list` for this field) is monodb the way to go?", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T17:07:50.894044+00:00", "nick": "fpghost84", "message": "(Are there any actual advantages to doing this too? i.e. proper db storage rather than JSON text files)", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T17:07:53.276029+00:00", "nick": "DanielW", "message": "you mean mongodb? (", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T17:08:02.997023+00:00", "nick": "fpghost84", "message": "yeah mongodb*", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T17:08:43.933986+00:00", "nick": "DanielW", "message": "i have never used it.  i am using sqlalchemy to store my scraped data", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T17:08:46.664724+00:00", "nick": "nyov", "message": "nothing wrong with mongodb, I used it quite a bit for variable items in json style", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T17:09:11.893519+00:00", "nick": "fpghost84", "message": "DanielW: with postgres?", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T17:09:30.174493+00:00", "nick": "fpghost84", "message": "nyov: thanks. Speed is definitely a requisite for me", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T17:09:33.179263+00:00", "nick": "nyov", "message": "sqlalchemy OTOH isn't really nice to go with twisted. DanielW, how do you handle ORM sessions?", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T17:10:23.218670+00:00", "nick": "fpghost84", "message": "nyov: do you use http://goo.gl/v9XKjo for mongo pipelining?", "links": ["http://goo.gl/v9XKjo"], "channel": "scrapy"},
{"date": "2014-10-03T17:10:23.519201+00:00", "nick": "DanielW", "message": "at the moment just sqlite.  it is easier to use at development time (you can make a backup of it very quickly) and i don't have that much data", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T17:10:44.874107+00:00", "nick": "nyov", "message": "no url shortener please? lol", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T17:11:05.079207+00:00", "nick": "fpghost84", "message": "https://github.com/sebdah/scrapy-mongodb", "links": ["https://github.com/sebdah/scrapy-mongodb"], "channel": "scrapy"},
{"date": "2014-10-03T17:11:20.981970+00:00", "nick": "nyov", "message": "ah. no I'm not using that one", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T17:11:26.459877+00:00", "nick": "nyov", "message": "but it works", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T17:11:57.815388+00:00", "nick": "fpghost84", "message": "ok, which one are you using out of interest or is something custom?", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T17:12:04.140289+00:00", "nick": "nyov", "message": "https://github.com/nyov/scrapyext/blob/master/s...", "links": ["https://github.com/nyov/scrapyext/blob/master/scrapyext/mongo.py"], "channel": "scrapy"},
{"date": "2014-10-03T17:12:22.872865+00:00", "nick": "fpghost84", "message": "thanks", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T17:13:04.125859+00:00", "nick": "nyov", "message": "okay, that's more than just the basic class. some useless examples in there you'd have to scrap first", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T17:13:39.782384+00:00", "nick": "DanielW", "message": "nyov: i don't have any problems with sqlalchemy. i create a session in the spider class and pass it down to the code which does the writing. (and session.commit() in spider.closed()) . but that is propably not a really clean design. but it works", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T17:13:42.459291+00:00", "nick": "nyov", "message": "and since I just overhauled the mongodb cahce storage class I should probably fix that code too", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T17:14:33.192780+00:00", "nick": "nyov", "message": "DanielW: i'm surprised that even works actually", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T17:15:41.414673+00:00", "nick": "nyov", "message": "i've been using alchemy as well of course, but there can be issues with twisted", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T17:15:59.960428+00:00", "nick": "nyov", "message": "if you don't need ORM, a better choice might be to go with https://pypi.python.org/pypi/alchimia/0.4", "links": ["https://pypi.python.org/pypi/alchimia/0.4"], "channel": "scrapy"},
{"date": "2014-10-03T17:19:44.865324+00:00", "nick": "DanielW", "message": "i don't use the item pipelines maybe that is the reason it works...   i couldn't really figure out how to make it work in my use case. (i want to scrape some sites daily and store the history of the items someshwere). but i also don't do rescrape the complete site everyday.  i first scrape the listing pages of the website in question and then figure out which items are new and just scrape those and also figure out which are gone and mark t", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T17:19:44.967552+00:00", "nick": "DanielW", "message": "hose deleted in my db.  i don't see a way of doing this with the item pipelines", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T17:58:56.468677+00:00", "nick": "AndroidLoverInSF", "message": "anyone have experience with whats more effective for bypass simple captchas? either using some oct library, vs using rotating proxies, or something else", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T18:24:53.872570+00:00", "nick": "DanielW", "message": "AndroidLoverInSF: i have used a paid service a few years ago to solve captchas (real humans do it there). but that can quickly get expensive", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T18:59:25.198084+00:00", "nick": "fpghost84", "message": "Is there anyway I can define the convenience method \".first()\" to be used with \"xpath('/path/to/...').extract().first()\" instead of having to do the whole try except KeyError thing when I use [0]? (I'm taking inspiration from the first() method in later django versions)", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T18:59:36.044367+00:00", "nick": "fpghost84", "message": "this would save me a lot of lines in my spiders", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T19:00:08.370349+00:00", "nick": "fpghost84", "message": "(In other words it returns None if list is empty)", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T19:04:24.606093+00:00", "nick": "nramirezuy", "message": "oh dear, there is a lot of info on our github repo :D", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T19:05:28.486026+00:00", "nick": "nramirezuy", "message": "you could also extend the Selector API or create your own function", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T19:08:52.666562+00:00", "nick": "fpghost84", "message": "nramirezuy: would item loaders actually be the way to go?", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T19:10:55.338831+00:00", "nick": "nramirezuy", "message": "if you like it, I use them a lot. I barely use selectors", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T19:11:07.241953+00:00", "nick": "nyov", "message": "nramirezuy: hahaha, that is one big ugly discussion :D", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T19:15:07.382907+00:00", "nick": "nramirezuy", "message": "you can also do", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T19:15:07.675794+00:00", "nick": "nramirezuy", "message": "import TakeFirst", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T19:15:07.773065+00:00", "nick": "nramirezuy", "message": "take_fisrt = TakeFirst()", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T19:15:07.773131+00:00", "nick": "nramirezuy", "message": "take_first(selector.xpath('//text()').extract())", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T19:15:54.254262+00:00", "nick": "fpghost84", "message": "thanks...I'm exploring using Item loaders anyway, perhaps they'll help with other things...but that method is nice too", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T19:16:36.118792+00:00", "nick": "nramirezuy", "message": "TakeFirst already handles empty strings inside extract, with is nice", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T19:16:46.638631+00:00", "nick": "nramirezuy", "message": "which*", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T19:18:27.896146+00:00", "nick": "fpghost84", "message": "Could item loaders help me with dates in multiple formats? One problem I often encounter (and have dealt with on spider by spider basis so far) is different formatting of data strings I grab, e.g. \"Sat 12 Oct\", \"12/10/14\", \"12th Oct\", and it would be great to have one big date parser that could deal with all of these from each spider", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T19:20:34.915381+00:00", "nick": "nramirezuy", "message": "what you can do is parse the date in each spider as a datetime object, and in a single place (item loader) output it as a string", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T19:20:35.509524+00:00", "nick": "nyov", "message": "try dateutil.parser", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T19:20:44.427039+00:00", "nick": "nyov", "message": "but it sucks for some strings", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T19:22:15.692423+00:00", "nick": "fpghost84", "message": "nramirezuy: well it's actually the other way round, I grab it as a string, and need to parse it into a datetime.datetime obj...at the moment in each spider I might do  date = datetime.datetime.strptime(date, '%d/%m').strftime('%m %d')....but that '%d/%m' is different for each spider", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T19:22:28.858327+00:00", "nick": "fpghost84", "message": "nyov: looks interesting", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T19:24:03.457696+00:00", "nick": "nramirezuy", "message": "string (from website) -> datetime -> string (common format, done in item loader)", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T19:24:20.789739+00:00", "nick": "nramirezuy", "message": "if you have to change the format later, you just go to the item loader and change it", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T19:25:31.677622+00:00", "nick": "fpghost84", "message": "ok but the initial step  string (from website) -> datetime  is the tricky bit, as string(from website) can be different per spider, so parsing into datetime obj is difficult but I guess that's not a scrapy question, I just need a robust function", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T19:26:14.016401+00:00", "nick": "fpghost84", "message": "so assume I had this function (or use nyov's suggestion of dateutil.parser for it) would I just do this parsing in the item loader input processor/output processor?", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T19:26:15.086989+00:00", "nick": "nyov", "message": "but datetime object is what you want as intermediate format", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T19:27:52.529548+00:00", "nick": "fpghost84", "message": "yeah I fully agree with that, so that then my final string can be formatted however I desire with an easy tweak later I suppose", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T19:28:16.841099+00:00", "nick": "nyov", "message": "depends. you can of course just say loader.add_value('date', dateutil.parser.parse(somedate))", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T19:28:39.361486+00:00", "nick": "nyov", "message": "but then the same could be done without itemloaders. or define a custom processor", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T19:29:36.938100+00:00", "nick": "nyov", "message": "def parse_date(date): [...] return datetime", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T19:29:47.810760+00:00", "nick": "nyov", "message": "define that in your itemloader class", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T19:30:05.781249+00:00", "nick": "nyov", "message": "then use it as date_in = MapCompose(parse_date)", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T19:30:08.765357+00:00", "nick": "nyov", "message": "or something", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T19:30:12.702838+00:00", "nick": "fpghost84", "message": "I think I might need custom, as sometimes the format is spanish or french dates like \"Mardi 11 Novombre\", until dateutil.parser really is awesome", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T19:30:55.721759+00:00", "nick": "fpghost84", "message": "(my solution when a spider gets dates like this so far has been to set and unset locales before parsing)", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T19:32:09.143943+00:00", "nick": "nyov", "message": "well, learn about itemloader contexts maybe", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T19:32:53.800017+00:00", "nick": "fpghost84", "message": "ok. Lots of reading to do. Thanks for the putting me on the right path", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T19:35:26.447606+00:00", "nick": "nyov", "message": "I have no easy solution for you. date formats are ugly, especially american ones. never know if you got the month or the day", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T19:36:29.829185+00:00", "nick": "nyov", "message": "and all the stuff that isn't even a format... \"23 hours ago\" -- what the fuck's with those, I hate everyone using them", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T19:37:32.656950+00:00", "nick": "fpghost84", "message": "Indeed, thankfully no of my sites will have that at least...but yes the bulk of my work per spider is rewriting date parsing at the moment it seems", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T19:38:41.963626+00:00", "nick": "nyov", "message": "good luck. that job sucks, I know it ;)", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T19:42:25.490807+00:00", "nick": "nyov", "message": "and I could rant about those pretty printed dates forever. 4 months ago/2 years ago ...who cares about that? what I want from a date is to know if stuff happened, e.g. on a friday before weekend, if my coworker already left for holidays after writing a text or not... you lose all the real information with these vague dates", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T19:43:33.654155+00:00", "nick": "nyov", "message": "anyway. next PR. lol", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T19:46:47.115421+00:00", "nick": "nyov", "message": "whoa. i got issue #911. cool", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T20:14:58.763585+00:00", "nick": "nyov", "message": ":D", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T20:41:02.193160+00:00", "nick": "fpghost84", "message": "One question, how do I use item loaders with relative Xpaths? namely if I do eventSelection = sel.xpath('//div[@class=\"matches \"]//div[@class=\"match featured-match\"]') to load some football matches, then loop over them with  \"for event in eventSelection\" , I normally get the data for the individual event with event.xpath('div[@class=\"date-time\"]/text()') and assign it to my item field, but how to do this with item loaders?", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T20:41:41.915122+00:00", "nick": "fpghost84", "message": "I mean other than just using l.add_value at some point", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T20:43:46.219261+00:00", "nick": "nyov", "message": "I have forgotten, nramirezuy would likely know", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T20:45:36.491979+00:00", "nick": "nramirezuy", "message": "there is no official solution for it, yet. this is the most close https://github.com/scrapy/scrapy/pull/818", "links": ["https://github.com/scrapy/scrapy/pull/818"], "channel": "scrapy"},
{"date": "2014-10-03T20:47:05.480192+00:00", "nick": "fpghost84", "message": "ok, thanks", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T20:52:12.782174+00:00", "nick": "nyov", "message": "that's why I don't like ItemLoaders. they only make things even more complicated :O", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T20:53:28.704847+00:00", "nick": "nyov", "message": "of course that's not true", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T20:55:23.198556+00:00", "nick": "eVRiAL", "message": "I have 350 mb of cache stored in leveldb and I got some 407 error cached. How can I remove/recache some of the requests?", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T20:55:42.022805+00:00", "nick": "eVRiAL", "message": "HTTPCACHE_IGNORE_HTTP_CODES = [407] didn't helped", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T20:55:52.034253+00:00", "nick": "eVRiAL", "message": "maybe it's late of this setting", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T20:56:38.038671+00:00", "nick": "nyov", "message": "it's ['407']", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T20:56:46.661758+00:00", "nick": "fpghost84", "message": "nyov: certainly beginning to wonder that!", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T20:56:49.606406+00:00", "nick": "nyov", "message": "or not?", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T20:57:42.308031+00:00", "nick": "eVRiAL", "message": "well didn't changed", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T20:57:55.555448+00:00", "nick": "eVRiAL", "message": "so no", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T20:58:02.217264+00:00", "nick": "eVRiAL", "message": "it's already there", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T20:58:04.128181+00:00", "nick": "nyov", "message": "eVRiAL: you're right, once it's cached, it'll be returned", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T20:58:07.781325+00:00", "nick": "nyov", "message": "just checked", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T20:58:29.248910+00:00", "nick": "nyov", "message": "so it depend on your cache storage how you get the stuff filtered out again", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T20:58:46.976472+00:00", "nick": "eVRiAL", "message": "yeah :)", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T20:59:18.179080+00:00", "nick": "nyov", "message": "or you could amend your CacheStorage class to check the HTTPCACHE_IGNORE_HTTP_CODES as well before returning a cached response", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T20:59:53.445885+00:00", "nick": "nyov", "message": "or even removing it again at that point", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:00:05.910580+00:00", "nick": "eVRiAL", "message": "yep, thx", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:01:48.856603+00:00", "nick": "eVRiAL", "message": "418 I'm a teapot (RFC 2324)", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:01:59.724911+00:00", "nick": "nyov", "message": "you got that from a real server?", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:02:00.420934+00:00", "nick": "eVRiAL", "message": "wish I wouldn't need to add this", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:02:10.616306+00:00", "nick": "eVRiAL", "message": "hehe", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:02:14.816864+00:00", "nick": "nyov", "message": ";)", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:12:14.334909+00:00", "nick": "bwana", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:19:07.638597+00:00", "nick": "bwana", "message": "anybody have any suggestions for a scrapy nooby for tuts?", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:19:43.407714+00:00", "nick": "nyov", "message": "have you found the official docs already?", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:19:55.293034+00:00", "nick": "bwana", "message": "http://doc.scrapy.org/en/latest/intro/overview....", "links": ["http://doc.scrapy.org/en/latest/intro/overview.html"], "channel": "scrapy"},
{"date": "2014-10-03T21:20:31.569559+00:00", "nick": "bwana", "message": "i am trying to scrape info from a website that is served by a juniper box through a vpn", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:20:33.779857+00:00", "nick": "nyov", "message": "yeah, what in particular are you looking for, then?", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:20:48.417832+00:00", "nick": "bwana", "message": "the site has a login", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:20:58.253999+00:00", "nick": "bwana", "message": "yes i have credentials on that site", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:21:10.837534+00:00", "nick": "bwana", "message": "and it is all above board", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:21:25.520098+00:00", "nick": "bwana", "message": "but the juniper box sends the vpn as java code", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:21:32.086234+00:00", "nick": "bwana", "message": "and it TAKES FOREVER", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:21:37.674114+00:00", "nick": "bwana", "message": "to get going", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:21:45.357986+00:00", "nick": "nyov", "message": "the vpn as java code?", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:21:52.023574+00:00", "nick": "bwana", "message": "yeah", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:21:57.703315+00:00", "nick": "nyov", "message": "you mean the VPN client is in java. or what?", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:22:11.522578+00:00", "nick": "bwana", "message": "the vpn software is pushed to the browser and runs in it", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:22:18.420651+00:00", "nick": "nyov", "message": "uh oh", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:22:27.416091+00:00", "nick": "bwana", "message": "yeah that's what i mean, the vpn client is java", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:22:41.888824+00:00", "nick": "nyov", "message": "can you run it standalone, from a script?", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:22:52.706452+00:00", "nick": "nyov", "message": "is it java-ws code?", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:23:13.040728+00:00", "nick": "bwana", "message": "no, i cannot instantiate the vpn connection without logging in first", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:23:36.270392+00:00", "nick": "bwana", "message": "dont know what java-ws code is", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:23:37.082036+00:00", "nick": "nyov", "message": "well that wouldn't matter, if you logged in from a script aswell (scrapy)", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:24:01.861282+00:00", "nick": "bwana", "message": "yea, i guess i have to hard code my creds into the python script", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:24:09.301294+00:00", "nick": "bwana", "message": "and put in a massive delay", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:24:20.044634+00:00", "nick": "bwana", "message": "to wait for the site to come up", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:25:01.468643+00:00", "nick": "nyov", "message": "how do you even get at that, if it's in a java applet?", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:25:12.833416+00:00", "nick": "bwana", "message": "then i have to figure out how to code in the right button clicks to get to the search page so i can get a list of the records i am interested in", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:25:30.998749+00:00", "nick": "bwana", "message": "the java is just the vpn connection", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:25:56.164657+00:00", "nick": "bwana", "message": "once that's up, the webpage is running in internet explorer", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:26:01.881468+00:00", "nick": "nyov", "message": "hm, I'm intrigued. never had something like that", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:26:39.111680+00:00", "nick": "bwana", "message": "https://access.healthalliance.com/dana-na/auth/...", "links": ["https://access.healthalliance.com/dana-na/auth/url_default/welcome.cgi"], "channel": "scrapy"},
{"date": "2014-10-03T21:26:49.913896+00:00", "nick": "bwana", "message": "yes that's a CGI script!", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:27:00.837340+00:00", "nick": "nyov", "message": "nothing wrong with cgi scripts ;)", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:28:02.088715+00:00", "nick": "bwana", "message": "anyway, the login results in a vpn session starting", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:28:24.305761+00:00", "nick": "nyov", "message": "I guess I can't get it to run the java thing without credentials?", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:29:16.591266+00:00", "nick": "bwana", "message": "not only that but it will only run in internet explorer because it uses ActiveX", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:29:34.135383+00:00", "nick": "nyov", "message": "ah shit.", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:30:22.566260+00:00", "nick": "nyov", "message": "then you won't have much luck in scripting it I guess. unless you know how to script IE or hook activex maybe", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:31:48.076609+00:00", "nick": "bwana", "message": "it's a challenge that i have to work up to i guess", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:32:30.003024+00:00", "nick": "nyov", "message": "don't they maybe have an alternative access if you try to use it with another browser?", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:32:40.786802+00:00", "nick": "bwana", "message": "and not only that but the webapp will only run in win 7", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:33:05.908681+00:00", "nick": "nyov", "message": "i can't believe people still write websites like that", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:33:19.093191+00:00", "nick": "bwana", "message": "no, active X is needed i guess because of the proprietary nature of the database", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:33:49.227170+00:00", "nick": "bwana", "message": "i can get the vpn to come up on a win7 box with chrome but once the vpn is up, i cannot get the webapp to run", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:34:11.571728+00:00", "nick": "bwana", "message": "not really a webapp, it runs on their windows server behind the firewall", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:34:24.180917+00:00", "nick": "nyov", "message": "hmm just looking at other ways to run activex", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:35:21.642118+00:00", "nick": "nyov", "message": "https://en.wikipedia.org/wiki/ActiveX", "links": ["https://en.wikipedia.org/wiki/ActiveX"], "channel": "scrapy"},
{"date": "2014-10-03T21:35:32.717861+00:00", "nick": "bwana", "message": "btw, how do you get your name to say 'unaffiliated/nyov'", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:35:51.449091+00:00", "nick": "nyov", "message": "\"FF ActiveX Host can run ActiveX controls in Mozilla Firefox for Windows.\" << https://github.com/leeor/ff-activex-host", "links": ["https://github.com/leeor/ff-activex-host"], "channel": "scrapy"},
{"date": "2014-10-03T21:36:12.956718+00:00", "nick": "bwana", "message": "wow, ty", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:36:15.341923+00:00", "nick": "nyov", "message": "bwana: you get a hostmask from the freenode team", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:36:33.168688+00:00", "nick": "nyov", "message": "check http://freenode.net/", "links": ["http://freenode.net/"], "channel": "scrapy"},
{"date": "2014-10-03T21:36:48.496493+00:00", "nick": "bwana", "message": "ok, well that's two things i gotta do now", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:37:19.775069+00:00", "nick": "nyov", "message": "https://freenode.net/faq.shtml#cloaks", "links": ["https://freenode.net/faq.shtml#cloaks"], "channel": "scrapy"},
{"date": "2014-10-03T21:38:45.884641+00:00", "nick": "bwana", "message": "i guess i can register my nickserv acct in the #freenode", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:40:26.576725+00:00", "nick": "bwana", "message": "i am using this new irc client icechat", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:42:54.500864+00:00", "nick": "nyov", "message": "no, /msg NickServ for registration", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:43:22.449884+00:00", "nick": "bwana", "message": "/msg NickServ", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:43:29.723327+00:00", "nick": "nyov", "message": "no space", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:43:43.708367+00:00", "nick": "bwana", "message": "/msgNickServ", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:44:02.660713+00:00", "nick": "bwana", "message": "duh", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:44:11.144678+00:00", "nick": "nyov", "message": "what I wanted to say about the activex thing, maybe you can use that to script your session in windows or using wine", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:57:25.048267+00:00", "nick": "bwanaaa", "message": "ty nyov. i am lving now. bye", "links": [], "channel": "scrapy"},
{"date": "2014-10-03T21:58:37.839048+00:00", "nick": "nyov", "message": "bye", "links": [], "channel": "scrapy"},
{"date": "2014-10-04T01:18:20.319476+00:00", "nick": "eVRiAL", "message": "oh, http://doc.scrapy.org/en/latest/topics/leaks.ht...", "links": ["http://doc.scrapy.org/en/latest/topics/leaks.html#leaks-without-leaks"], "channel": "scrapy"},
{"date": "2014-10-04T01:18:20.418556+00:00", "nick": "eVRiAL", "message": "~ 18k requests eats 2 gb of ram", "links": [], "channel": "scrapy"},
{"date": "2014-10-04T01:18:50.033049+00:00", "nick": "eVRiAL", "message": "and killed without swap :x", "links": [], "channel": "scrapy"},
{"date": "2014-10-04T02:15:45.146317+00:00", "nick": "nyov", "message": "props on a job well done, +1", "links": [], "channel": "scrapy"},
{"date": "2014-10-04T14:17:22.487554+00:00", "nick": "spider_", "message": "is there a good resource for quick learning scrapy ?", "links": [], "channel": "scrapy"},
{"date": "2014-10-04T15:16:46.015342+00:00", "nick": "seni_", "message": "Hi!", "links": [], "channel": "scrapy"},
{"date": "2014-10-05T15:47:22.930889+00:00", "nick": "seni", "message": "Hi!", "links": [], "channel": "scrapy"},
{"date": "2014-10-05T16:40:18.162292+00:00", "nick": "seni", "message": "I'm trying to crawl a page but I get \"HTTP Error 400. The request has an invalid header name.\".", "links": [], "channel": "scrapy"},
{"date": "2014-10-05T16:41:24.495740+00:00", "nick": "seni", "message": "Actually I'm POSTING to a URL with payload, so I'm setting the headers : Content-Length and Content-Type.", "links": [], "channel": "scrapy"},
{"date": "2014-10-05T16:41:36.348809+00:00", "nick": "seni", "message": "Any idea?", "links": [], "channel": "scrapy"},
{"date": "2014-10-05T17:08:46.057102+00:00", "nick": "seni", "message": "Nevermind, I just fixed that problem by sending the correct content-length (calculated from the payload I'm sending).", "links": [], "channel": "scrapy"},
{"date": "2014-10-05T19:55:35.596157+00:00", "nick": "DanielW", "message": "oh, i can't restart the twisted reactor :(", "links": [], "channel": "scrapy"},
{"date": "2014-10-05T21:12:19.851577+00:00", "nick": "DanielW", "message": "scripting multiple spider runs in one python script is not that easy :(", "links": [], "channel": "scrapy"},
{"date": "2014-10-05T21:42:49.106844+00:00", "nick": "toothrot", "message": "i thought they worked towards making that pretty simple", "links": [], "channel": "scrapy"},
{"date": "2014-10-05T21:44:51.810286+00:00", "nick": "DanielW", "message": "it looks like CrawlerProcess is made to make it easier.  but i have to make some validations to decide if i want to run another spider or not. and also do email notication and backup of database depending on the outcome of spider runs", "links": [], "channel": "scrapy"},
{"date": "2014-10-05T23:02:16.119433+00:00", "nick": "DanielW", "message": "when Spider.closed() raises an exception  i am not getting either of the signals signals.spider_closed, signals.spider_error or signals.engine_stopped  :(", "links": [], "channel": "scrapy"},
{"date": "2014-10-05T23:03:44.024460+00:00", "nick": "DanielW", "message": "that means i don't stop the twisted reactor...   and my program will never end.   and it also means i can not react in any way to it (like start an other crawler or write an email)", "links": [], "channel": "scrapy"},
{"date": "2014-10-05T23:04:20.582149+00:00", "nick": "DanielW", "message": "i can not figure out how to script scrapy spiders for a nigthly run", "links": [], "channel": "scrapy"},
{"date": "2014-10-05T23:07:36.660875+00:00", "nick": "DanielW", "message": "i am starting to believe that the best way to to it, ist just call  the scrapy shell multiple times and parse the log file between it", "links": [], "channel": "scrapy"},
{"date": "2014-10-05T23:08:03.131349+00:00", "nick": "DanielW", "message": "i can't be the first who has this problem?", "links": [], "channel": "scrapy"},
{"date": "2014-10-06T09:18:47.201564+00:00", "nick": "Newbie0086", "message": "anyone here?", "links": [], "channel": "scrapy"},
{"date": "2014-10-06T09:18:56.790259+00:00", "nick": "Newbie0086", "message": "help", "links": [], "channel": "scrapy"},
{"date": "2014-10-06T09:19:26.352829+00:00", "nick": "Newbie0086", "message": "...", "links": [], "channel": "scrapy"},
{"date": "2014-10-06T10:23:17.592778+00:00", "nick": "bobsuncle", "message": "Hi guys! I have a question about feed exporters. I want to disable them entirely since I'm using DjangoItem to persist scraped items. How would I go about doing this without writing my own dummy feed exporter that does nothing?", "links": [], "channel": "scrapy"},
{"date": "2014-10-06T10:28:32.316516+00:00", "nick": "bobsuncle", "message": "Hhmm would adding EXTENSIONS_BASE = {'scrapy.contrib.feedexport.FeedExporter': None} accomplish this?", "links": [], "channel": "scrapy"},
{"date": "2014-10-06T10:38:19.300190+00:00", "nick": "bobsuncle", "message": "Yup, I think that did it. But I had to add the entire default EXTENSIONS_BASE dict.", "links": [], "channel": "scrapy"},
{"date": "2014-10-06T14:05:53.959121+00:00", "nick": "Xionbox", "message": "Hi! It seems that whenever a DropItem is raised, scrapy will print out the whole item. Is there a way to prevent that? When I raise DropItem in my pipeline I call it as follows: raise DropItem('Duplicate {} item link found: {}.'.format(item.django_model.__name__, item['link'])) . How come this prints the whole item?", "links": [], "channel": "scrapy"},
{"date": "2014-10-06T14:05:55.427328+00:00", "nick": "Xionbox", "message": "Thanks.", "links": [], "channel": "scrapy"},
{"date": "2014-10-06T14:09:40.328139+00:00", "nick": "nikolaosk", "message": "lol, no idea", "links": [], "channel": "scrapy"},
{"date": "2014-10-06T14:09:48.483289+00:00", "nick": "nikolaosk", "message": "but that's a good question", "links": [], "channel": "scrapy"},
{"date": "2014-10-06T14:10:30.171085+00:00", "nick": "nikolaosk", "message": "and a reason to dislike exceptions", "links": [], "channel": "scrapy"},
{"date": "2014-10-06T14:58:11.816060+00:00", "nick": "Xionbox", "message": "nikolaosk, then again scrapy's pipelines are designed for using exceptions.", "links": [], "channel": "scrapy"},
{"date": "2014-10-06T18:55:40.557904+00:00", "nick": "asd", "message": "are there any books about Scrapy?", "links": [], "channel": "scrapy"},
{"date": "2014-10-06T19:52:40.491695+00:00", "nick": "nyov", "message": "asd: I don't think so, you could be the first to write one!", "links": [], "channel": "scrapy"},
{"date": "2014-10-06T19:53:24.621174+00:00", "nick": "nyov", "message": "not that I think that'd work either. the framework is so flexible and in flux, any book written would be completely outdated in a year, two at most", "links": [], "channel": "scrapy"},
{"date": "2014-10-06T19:54:59.199302+00:00", "nick": "nyov", "message": "besides, the documentation id pretty much book enough for most folks", "links": [], "channel": "scrapy"},
{"date": "2014-10-06T19:55:03.228904+00:00", "nick": "nyov", "message": "*is", "links": [], "channel": "scrapy"},
{"date": "2014-10-06T19:57:17.162648+00:00", "nick": "asd", "message": "nyov, thanks for answer", "links": [], "channel": "scrapy"},
{"date": "2014-10-06T19:57:25.868795+00:00", "nick": "nyov", "message": "asd: were you lokoing for one?", "links": [], "channel": "scrapy"},
{"date": "2014-10-06T19:57:48.933755+00:00", "nick": "nyov", "message": "https://media.readthedocs.org/pdf/scrapy/latest...", "links": ["https://media.readthedocs.org/pdf/scrapy/latest/scrapy.pdf"], "channel": "scrapy"},
{"date": "2014-10-06T19:57:50.526196+00:00", "nick": "nyov", "message": "https://media.readthedocs.org/epub/scrapy/lates...", "links": ["https://media.readthedocs.org/epub/scrapy/latest/scrapy.epub"], "channel": "scrapy"},
{"date": "2014-10-06T19:59:01.164717+00:00", "nick": "asd", "message": "yea that is docs, i've read that", "links": [], "channel": "scrapy"},
{"date": "2014-10-06T19:59:06.587897+00:00", "nick": "nyov", "message": "okay", "links": [], "channel": "scrapy"},
{"date": "2014-10-06T19:59:56.853440+00:00", "nick": "asd", "message": "nyov, is scrapy good for scraping content from multiple web pages", "links": [], "channel": "scrapy"},
{"date": "2014-10-06T20:00:09.200389+00:00", "nick": "asd", "message": "when i say multiple i mean 100+ probably more", "links": [], "channel": "scrapy"},
{"date": "2014-10-06T20:00:10.866949+00:00", "nick": "nyov", "message": "absolutely, it was made for that", "links": [], "channel": "scrapy"},
{"date": "2014-10-06T20:00:58.686375+00:00", "nick": "nyov", "message": "read the use-cases here http://scrapy.org/companies/", "links": ["http://scrapy.org/companies/"], "channel": "scrapy"},
{"date": "2014-10-06T20:01:50.880740+00:00", "nick": "asd", "message": "i've somehow missed that link, thanks", "links": [], "channel": "scrapy"},
{"date": "2014-10-06T20:05:38.532614+00:00", "nick": "asd", "message": "nyov, do u have expirience with freelance gigs using scrapy?", "links": [], "channel": "scrapy"},
{"date": "2014-10-06T20:08:18.025788+00:00", "nick": "nyov", "message": "what do you mean by that? freelancers who do scrapy?", "links": [], "channel": "scrapy"},
{"date": "2014-10-06T20:09:03.097331+00:00", "nick": "nyov", "message": "(there are lots of those)", "links": [], "channel": "scrapy"},
{"date": "2014-10-06T20:09:31.501227+00:00", "nick": "asd", "message": "yes, searched around elance, odesk, found only couple of scrapy related projects", "links": [], "channel": "scrapy"},
{"date": "2014-10-06T20:09:58.704666+00:00", "nick": "asd", "message": "any other web sites i should be looking for when searching?", "links": [], "channel": "scrapy"},
{"date": "2014-10-06T20:10:34.992595+00:00", "nick": "nyov", "message": "only a couple? I've seen lots on those. well, besides those two, freelancer.com probably", "links": ["http://freelancer.com"], "channel": "scrapy"},
{"date": "2014-10-06T20:11:14.615812+00:00", "nick": "asd", "message": "https://www.elance.com/r/jobs/q-scrapy/", "links": ["https://www.elance.com/r/jobs/q-scrapy/"], "channel": "scrapy"},
{"date": "2014-10-06T20:12:09.579090+00:00", "nick": "asd", "message": "when i searched web scraping there are a lot more ofc", "links": [], "channel": "scrapy"},
{"date": "2014-10-06T20:12:54.126403+00:00", "nick": "nyov", "message": "right, most don't specify scrapy as a requirement.", "links": [], "channel": "scrapy"},
{"date": "2014-10-06T20:19:27.111598+00:00", "nick": "asd", "message": "looking at this gig atm: https://www.odesk.com/o/jobs/job/_~0190e7a98942...", "links": ["https://www.odesk.com/o/jobs/job/_~0190e7a98942cdff29/"], "channel": "scrapy"},
{"date": "2014-10-06T20:19:48.243549+00:00", "nick": "asd", "message": "what is directory scrappers?", "links": [], "channel": "scrapy"},
{"date": "2014-10-06T23:02:42.571644+00:00", "nick": "danielw_", "message": "i read a bit about twisted, but stilll can't figure out how to handle an exception in spider.close()   after crawler.start()", "links": [], "channel": "scrapy"},
{"date": "2014-10-06T23:03:03.010578+00:00", "nick": "danielw_", "message": "it seems there is no way to get an information back to my code when thats happens", "links": [], "channel": "scrapy"},
{"date": "2014-10-06T23:03:19.182874+00:00", "nick": "danielw_", "message": "so i have no way to stop the reactor or handle that case in any way", "links": [], "channel": "scrapy"},
{"date": "2014-10-06T23:08:30.399054+00:00", "nick": "danielw_", "message": "i don't get the engine_closed signal nor the spider_closed signal so i am lost after crawler.configure() and crawler.start() when spider.close() throws an exception", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:12:07.008720+00:00", "nick": "toothrot", "message": "\"script scrapy spiders for a nigthly run\" how many spiders are we talking about here", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:12:37.524174+00:00", "nick": "toothrot", "message": "danielw_, ^", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:14:02.815907+00:00", "nick": "danielw_", "message": "toothrot: at the moment 3", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:14:26.097635+00:00", "nick": "toothrot", "message": "do they need to be run in the same process?", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:16:10.237236+00:00", "nick": "DanielW", "message": "it would help, because depending on the result of spider i want to start the next or skip one.  at the end i want to send me an email with a report of this run (did the spiders run without errors, how many new items, how many deleted items and so on)", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:16:46.355905+00:00", "nick": "DanielW", "message": "result meaning: i need the stats and the information if any exception has happend", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:17:54.423460+00:00", "nick": "DanielW", "message": "if i run them in a second process (with subprocess or something), i would have to parse the log to get the info?", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:18:13.048163+00:00", "nick": "toothrot", "message": "what information specifically?", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:18:40.238022+00:00", "nick": "DanielW", "message": "did it run with or without exceptions and the stats dictionary", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:21:21.705679+00:00", "nick": "DanielW", "message": "i could write the stats dict into a file in spider.close()  but that seems very hacky to me", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:22:01.883771+00:00", "nick": "toothrot", "message": "maybe, the crawlers i have all write JSON files out, and then a processing script consumes those files afterwards", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:22:43.147600+00:00", "nick": "toothrot", "message": "i used to run my crawlers from within a script and getting rid of that setup got rid of a headache", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:23:08.233406+00:00", "nick": "toothrot", "message": "it's a bit different though", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:23:28.757308+00:00", "nick": "toothrot", "message": "my crawlers were independent from each other, only the post processing of items was the same", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:23:51.347825+00:00", "nick": "toothrot", "message": "one of mine has an extension that writes a file out with the last successful run time (if no excpetions)", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:24:06.571211+00:00", "nick": "toothrot", "message": "and it loads that on the next run (and uses it to decide what/how to crawl)", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:24:54.279840+00:00", "nick": "DanielW", "message": "i find it even hard to figure out if a run was sucessful (there will also be no exception when there were way less items than expected)", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:25:21.863842+00:00", "nick": "toothrot", "message": "well, you can check that for yourself and then throw your own exception", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:25:44.909588+00:00", "nick": "toothrot", "message": "anyways... what's happening when you try to handle an exception in spider.close() ? by spider.close do you mean a signal handler?", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:26:26.197184+00:00", "nick": "toothrot", "message": "or did you mean http://doc.scrapy.org/en/latest/topics/spiders....", "links": ["http://doc.scrapy.org/en/latest/topics/spiders.html#scrapy.spider.Spider.closed"], "channel": "scrapy"},
{"date": "2014-10-07T00:26:35.344545+00:00", "nick": "DanielW", "message": "yeah spider.closed", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:26:35.741935+00:00", "nick": "toothrot", "message": "well, that's the same thing i guess", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:27:21.071551+00:00", "nick": "DanielW", "message": "when for some reason my code raises an exception in spider.closed()  it stops my script", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:27:54.576098+00:00", "nick": "DanielW", "message": "(no signal gets called, and the twisted reactor is then stuck waiting)", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:28:35.726717+00:00", "nick": "toothrot", "message": "why are you raising an exception there?", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:29:15.976582+00:00", "nick": "DanielW", "message": "i raised it to indicate that the spider run was not success full", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:29:41.575915+00:00", "nick": "DanielW", "message": "in hope to somehow get/handle this exception in my calling script", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:31:43.531679+00:00", "nick": "DanielW", "message": "maybe your suggestion to call each spider in a different process and write the needed data into a file for the caller to read and decide is the best way", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:32:26.819925+00:00", "nick": "toothrot", "message": "where are you trying to catch it? probably hard to say much without seeing your main spider-running script", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:33:08.338413+00:00", "nick": "DanielW", "message": "i just don't understand enough about twisted to really understand what is happening inside of scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:34:03.816597+00:00", "nick": "DanielW", "message": "toothrot: no where.    i connected the three signals spider_closed, engine_stopped and spider_error", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:34:32.200416+00:00", "nick": "DanielW", "message": "and expected that one of the three gets called even when an excepion was raised in Spider.closed()", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:35:22.828943+00:00", "nick": "DanielW", "message": "but it looks like that is not the case.  so i have no chance to either stop the twisted reactor or well do anything at all. it just stops there because i don't get back control", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:35:34.834218+00:00", "nick": "toothrot", "message": "DanielW, why don't you use some attribute of the spider that you can check after the run", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:36:29.638524+00:00", "nick": "toothrot", "message": "eg `if somespider.success: startthenextone()`", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:36:51.969929+00:00", "nick": "toothrot", "message": "that way you can let the spider close down cleanly", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:37:12.388651+00:00", "nick": "toothrot", "message": "also, you can't restart the reactor", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:37:27.644319+00:00", "nick": "DanielW", "message": "yes i could do that (well it is still quite complicated to do something after the last spider closed, because i have to do it in one of the 3 callbacks)", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:38:21.880304+00:00", "nick": "toothrot", "message": "why do you have to do it in one of the 3 callbacks?", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:38:22.957340+00:00", "nick": "DanielW", "message": "but i don't really like the idea that everything will stop if for some reason either spider.closed()  or start_requests()  raises an exception (could be other code there (reading from database) which raises an exception)", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:38:49.691583+00:00", "nick": "DanielW", "message": "toothrot: that is the only way to get control back into my code", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:39:19.731268+00:00", "nick": "DanielW", "message": "toothrot: http://doc.scrapy.org/en/latest/topics/practice...  see there", "links": ["http://doc.scrapy.org/en/latest/topics/practices.html"], "channel": "scrapy"},
{"date": "2014-10-07T00:39:26.667295+00:00", "nick": "DanielW", "message": "the first source example", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:39:38.232038+00:00", "nick": "DanielW", "message": "after reactor.run() the code blocks", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:41:49.055186+00:00", "nick": "toothrot", "message": "right, i'm familiar with twisted", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:44:25.128240+00:00", "nick": "DanielW", "message": "i watched a introduction video about twisted and read a few pages of some tutorials a few hours ago . thats all i know about twisted", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:44:30.856619+00:00", "nick": "toothrot", "message": "looks like you can use CrawlerProcess.join() to get a deferred that will fire when the spiders are all done", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:46:15.129431+00:00", "nick": "DanielW", "message": "in crawler.py?  in my version there is no join() in CrawlerProcess", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:46:33.521763+00:00", "nick": "toothrot", "message": "it's parent class defines it", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:46:39.345049+00:00", "nick": "toothrot", "message": "its", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:46:57.283664+00:00", "nick": "toothrot", "message": "but anyhow, i'm going to try and test this exception in spider_closed", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:47:55.138202+00:00", "nick": "DanielW", "message": "maybe i have a different version: class CrawlerProcess(object):   so \"no\" parent class", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:48:22.230676+00:00", "nick": "toothrot", "message": "what version are you on?", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:48:53.446089+00:00", "nick": "DanielW", "message": "0.24.2 and python 2.7", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:50:14.283260+00:00", "nick": "toothrot", "message": "yeah, it was recently added, my mistake", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:50:38.632273+00:00", "nick": "toothrot", "message": "anyhow, let me test the exception thing here", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:52:30.406307+00:00", "nick": "DanielW", "message": "ok", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T00:55:12.477831+00:00", "nick": "DanielW", "message": "i really like scrapy so far. but support for scripting of scrapy runs is one area where there is room for improvement.  or is it just me, how find it hard to find a best way to do it?", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T01:03:47.240661+00:00", "nick": "toothrot", "message": "just so i", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T01:05:33.071583+00:00", "nick": "toothrot", "message": "am clear, you want to raise an exception in spider_closed and catch that from your spider-running script?", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T01:06:15.644256+00:00", "nick": "toothrot", "message": "you can't do it, because you're not calling that code yourself, and scrapy doesn't expose the deferreds/etc so you can attach an errBack", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T01:07:29.150755+00:00", "nick": "toothrot", "message": "why don't you just wrap up the code in question in a try/except, and do a callLater/deferLater to call into your spiderFailedHandler", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T01:11:59.651485+00:00", "nick": "DanielW", "message": "ok thanks, that right, that's what i want to do", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T01:12:49.595978+00:00", "nick": "DanielW", "message": "ok callLater and deferLater  I don't know how to use them. but i will look them up", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T01:13:04.311334+00:00", "nick": "DanielW", "message": "what do it need to call those methods on?", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T01:18:13.936832+00:00", "nick": "toothrot", "message": "https://twistedmatrix.com/documents/current/cor...", "links": ["https://twistedmatrix.com/documents/current/core/howto/time.html"], "channel": "scrapy"},
{"date": "2014-10-07T01:21:29.465549+00:00", "nick": "DanielW", "message": "thank you,  i will look into it tomorrow (it's 3:20 am here)", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T01:22:57.925362+00:00", "nick": "toothrot", "message": "doesn't the spider_closed signal get called for any spider in the crawler closing?", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T01:27:59.542325+00:00", "nick": "toothrot", "message": "personally I'd use the standard scrapy command line bits and find a way to communicate between the processes (a file, database, network connection, etc).", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T01:28:52.143777+00:00", "nick": "toothrot", "message": "like i said, i did the run from a script thing... and then upgrading scrapy broke it and i decided i was working against the tool's design", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T01:33:08.760369+00:00", "nick": "DanielW", "message": "from what i understand it spider_closed gets called from close_spider() of the ExecutionEngine", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T01:33:26.511984+00:00", "nick": "DanielW", "message": "but with self.signals.send_catch_log_deferred", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T01:35:57.555415+00:00", "nick": "DanielW", "message": "but anyway i will think about it tomorrow and maybe if will go your way  and just use the \"scrapy crawl\" shell and start my spider with that and figure out the best way to share the results with my controlling script", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T01:36:09.147454+00:00", "nick": "DanielW", "message": "I not if", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T01:37:40.764768+00:00", "nick": "DanielW", "message": "i thought about (even before i was working on the scripting) having spider run protocol in a database", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T01:42:27.214750+00:00", "nick": "DanielW", "message": "thank you :)", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T01:42:31.225078+00:00", "nick": "DanielW", "message": "and good night", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T01:59:30.258348+00:00", "nick": "MrMethodMan", "message": "Can someone please help me?", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T02:01:45.218905+00:00", "nick": "MrMethodMan", "message": "Hello?", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T02:02:06.110589+00:00", "nick": "MrMethodMan", "message": "Is it easy to scrape data from a website?", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T04:16:14.492804+00:00", "nick": "Guest56566", "message": "Hello?", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T04:16:52.999792+00:00", "nick": "Guest56566", "message": "Does anyone know an easy way to scrape/harvest data from a phone directory such as yellowpages or whitepages?", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T07:59:08.792731+00:00", "nick": "Newbie0086", "message": "any one can give me a demo about scrapy-redis's distributed crawler", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T07:59:25.726809+00:00", "nick": "Newbie0086", "message": "help", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T07:59:28.353115+00:00", "nick": "Newbie0086", "message": "me", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T07:59:31.856570+00:00", "nick": "Newbie0086", "message": "please", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T09:30:14.121800+00:00", "nick": "Newbie0086", "message": "fuck", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T11:29:31.109758+00:00", "nick": "ariba", "message": "Hi! Every time I run my scraper, I am getting my items.json from the JsonWriterPipeline as one json object per line. I want to merge that file with data.json addign only new items, is there a proper way to do it with Scrapy?", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T11:35:16.870362+00:00", "nick": "nyov", "message": "ariba: load data.json in your pipeline, potentially only the unique key of each item and use those to verify your new items against. discard if a matching key was found, else write out to items.json", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T11:41:41.689772+00:00", "nick": "ariba", "message": "nyov: ok, I see. I am giving it a try", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T12:30:16.935905+00:00", "nick": "ariba", "message": "nyov: it worked thanks, I ended up creating a new pipeline DuplicatesPipeline to drop items already in items.json, and it runs before JsonWriterPipeline", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T14:39:02.651569+00:00", "nick": "splt3r", "message": "hello", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T14:39:53.360236+00:00", "nick": "splt3r", "message": "What is the best way to run spiders from a for loop?", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T14:43:05.131755+00:00", "nick": "splt3r", "message": "and when all spiders are finished to close the reactor?", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T14:44:49.533664+00:00", "nick": "splt3r", "message": "I've tried using the last spider from the loop to send the close signal to the reactor", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T14:49:35.395098+00:00", "nick": "Zladivliba", "message": "hello !", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T14:49:59.256757+00:00", "nick": "Zladivliba", "message": "I'm having a bug when processing items, and it's related to the fact that the website has only one page", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T14:50:13.314284+00:00", "nick": "Zladivliba", "message": "is there somehting sent in the item thing when there's only one page ?", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T15:09:58.207348+00:00", "nick": "oystein", "message": "I am running multiple spiders in the same process, how do I stop the reactor when all the spiders have finished?", "links": [], "channel": "scrapy"},
{"date": "2014-10-07T23:19:38.817758+00:00", "nick": "Crispies", "message": "Im trying to scrape video links from a site but on the site its just /video####### but it is a link in the code. I have (//a/@href) which pulls the /video####### but is there a way to make it pull the full url from that like you would copying from link location?", "links": [], "channel": "scrapy"},
{"date": "2014-10-08T11:33:49.149522+00:00", "nick": "cyreX11", "message": "Hello!  New to Scrapy and am trying to set up a crawler to return home page only URLs of unknown websites matching specific keyword(s).  Can anyone point me to a good tutorial or provide assistance?  thx!", "links": [], "channel": "scrapy"},
{"date": "2014-10-08T13:23:29.957514+00:00", "nick": "jojo1414", "message": "Do people typically use scrapy to create json/xml files and then use other programs to do post processing or do you typically build extensions", "links": [], "channel": "scrapy"},
{"date": "2014-10-09T08:58:27.232403+00:00", "nick": "blusteal", "message": "nyov I was looking at the github sample to run multiple spiders and saw that you wrote a large chunk of it. If I create a spider that I'd like to run, it relies on custom middlewares and stuff like that, how can i integrate that into one of those multiple spider/ same process type projects", "links": [], "channel": "scrapy"},
{"date": "2014-10-09T08:59:41.701039+00:00", "nick": "blusteal", "message": "do I just put the project with the spider inside a folder in the new project?", "links": [], "channel": "scrapy"},
{"date": "2014-10-09T09:02:37.960276+00:00", "nick": "nyov", "message": "blusteal: I'm not sure I understand the question right", "links": [], "channel": "scrapy"},
{"date": "2014-10-09T09:03:21.514085+00:00", "nick": "blusteal", "message": "i created a scrapy project and made a spider that works well, i'd like to run multiple of that spider with different domains", "links": [], "channel": "scrapy"},
{"date": "2014-10-09T09:03:48.489215+00:00", "nick": "blusteal", "message": "I was looking at : http://doc.scrapy.org/en/latest/topics/practice... and : https://github.com/scrapinghub/testspiders", "links": ["http://doc.scrapy.org/en/latest/topics/practices.html", "https://github.com/scrapinghub/testspiders"], "channel": "scrapy"},
{"date": "2014-10-09T09:04:17.416285+00:00", "nick": "blusteal", "message": "it looks like you created a new scrapy project and just dropped your spiders into the testspiders folder", "links": [], "channel": "scrapy"},
{"date": "2014-10-09T09:05:02.160276+00:00", "nick": "blusteal", "message": "the thing is my spider uses some outside classes; those outside py modules are in the spider project folder structure though", "links": [], "channel": "scrapy"},
{"date": "2014-10-09T09:05:54.206753+00:00", "nick": "blusteal", "message": "i'd like to be able to use this one spider that i created and run multiple instances of it", "links": [], "channel": "scrapy"},
{"date": "2014-10-09T09:06:08.846443+00:00", "nick": "nyov", "message": "well to have multiple spiders per project, just add them into spiders/ or whatever your SPIDER_MODULES in settings.py says", "links": [], "channel": "scrapy"},
{"date": "2014-10-09T09:06:42.660076+00:00", "nick": "nyov", "message": "to run multiple instances of the same spider, just start it multiple times I guess", "links": [], "channel": "scrapy"},
{"date": "2014-10-09T09:08:17.378137+00:00", "nick": "nyov", "message": "I think I don't understand the question really", "links": [], "channel": "scrapy"},
{"date": "2014-10-09T09:10:00.890250+00:00", "nick": "blusteal", "message": "this is the code that will run a spider : http://pastebin.com/PT7XCeN9", "links": ["http://pastebin.com/PT7XCeN9"], "channel": "scrapy"},
{"date": "2014-10-09T09:10:22.812515+00:00", "nick": "blusteal", "message": "now typically we just go into the main folder and call scrapy crawl [spidername]", "links": [], "channel": "scrapy"},
{"date": "2014-10-09T09:10:28.098473+00:00", "nick": "nyov", "message": "aaah", "links": [], "channel": "scrapy"},
{"date": "2014-10-09T09:10:43.431258+00:00", "nick": "blusteal", "message": "how do i put my already created spider project into this structure", "links": [], "channel": "scrapy"},
{"date": "2014-10-09T09:14:58.098809+00:00", "nick": "nyov", "message": "heh, I guess that depends", "links": [], "channel": "scrapy"},
{"date": "2014-10-09T09:15:26.388015+00:00", "nick": "blusteal", "message": "is that project that calls the spider a different spider project", "links": [], "channel": "scrapy"},
{"date": "2014-10-09T09:15:44.784956+00:00", "nick": "blusteal", "message": "or is it just a class inside the overall project that does this init/ calling stuff?", "links": [], "channel": "scrapy"},
{"date": "2014-10-09T09:17:37.080523+00:00", "nick": "nyov", "message": "I don't follow :( which project, if you just run this as a script?", "links": [], "channel": "scrapy"},
{"date": "2014-10-09T09:22:52.344781+00:00", "nick": "blusteal", "message": "can you take a look really quick, i msg'd you", "links": [], "channel": "scrapy"},
{"date": "2014-10-09T09:23:26.836070+00:00", "nick": "nyov", "message": "oh right", "links": [], "channel": "scrapy"},
{"date": "2014-10-09T11:11:58.010533+00:00", "nick": "nikolaosk", "message": "the extension manager, unlike the middleware manager, does not connect the open/close signals to the accordingly named methods in the extension", "links": [], "channel": "scrapy"},
{"date": "2014-10-09T11:12:02.728627+00:00", "nick": "nikolaosk", "message": "is this desirable?", "links": [], "channel": "scrapy"},
{"date": "2014-10-10T16:21:54.026681+00:00", "nick": "gadams", "message": "Hi! I want to use scrapy to find all subdomains from a primaru url. Has someone already written a tool like this?", "links": [], "channel": "scrapy"},
{"date": "2014-10-10T16:22:05.134492+00:00", "nick": "gadams", "message": "s/primaru/primary", "links": [], "channel": "scrapy"},
{"date": "2014-10-10T20:05:28.827518+00:00", "nick": "Lingo___", "message": "gadams are there links that lead to it on the site you're scraping?", "links": [], "channel": "scrapy"},
{"date": "2014-10-10T20:33:10.901573+00:00", "nick": "asd__", "message": "do anyone have idea how to scrape some information from multipe 100+ sites", "links": [], "channel": "scrapy"},
{"date": "2014-10-10T20:33:33.609004+00:00", "nick": "asd__", "message": "but not writing each spider for each site", "links": [], "channel": "scrapy"},
{"date": "2014-10-10T20:57:02.518935+00:00", "nick": "gadams", "message": ":( Lingo___ is gone", "links": [], "channel": "scrapy"},
{"date": "2014-10-12T17:26:23.444162+00:00", "nick": "Noteworthy", "message": "Hello, any example which show how to collect emails using scrapy ?", "links": [], "channel": "scrapy"},
{"date": "2014-10-13T18:52:22.638481+00:00", "nick": "blusteal", "message": "guys i'm getting this error [ScrapyDeprecationWarning: Module `scrapy.conf` is deprecated, use `crawler.settings` attribute instead", "links": [], "channel": "scrapy"},
{"date": "2014-10-13T18:52:23.020773+00:00", "nick": "blusteal", "message": "from scrapy.conf import settings] because I'm doing request.meta['some stuff'] to add some additional data to my request object, how can I fix that?", "links": [], "channel": "scrapy"},
{"date": "2014-10-13T18:55:46.708911+00:00", "nick": "nramirezuy", "message": "use crawler.settings", "links": [], "channel": "scrapy"},
{"date": "2014-10-13T18:56:28.219228+00:00", "nick": "blusteal", "message": "im not quite sure how to do that nramirezuy", "links": [], "channel": "scrapy"},
{"date": "2014-10-13T18:56:45.275444+00:00", "nick": "nramirezuy", "message": "if is in the spider you can do self.crawler.settings", "links": [], "channel": "scrapy"},
{"date": "2014-10-13T18:59:06.649607+00:00", "nick": "blusteal", "message": "so I have to do self.crawler.settings['addiitonal meta'] = some data?", "links": [], "channel": "scrapy"},
{"date": "2014-10-13T18:59:19.952302+00:00", "nick": "nramirezuy", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2014-10-13T18:59:48.263376+00:00", "nick": "nramirezuy", "message": "http://doc.scrapy.org/en/master/topics/api.html...", "links": ["http://doc.scrapy.org/en/master/topics/api.html#module-scrapy.settings"], "channel": "scrapy"},
{"date": "2014-10-13T19:01:10.654910+00:00", "nick": "blusteal", "message": "thanks, for the link", "links": [], "channel": "scrapy"},
{"date": "2014-10-13T20:11:41.262513+00:00", "nick": "stav", "message": "Any spider authors here tired of manually routing requests from method to method preserving loader data so that it can be yielded after the last request is satisfied?", "links": [], "channel": "scrapy"},
{"date": "2014-10-13T20:12:31.162583+00:00", "nick": "stav", "message": "I just created a RouteSpider that does this stuff behind the scenes, and I am trying to gauge the interest.", "links": [], "channel": "scrapy"},
{"date": "2014-10-13T20:23:14.269332+00:00", "nick": "blusteal", "message": "one more quick question, using the internal commands when you run scrapy from command line [not scrapy crawl <spidername>] I was just testing for a bit and noticed running the program from api I get terminal output but it doesn't actually print the resulting items. I would typically do scrapy crawl <spidername> -o t.csv -t csv just to look at the file after a test run, how can I do that using the method described here: http://doc.scrap", "links": ["http://doc.scrap"], "channel": "scrapy"},
{"date": "2014-10-13T20:23:15.039569+00:00", "nick": "blusteal", "message": "y.org/en/latest/topics/practices.html#run-from-script", "links": [], "channel": "scrapy"},
{"date": "2014-10-13T20:35:53.972329+00:00", "nick": "stav", "message": "@blusteal: The item is rendered in the log as DEBUG level, what level are you using in your settings from API?", "links": [], "channel": "scrapy"},
{"date": "2014-10-13T20:39:54.945239+00:00", "nick": "blusteal", "message": "stav here's the pastebin: http://pastebin.com/6EA4FerF running the code from commandline gives the dump at the end but no actual items running the program scrapy crawl tspider i see everything", "links": ["http://pastebin.com/6EA4FerF"], "channel": "scrapy"},
{"date": "2014-10-13T20:42:12.635256+00:00", "nick": "stav", "message": "\" the dump at the end\"? ...I don't see any dump at the end, just the code, the last line is `reactor.run()`.", "links": [], "channel": "scrapy"},
{"date": "2014-10-13T20:43:12.571890+00:00", "nick": "stav", "message": "I notice this... \u00a0 \u00a0 crawler.settings.set('LOG_LEVEL', 'INFO')", "links": [], "channel": "scrapy"},
{"date": "2014-10-13T20:43:31.617472+00:00", "nick": "blusteal", "message": "http://pastebin.com/n4hm5Jhs", "links": ["http://pastebin.com/n4hm5Jhs"], "channel": "scrapy"},
{"date": "2014-10-13T20:43:33.632532+00:00", "nick": "stav", "message": "you need to set the log_level to DEBUG to see the items.", "links": [], "channel": "scrapy"},
{"date": "2014-10-13T20:44:54.884525+00:00", "nick": "blusteal", "message": "I tried that as well", "links": [], "channel": "scrapy"},
{"date": "2014-10-13T21:52:36.761854+00:00", "nick": "stav", "message": "@blusteal: I don't think you can change the log_level in Scrapy 0.24... can you try it with the current version from master: 0.25", "links": [], "channel": "scrapy"},
{"date": "2014-10-14T01:03:10.882441+00:00", "nick": "ggv", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2014-10-14T02:39:04.505659+00:00", "nick": "blusteal", "message": "i'm using pip on a mac, but it seems only scrapy 0.24 is available, how do i upgrade?", "links": [], "channel": "scrapy"},
{"date": "2014-10-14T03:02:26.495709+00:00", "nick": "agedman", "message": ">_<", "links": [], "channel": "scrapy"},
{"date": "2014-10-14T03:42:16.942984+00:00", "nick": "ggv", "message": "rcit", "links": [], "channel": "scrapy"},
{"date": "2014-10-14T15:00:39.833401+00:00", "nick": "petapetapeta", "message": "Hello, is there a way of removing items from the request queue? I need it because I want to limit the amount of scraped responses from a given site to 1000", "links": [], "channel": "scrapy"},
{"date": "2014-10-14T15:01:37.210943+00:00", "nick": "petapetapeta", "message": "Or do I write a downloader middleware", "links": [], "channel": "scrapy"},
{"date": "2014-10-14T15:42:53.441694+00:00", "nick": "mushroomed", "message": "Hi petapetapeta It looks like Close spider extension (http://doc.scrapy.org/en/latest/topics/extensio...) could help you with its CLOSESPIDER_ITEMCOUNT (http://doc.scrapy.org/en/latest/topics/extensio...)", "links": ["http://doc.scrapy.org/en/latest/topics/extensions.html#module-scrapy.contrib.closespider", "http://doc.scrapy.org/en/latest/topics/extensions.html#std:setting-CLOSESPIDER_ITEMCOUNT"], "channel": "scrapy"},
{"date": "2014-10-14T20:35:54.316467+00:00", "nick": "seni", "message": "Hi!", "links": [], "channel": "scrapy"},
{"date": "2014-10-14T20:36:42.363546+00:00", "nick": "seni", "message": "I want a stats exporter extension to database.", "links": [], "channel": "scrapy"},
{"date": "2014-10-14T20:37:00.937426+00:00", "nick": "seni", "message": "There was one at : http://mahmoud.abdel-fattah.net/2012/07/23/scra... but it is not working for me now.", "links": ["http://mahmoud.abdel-fattah.net/2012/07/23/scrapy-extension-to-store-spider-statistics-to-postgesql-db/"], "channel": "scrapy"},
{"date": "2014-10-14T20:37:14.327579+00:00", "nick": "seni", "message": "Any help will be very appreciated", "links": [], "channel": "scrapy"},
{"date": "2014-10-14T21:45:42.291137+00:00", "nick": "tightflks_", "message": "Hi!", "links": [], "channel": "scrapy"},
{"date": "2014-10-14T21:45:47.878907+00:00", "nick": "tightflks_", "message": "anyone in here to offer some help?", "links": [], "channel": "scrapy"},
{"date": "2014-10-15T06:54:26.884828+00:00", "nick": "AndyRez", "message": "hi all", "links": [], "channel": "scrapy"},
{"date": "2014-10-15T07:13:40.422139+00:00", "nick": "tightflks_", "message": "hey", "links": [], "channel": "scrapy"},
{"date": "2014-10-15T07:17:50.193985+00:00", "nick": "AndyRez", "message": "tightflks_: I am having a hard time trying to get the value 3 in this html document <strong>Chambres : </strong>3<br>", "links": [], "channel": "scrapy"},
{"date": "2014-10-15T07:18:22.245464+00:00", "nick": "AndyRez", "message": "all the xpath's I have tried can't provide the value \"3\"", "links": [], "channel": "scrapy"},
{"date": "2014-10-15T07:18:48.643002+00:00", "nick": "tightflks_", "message": "i am a noob just started today, i have been using beautifulsoup to get the data", "links": [], "channel": "scrapy"},
{"date": "2014-10-15T07:20:23.515110+00:00", "nick": "AndyRez", "message": "ah, ok..", "links": [], "channel": "scrapy"},
{"date": "2014-10-15T08:15:09.427228+00:00", "nick": "seni_", "message": "Hi Andy. let me check the webpage for you.", "links": [], "channel": "scrapy"},
{"date": "2014-10-15T10:47:00.784639+00:00", "nick": "root3d", "message": "hey folks . just a shoutout for the scrapy .", "links": [], "channel": "scrapy"},
{"date": "2014-10-15T12:29:28.063742+00:00", "nick": "telec0mmute", "message": "Can someone help with me SSL issue I started having today? scrapy shell https://w24.fillz.com/exec/api1", "links": ["https://w24.fillz.com/exec/api1"], "channel": "scrapy"},
{"date": "2014-10-15T12:29:45.646310+00:00", "nick": "telec0mmute", "message": "this url used to work from scrapy till yesterday", "links": [], "channel": "scrapy"},
{"date": "2014-10-15T12:30:22.416430+00:00", "nick": "telec0mmute", "message": "but now gives: <twisted.python.failure.Failure <class 'OpenSSL.SSL.Error'>>", "links": [], "channel": "scrapy"},
{"date": "2014-10-15T12:37:16.203038+00:00", "nick": "eric_", "message": "morning everyone", "links": [], "channel": "scrapy"},
{"date": "2014-10-15T12:39:52.539912+00:00", "nick": "telec0mmute", "message": "Hi - can someone help with with an SSL issue with scrapy?", "links": [], "channel": "scrapy"},
{"date": "2014-10-15T12:40:04.025225+00:00", "nick": "telec0mmute", "message": "scrapy shell https://w24.fillz.com/exec/api1", "links": ["https://w24.fillz.com/exec/api1"], "channel": "scrapy"},
{"date": "2014-10-15T12:40:16.987164+00:00", "nick": "telec0mmute", "message": "this url doesn't work with scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-10-15T12:40:25.860351+00:00", "nick": "telec0mmute", "message": "gives: <twisted.python.failure.Failure <class 'OpenSSL.SSL.Error'>>", "links": [], "channel": "scrapy"},
{"date": "2014-10-15T13:12:32.264099+00:00", "nick": "eric_", "message": "I'm having issues deploying a spider with scrapyd", "links": [], "channel": "scrapy"},
{"date": "2014-10-15T13:13:11.903293+00:00", "nick": "eric_", "message": "getting error IO error permission denied.  I read that scrapyd should not be installed with sudo", "links": [], "channel": "scrapy"},
{"date": "2014-10-15T13:13:17.392526+00:00", "nick": "eric_", "message": "is this true?", "links": [], "channel": "scrapy"},
{"date": "2014-10-15T13:55:48.970606+00:00", "nick": "telec0mmute", "message": "scrapy shell https://w24.fillz.com/exec/api1", "links": ["https://w24.fillz.com/exec/api1"], "channel": "scrapy"},
{"date": "2014-10-15T13:56:05.670737+00:00", "nick": "telec0mmute", "message": "this gives: <twisted.python.failure.Failure <class 'OpenSSL.SSL.Error'>>", "links": [], "channel": "scrapy"},
{"date": "2014-10-15T13:56:23.912925+00:00", "nick": "telec0mmute", "message": "but works with curl", "links": [], "channel": "scrapy"},
{"date": "2014-10-15T21:00:30.583105+00:00", "nick": "spaceturtle", "message": "Does anyone know if it is possible to make a custom spider template? I am not seeing anything in the documentation", "links": [], "channel": "scrapy"},
{"date": "2014-10-15T23:56:23.713892+00:00", "nick": "phpguy306", "message": "anyone point me to tutorials for scraping pdfs? I'm trying to use pdfminer unsuccessfully", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T00:02:16.848072+00:00", "nick": "toothrot", "message": "phpguy306, i use poppler for that", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T00:02:29.693459+00:00", "nick": "toothrot", "message": "although i modified it a bit to return lines/blocks", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T00:02:44.987339+00:00", "nick": "toothrot", "message": "pdfminer is slow, but can probably do it just as well", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T00:03:02.783388+00:00", "nick": "toothrot", "message": "been a long time since i looked at pdfminer (5 years maybe)", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T00:04:39.929666+00:00", "nick": "phpguy306", "message": "checking out popler", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T00:05:33.623551+00:00", "nick": "toothrot", "message": "poppler is a c++ lib, but it has glib bindings, and there's popplerqt4", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T00:06:05.359343+00:00", "nick": "toothrot", "message": "python/glib", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T00:06:25.963587+00:00", "nick": "toothrot", "message": "my application had to display pdfs too though, not just scrape them (not scrapy related at all)", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T00:07:00.750983+00:00", "nick": "toothrot", "message": "phpguy306, how exactly do you need to scrape them? do you need locations/structure of text? or just the text itself?", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T00:09:50.031117+00:00", "nick": "phpguy306", "message": "i need to scrape address information, and phone numbers each being in a different column.", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T12:35:03.467083+00:00", "nick": "evalente", "message": "morning", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T14:12:27.132522+00:00", "nick": "rcosta", "message": "Hello", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T14:46:47.991034+00:00", "nick": "rcosta", "message": "Hi!", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T15:14:02.642957+00:00", "nick": "vipul_", "message": "join #django", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T18:49:00.938496+00:00", "nick": "evalente", "message": "hey all anyone here?", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T18:52:37.212356+00:00", "nick": "jheyrlla", "message": "Hi! I'm using Django and I need to work with Scrapy. Should I make a new project or how should I set it up so it can work with my Django project?", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T18:53:09.689639+00:00", "nick": "jheyrlla", "message": "(I'm fairly new to Django as well as Python in general)", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T18:54:47.760599+00:00", "nick": "jheyrlla", "message": "anybody?", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T19:13:37.038763+00:00", "nick": "jheyrlla", "message": "Guys, I'd need help", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T19:13:38.705689+00:00", "nick": "jheyrlla", "message": "http://dpaste.com/0BCX4P8", "links": ["http://dpaste.com/0BCX4P8"], "channel": "scrapy"},
{"date": "2014-10-16T19:13:51.620313+00:00", "nick": "jheyrlla", "message": "I can't start scrapy inside my virtualenv", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T19:21:47.153761+00:00", "nick": "jheyrlla", "message": "help please?", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T19:36:03.713229+00:00", "nick": "jheyrlla", "message": "can anybody help me please/!?!?!?", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T19:58:40.269184+00:00", "nick": "rcosta__", "message": "jheyrlla, I may be able to help you", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T19:58:49.681817+00:00", "nick": "rcosta__", "message": "You have a problem installing scrapy yes?", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T19:58:56.246688+00:00", "nick": "jheyrlla", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T19:59:03.044740+00:00", "nick": "jheyrlla", "message": "actually i've installed it", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T19:59:04.388229+00:00", "nick": "rcosta__", "message": "I had a problem installing it too.", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T19:59:16.613131+00:00", "nick": "jheyrlla", "message": "and right now i'm stuck at http://dpaste.com/2PJW9MM", "links": ["http://dpaste.com/2PJW9MM"], "channel": "scrapy"},
{"date": "2014-10-16T19:59:54.664152+00:00", "nick": "rcosta__", "message": "Can you run scrapy fine?", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T20:00:06.082926+00:00", "nick": "jheyrlla", "message": "$ scrapy startproject tutorial", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T20:00:07.819883+00:00", "nick": "jheyrlla", "message": "doesn't work", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T20:00:40.860747+00:00", "nick": "rcosta__", "message": "Do you have python2.7?", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T20:01:01.414553+00:00", "nick": "jheyrlla", "message": "yes but I wanna use python3 so inside my virtualenv i've installed py3", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T20:01:09.300563+00:00", "nick": "rcosta__", "message": "humm", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T20:01:30.115124+00:00", "nick": "rcosta__", "message": "Does it work with python2.7? Maybe that is the issue.", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T20:01:42.922591+00:00", "nick": "jheyrlla", "message": "haven't tried", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T20:02:12.377703+00:00", "nick": "jheyrlla", "message": "want me to try it?", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T20:02:15.660934+00:00", "nick": "rcosta__", "message": "If it works with 2.7 and does not with 3 that may be the problem. Some dists have the package python27", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T20:02:36.450744+00:00", "nick": "jheyrlla", "message": "i'm sure it should work", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T20:02:40.266131+00:00", "nick": "jheyrlla", "message": "i'm using django with py3", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T20:02:45.489868+00:00", "nick": "jheyrlla", "message": "inside the same virtualenv", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T20:02:49.881227+00:00", "nick": "rcosta__", "message": "My time is up, sorry, have to catch the bus home.", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T20:03:03.015649+00:00", "nick": "jheyrlla", "message": "alright, have a nice day", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T20:03:08.146967+00:00", "nick": "rcosta__", "message": "Gonna copy and paste the setup that worked for me.", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T20:03:24.064561+00:00", "nick": "jheyrlla", "message": "ok where can you send it to me?", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T20:03:24.991487+00:00", "nick": "rcosta__", "message": "yum install python27", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T20:03:25.315621+00:00", "nick": "rcosta__", "message": "yum install gcc", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T20:03:25.315694+00:00", "nick": "rcosta__", "message": "yum install python-devel", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T20:03:25.316608+00:00", "nick": "rcosta__", "message": "yum install python27-devel", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T20:03:25.317570+00:00", "nick": "rcosta__", "message": "yum install libffi-devel", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T20:03:25.647138+00:00", "nick": "rcosta__", "message": "yum install openssl-devel", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T20:03:27.400341+00:00", "nick": "rcosta__", "message": "yum install libxml2-devel", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T20:03:29.455518+00:00", "nick": "rcosta__", "message": "yum install libxslt-devel", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T20:03:31.439203+00:00", "nick": "rcosta__", "message": "wget https://bootstrap.pypa.io/ez_setup.py -O - | python27", "links": ["https://bootstrap.pypa.io/ez_setup.py"], "channel": "scrapy"},
{"date": "2014-10-16T20:03:33.413549+00:00", "nick": "rcosta__", "message": "wget https://bootstrap.pypa.io/get-pip.py", "links": ["https://bootstrap.pypa.io/get-pip.py"], "channel": "scrapy"},
{"date": "2014-10-16T20:03:35.422996+00:00", "nick": "rcosta__", "message": "python27 get-pip.py", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T20:03:37.420467+00:00", "nick": "rcosta__", "message": "pip install Scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T20:03:39.438883+00:00", "nick": "rcosta__", "message": "pip install service_identity", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T20:03:47.835058+00:00", "nick": "rcosta__", "message": "Good luck, bye", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T20:03:58.255192+00:00", "nick": "jheyrlla", "message": "thanks!", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T20:34:32.063181+00:00", "nick": "dhruvagga", "message": "hey anyone out there ?", "links": [], "channel": "scrapy"},
{"date": "2014-10-16T21:18:50.843485+00:00", "nick": "dhruvagga", "message": "Hi , I am new to this org and would like to contribute, it would be great if anyone would let me know where to start . thanks", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T05:47:40.667285+00:00", "nick": "root3d", "message": "hey how does xpath works i type //span .nothings happens ?", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T08:39:22.509301+00:00", "nick": "Newbie0086", "message": "hello", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T08:39:27.845133+00:00", "nick": "Newbie0086", "message": "anyone here", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T08:39:31.522678+00:00", "nick": "Newbie0086", "message": "?", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T11:10:55.434053+00:00", "nick": "AndyRez", "message": "Hi all, can someone please help me with getting the xpath of the image in this url..", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T11:10:58.606552+00:00", "nick": "AndyRez", "message": "http://www.zitounaimmobilier.com/detail/2451/fr", "links": ["http://www.zitounaimmobilier.com/detail/2451/fr"], "channel": "scrapy"},
{"date": "2014-10-17T11:11:20.316509+00:00", "nick": "AndyRez", "message": "I tried response.xpath('//img/@src').extract()", "links": ["mailto:response.xpath('//img/@src').extract()"], "channel": "scrapy"},
{"date": "2014-10-17T11:11:30.406541+00:00", "nick": "AndyRez", "message": "but it doesn't seem to spit out the images i see in the source..", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T12:02:30.394412+00:00", "nick": "toothrot", "message": "AndyRez, looks like its pulled in through AJAX, did you actually inspect the html?", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T12:02:45.380563+00:00", "nick": "toothrot", "message": "the html [your spider receives]?", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T12:03:58.101246+00:00", "nick": "toothrot", "message": "AndyRez_, looks like its pulled in through AJAX, did you actually inspect the html your spider receives?", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T12:04:54.315758+00:00", "nick": "AndyRez_", "message": "Not sure I understand what you mean by inspect in spider recieves", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T12:41:40.965513+00:00", "nick": "jheyrlla", "message": "hello! i'm having troubles with the first scrapy tutorial, where we use it to save data from dmoz: i've called my project scrapy_tut and I get an error when I write: from scrapy_tut.items import DmozItem", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T12:42:49.011139+00:00", "nick": "jheyrlla", "message": "can anybody help me?", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T13:06:35.354330+00:00", "nick": "evalent__", "message": "hey guys", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T13:08:16.377182+00:00", "nick": "evalent__", "message": "where do I put a command in a spider to get run FIRST and only once, and also, where can I execute something when scrapy finishes?", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T13:39:52.694167+00:00", "nick": "rodrigo5244", "message": "hello", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T13:40:16.704148+00:00", "nick": "rodrigo5244", "message": "evalent__, you can add code to __init__.py", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T13:40:50.892459+00:00", "nick": "rodrigo5244", "message": "evalent__, or start_requests()", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T13:41:15.206914+00:00", "nick": "evalent__", "message": "does __init__ only run once?", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T13:41:22.100167+00:00", "nick": "evalent__", "message": "and is __del__ the place to put the FINAL command?", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T13:41:30.042826+00:00", "nick": "rodrigo5244", "message": "You can use a middleware to add code when scrapy finishes. I don't know if there is an easier way.", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T13:42:09.104777+00:00", "nick": "rodrigo5244", "message": "I believe __init__ runs once. I have being using it and so far I didn't have have any problems.", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T13:42:16.553734+00:00", "nick": "evalent__", "message": "is __init__ run for each URL in the start URLS?", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T13:42:46.603609+00:00", "nick": "rodrigo5244", "message": "You are talking about __init__ in the class spider right?", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T13:43:28.993144+00:00", "nick": "rodrigo5244", "message": "I can't see any reason for that __init__ be run twice.", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T13:44:16.034162+00:00", "nick": "rodrigo5244", "message": "jheyrlla, have you added that class to items.py?", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T13:44:42.173621+00:00", "nick": "jheyrlla", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T13:45:01.818367+00:00", "nick": "jheyrlla", "message": "it's running fine even if pycharm says there's an error there, so", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T13:48:16.472001+00:00", "nick": "rodrigo5244", "message": "jheyrlla, are you the one from yesterday with problems running the tutorial?", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T13:48:32.347647+00:00", "nick": "jheyrlla", "message": "i am but i've fixed the ones i had yesterday", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T13:51:34.841265+00:00", "nick": "rodrigo5244", "message": "Was it the python version or something else?", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T13:51:41.568136+00:00", "nick": "jheyrlla", "message": "yeah", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T13:51:46.580822+00:00", "nick": "jheyrlla", "message": "it took 3 hours to figure out", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T13:52:17.609880+00:00", "nick": "rodrigo5244", "message": "I am sorry to hear that.", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T13:54:09.118225+00:00", "nick": "jheyrlla", "message": "can i ask you a question while i'm here? i should google it a bit but", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T13:54:27.388028+00:00", "nick": "Noteworthy", "message": "Hello everybody", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T13:54:34.431305+00:00", "nick": "rodrigo5244", "message": "jheyrlla go ahead.", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T13:54:39.436066+00:00", "nick": "rodrigo5244", "message": "Hello Noteworthy", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T13:54:53.561807+00:00", "nick": "Noteworthy", "message": "sel.select('/html/body/div[1]/div[@id=\"...:", "links": ["mailto:sel.select('/html/body/div[1]/div[@id=\"main\"]/div/div/section[@id=\"result\"]/article/ul[1]/li[1]/header/h3/a/@href').extract()"], "channel": "scrapy"},
{"date": "2014-10-17T13:55:24.336825+00:00", "nick": "Noteworthy", "message": "How can I do it with re ?", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T13:55:35.137751+00:00", "nick": "Noteworthy", "message": "Thanks.", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T13:56:15.756044+00:00", "nick": "Noteworthy", "message": "-> /ul[1]/li[1], I want to replace '1' with any digit", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T13:57:05.135891+00:00", "nick": "jheyrlla", "message": "rodrigo5244: i want to build a website that uses scrapy: basically i set the url to, let's say, www.example.com, and i want two inputs from the user, so it'll be example.com/input1/blahblah/input2 . then i want scrapy to run my spider", "links": ["http://www.example.com", "http://example.com/input1/blahblah/input2"], "channel": "scrapy"},
{"date": "2014-10-17T13:58:32.479902+00:00", "nick": "jheyrlla", "message": "(using django)", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T13:58:34.562886+00:00", "nick": "jheyrlla", "message": "is it possible?", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T13:58:39.276945+00:00", "nick": "rodrigo5244", "message": "Maybe there is something in the cmd interface where you can pass stuff to your spider, but I am not sure.", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T13:58:43.974993+00:00", "nick": "rodrigo5244", "message": "It should be.", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T14:01:01.350338+00:00", "nick": "Noteworthy", "message": "rodrigo5244, any solution for my problem ?", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T14:01:05.099441+00:00", "nick": "rodrigo5244", "message": "I have never used django before, I am kind of new to Python to tell the truth, being working with it for 2 months now.", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T14:01:25.075570+00:00", "nick": "rodrigo5244", "message": "Noteworthy you can break your select into pieces.", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T14:01:58.180021+00:00", "nick": "rodrigo5244", "message": "when you reach the article you can perform a regular expression select, then go back to using xpath.", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T14:02:16.692359+00:00", "nick": "rodrigo5244", "message": "the return of a select has the selection methods.", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T14:02:48.988475+00:00", "nick": "jheyrlla", "message": "rodrigo5244: yes there is a cmd command to do that, but i'm not sure how to run it actually via django / web page", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T14:03:40.006081+00:00", "nick": "rodrigo5244", "message": "<Noteworthy>, check http://doc.scrapy.org/en/latest/topics/selector...", "links": ["http://doc.scrapy.org/en/latest/topics/selectors.html#nesting-selectors"], "channel": "scrapy"},
{"date": "2014-10-17T14:05:01.143378+00:00", "nick": "rodrigo5244", "message": "<jheyrlla>, django should have a place where you get a POST from the user. There you can run a different process.", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T14:05:18.454782+00:00", "nick": "rodrigo5244", "message": "I am sorry, but I don't know more about it.", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T14:05:27.997360+00:00", "nick": "jheyrlla", "message": "ok thanks ;)", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T14:05:35.702707+00:00", "nick": "Noteworthy", "message": "Thank you so much rodrigo5244", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T14:06:03.740563+00:00", "nick": "rodrigo5244", "message": "Noteworthy, you are welcome.", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T14:08:08.252142+00:00", "nick": "jheyrlla", "message": "do you know how i can see if the url response == 404?", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T14:08:13.791359+00:00", "nick": "jheyrlla", "message": "http://stackoverflow.com/questions/15865611/che... doesn't work", "links": ["http://stackoverflow.com/questions/15865611/checking-a-url-for-a-404-error-scrapy"], "channel": "scrapy"},
{"date": "2014-10-17T14:10:31.930172+00:00", "nick": "rodrigo5244", "message": "jheyrlla, there is one middleware removes thoses erros from the queue.", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T14:11:20.295034+00:00", "nick": "jheyrlla", "message": "what does it mean?", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T14:11:51.928261+00:00", "nick": "rodrigo5244", "message": "HttpErrorMiddleware is a middleware that filters those pages I think.", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T14:12:01.183711+00:00", "nick": "rodrigo5244", "message": "you have to disable it in settings.py", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T14:12:56.752572+00:00", "nick": "rodrigo5244", "message": "here is the middleware http://doc.scrapy.org/en/latest/topics/spider-m...", "links": ["http://doc.scrapy.org/en/latest/topics/spider-middleware.html?highlight=httperrormiddleware#module-scrapy.contrib.spidermiddleware.httperror"], "channel": "scrapy"},
{"date": "2014-10-17T14:13:19.487729+00:00", "nick": "rodrigo5244", "message": "here is the doc on middlewares: http://doc.scrapy.org/en/latest/topics/spider-m...", "links": ["http://doc.scrapy.org/en/latest/topics/spider-middleware.html?highlight=httperrormiddleware#module-scrapy.contrib.spidermiddleware.httperror"], "channel": "scrapy"},
{"date": "2014-10-17T14:14:57.609141+00:00", "nick": "jheyrlla", "message": "handle_httpstatus_list = [404]", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T14:15:00.550216+00:00", "nick": "jheyrlla", "message": "doesn't work actually", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T14:15:06.342035+00:00", "nick": "jheyrlla", "message": "wait", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T14:16:24.858492+00:00", "nick": "jheyrlla", "message": "works well :D", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T14:16:27.469423+00:00", "nick": "jheyrlla", "message": "thanks!", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T14:16:43.360847+00:00", "nick": "jheyrlla", "message": "basically with this line:     handle_httpstatus_list = [404]", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T14:17:00.443673+00:00", "nick": "jheyrlla", "message": "it handles 404 AND [200, 300] right?", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T14:20:04.668834+00:00", "nick": "rodrigo5244", "message": "not sure. let me see", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T14:20:46.718211+00:00", "nick": "rodrigo5244", "message": "do handle_httpstatus_list = [404, 200, 300] to be sure.", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T14:22:14.890691+00:00", "nick": "jheyrlla", "message": "i've tried with 404 only and it worked with 200 too", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T14:22:23.675375+00:00", "nick": "jheyrlla", "message": "i've set: if (... 404) else ...", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T14:22:26.696414+00:00", "nick": "jheyrlla", "message": "and both worked", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T14:22:30.026139+00:00", "nick": "rodrigo5244", "message": "That is weird.", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T14:22:50.307385+00:00", "nick": "jheyrlla", "message": ":O", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T14:23:18.346478+00:00", "nick": "jheyrlla", "message": "well, ok, [404, 200, 300] right?", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T14:23:20.249744+00:00", "nick": "jheyrlla", "message": "to be sure", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T14:37:31.046728+00:00", "nick": "rodrigo5244", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T14:44:13.619813+00:00", "nick": "jheyrlla", "message": "do you know how can i check if a specific tag exists?", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T14:44:26.279545+00:00", "nick": "jheyrlla", "message": "for example, <title>", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T14:44:30.804017+00:00", "nick": "jheyrlla", "message": "otherwise it throws an error", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T14:45:06.309027+00:00", "nick": "jheyrlla", "message": "http://stackoverflow.com/questions/24672048/how...", "links": ["http://stackoverflow.com/questions/24672048/how-to-check-if-a-specific-button-exists-in-scrapy"], "channel": "scrapy"},
{"date": "2014-10-17T14:45:13.217975+00:00", "nick": "jheyrlla", "message": "maybe this is the answer, let me try", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T14:51:15.106245+00:00", "nick": "rodrigo5244", "message": "looks like it.", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T14:55:02.333255+00:00", "nick": "jheyrlla", "message": "by the way", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T14:55:04.094836+00:00", "nick": "jheyrlla", "message": "According to the HTTP standard, successful responses are those whose status codes are in the 200-300 range", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T14:55:11.103967+00:00", "nick": "jheyrlla", "message": "200 -> 300, not 200,300", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T14:55:33.239058+00:00", "nick": "jheyrlla", "message": "how can i put every value from 200 to 300 inside handle_httpstatus_list", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T15:05:37.090497+00:00", "nick": "jheyrlla", "message": "has anybody ever used scrapy's stats?", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T15:17:24.600110+00:00", "nick": "evalente_", "message": "can you deploy to multiple targets in one command?  scrapy deploy ALLHOSTS -p project=etc", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T15:22:06.887500+00:00", "nick": "rodrigo5244", "message": "jheyrlla, use range. handle_httpstatus_list = range(200, 300 + 1)", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T15:22:31.091906+00:00", "nick": "jheyrlla", "message": "and 404?", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T15:23:04.444891+00:00", "nick": "rodrigo5244", "message": "jheyrlla, use range. handle_httpstatus_list = range(200, 300 + 1) + [404]", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T15:23:12.439919+00:00", "nick": "jheyrlla", "message": ":D", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T15:23:15.876076+00:00", "nick": "jheyrlla", "message": "thanks!", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T15:23:19.959096+00:00", "nick": "rodrigo5244", "message": "welcome", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T15:29:28.958055+00:00", "nick": "jheyrlla", "message": "how can i call a spider from another spider?", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T15:29:48.652887+00:00", "nick": "jheyrlla", "message": "for example: i get 404, then try another spider with other settings", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T15:31:26.607275+00:00", "nick": "rodrigo5244", "message": "Is this a bug? Can't pickle <class 'scrapy.squeue.SerializableQueue'>: it's not found as scrapy.squeue.SerializableQueue", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T15:32:37.169771+00:00", "nick": "rodrigo5244", "message": "In you spider you could call parse functions of the other spider.", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T15:32:46.843362+00:00", "nick": "rodrigo5244", "message": "and import the the other spider.", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T15:35:40.298501+00:00", "nick": "jheyrlla", "message": "rodrigo5244: could you write an example please?", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T15:36:38.306929+00:00", "nick": "rodrigo5244", "message": "pickle.dumps(request)", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T15:37:01.464843+00:00", "nick": "rodrigo5244", "message": "getting request from def process_exception(self, request, exception, spider):", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T15:37:36.315346+00:00", "nick": "rodrigo5244", "message": "in a middleware downloader", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T16:21:16.412166+00:00", "nick": "jheyrlla", "message": "i didn't understand :(", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T16:21:27.024884+00:00", "nick": "jheyrlla", "message": "rodrigo5244: can you explain it please", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T16:22:15.451662+00:00", "nick": "rodrigo5244", "message": "jheyrlla, Scrapy has API for django I heard, you should look that up.", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T16:22:36.523182+00:00", "nick": "jheyrlla", "message": "i think i've seen them once", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T16:22:40.879476+00:00", "nick": "rodrigo5244", "message": "I want to save the request using pickle to a file, but I am getting errors.", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T16:23:00.355063+00:00", "nick": "rodrigo5244", "message": "I want to save the requests the failed to try again latter, or to see what went wrong.", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T16:37:11.467395+00:00", "nick": "rodrigo5244", "message": "jheyrlla, this might be interesting to you: http://stackoverflow.com/questions/12996910/how...", "links": ["http://stackoverflow.com/questions/12996910/how-to-setup-and-launch-a-scrapy-spider-programmatically-urls-and-settings"], "channel": "scrapy"},
{"date": "2014-10-17T16:38:11.085452+00:00", "nick": "jheyrlla", "message": "yes, this is my next step :) but right now i have to see how to call another spider after i get a 404 response", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T17:11:06.280996+00:00", "nick": "rodrigo5244", "message": "Can you use os.popen(\"scrapy\", ...)?", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T17:11:23.566529+00:00", "nick": "jheyrlla", "message": "I don't know :(", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T17:11:26.487214+00:00", "nick": "jheyrlla", "message": "i'll try later", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T17:11:33.864508+00:00", "nick": "jheyrlla", "message": "are you online in an hour?", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T17:11:46.340285+00:00", "nick": "jheyrlla", "message": "i have to go now, talk to you when i'm back", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T17:20:14.101392+00:00", "nick": "rodrigo5244", "message": "I will be.", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T17:31:06.624310+00:00", "nick": "Zladivliba", "message": "hello everyone !", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T17:31:23.213325+00:00", "nick": "Zladivliba", "message": "I'm having tons of errors filtering pages to scrape", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T17:32:02.677258+00:00", "nick": "Zladivliba", "message": "like https://twitter.com/intent/tweet?via=https-my-s... is beeing scraped", "links": ["https://twitter.com/intent/tweet?via=https-my-scraped-website.com"], "channel": "scrapy"},
{"date": "2014-10-17T17:32:21.339647+00:00", "nick": "Zladivliba", "message": "is there a way for me to filter the urls that are scraped or not inside scrapy ?", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T17:47:26.314572+00:00", "nick": "Werel", "message": "I'm having trouble installing lxml, #lxml doesn't really have anyone around... c.c", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T17:53:05.005592+00:00", "nick": "Werel", "message": "I got pip and setuptoolds installed on my raspberry pi, and I installed the other packages lxml needs, but when I go to sudo pip install lxml , it hangs here https://dl.dropboxusercontent.com/u/35098555/Pi...", "links": ["https://dl.dropboxusercontent.com/u/35098555/Pictures/lmxlhang.png"], "channel": "scrapy"},
{"date": "2014-10-17T17:53:47.271731+00:00", "nick": "Werel", "message": "I'm thinking it's using a gcc command expecting an x86 architecture, but I'm on raspberri pi which is ARM ?", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T17:53:55.500564+00:00", "nick": "Werel", "message": "or does that have nothing to do with it", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T17:53:59.989803+00:00", "nick": "Werel", "message": "does maybe it just take a while anyway? :P", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T18:31:37.450249+00:00", "nick": "Werel", "message": "okay, was just gcc taking forever on the raspberry pi, now I'm installing scrapy :)", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T18:39:49.294391+00:00", "nick": "Werel", "message": "Scrapy failed to install, looks like it was missing a libffi :(", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T19:20:41.131327+00:00", "nick": "rodrigo5244", "message": "<Werel>, I have a list of some dependencies.", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T19:21:09.683932+00:00", "nick": "rodrigo5244", "message": "Here are they:", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T19:21:10.098214+00:00", "nick": "rodrigo5244", "message": "yum install python27", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T19:21:10.408038+00:00", "nick": "rodrigo5244", "message": "yum install gcc", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T19:21:10.408133+00:00", "nick": "rodrigo5244", "message": "yum install python-devel", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T19:21:10.410140+00:00", "nick": "rodrigo5244", "message": "yum install python27-devel", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T19:21:10.730841+00:00", "nick": "rodrigo5244", "message": "yum install libffi-devel", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T19:21:12.569662+00:00", "nick": "rodrigo5244", "message": "yum install openssl-devel", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T19:21:14.657995+00:00", "nick": "rodrigo5244", "message": "yum install libxml2-devel", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T19:21:16.594946+00:00", "nick": "rodrigo5244", "message": "yum install libxslt-devel", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T19:21:19.676551+00:00", "nick": "Werel", "message": "rodrigo5244, sure :)  I'm getting some help over at #python, I'm rebuilding some stuff right now, but that definitely helps", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T19:21:20.613326+00:00", "nick": "rodrigo5244", "message": "wget https://bootstrap.pypa.io/ez_setup.py -O - | python27", "links": ["https://bootstrap.pypa.io/ez_setup.py"], "channel": "scrapy"},
{"date": "2014-10-17T19:21:22.629135+00:00", "nick": "rodrigo5244", "message": "wget https://bootstrap.pypa.io/get-pip.py", "links": ["https://bootstrap.pypa.io/get-pip.py"], "channel": "scrapy"},
{"date": "2014-10-17T19:21:24.596930+00:00", "nick": "rodrigo5244", "message": "python27 get-pip.py", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T19:21:26.697993+00:00", "nick": "rodrigo5244", "message": "pip install Scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T19:21:28.611448+00:00", "nick": "rodrigo5244", "message": "pip install service_identity", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T19:21:37.611954+00:00", "nick": "Werel", "message": "service identity?", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T19:21:50.204972+00:00", "nick": "rodrigo5244", "message": "if you don't install that scrapy gives a warning.", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T19:52:09.195573+00:00", "nick": "jheyrlla", "message": "rodrigo5244: are you there?", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T20:01:03.954265+00:00", "nick": "rodrigo5244", "message": "jheyrlla, sorry, needs to grab my bus again.", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T20:01:15.965343+00:00", "nick": "jheyrlla", "message": "sure :) see you next time", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T20:01:23.832714+00:00", "nick": "rodrigo5244", "message": "see you", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T21:26:37.536118+00:00", "nick": "jheyrlla", "message": "Hi! I need to check two URLs with the same spider. How can I do it?", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T22:24:44.631745+00:00", "nick": "evalente", "message": "anyone ahve trouble with s3 FEED_URI's?", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T22:24:48.427492+00:00", "nick": "evalente", "message": "can't get it to work no matter waht I try", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T22:24:51.761277+00:00", "nick": "evalente", "message": "I know key is good", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T22:24:54.492415+00:00", "nick": "evalente", "message": "and patry", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T22:24:55.670013+00:00", "nick": "evalente", "message": "path", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T22:25:09.889942+00:00", "nick": "evalente", "message": "i have it as FEED_URI='s3://name/path/test.csv'", "links": [], "channel": "scrapy"},
{"date": "2014-10-17T22:25:17.060999+00:00", "nick": "evalente", "message": "and FEED_FORMAT = 'csv'", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T05:15:21.263507+00:00", "nick": "HowardwLo", "message": "do I need to account for CSRF token\u2019s when using form requests?", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T05:15:26.042110+00:00", "nick": "HowardwLo", "message": "*Submitting forms", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T06:39:24.727456+00:00", "nick": "root3d", "message": "in scrapy i am using //a/@onclick/text()  to select onclick attribute . but doesn't work . i am rght ?", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T07:06:01.913195+00:00", "nick": "root3d", "message": "thanx anywasy", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T09:59:23.695665+00:00", "nick": "jheyrlla", "message": "hello", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T10:00:26.589820+00:00", "nick": "jheyrlla", "message": "i need to crawl two urls with the same spider: example.com/folder/ and example.com/folder/fold2 and retrieve two different things for each url", "links": ["http://example.com/folder/", "http://example.com/folder/fold2"], "channel": "scrapy"},
{"date": "2014-10-18T10:00:30.064701+00:00", "nick": "jheyrlla", "message": "how can i do it?", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T10:32:34.257837+00:00", "nick": "jheyrlla", "message": "i need to crawl two urls with the same spider: example.com/folder/ and example.com/folder/fold2 and retrieve two different things for each url. how can i do that?", "links": ["http://example.com/folder/", "http://example.com/folder/fold2"], "channel": "scrapy"},
{"date": "2014-10-18T12:27:59.215621+00:00", "nick": "Zladivliba", "message": "hey guys", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T12:28:04.646851+00:00", "nick": "Zladivliba", "message": "anyone here?", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T12:43:15.504351+00:00", "nick": "Zladivliba", "message": "hello everyone !", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T12:43:20.938904+00:00", "nick": "toothrot", "message": "heh. hello.", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T12:43:48.246154+00:00", "nick": "Zladivliba", "message": "I'm having big problems with requests that are made outside the authorized domains", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T12:43:53.783336+00:00", "nick": "Zladivliba", "message": "any idea how to debug this ?", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T12:44:00.136912+00:00", "nick": "toothrot", "message": "was that a misbehaving bnc/client or just being a funny guy?", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T12:44:10.006257+00:00", "nick": "Zladivliba", "message": "what ?", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T12:45:06.937421+00:00", "nick": "toothrot", "message": "https://bpaste.net/show/21a9e376007f", "links": ["https://bpaste.net/show/21a9e376007f"], "channel": "scrapy"},
{"date": "2014-10-18T12:45:35.826602+00:00", "nick": "Zladivliba", "message": "toothrot: sorry no", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T12:45:50.825709+00:00", "nick": "Zladivliba", "message": "I don't use irc that much and my client is a pain in the ass", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T12:45:51.009940+00:00", "nick": "toothrot", "message": "what? just curious.", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T12:45:55.698918+00:00", "nick": "Zladivliba", "message": "I'm not sure what u meant", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T12:46:03.090181+00:00", "nick": "Zladivliba", "message": "i thought i was connected to the chan", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T12:46:15.166413+00:00", "nick": "toothrot", "message": "okay, no worries. the channel should really be +n", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T12:46:28.160119+00:00", "nick": "Zladivliba", "message": "anyway any idea how to stop scrapy from doing resquests ouside allowed_domains ?", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T12:46:56.332324+00:00", "nick": "toothrot", "message": "what's your spider's parent class?", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T12:47:36.777345+00:00", "nick": "Zladivliba", "message": "CrawlSpider", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T12:48:14.310932+00:00", "nick": "toothrot", "message": "are you overriding the parse method?", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T12:48:47.021640+00:00", "nick": "Zladivliba", "message": "I have a parse_item function", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T12:48:54.109984+00:00", "nick": "Zladivliba", "message": "but that's it", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T12:50:07.740112+00:00", "nick": "toothrot", "message": "can you share your code at this point?", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T12:50:23.364237+00:00", "nick": "Zladivliba", "message": "https://gist.github.com/anonymous/c99c7239eced0...", "links": ["https://gist.github.com/anonymous/c99c7239eced00af760e"], "channel": "scrapy"},
{"date": "2014-10-18T12:50:28.816517+00:00", "nick": "Zladivliba", "message": "toothrot: here's a bit of it", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T12:50:38.912119+00:00", "nick": "Zladivliba", "message": "I can't share everything, unfortunately :(", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T12:51:22.438132+00:00", "nick": "toothrot", "message": "i understand", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T12:51:33.400833+00:00", "nick": "Zladivliba", "message": "yearh my boss would just fire me :(", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T12:51:45.240411+00:00", "nick": "Zladivliba", "message": "any idea what's going on ?", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T12:51:51.292533+00:00", "nick": "toothrot", "message": "although unrelated it looks like some encoding snafu happened: https://gist.githubusercontent.com/anonymous/c9...", "links": ["https://gist.githubusercontent.com/anonymous/c99c7239eced00af760e/raw/44a20a10b30233be26e7128070d33a28ee2fddaf/gistfile1.txt"], "channel": "scrapy"},
{"date": "2014-10-18T12:52:02.366871+00:00", "nick": "toothrot", "message": "eg. name = \u00e2\u20ac\u02dctest\u00e2\u20ac\u2122", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T12:52:18.193894+00:00", "nick": "Zladivliba", "message": "I was thinking maybe I'd check manually if the page was related to the current domain name", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T12:52:48.033985+00:00", "nick": "Zladivliba", "message": "ok toothrot : no that's not the case", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T12:52:53.047955+00:00", "nick": "Zladivliba", "message": "it's just a copy/paste problem", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T12:53:05.030984+00:00", "nick": "toothrot", "message": "right, i know it's not the cause, but it makes it harder for me to look at it.", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T12:53:15.200073+00:00", "nick": "toothrot", "message": "and test it, etc", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T12:53:15.900562+00:00", "nick": "Zladivliba", "message": "name = \u2018test\u2019", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T12:53:15.999956+00:00", "nick": "Zladivliba", "message": "website = \"http://test.com\u00a0\u00bb", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T12:53:17.112871+00:00", "nick": "Zladivliba", "message": "I have", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T12:53:27.007591+00:00", "nick": "Zladivliba", "message": "hummm", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T12:53:43.628971+00:00", "nick": "toothrot", "message": "those don't appear to be actual quotes around 'test'", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T12:54:01.821854+00:00", "nick": "Zladivliba", "message": "ah sorry, again mis-copy/paste", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T12:54:04.947178+00:00", "nick": "toothrot", "message": "but anyways, that's from the docs, so i'll figure it out", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T12:54:56.621233+00:00", "nick": "Zladivliba", "message": "tomwardill: here's one url taht's messed up : 'url': 'https://twitter.com/intent/tweet?via=httpstwittercomRenataLahalle1&text=Renoir%2C+Picasso%2C+Hokusai%2C+...&url=http%3A%2F%2Flesimages2renata.com%2Frenoir-picasso-hokusai%2F'}", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T12:55:28.925798+00:00", "nick": "Zladivliba", "message": "I'm scraping the website lesimages2renata.com", "links": ["http://lesimages2renata.com"], "channel": "scrapy"},
{"date": "2014-10-18T12:56:20.296898+00:00", "nick": "toothrot", "message": "and it shouldn't be following that link ? is that what you're saying ?", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T12:56:26.181378+00:00", "nick": "Zladivliba", "message": "nope", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T12:56:46.701765+00:00", "nick": "Zladivliba", "message": "the rules say : stay within the website lesimages2renata.com", "links": ["http://lesimages2renata.com"], "channel": "scrapy"},
{"date": "2014-10-18T12:57:04.975546+00:00", "nick": "Zladivliba", "message": "and I'm scraping twitter.com/blabla/lesimages2renata.com", "links": ["http://twitter.com/blabla/lesimages2renata.com"], "channel": "scrapy"},
{"date": "2014-10-18T12:57:43.731536+00:00", "nick": "Zladivliba", "message": "toothrot: do you know if there's a way inside parse_item to say \"don't parse this item\" ?", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T12:58:12.188617+00:00", "nick": "Zladivliba", "message": "I could just hack this function and stop parsing the item if it's not the same domain", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T12:59:24.162241+00:00", "nick": "toothrot", "message": "i think the problem is your rule", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T12:59:37.695535+00:00", "nick": "toothrot", "message": "it's matching everything", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T12:59:41.092902+00:00", "nick": "toothrot", "message": "(i think)", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T13:00:05.208670+00:00", "nick": "toothrot", "message": "or i guess that only applies to the url, not the domain?", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T13:00:14.613957+00:00", "nick": "toothrot", "message": "yeah.... my mistake", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T13:00:26.523390+00:00", "nick": "Zladivliba", "message": "hummm I'm not sure because I only scrape the current websites", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T13:00:30.008761+00:00", "nick": "Zladivliba", "message": "I mean most of the time", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T13:00:39.647553+00:00", "nick": "Zladivliba", "message": "somtimes I do scrape one or 2 pages here or there...", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T13:00:44.532213+00:00", "nick": "Zladivliba", "message": "But I don't understand why...", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T13:00:46.684552+00:00", "nick": "toothrot", "message": "no, i'm mistaken", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T13:00:59.571056+00:00", "nick": "toothrot", "message": "the rule has nothing to do with the domain", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T13:01:09.391585+00:00", "nick": "toothrot", "message": "can you tell mme which page specifically you're looking at?", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T13:02:31.005873+00:00", "nick": "Zladivliba", "message": "what do u mean ?", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T13:02:53.871871+00:00", "nick": "toothrot", "message": "i want to try it here, but i'm not sure which page you're scraping (if it can be shared at all)", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T13:03:17.867205+00:00", "nick": "Zladivliba", "message": "oh year try : lesimages2renata.com", "links": ["http://lesimages2renata.com"], "channel": "scrapy"},
{"date": "2014-10-18T13:04:05.641330+00:00", "nick": "toothrot", "message": "okay, one sec", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T13:05:10.755535+00:00", "nick": "Zladivliba", "message": "my rules are definetly fucked up...", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T13:05:40.297954+00:00", "nick": "toothrot", "message": "one more question", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T13:05:44.811618+00:00", "nick": "toothrot", "message": "is scrapy actually crawling these links", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T13:05:45.155274+00:00", "nick": "Zladivliba", "message": "yep", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T13:05:49.968853+00:00", "nick": "Zladivliba", "message": "yep", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T13:05:56.844535+00:00", "nick": "toothrot", "message": "or are they just showing up in your items callback?", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T13:06:03.932269+00:00", "nick": "Zladivliba", "message": "2014-10-18 15:04:13+0200 [legiscope] DEBUG: Scraped from <200 https://twitter.com/intent/tweet?via=httpstwitt...;", "links": ["https://twitter.com/intent/tweet?via=httpstwittercomRenataLahalle1&amp=&text=Renoir%2C+Picasso%2C+Hokusai%2C+...&amp=&url=http%3A%2F%2Flesimages2renata.com%2Frenoir-picasso-hokusai%2F&gt="], "channel": "scrapy"},
{"date": "2014-10-18T13:06:09.060127+00:00", "nick": "toothrot", "message": "okay..", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T13:06:09.774628+00:00", "nick": "Zladivliba", "message": "here's what I got", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T13:06:38.092703+00:00", "nick": "Zladivliba", "message": "2014-10-18 15:04:17+0200 [legiscope] DEBUG: Crawled (200) <GET https://www.tumblr.com/login?s=&t=%231+%3E+...; (referer: http://lesimages2renata.com) ['partial']", "links": ["https://www.tumblr.com/login?s=&amp=&t=%231+%3E+Couple+%26+Art+%3A+Jean+Tinguely++%26+Niki+de+St+Phale&amp=&u=http%3A%2F%2Flesimages2renata.com%2F1-couple-art-jean-tinguely-niki-de-st-phale%2F&amp=&v=3&gt=", "http://lesimages2renata.com"], "channel": "scrapy"},
{"date": "2014-10-18T13:06:41.832783+00:00", "nick": "Zladivliba", "message": "or this too...", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T13:07:32.494199+00:00", "nick": "Zladivliba", "message": "2014-10-18 15:04:13+0200 [legiscope] DEBUG: Scraped from <200 https://accounts.google.com/ServiceLogin?servic...;", "links": ["https://accounts.google.com/ServiceLogin?service=oz&amp=&passive=1209600&amp=&continue=https%3A%2F%2Fplus.google.com%2Fshare%3Furl%3Dhttp%3A%2F%2Flesimages2renata.com%2Frenoir-picasso-hokusai%2F%26gpsrc%3Dframeless&amp=&btmpl=popup&gt="], "channel": "scrapy"},
{"date": "2014-10-18T13:07:37.198593+00:00", "nick": "Zladivliba", "message": "same here...", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T13:12:50.931910+00:00", "nick": "toothrot", "message": "Zladivliba, i might've been on the right track a minute ago", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T13:12:53.910850+00:00", "nick": "toothrot", "message": "look at this: http://doc.scrapy.org/en/latest/topics/link-ext...", "links": ["http://doc.scrapy.org/en/latest/topics/link-extractors.html#scrapy.contrib.linkextractors.sgml.SgmlLinkExtractor"], "channel": "scrapy"},
{"date": "2014-10-18T13:13:11.743843+00:00", "nick": "toothrot", "message": "SgmlLinkExtractor has an allow_domains argument", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T13:14:38.091267+00:00", "nick": "toothrot", "message": "so you can probably do `SgmlLinkExtractor(allow=(), allow_domains=allowed_domains)`", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T13:14:45.681326+00:00", "nick": "Zladivliba", "message": "allowed_domains = [urlparse.urlparse(website).hostname]", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T13:14:46.457946+00:00", "nick": "Zladivliba", "message": "toothrot: yep it's what I'm currently using :", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T13:15:04.642297+00:00", "nick": "toothrot", "message": "i think you are misunderstanding.", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T13:15:12.535858+00:00", "nick": "Zladivliba", "message": "toothrot: ah ok I get it", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T13:15:15.838210+00:00", "nick": "Zladivliba", "message": "I understand", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T13:15:16.593733+00:00", "nick": "toothrot", "message": "in your Rule, SgmlLinkExtractor(allow=(), allow_domains=allowed_domains)", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T13:15:22.566206+00:00", "nick": "Zladivliba", "message": "good idea ! I'll try that !", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T13:15:27.573480+00:00", "nick": "toothrot", "message": "try it out and let me know", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T13:19:42.048246+00:00", "nick": "Zladivliba", "message": "ok still testing...", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T13:22:24.877169+00:00", "nick": "Zladivliba", "message": "toothrot: nope same shit", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T13:22:33.109235+00:00", "nick": "Zladivliba", "message": "still hitting every here and there", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T13:22:44.568153+00:00", "nick": "Zladivliba", "message": "2014-10-18 15:20:40+0200 [legiscope] DEBUG: Scraped from <200 https://twitter.com/intent/tweet?via=httpstwitt...;", "links": ["https://twitter.com/intent/tweet?via=httpstwittercomRenataLahalle1&amp=&text=Renoir%2C+Picasso%2C+Hokusai%2C+...&amp=&url=http%3A%2F%2Flesimages2renata.com%2Frenoir-picasso-hokusai%2F&gt="], "channel": "scrapy"},
{"date": "2014-10-18T13:25:55.329045+00:00", "nick": "Zladivliba", "message": "toothrot: still there's a problem in the fact that it should not hit pages that are limited through allowed_domains", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T13:28:44.231752+00:00", "nick": "Zladivliba", "message": "toothrot: ok check this page : http://lesimages2renata.com/1-couple-art-jean-t...", "links": ["http://lesimages2renata.com/1-couple-art-jean-tinguely-niki-de-st-phale/?share=facebook"], "channel": "scrapy"},
{"date": "2014-10-18T13:29:04.313192+00:00", "nick": "Zladivliba", "message": "I think what's happening is that the robot is following 301 redirections and is mislead to another domain", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T13:32:07.193900+00:00", "nick": "Zladivliba", "message": "ok seems to be working better now !", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T13:34:35.281024+00:00", "nick": "toothrot", "message": "i think redirects are logged", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T13:34:50.974360+00:00", "nick": "Zladivliba", "message": "yearh, it's seems it's really working now", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T13:35:17.575954+00:00", "nick": "Zladivliba", "message": "What happened is that lots of requests were redirected on this website, so this is the reason I got external urls", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T13:35:32.904329+00:00", "nick": "toothrot", "message": "cool, glAD you figured it out", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T13:35:39.549243+00:00", "nick": "Zladivliba", "message": "yearh thanks ;)))", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T13:35:41.030193+00:00", "nick": "toothrot", "message": "(can you tell i don't use CrawlSpider)", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T13:35:50.224241+00:00", "nick": "Zladivliba", "message": "thanks for the help", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T13:36:06.769469+00:00", "nick": "Zladivliba", "message": "ok worked enitrely now !", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T13:36:10.363187+00:00", "nick": "Zladivliba", "message": "good ;)))", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T13:36:14.695062+00:00", "nick": "Zladivliba", "message": "I'm happy ;)))", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T13:47:17.015222+00:00", "nick": "jheyrlla", "message": "how can i call a spider from another spider?", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T14:44:46.069165+00:00", "nick": "fpghost84", "message": "Hi, currently I run multiple spiders in turn from a bash script just invoking \"scrapy crawl Spider1\" etc. What's the best way to run these from a python controller script instead?", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T14:45:13.016648+00:00", "nick": "fpghost84", "message": "(the bash way works fine, I just need python for some other things)", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T14:51:25.159221+00:00", "nick": "jheyrlla", "message": "have you considered subprocesses?", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T15:01:16.296662+00:00", "nick": "fpghost84", "message": "jheyrlla: yeah guess that would be the easy way to do it..", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T15:01:24.726637+00:00", "nick": "jheyrlla", "message": ":)", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T15:01:35.649540+00:00", "nick": "jheyrlla", "message": "do you know how to call a spider from another spider?", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T15:02:17.393822+00:00", "nick": "fpghost84", "message": "I think that might get messy too quickly. I have over 60 spiders, and different things going on in between groups of around 5 spiders etc :s", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T15:03:25.044402+00:00", "nick": "jheyrlla", "message": "are you referring to my question or to subprocess?", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T15:03:55.457581+00:00", "nick": "fpghost84", "message": "question", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T15:04:28.207198+00:00", "nick": "fpghost84", "message": "subprocesses would be like how I do it now with bash, so should be good I guess, just wanted to make sure there wasn't a better way", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T15:05:15.208100+00:00", "nick": "jheyrlla", "message": "i don't know, i've said subprocess because i've looked for that a while ago... but it doesn't work well for me", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T15:06:05.380799+00:00", "nick": "jheyrlla", "message": "exception OS error: no such file or directory", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T15:06:07.132204+00:00", "nick": "jheyrlla", "message": ":(", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T15:07:37.634114+00:00", "nick": "fpghost84", "message": "Strange, I'll have to give it a try", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T15:08:39.797754+00:00", "nick": "jheyrlla", "message": "ok let me know", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T15:08:55.771176+00:00", "nick": "jheyrlla", "message": "so you use many spiders together, how do you do that?", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T15:09:13.491888+00:00", "nick": "jheyrlla", "message": "i just need to call a spider when another spider has finished, and this seems impossible to me right now :(", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T15:24:02.990958+00:00", "nick": "jheyrlla", "message": "fpghost84: are you there?", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T15:28:46.462099+00:00", "nick": "fpghost84", "message": "jheyrlla: hi sorry", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T15:29:27.474431+00:00", "nick": "fpghost84", "message": "like I say at the moment to string many spiders together I have a \"controller\" bash script that just invokes the \"scrapy crawl MySpider\" commands in turn", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T15:30:00.384379+00:00", "nick": "fpghost84", "message": "there are way like this to though http://stackoverflow.com/questions/13437402/how...", "links": ["http://stackoverflow.com/questions/13437402/how-to-run-scrapy-from-within-a-python-script"], "channel": "scrapy"},
{"date": "2014-10-18T15:30:06.497339+00:00", "nick": "fpghost84", "message": "too*", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T15:30:41.962545+00:00", "nick": "fpghost84", "message": "I think that way would probably let you call the other spider when first had finished", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T15:30:49.541299+00:00", "nick": "fpghost84", "message": "although never tried it myself", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T15:31:19.538509+00:00", "nick": "fpghost84", "message": "(see also http://scrapy.readthedocs.org/en/0.16/topics/pr...)", "links": ["http://scrapy.readthedocs.org/en/0.16/topics/practices.html"], "channel": "scrapy"},
{"date": "2014-10-18T15:32:23.053029+00:00", "nick": "jheyrlla", "message": "fpghost84: thanks :) from the link you gave me on stackoverflow, i don't understand how to run THE spider i need", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T15:32:30.208360+00:00", "nick": "jheyrlla", "message": "where do i put its name?", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T15:35:01.460780+00:00", "nick": "fpghost84", "message": "looks like crawler.crawl('spider1')", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T15:35:15.438583+00:00", "nick": "fpghost84", "message": "so replace 'spider1' with your spider's name", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T15:44:46.204725+00:00", "nick": "jheyrlla", "message": "fpghost84: doesn't work :\\", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T15:45:53.919931+00:00", "nick": "fpghost84", "message": "jheyrlla: like I say I've never tried it myself, but I'd at the very least expect the example in the scrapy docs to work http://scrapy.readthedocs.org/en/0.16/topics/pr...", "links": ["http://scrapy.readthedocs.org/en/0.16/topics/practices.html"], "channel": "scrapy"},
{"date": "2014-10-18T15:46:21.647699+00:00", "nick": "jheyrlla", "message": "that is the code i'm trying to run", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T15:46:21.747138+00:00", "nick": "fpghost84", "message": "you'll need to import your own spider instead of \"from testspiders.spiders.followall import FollowAllSpider\"", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T15:46:31.497626+00:00", "nick": "jheyrlla", "message": "oh", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T15:46:34.121712+00:00", "nick": "jheyrlla", "message": "lets try", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T15:46:58.818761+00:00", "nick": "fpghost84", "message": "then if you already set a domain in your spider class, you don't need to pass domain='scrapinghub.com'", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T15:47:30.719623+00:00", "nick": "jheyrlla", "message": "so like spider = dmoz() >", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T15:47:32.622000+00:00", "nick": "jheyrlla", "message": "?", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T15:48:13.740920+00:00", "nick": "fpghost84", "message": "yeah, or if you aren't setting a domain in the class, then spider=dmoz(domain=\"websiteyouwanttoscrape.com\")", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T15:50:20.320413+00:00", "nick": "jheyrlla", "message": "fpghost84: no luck", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T15:50:37.692153+00:00", "nick": "jheyrlla", "message": "reactor is not defined", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T15:50:54.827852+00:00", "nick": "jheyrlla", "message": "then followall doesn't exist", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T15:51:05.304321+00:00", "nick": "jheyrlla", "message": "then :\\", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T15:51:31.969596+00:00", "nick": "fpghost84", "message": "yeah followall is their spider", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T15:51:42.388227+00:00", "nick": "fpghost84", "message": "you need to replace that with your spider", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T15:52:04.735870+00:00", "nick": "fpghost84", "message": "as for reactor, did you make sure you included the line from twisted.internet import reactor?", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T15:52:37.328517+00:00", "nick": "jheyrlla", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T15:52:43.060565+00:00", "nick": "jheyrlla", "message": "twisted is installed too", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T15:55:01.366956+00:00", "nick": "fpghost84", "message": "so wht happens if you run the import reactor line? is the module being imported correctly?", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T15:56:20.747277+00:00", "nick": "jheyrlla", "message": "i get an error: \"reacor already running\"", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T15:56:45.428822+00:00", "nick": "jheyrlla", "message": "if i delete the lines with reactor, the script runs but the second spider doesn't run", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T16:01:14.148661+00:00", "nick": "fpghost84", "message": "maybe take a look at http://stackoverflow.com/questions/18787750/how...", "links": ["http://stackoverflow.com/questions/18787750/how-to-stop-the-reactor-while-several-scrapy-spiders-are-running-in-the-same-pro"], "channel": "scrapy"},
{"date": "2014-10-18T16:01:47.389788+00:00", "nick": "fpghost84", "message": "I'm afraid I don't have any direct experience doing this to know more, but sure an expert will be along sooner or later", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T16:04:25.739428+00:00", "nick": "fpghost84", "message": "I think you only want to run the reactor once you've set up the crawlers for all your spiders...then use the signal at the end to stop the reactor if necessary....really couldn't tell you more though", "links": [], "channel": "scrapy"},
{"date": "2014-10-18T16:11:28.130884+00:00", "nick": "fpghost84", "message": "got to go, but good luck! sure someone more knowledgeable will be along here soon too", "links": [], "channel": "scrapy"},
{"date": "2014-10-19T10:18:11.843502+00:00", "nick": "Zladivliba", "message": "hello everyone !", "links": [], "channel": "scrapy"},
{"date": "2014-10-19T10:18:20.103850+00:00", "nick": "Zladivliba", "message": "anyone knows how to solve this error : sgmllib.SGMLParseError: expected name token at '<!\\xe2\\x80\\x94-\\n    * {\\n     '", "links": [], "channel": "scrapy"},
{"date": "2014-10-19T10:19:15.756752+00:00", "nick": "Zladivliba", "message": "Here's the entire error : https://gist.github.com/anonymous/44c2bab1e5c88...", "links": ["https://gist.github.com/anonymous/44c2bab1e5c8824f7f31"], "channel": "scrapy"},
{"date": "2014-10-19T12:54:12.718468+00:00", "nick": "Zladivli_", "message": "hello everyone !", "links": [], "channel": "scrapy"},
{"date": "2014-10-19T12:54:15.273370+00:00", "nick": "Zladivli_", "message": "anyone knows how to solve this error : sgmllib.SGMLParseError: expected name token at '<!\\xe2\\x80\\x94-\\n    * {\\n     '", "links": [], "channel": "scrapy"},
{"date": "2014-10-19T12:54:17.498452+00:00", "nick": "Zladivli_", "message": "Here's the entire error : https://gist.github.com/anonymous/44c2bab1e5c88...", "links": ["https://gist.github.com/anonymous/44c2bab1e5c8824f7f31"], "channel": "scrapy"},
{"date": "2014-10-19T17:58:51.784970+00:00", "nick": "root3d", "message": "hello fellas", "links": [], "channel": "scrapy"},
{"date": "2014-10-19T17:59:05.207347+00:00", "nick": "root3d", "message": "https://gist.github.com/igauravsehrawat/efdf454... is am facing weird problem in this", "links": ["https://gist.github.com/igauravsehrawat/efdf454dd9d11e22b2b4"], "channel": "scrapy"},
{"date": "2014-10-19T17:59:17.279945+00:00", "nick": "root3d", "message": "it isn't scraping anymore !", "links": [], "channel": "scrapy"},
{"date": "2014-10-19T23:27:47.704818+00:00", "nick": "akhat", "message": "Hi", "links": [], "channel": "scrapy"},
{"date": "2014-10-19T23:28:03.457250+00:00", "nick": "akhat", "message": "I have a problem regarding dynamic creation of classes", "links": [], "channel": "scrapy"},
{"date": "2014-10-19T23:29:32.713983+00:00", "nick": "akhat", "message": "ERROR: Spider must return Request, BaseItem or None, got 'type' in", "links": [], "channel": "scrapy"},
{"date": "2014-10-19T23:30:10.079283+00:00", "nick": "akhat", "message": "After using yield", "links": [], "channel": "scrapy"},
{"date": "2014-10-19T23:38:16.457405+00:00", "nick": "toothrot", "message": "akhat, probably need to see the offending code", "links": [], "channel": "scrapy"},
{"date": "2014-10-19T23:38:37.948515+00:00", "nick": "toothrot", "message": "seems clear though, you're yielding or returning something unexpected", "links": [], "channel": "scrapy"},
{"date": "2014-10-19T23:38:45.289447+00:00", "nick": "akhat", "message": "Should I use pastebin?", "links": [], "channel": "scrapy"},
{"date": "2014-10-19T23:38:49.121856+00:00", "nick": "toothrot", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2014-10-19T23:40:01.774887+00:00", "nick": "akhat", "message": "I am following the link at http://doc.scrapy.org/en/latest/topics/practice....", "links": ["http://doc.scrapy.org/en/latest/topics/practices.html#dynamic-creation-of-item-classes"], "channel": "scrapy"},
{"date": "2014-10-19T23:40:05.577672+00:00", "nick": "akhat", "message": "Pastebin is http://pastebin.com/dt6AmqFU", "links": ["http://pastebin.com/dt6AmqFU"], "channel": "scrapy"},
{"date": "2014-10-19T23:41:01.844254+00:00", "nick": "akhat", "message": "Dynamic creation is resulting in a 'type' object, instead of usual Item or Request. Not sure how to yield it though.", "links": [], "channel": "scrapy"},
{"date": "2014-10-19T23:41:35.436560+00:00", "nick": "toothrot", "message": "you're yield a list, when i think you want to yield each item from the list, or simply return the list itself", "links": [], "channel": "scrapy"},
{"date": "2014-10-19T23:42:09.491746+00:00", "nick": "toothrot", "message": "sorry..", "links": [], "channel": "scrapy"},
{"date": "2014-10-19T23:42:12.381483+00:00", "nick": "toothrot", "message": "i am mistaken", "links": [], "channel": "scrapy"},
{"date": "2014-10-19T23:42:15.723081+00:00", "nick": "akhat", "message": "I want to yield a particualr item object.", "links": [], "channel": "scrapy"},
{"date": "2014-10-19T23:42:43.441793+00:00", "nick": "akhat", "message": "the relevant code for crawler is      malcode=crawler.Spider('Malcode', \"http://malc0de.com/bl/IP_Blacklist.txt\", \"([0-9.]*)\",[\"ip_address\"],5)", "links": [], "channel": "scrapy"},
{"date": "2014-10-19T23:43:31.598625+00:00", "nick": "toothrot", "message": "is it a tuple?", "links": [], "channel": "scrapy"},
{"date": "2014-10-19T23:43:38.448849+00:00", "nick": "akhat", "message": "Yes", "links": [], "channel": "scrapy"},
{"date": "2014-10-19T23:44:04.429429+00:00", "nick": "toothrot", "message": "a tuple isn't an item", "links": [], "channel": "scrapy"},
{"date": "2014-10-19T23:44:07.423143+00:00", "nick": "toothrot", "message": "Item", "links": [], "channel": "scrapy"},
{"date": "2014-10-19T23:45:47.560952+00:00", "nick": "toothrot", "message": "apparently i'm chronically confused.", "links": [], "channel": "scrapy"},
{"date": "2014-10-19T23:45:50.469731+00:00", "nick": "akhat", "message": "I am making a dynamic class, which should have Item type. Then I am using setattr to set its fileds using the tuples. Should I change the tuple to list?", "links": [], "channel": "scrapy"},
{"date": "2014-10-19T23:46:04.544222+00:00", "nick": "toothrot", "message": "no, but you're aren't returning an instace of that class", "links": [], "channel": "scrapy"},
{"date": "2014-10-19T23:46:12.498221+00:00", "nick": "toothrot", "message": "you're returning the type itself", "links": [], "channel": "scrapy"},
{"date": "2014-10-19T23:47:17.526065+00:00", "nick": "toothrot", "message": "one sec", "links": [], "channel": "scrapy"},
{"date": "2014-10-19T23:47:24.449802+00:00", "nick": "akhat", "message": "Ok. Got the error.", "links": [], "channel": "scrapy"},
{"date": "2014-10-19T23:47:39.267733+00:00", "nick": "akhat", "message": "Now any way to instantiate a metaclass?", "links": [], "channel": "scrapy"},
{"date": "2014-10-19T23:48:12.351606+00:00", "nick": "toothrot", "message": "do you need to use setattr instead of normal dict syntax?", "links": [], "channel": "scrapy"},
{"date": "2014-10-19T23:49:10.109773+00:00", "nick": "toothrot", "message": "i'm modifying your paste", "links": [], "channel": "scrapy"},
{"date": "2014-10-19T23:49:10.385343+00:00", "nick": "akhat", "message": "Normal dict wasn't working. Which is probably because I was working with the class instead of the object as you said.", "links": [], "channel": "scrapy"},
{"date": "2014-10-19T23:49:34.675467+00:00", "nick": "akhat", "message": "I think using an object should allow the normal dict syntax to work", "links": [], "channel": "scrapy"},
{"date": "2014-10-19T23:51:09.043975+00:00", "nick": "toothrot", "message": "akhat, https://bpaste.net/show/6143745ea06f", "links": ["https://bpaste.net/show/6143745ea06f"], "channel": "scrapy"},
{"date": "2014-10-19T23:51:24.335708+00:00", "nick": "toothrot", "message": "(untested, of course-- hopefully i did not make a mistake)", "links": [], "channel": "scrapy"},
{"date": "2014-10-19T23:55:54.947327+00:00", "nick": "toothrot", "message": "akhat, any feedback?", "links": [], "channel": "scrapy"},
{"date": "2014-10-19T23:56:04.946945+00:00", "nick": "akhat", "message": "Not working as is. I think I'll google more on metaclasses and ask again in say 20 minutes, if I am still not able to work it out.", "links": [], "channel": "scrapy"},
{"date": "2014-10-19T23:57:14.846240+00:00", "nick": "toothrot", "message": "what's the error", "links": [], "channel": "scrapy"},
{"date": "2014-10-19T23:58:02.785022+00:00", "nick": "toothrot", "message": "ahkat, the problem isn't anything to do with thhe dynamic class.", "links": [], "channel": "scrapy"},
{"date": "2014-10-19T23:58:16.667287+00:00", "nick": "toothrot", "message": "except that you weren't actually creating an instance of it.", "links": [], "channel": "scrapy"},
{"date": "2014-10-19T23:58:51.113275+00:00", "nick": "toothrot", "message": "what's the traceback now? it could be something simple I typo'ed..", "links": [], "channel": "scrapy"},
{"date": "2014-10-19T23:59:09.881620+00:00", "nick": "akhat", "message": "Just a second", "links": [], "channel": "scrapy"},
{"date": "2014-10-20T00:03:35.601629+00:00", "nick": "toothrot", "message": "the class created does not seem correct. the fields attribute shows an empty dict.", "links": [], "channel": "scrapy"},
{"date": "2014-10-20T00:04:03.876997+00:00", "nick": "akhat", "message": "Yes. the error is exceptions.KeyError: 'item does not support field: ip_address'", "links": [], "channel": "scrapy"},
{"date": "2014-10-20T00:08:19.641362+00:00", "nick": "toothrot", "message": "akhat, you could do this as a quick work-around https://bpaste.net/show/1e1f5b95c83c", "links": ["https://bpaste.net/show/1e1f5b95c83c"], "channel": "scrapy"},
{"date": "2014-10-20T00:09:40.157594+00:00", "nick": "akhat", "message": "That works. Thanks", "links": [], "channel": "scrapy"},
{"date": "2014-10-20T00:09:58.786008+00:00", "nick": "toothrot", "message": "for my crawer, i just use basically a  `class BasicJobItem(dict, BaseItem):pass` and treat it like a dict... but i don't need the values to actually be Field()'s", "links": [], "channel": "scrapy"},
{"date": "2014-10-20T00:10:26.568218+00:00", "nick": "toothrot", "message": "do you actually need arbitrary fields?", "links": [], "channel": "scrapy"},
{"date": "2014-10-20T00:11:25.383003+00:00", "nick": "akhat", "message": "The crawler is being used for lots of different websites with different attributes that are used through the regex. So I think so. Not sure if there is an easier solution", "links": [], "channel": "scrapy"},
{"date": "2014-10-20T00:13:07.734276+00:00", "nick": "toothrot", "message": "gotcha", "links": [], "channel": "scrapy"},
{"date": "2014-10-20T00:17:49.433795+00:00", "nick": "akshat", "message": "toothrot, sorry I got logged out.", "links": [], "channel": "scrapy"},
{"date": "2014-10-20T00:18:09.321442+00:00", "nick": "toothrot", "message": "no problem, i didn't say anything else. did you ask another question?", "links": [], "channel": "scrapy"},
{"date": "2014-10-20T00:18:31.881324+00:00", "nick": "akshat", "message": "What exactly is the code \"cls.fields = field_dict\" doing?", "links": [], "channel": "scrapy"},
{"date": "2014-10-20T00:19:46.020080+00:00", "nick": "akshat", "message": "I am guessing a class has a field attribute which is being set to the dict values, am I write?", "links": [], "channel": "scrapy"},
{"date": "2014-10-20T00:19:58.102623+00:00", "nick": "toothrot", "message": "https://github.com/scrapy/scrapy/blob/master/sc...", "links": ["https://github.com/scrapy/scrapy/blob/master/scrapy/item.py#L43-L58"], "channel": "scrapy"},
{"date": "2014-10-20T00:20:26.166580+00:00", "nick": "toothrot", "message": "the class attribute `fields` is checked for the key being used", "links": [], "channel": "scrapy"},
{"date": "2014-10-20T00:21:15.479571+00:00", "nick": "akshat", "message": "Ok. Got it.", "links": [], "channel": "scrapy"},
{"date": "2014-10-20T00:21:37.602355+00:00", "nick": "toothrot", "message": "i'm looking at the docs for type()", "links": [], "channel": "scrapy"},
{"date": "2014-10-20T00:21:46.252184+00:00", "nick": "toothrot", "message": "to see what's supposed to happen there", "links": [], "channel": "scrapy"},
{"date": "2014-10-20T04:38:36.094787+00:00", "nick": "Zladivli_", "message": "hello everyone !", "links": [], "channel": "scrapy"},
{"date": "2014-10-20T14:31:14.739070+00:00", "nick": "tr0n1c", "message": "hey guys, just installed the latest version of Scrapy and I'm going through the tutorials. At the moment I follow the \"scrapy shell\" tutorial. When I run <scrapy shell <url>>, there is no \"sel\" (Selector) scrapy object. Could you give me a hand here?", "links": [], "channel": "scrapy"},
{"date": "2014-10-20T14:39:08.136334+00:00", "nick": "rodrigo5244", "message": "Try response.selector that should give you the selector.", "links": [], "channel": "scrapy"},
{"date": "2014-10-20T14:41:14.218967+00:00", "nick": "rodrigo5244", "message": "For me \"sel\" worked for \"scrapy shell \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\"\"", "links": [], "channel": "scrapy"},
{"date": "2014-10-20T14:44:12.024054+00:00", "nick": "tr0n1c", "message": "yeah indeed response.selector is there", "links": [], "channel": "scrapy"},
{"date": "2014-10-20T14:49:10.825708+00:00", "nick": "tr0n1c", "message": "thanks rodrigo5244 !", "links": [], "channel": "scrapy"},
{"date": "2014-10-20T14:49:20.768878+00:00", "nick": "rodrigo5244", "message": "Welcome", "links": [], "channel": "scrapy"},
{"date": "2014-10-20T15:54:53.020324+00:00", "nick": "nramirezuy", "message": "that depends on the version", "links": [], "channel": "scrapy"},
{"date": "2014-10-20T15:54:55.239453+00:00", "nick": "nramirezuy", "message": "you installed", "links": [], "channel": "scrapy"},
{"date": "2014-10-20T15:56:00.033356+00:00", "nick": "nramirezuy", "message": "run scrapy version and look at the documentation for that version", "links": [], "channel": "scrapy"},
{"date": "2014-10-20T16:57:57.767015+00:00", "nick": "dhev", "message": ".", "links": [], "channel": "scrapy"},
{"date": "2014-10-20T20:02:00.123754+00:00", "nick": "asd", "message": "Hey guys can somebody help me out on this example? http://doc.scrapy.org/en/0.24/topics/item-pipel...", "links": ["http://doc.scrapy.org/en/0.24/topics/item-pipeline.html#price-validation-and-dropping-items-with-no-prices"], "channel": "scrapy"},
{"date": "2014-10-20T20:03:12.277688+00:00", "nick": "asd", "message": "why is item['price'] multiplying with vat_factor?", "links": [], "channel": "scrapy"},
{"date": "2014-10-21T11:05:32.438449+00:00", "nick": "Zladivliba", "message": "hi everyone", "links": [], "channel": "scrapy"},
{"date": "2014-10-21T11:05:51.904166+00:00", "nick": "rodrigo5244", "message": "Hey", "links": [], "channel": "scrapy"},
{"date": "2014-10-21T11:06:00.603677+00:00", "nick": "Zladivliba", "message": "is there a way to scrape multiple websites with scrapy like 200 pages per website ?", "links": [], "channel": "scrapy"},
{"date": "2014-10-21T11:06:03.667658+00:00", "nick": "Zladivliba", "message": "hey rodrigo5244", "links": [], "channel": "scrapy"},
{"date": "2014-10-21T11:06:27.823757+00:00", "nick": "rodrigo5244", "message": "Not sure if I understand your question.", "links": [], "channel": "scrapy"},
{"date": "2014-10-21T11:06:49.999267+00:00", "nick": "rodrigo5244", "message": "Do you want run more than one spider at a time?", "links": [], "channel": "scrapy"},
{"date": "2014-10-21T11:07:50.684342+00:00", "nick": "Zladivliba", "message": "humm yes and no", "links": [], "channel": "scrapy"},
{"date": "2014-10-21T11:08:02.559592+00:00", "nick": "Zladivliba", "message": "I'd like to scrape 20.000 websites", "links": [], "channel": "scrapy"},
{"date": "2014-10-21T11:08:16.090464+00:00", "nick": "Zladivliba", "message": "I'm trying to collect every domain name in .dk", "links": [], "channel": "scrapy"},
{"date": "2014-10-21T11:08:22.738019+00:00", "nick": "Zladivliba", "message": "so I need to check a LOT of websites", "links": [], "channel": "scrapy"},
{"date": "2014-10-21T11:08:31.405021+00:00", "nick": "Zladivliba", "message": "and check like lots of pages", "links": [], "channel": "scrapy"},
{"date": "2014-10-21T11:08:53.970629+00:00", "nick": "Zladivliba", "message": "and then collect the links that I find that will have the extension .dk at the end", "links": [], "channel": "scrapy"},
{"date": "2014-10-21T11:10:18.131392+00:00", "nick": "rodrigo5244", "message": "And you are wondering if there is an efficient way of doing that?", "links": [], "channel": "scrapy"},
{"date": "2014-10-21T11:11:45.012745+00:00", "nick": "rodrigo5244", "message": "I think this is what you are looking for: http://doc.scrapy.org/en/latest/topics/broad-cr...", "links": ["http://doc.scrapy.org/en/latest/topics/broad-crawls.html"], "channel": "scrapy"},
{"date": "2014-10-21T11:11:48.607851+00:00", "nick": "Zladivliba", "message": "well I have some other reasons to scrape theses websites so basically yes that part is checked (I know it would not be very efficient just for domains collecting)", "links": [], "channel": "scrapy"},
{"date": "2014-10-21T11:54:44.589860+00:00", "nick": "Zladivliba", "message": "rodrigo5244: do you know if there's a way to tell scrapy to follow only a path within a url ?", "links": [], "channel": "scrapy"},
{"date": "2014-10-21T11:55:06.183854+00:00", "nick": "Zladivliba", "message": "like for example crawl everything under test.com/intresting_stuff/", "links": ["http://test.com/intresting_stuff/"], "channel": "scrapy"},
{"date": "2014-10-21T11:55:18.559188+00:00", "nick": "rodrigo5244", "message": "I think there is.", "links": [], "channel": "scrapy"},
{"date": "2014-10-21T11:56:00.608560+00:00", "nick": "rodrigo5244", "message": "http://doc.scrapy.org/en/latest/topics/spiders....", "links": ["http://doc.scrapy.org/en/latest/topics/spiders.html#crawlspider"], "channel": "scrapy"},
{"date": "2014-10-21T11:56:17.347240+00:00", "nick": "rodrigo5244", "message": "I think you can make a rule for that.", "links": [], "channel": "scrapy"},
{"date": "2014-10-21T11:56:54.538516+00:00", "nick": "rodrigo5244", "message": "http://doc.scrapy.org/en/latest/topics/link-ext...", "links": ["http://doc.scrapy.org/en/latest/topics/link-extractors.html#link-extractors"], "channel": "scrapy"},
{"date": "2014-10-21T11:57:38.577929+00:00", "nick": "rodrigo5244", "message": "allow=\"test.com/intresting_stuff/\";", "links": ["http://allow=\"test.com/intresting_stuff/\""], "channel": "scrapy"},
{"date": "2014-10-21T12:08:44.630993+00:00", "nick": "Zladivliba", "message": "ok cool thanks !!!", "links": [], "channel": "scrapy"},
{"date": "2014-10-21T12:09:42.080590+00:00", "nick": "Zladivliba", "message": "rodrigo5244: do you think that if I'm doing a broad crawl there's a way to tell scrapy don't hit on the same website more than one time per second ?", "links": [], "channel": "scrapy"},
{"date": "2014-10-21T12:18:21.241542+00:00", "nick": "rodrigo5244", "message": "You are welcome.", "links": [], "channel": "scrapy"},
{"date": "2014-10-21T12:18:28.108644+00:00", "nick": "rodrigo5244", "message": "Do you mean the same server or domain?", "links": [], "channel": "scrapy"},
{"date": "2014-10-21T12:18:45.398229+00:00", "nick": "Zladivliba", "message": "well either one of them", "links": [], "channel": "scrapy"},
{"date": "2014-10-21T12:19:13.134253+00:00", "nick": "Zladivliba", "message": "I just dont want to use to many resources of one server rodrigo5244", "links": [], "channel": "scrapy"},
{"date": "2014-10-21T12:20:07.990566+00:00", "nick": "rodrigo5244", "message": "I see. I have seen some websites go done because of a crawler.", "links": [], "channel": "scrapy"},
{"date": "2014-10-21T12:20:14.871486+00:00", "nick": "rodrigo5244", "message": "down", "links": [], "channel": "scrapy"},
{"date": "2014-10-21T12:20:57.289111+00:00", "nick": "rodrigo5244", "message": "There maybe a config for that, but I don't know.", "links": [], "channel": "scrapy"},
{"date": "2014-10-21T12:21:04.040154+00:00", "nick": "rodrigo5244", "message": "may be", "links": [], "channel": "scrapy"},
{"date": "2014-10-21T12:23:11.251868+00:00", "nick": "Zladivliba", "message": "ok", "links": [], "channel": "scrapy"},
{"date": "2014-10-21T12:28:13.113695+00:00", "nick": "Zladivliba", "message": "thanks a lot rodrigo5244 for ur help", "links": [], "channel": "scrapy"},
{"date": "2014-10-21T12:29:07.505225+00:00", "nick": "rodrigo5244", "message": "You are welcome.", "links": [], "channel": "scrapy"},
{"date": "2014-10-21T13:29:15.125602+00:00", "nick": "whateveridc", "message": "inherits from deprecated class scrapy.spider.BaseSpider, please inherit from scrapy.spider.Spider.", "links": [], "channel": "scrapy"},
{"date": "2014-10-21T13:29:27.895198+00:00", "nick": "whateveridc", "message": "that import am i missing ?", "links": [], "channel": "scrapy"},
{"date": "2014-10-21T13:43:36.863358+00:00", "nick": "rodrigo5244", "message": "whateveridc: I think you are looking for this: https://github.com/scrapy/scrapy/blob/8fece4b0b...", "links": ["https://github.com/scrapy/scrapy/blob/8fece4b0b8eb8772a07673b4166cdcdb5c017eb8/docs/news.rst#enhancements-1"], "channel": "scrapy"},
{"date": "2014-10-21T13:43:44.209158+00:00", "nick": "rodrigo5244", "message": "Rename scrapy.spider.BaseSpider to scrapy.spider.Spider (:issue:`510`, :issue:`519`)", "links": [], "channel": "scrapy"},
{"date": "2014-10-21T15:01:25.473495+00:00", "nick": "kaido", "message": "hello, i started learning scrapy and i am having some issues with the encoding, can someone please give me some advice ?", "links": [], "channel": "scrapy"},
{"date": "2014-10-21T15:08:33.124439+00:00", "nick": "Zladivliba", "message": "hey !", "links": [], "channel": "scrapy"},
{"date": "2014-10-21T15:08:48.008063+00:00", "nick": "Zladivliba", "message": "I have these settings https://gist.github.com/anonymous/074f82757e050...", "links": ["https://gist.github.com/anonymous/074f82757e0501788dea"], "channel": "scrapy"},
{"date": "2014-10-21T15:08:57.904392+00:00", "nick": "Zladivliba", "message": "and I can't get scrapy to consume more than 10% of the processor", "links": [], "channel": "scrapy"},
{"date": "2014-10-21T15:09:00.560160+00:00", "nick": "Zladivliba", "message": "what's wrong ?", "links": [], "channel": "scrapy"},
{"date": "2014-10-21T15:09:06.782830+00:00", "nick": "Zladivliba", "message": "I'm doing a broad crawl...", "links": [], "channel": "scrapy"},
{"date": "2014-10-21T15:11:02.574881+00:00", "nick": "Zladivliba", "message": "ohhh I just realized i'm not multithread... ok that makes sense", "links": [], "channel": "scrapy"},
{"date": "2014-10-21T15:44:05.931735+00:00", "nick": "Zladivliba", "message": "but still the load is really low... humm any idea what's going on ?", "links": [], "channel": "scrapy"},
{"date": "2014-10-21T16:06:34.324045+00:00", "nick": "rodrigo5244", "message": "Zladivliba: You mean the crawler is going slow?", "links": [], "channel": "scrapy"}][{"date": "2014-10-22T19:38:10.559475+00:00", "nick": "draoibit", "message": "Hello all", "links": [], "channel": "scrapy"},
{"date": "2014-10-22T19:49:31.626172+00:00", "nick": "Zladivliba", "message": "hello", "links": [], "channel": "scrapy"},
{"date": "2014-10-22T19:49:47.736398+00:00", "nick": "Zladivliba", "message": "Is there a way I can accept redirect ONLY when they are on the same domain ?", "links": [], "channel": "scrapy"},
{"date": "2014-10-23T10:01:51.514397+00:00", "nick": "AndyRez", "message": "How do I order the values of a scrapped item in output csv...", "links": [], "channel": "scrapy"},
{"date": "2014-10-23T10:02:15.300251+00:00", "nick": "AndyRez", "message": "when printing the log to screen its arranged alphabetically but when it writes to csv it isn't", "links": [], "channel": "scrapy"},
{"date": "2014-10-23T14:10:02.408723+00:00", "nick": "gcfhvjbkn", "message": "hello", "links": [], "channel": "scrapy"},
{"date": "2014-10-23T14:10:03.164684+00:00", "nick": "gcfhvjbkn", "message": "Could not open CONNECT tunnel.", "links": [], "channel": "scrapy"},
{"date": "2014-10-23T14:10:08.171298+00:00", "nick": "gcfhvjbkn", "message": "i get this error for some reason", "links": [], "channel": "scrapy"},
{"date": "2014-10-23T14:10:15.113342+00:00", "nick": "gcfhvjbkn", "message": "why could that be?", "links": [], "channel": "scrapy"},
{"date": "2014-10-23T14:10:51.073228+00:00", "nick": "gcfhvjbkn", "message": "i use a http proxy with authorisation that works well enough in the browser", "links": [], "channel": "scrapy"},
{"date": "2014-10-23T15:50:13.654141+00:00", "nick": "Zladivliba", "message": "hi everyone", "links": [], "channel": "scrapy"},
{"date": "2014-10-23T15:50:33.508684+00:00", "nick": "Zladivliba", "message": "how do I accept 301 redirects only if they are within the current website ?", "links": [], "channel": "scrapy"},
{"date": "2014-10-23T15:56:28.619259+00:00", "nick": "rodrigo5244", "message": "Zladivliba, are you using allowed_domains in your spider?", "links": [], "channel": "scrapy"},
{"date": "2014-10-23T15:56:39.410361+00:00", "nick": "Zladivliba", "message": "rodrigo5244: yes !", "links": [], "channel": "scrapy"},
{"date": "2014-10-23T15:56:47.863414+00:00", "nick": "Zladivliba", "message": "it had no impact unfortunately", "links": [], "channel": "scrapy"},
{"date": "2014-10-23T15:56:54.052667+00:00", "nick": "rodrigo5244", "message": "humm", "links": [], "channel": "scrapy"},
{"date": "2014-10-23T15:57:01.224541+00:00", "nick": "Zladivliba", "message": "when 301 my scrapy hit it anyway", "links": [], "channel": "scrapy"},
{"date": "2014-10-23T15:57:17.040572+00:00", "nick": "Zladivliba", "message": "this is why I had to remove following 301...", "links": [], "channel": "scrapy"},
{"date": "2014-10-23T15:57:40.932837+00:00", "nick": "Zladivliba", "message": "any idea how to stop scrapy from following 301 from other domains ?", "links": [], "channel": "scrapy"},
{"date": "2014-10-23T15:57:57.602946+00:00", "nick": "rodrigo5244", "message": "You could check if the url matches the domain yourself and stop the crawler there.", "links": [], "channel": "scrapy"},
{"date": "2014-10-23T15:58:44.760726+00:00", "nick": "Zladivliba", "message": "ok how do I do that ?", "links": [], "channel": "scrapy"},
{"date": "2014-10-23T15:59:44.616644+00:00", "nick": "rodrigo5244", "message": "the url is in response.url", "links": [], "channel": "scrapy"},
{"date": "2014-10-23T16:00:25.413208+00:00", "nick": "rodrigo5244", "message": "Use urlparse to parse it.", "links": [], "channel": "scrapy"},
{"date": "2014-10-23T16:00:35.005010+00:00", "nick": "Zladivliba", "message": "ok and how do I tell scrapy to just pass this url ?", "links": [], "channel": "scrapy"},
{"date": "2014-10-23T16:01:41.595846+00:00", "nick": "rodrigo5244", "message": "Try returning [] that should tell scrapy that you hit a dead end. It is an iterable with nothing.", "links": [], "channel": "scrapy"},
{"date": "2014-10-23T16:01:52.244192+00:00", "nick": "rodrigo5244", "message": "http://stackoverflow.com/questions/9626535/get-... get the domain this way.", "links": ["http://stackoverflow.com/questions/9626535/get-domain-name-from-url"], "channel": "scrapy"},
{"date": "2014-10-23T16:01:57.848654+00:00", "nick": "Zladivliba", "message": "ok cool rodrigo5244 I'll try that ;)", "links": [], "channel": "scrapy"},
{"date": "2014-10-23T16:02:02.625666+00:00", "nick": "Zladivliba", "message": "thanks a lot for the idea !", "links": [], "channel": "scrapy"},
{"date": "2014-10-23T16:02:09.002038+00:00", "nick": "rodrigo5244", "message": "You are welcome.", "links": [], "channel": "scrapy"},
{"date": "2014-10-23T16:14:46.610651+00:00", "nick": "Zladivliba", "message": "ok seems to work so far ;)))", "links": [], "channel": "scrapy"},
{"date": "2014-10-23T16:24:47.698068+00:00", "nick": "rodrigo5244", "message": "Good", "links": [], "channel": "scrapy"},
{"date": "2014-10-23T17:35:31.390346+00:00", "nick": "ericc", "message": "hi everyone, having a hell of a time getting a mysql pipeline to work.  my select statement is not working, not 100% sure how to pass the item variables", "links": [], "channel": "scrapy"},
{"date": "2014-10-23T17:35:36.348063+00:00", "nick": "ericc", "message": "anyone around to help?", "links": [], "channel": "scrapy"},
{"date": "2014-10-23T17:38:11.305990+00:00", "nick": "ericc", "message": "self.cursor.execute(\"\"\"SELECT * FROM Count WHERE country=%s AND titleId=%s AND date=%s\"\"\", (item['country'], item['titleId'], item['date']))", "links": [], "channel": "scrapy"},
{"date": "2014-10-23T17:38:19.029528+00:00", "nick": "ericc", "message": "its saying:    exceptions.TypeError: not enough arguments for format string", "links": [], "channel": "scrapy"},
{"date": "2014-10-24T17:41:44.441420+00:00", "nick": "jsjc", "message": "Hi! I wonder if there is a way to change settings per spider, like for example DELAYs.....", "links": [], "channel": "scrapy"},
{"date": "2014-10-24T17:54:00.517639+00:00", "nick": "fpghost84", "message": "jsjc: yeah you can do that, e.g. download_delay = 0.4 in the spider class", "links": [], "channel": "scrapy"},
{"date": "2014-10-24T17:57:43.423257+00:00", "nick": "fpghost84", "message": "In general I think they are lower case versions of the settings http://doc.scrapy.org/en/latest/topics/settings... (but not every setting can be adjusted per spider, see the docs for which)", "links": ["http://doc.scrapy.org/en/latest/topics/settings.html"], "channel": "scrapy"},
{"date": "2014-10-24T18:05:39.002621+00:00", "nick": "jsjc", "message": "fpghost84:  Thanks! sounds nice.", "links": [], "channel": "scrapy"},
{"date": "2014-10-25T08:17:21.217165+00:00", "nick": "Newbie0086", "message": "hello", "links": [], "channel": "scrapy"},
{"date": "2014-10-25T08:17:25.370266+00:00", "nick": "Newbie0086", "message": "anyone here", "links": [], "channel": "scrapy"},
{"date": "2014-10-25T08:18:02.331888+00:00", "nick": "Newbie0086", "message": "if i can in one spider include another xml spider", "links": [], "channel": "scrapy"},
{"date": "2014-10-25T08:20:02.435021+00:00", "nick": "Newbie0086", "message": "i user baseSpider to crawl some website,but i want to use xml spider to find other items", "links": [], "channel": "scrapy"},
{"date": "2014-10-25T08:20:04.907216+00:00", "nick": "Newbie0086", "message": "can i ?", "links": [], "channel": "scrapy"},
{"date": "2014-10-25T10:38:29.841358+00:00", "nick": "cheezee", "message": "anybody home?", "links": [], "channel": "scrapy"},
{"date": "2014-10-25T11:20:08.160119+00:00", "nick": "asd", "message": "Hi, does somebody know how to remove trailing whitespace when i have a list of items?", "links": [], "channel": "scrapy"},
{"date": "2014-10-25T14:34:40.482266+00:00", "nick": "f0reclone", "message": "Hi", "links": [], "channel": "scrapy"},
{"date": "2014-10-25T14:35:13.040355+00:00", "nick": "f0reclone", "message": "Are there any developers who can help me with my Python application?", "links": [], "channel": "scrapy"},
{"date": "2014-10-25T19:35:11.149788+00:00", "nick": "jsPy", "message": "Hi", "links": [], "channel": "scrapy"},
{"date": "2014-10-25T21:10:29.092695+00:00", "nick": "jsPy1", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2014-10-26T09:18:38.145172+00:00", "nick": "fpghost84", "message": "Hi, I am wondering is response.meta viewable by servers? and could this lead to a ban? or is it just an internal thing...", "links": [], "channel": "scrapy"},
{"date": "2014-10-26T17:52:18.607776+00:00", "nick": "Yves514", "message": "Hi all. I seem to be struggling with recursive functionality on Scrapy (0.24.4) I\u2019m using CrawlSpider and LinkExtractor rules targetting the Next Page link via restrict_xpaths (LinkExtractor). When I run the spider, it finds the first NextPage link, says it crawled it but doesn\u2019t try to crawl the subsequent Next Page link. Does CrawlSpider do recursive crawling? I\u2019m about to do a test using the base spider to see if it behaves any differently\u2026", "links": [], "channel": "scrapy"},
{"date": "2014-10-26T18:06:25.637596+00:00", "nick": "toothrot", "message": "Yves514, it should, did you dump the html, debug your rules, etc?", "links": [], "channel": "scrapy"},
{"date": "2014-10-26T18:18:05.744622+00:00", "nick": "Yves514", "message": "Thanks toothrot. Yes, I did but I decided to double check the links reported by my spider as crawled. And of course, the Next Page link that scrapy is returning is breaking the target site. Not sure what\u2019s going on but it\u2019s a lead!", "links": [], "channel": "scrapy"},
{"date": "2014-10-26T19:07:03.381072+00:00", "nick": "Yves514", "message": "Interesting. I think there is something strage going on in the LinkExtractor (or somewhere else). These are the GET parameters found on the NextPage link I\u2019m trying to scrape: \u201cOpenView&RP=2004-2005~1&Start=21&Count=20&lang=eng&\u201d get changed to \u201cCount=20&OpenView=&RP=2004-2005%7E1&Start=21&lang=eng\u201d. Notice it encoded the tilde and moved the Count parameter to the front. My destination site is sensitive to the order of the parameters. The parame", "links": [], "channel": "scrapy"},
{"date": "2014-10-26T19:07:03.961443+00:00", "nick": "Yves514", "message": "look find when I use response.xpath() via the scrapy shell.", "links": [], "channel": "scrapy"},
{"date": "2014-10-26T19:16:24.520103+00:00", "nick": "Yves514", "message": "Found it. Need to set canonicalize=False for the LinkExtractor for that spider.", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T02:16:58.157525+00:00", "nick": "Newbie0086", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T03:44:14.674158+00:00", "nick": "cheeze", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T10:55:47.392736+00:00", "nick": "fpghost84", "message": "Hi, I'm using some rotate user agent middleware (much like http://tangww.com/2013/06/UsingRandomAgent/), is there a way I can slightly tweak this so that sometimes I can choose from a pool of \"mobile\" user-agents and other times non-mobile", "links": ["http://tangww.com/2013/06/UsingRandomAgent/"], "channel": "scrapy"},
{"date": "2014-10-27T10:55:56.505238+00:00", "nick": "fpghost84", "message": "(on a per spider basis)", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T10:57:32.149004+00:00", "nick": "jnosal", "message": "Hi guys, I have on question about Accept-Language and how it plays with scrapy, is anyone online ?", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T14:36:36.090779+00:00", "nick": "wizo", "message": "Hello! I am using crawl spider in an attempt to crawl all the urls of a site. I also have a call back function defined, with \"follow\" set to true in the rule. when running the crawler, i see that the [DEBUG] output shows correctly URLs present in the start_urls page. Is there anyway i can access those URLs? I can't seem to find any documentation on how to access those urls. I want to do some stuff like changing the values of the paramet", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T14:41:01.965377+00:00", "nick": "csalazar", "message": "http://doc.scrapy.org/en/latest/topics/spiders....", "links": ["http://doc.scrapy.org/en/latest/topics/spiders.html#scrapy.contrib.spiders.Rule"], "channel": "scrapy"},
{"date": "2014-10-27T14:41:15.808432+00:00", "nick": "csalazar", "message": "process_request parameter, it accepts a function", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T14:45:53.528144+00:00", "nick": "wizo", "message": "hmmm thanks csalazar i'll take a look at that", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T14:49:32.717805+00:00", "nick": "wizo", "message": "i think process_links might work too, since it returns a list of the urls on the response page", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T15:13:27.770016+00:00", "nick": "wizo", "message": "hey csalazar process_request is great, gave me all the urls, thanks!", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T15:13:58.330075+00:00", "nick": "csalazar", "message": "you're welcome!", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T15:15:57.742632+00:00", "nick": "wizo", "message": "one other thing, it appears to give me all the urls, including those that are of other domain, does that mean I have to filter them out manual? by looking at the [DEBUG] output on crawlspider, it appears to auto filter out those third party urls, \"DEBUG: Filtered offsite request to\"", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T15:16:08.121745+00:00", "nick": "wizo", "message": "manually*", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T15:16:10.816637+00:00", "nick": "webmaven_work", "message": "Hey folks, Scrapy made BeautifulOpen!: http://beautifulopen.com/2014/10/24/scrapy/", "links": ["http://beautifulopen.com/2014/10/24/scrapy/"], "channel": "scrapy"},
{"date": "2014-10-27T15:42:04.163182+00:00", "nick": "wizo", "message": "oh I'm such a fool", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T15:43:22.132614+00:00", "nick": "wizo", "message": "named my callback wrongly, no wonder i wasn't able to get the urls", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T23:01:08.365778+00:00", "nick": "brobias", "message": "Hello, is anyone here available to answer a few scrapy questions?", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T23:02:07.594702+00:00", "nick": "toothrot", "message": "brobias, best to ask", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T23:02:11.346916+00:00", "nick": "toothrot", "message": "just ask*", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T23:02:15.789659+00:00", "nick": "brobias", "message": "ok, thanks", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T23:02:30.885684+00:00", "nick": "toothrot", "message": "then when somone sees it, they can answer instead of saying 'what is the question?' :P", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T23:03:49.553030+00:00", "nick": "brobias", "message": "So I am tring to iterate over several urls that return json. Each url has two counters that I need to iterate through. I am wondering how I shuld approach this. I basically need to start at one url and iterate over the url and depending on whether or not the length of the json response is > 0 or not, I need to parse it", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T23:05:29.480590+00:00", "nick": "brobias", "message": "an example url is this right here:", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T23:05:29.626071+00:00", "nick": "brobias", "message": "http://www.metal-archives.com/browse/ajax-lette...", "links": ["http://www.metal-archives.com/browse/ajax-letter/l/NBR/json/1?sEcho=1&amp=&iColumns=4&amp=&sColumns=&amp=&iDisplayStart=0&amp=&iDisplayLength=500&amp=&mDataProp_0=0&amp=&mDataProp_1=1&amp=&mDataProp_2=2&amp=&mDataProp_3=3&amp=&iSortCol_0=0&amp=&sSortDir_0=asc&amp=&iSortingCols=1&amp=&bSortable_0=true&amp=&bSortable_1=true&amp=&bSortable_2=true&amp=&bSortable_3=false&amp=&_=1414451093790"], "channel": "scrapy"},
{"date": "2014-10-27T23:06:05.565746+00:00", "nick": "toothrot", "message": "so, if aaData has items, crawl those links?", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T23:06:10.513537+00:00", "nick": "brobias", "message": "I need to iterate over ajax-letter/(letter) and DisplayStart=0", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T23:06:11.183774+00:00", "nick": "brobias", "message": "yep", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T23:06:26.030502+00:00", "nick": "brobias", "message": "so the max length of each response is 500, so I need to increment by 500 for that letter until there is no respnose", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T23:06:34.860207+00:00", "nick": "brobias", "message": "I can do this nonconcurrently no problem, but I am not sure how to do it with scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T23:07:11.199461+00:00", "nick": "toothrot", "message": "so you can figure out the total number of requests needed from the first request, right?", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T23:07:44.239050+00:00", "nick": "toothrot", "message": "if iTotalRecords > iTotalDisplayRecords? right?", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T23:18:34.364870+00:00", "nick": "toothrot", "message": "...", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T23:20:13.248210+00:00", "nick": "brobias", "message": "yeah I believe you can do that", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T23:20:15.494651+00:00", "nick": "brobias", "message": "I was doing it differently", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T23:20:25.994349+00:00", "nick": "brobias", "message": "but I wasn't thinking about doing it with scrapy before so I appreciate that", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T23:21:38.853960+00:00", "nick": "brobias", "message": "so using that number can I just populate the start urls array this way.", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T23:21:39.678074+00:00", "nick": "brobias", "message": "?", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T23:27:24.910018+00:00", "nick": "toothrot", "message": "i tried changing iDisplayLength param and nothing happened.", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T23:28:02.210448+00:00", "nick": "brobias", "message": "yeah so", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T23:28:05.457027+00:00", "nick": "brobias", "message": "the display start works", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T23:28:09.536109+00:00", "nick": "brobias", "message": "I increment that by 500", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T23:28:35.237991+00:00", "nick": "brobias", "message": "I wrote a simple function to iterate through every url and I can do it outside of scrapy but Im wondering how I could do this with scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T23:28:42.589598+00:00", "nick": "brobias", "message": "without measuring the response length", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T23:28:54.911714+00:00", "nick": "brobias", "message": "so I will try doing what you suggested by using the totalrecords number", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T23:29:12.404145+00:00", "nick": "brobias", "message": "that actaully should work...so I will just append to the starturls array", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T23:29:43.767297+00:00", "nick": "toothrot", "message": "well, wait", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T23:29:55.471137+00:00", "nick": "toothrot", "message": "that url is the one in start url?", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T23:30:06.495679+00:00", "nick": "toothrot", "message": "these arne't known up front are they?", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T23:30:18.936755+00:00", "nick": "brobias", "message": "no im using the urls in the json", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T23:30:20.694074+00:00", "nick": "toothrot", "message": "i was thinknig of somethin glike this in your parse method for the first response: https://bpaste.net/show/fdf0309ae011", "links": ["https://bpaste.net/show/fdf0309ae011"], "channel": "scrapy"},
{"date": "2014-10-27T23:31:36.243754+00:00", "nick": "brobias", "message": "so the urls I am actaully parsing are the ones in the json response", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T23:31:54.485620+00:00", "nick": "brobias", "message": "but what you just pasted is pretty much what I was going to do", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T23:35:11.214323+00:00", "nick": "brobias", "message": "I just need to write a function that gives me all of the urls and then populate the start urls array", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T23:36:38.276271+00:00", "nick": "toothrot", "message": "don't you get all the urls from the website though?", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T23:37:07.730550+00:00", "nick": "toothrot", "message": "and you need the totals from each response before you can generate the additional urls, right?", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T23:38:59.210033+00:00", "nick": "brobias", "message": "yeah I didn't think to do it that way", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T23:39:08.090623+00:00", "nick": "brobias", "message": "I just iterated by 500 everytime and measured the length of the response", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T23:39:18.769598+00:00", "nick": "brobias", "message": "and once the length of the respnose was 0, I moved to the next letter", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T23:39:30.485771+00:00", "nick": "brobias", "message": "I haven't figured out how to get all the urls otherwise", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T23:40:03.203724+00:00", "nick": "brobias", "message": "the urls by defaut are http://whatever.com/name/id", "links": ["http://whatever.com/name/id"], "channel": "scrapy"},
{"date": "2014-10-27T23:40:15.499399+00:00", "nick": "brobias", "message": "so maybe this is something that could be tackled with regex?", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T23:40:29.977919+00:00", "nick": "brobias", "message": "the json explicitely gives me all of the urls I need though", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T23:40:54.409743+00:00", "nick": "toothrot", "message": "don't think you need regex..", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T23:41:23.569510+00:00", "nick": "brobias", "message": "but the urls im actually extracting data from are like the above one and all of them are in that json request", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T23:41:42.371776+00:00", "nick": "brobias", "message": "so I think the way to do it is how you'er doing it and then just append to the start urls", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T23:41:54.121736+00:00", "nick": "toothrot", "message": "no, one sec", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T23:42:00.237273+00:00", "nick": "toothrot", "message": "i'll show you an example", "links": [], "channel": "scrapy"},
{"date": "2014-10-27T23:50:12.694171+00:00", "nick": "toothrot", "message": "sorry, hold tight, had to set up a test project", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T00:01:17.712096+00:00", "nick": "brobias", "message": "no wories", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T00:01:49.544918+00:00", "nick": "toothrot", "message": "brobias, okay, i got carried away", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T00:01:51.117268+00:00", "nick": "brobias", "message": "I was planning on just writing a separate function that returns a list of all the urls and sets the start urls to that", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T00:02:42.302102+00:00", "nick": "toothrot", "message": "https://bpaste.net/show/1afb83f47a11", "links": ["https://bpaste.net/show/1afb83f47a11"], "channel": "scrapy"},
{"date": "2014-10-28T00:02:55.419699+00:00", "nick": "toothrot", "message": "this only actually crawls the first page for each latter", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T00:02:57.454855+00:00", "nick": "toothrot", "message": "er, letter", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T00:03:05.932470+00:00", "nick": "toothrot", "message": "and ir prints the urls for the rest", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T00:03:15.168468+00:00", "nick": "toothrot", "message": "print url #Request(url, callback=self.parse_data)", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T00:03:29.390588+00:00", "nick": "toothrot", "message": "change that to `yield Request(url, callback=self.parse_data)`", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T00:03:37.059512+00:00", "nick": "toothrot", "message": "please set a delay on your crawler", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T00:04:53.243417+00:00", "nick": "toothrot", "message": "that'd be the DOWNLOAD_DELAY setting", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T00:05:10.110539+00:00", "nick": "toothrot", "message": "and set CONCURRENT_REQUESTS to be reasonable", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T00:05:31.578178+00:00", "nick": "toothrot", "message": "rather, CONCURRENT_REQUESTS_PER_DOMAIN", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T00:06:00.273662+00:00", "nick": "toothrot", "message": "brobias, ^", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T00:08:14.559062+00:00", "nick": "toothrot", "message": "gotta eat dinner, but hopefully that can give you a start", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T00:17:08.999142+00:00", "nick": "brobias", "message": "hey I greatly appreciate man, tahnks so much", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T00:29:02.114993+00:00", "nick": "toothrot", "message": "brobias, did you get it going?", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T00:31:32.671618+00:00", "nick": "brobias", "message": "yeah im just deciphering what you did", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T00:35:00.361593+00:00", "nick": "brobias", "message": "greatly appreciate this", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T00:35:15.800700+00:00", "nick": "brobias", "message": "I definitely was planning on throttling this", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T00:51:08.123940+00:00", "nick": "brobias", "message": "the meta stuff I find confusing so I'm reading the docs", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T09:23:09.948450+00:00", "nick": "Teodora", "message": "Hello, anybody here?", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T09:37:22.274809+00:00", "nick": "nikolaosk", "message": "just dump the question and wait", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T09:39:30.029443+00:00", "nick": "nikolaosk", "message": "if I don't know the answer probably someone else does", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T09:47:37.397420+00:00", "nick": "Teodora", "message": "I just posted in on stackoverflow, so http://stackoverflow.com/questions/26604872/scr...", "links": ["http://stackoverflow.com/questions/26604872/scrapy-no-such-host-crawler"], "channel": "scrapy"},
{"date": "2014-10-28T09:49:41.179861+00:00", "nick": "nikolaosk", "message": "aha", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T09:49:50.231401+00:00", "nick": "nikolaosk", "message": "I am not familiar with the crawl spider", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T09:50:13.347541+00:00", "nick": "nikolaosk", "message": "but it seems to me that you want to yield a special item in case of error instead of a log message", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T09:50:16.425275+00:00", "nick": "nikolaosk", "message": "am I right?", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T09:51:09.328233+00:00", "nick": "Teodora", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T09:51:16.149404+00:00", "nick": "Teodora", "message": "It seems that this is the case", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T09:52:04.231715+00:00", "nick": "Teodora", "message": "I thought I needed to catch a log message, but scrapy gives error when finds domains that are not hosted", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T09:52:27.972304+00:00", "nick": "nikolaosk", "message": "I see, I see", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T09:52:43.074538+00:00", "nick": "nikolaosk", "message": "well, first of all, this code is probably old", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T09:52:49.950246+00:00", "nick": "nikolaosk", "message": "with the last commit from 2013", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T09:53:35.634284+00:00", "nick": "nikolaosk", "message": "I think scrapy broke compatibility with version from back then", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T09:54:15.300006+00:00", "nick": "nikolaosk", "message": "are you going to use any logic in the page?", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T09:54:30.766621+00:00", "nick": "nikolaosk", "message": "eg, imagine the site return a 200 OK code", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T09:54:45.394730+00:00", "nick": "nikolaosk", "message": "but the page actually has a 404 message", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T09:55:57.549181+00:00", "nick": "nikolaosk", "message": "because if you don't", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T09:56:22.845332+00:00", "nick": "nikolaosk", "message": "you might be able to just add an errback", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T09:56:31.737010+00:00", "nick": "nikolaosk", "message": "to yield these items", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T09:56:53.542465+00:00", "nick": "Teodora", "message": "well, I just need to collect a specific errback", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T09:57:04.452740+00:00", "nick": "Teodora", "message": "truth be told, I'm really new to scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T09:57:11.365624+00:00", "nick": "Teodora", "message": "I have some experience with python", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T09:57:28.262198+00:00", "nick": "Teodora", "message": "and I thought it would be much easier to create or find such crawler", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T09:57:32.483849+00:00", "nick": "Teodora", "message": "because of the community", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T09:58:43.989605+00:00", "nick": "nikolaosk", "message": "I guess there is plenty of software out there checking for broken links", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T09:59:02.721022+00:00", "nick": "nikolaosk", "message": "are you going to check your own site?", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T10:00:36.175306+00:00", "nick": "Teodora", "message": "no, i basically want to crawl the web", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T10:01:01.143278+00:00", "nick": "Teodora", "message": "there are some other, but they are not specifically designed for this and they store many other responce codes", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T10:01:05.489462+00:00", "nick": "Teodora", "message": "so, they are much slower", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T10:01:08.812492+00:00", "nick": "Teodora", "message": "and heavier", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T10:07:23.111968+00:00", "nick": "nikolaosk", "message": "look at this setting http://doc.scrapy.org/en/latest/topics/spider-m...", "links": ["http://doc.scrapy.org/en/latest/topics/spider-middleware.html?highlight=error#std:setting-HTTPERROR_ALLOW_ALL"], "channel": "scrapy"},
{"date": "2014-10-28T10:07:41.836082+00:00", "nick": "nikolaosk", "message": "maybe you need to set it to true and try again", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T10:08:55.594374+00:00", "nick": "nikolaosk", "message": "what kind of sites are you crawling?", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T10:23:16.387493+00:00", "nick": "nikolaosk", "message": "did the setting work?", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T10:34:31.422194+00:00", "nick": "Teodora", "message": "i'm checking it right now", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T10:47:59.023390+00:00", "nick": "Teodora", "message": "hmmm", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T10:48:09.644960+00:00", "nick": "Teodora", "message": "how do i exactly use it?", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T10:48:26.545257+00:00", "nick": "nikolaosk", "message": "in settings.py", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T10:48:30.130695+00:00", "nick": "nikolaosk", "message": "of your project", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T10:48:45.792329+00:00", "nick": "nikolaosk", "message": "you assign it True", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T11:00:29.346596+00:00", "nick": "Teodora", "message": "still", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T11:00:30.536323+00:00", "nick": "Teodora", "message": "not found: [Errno 11001] getaddrinfo failed.", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T11:01:58.306712+00:00", "nick": "nikolaosk", "message": "well that was quite stupid of me", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T11:02:11.098881+00:00", "nick": "nikolaosk", "message": "it's not even an http error", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T11:04:03.698770+00:00", "nick": "nikolaosk", "message": "it's on a lower layer", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T11:07:55.201568+00:00", "nick": "Teodora", "message": "yeah", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T11:08:16.191683+00:00", "nick": "Teodora", "message": "thanks for your suggestion though ;)", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T11:08:44.550798+00:00", "nick": "nikolaosk", "message": "maybe a downloader middleware would do", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T11:08:50.192650+00:00", "nick": "Teodora", "message": "http://stackoverflow.com/questions/12026707/scr...", "links": ["http://stackoverflow.com/questions/12026707/scrapy-if-request-error-then-return-item"], "channel": "scrapy"},
{"date": "2014-10-28T11:08:55.663627+00:00", "nick": "Teodora", "message": "something likethis maybe", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T11:09:43.201118+00:00", "nick": "nikolaosk", "message": "yes but you are using the crawl spider", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T11:10:14.090599+00:00", "nick": "nikolaosk", "message": "I don't know how you define errors there", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T11:10:47.218268+00:00", "nick": "nikolaosk", "message": "errbacks*", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T11:11:35.930711+00:00", "nick": "Teodora", "message": "what spider should I use so that I can define errbacks?", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T11:12:55.758665+00:00", "nick": "nikolaosk", "message": "oh wait", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T11:14:07.915931+00:00", "nick": "nikolaosk", "message": "it is possible", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T11:14:13.882490+00:00", "nick": "nikolaosk", "message": "you will have to modify this spider", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T11:15:31.266423+00:00", "nick": "nikolaosk", "message": "instead of callback", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T11:15:35.665293+00:00", "nick": "nikolaosk", "message": "define errback in the rules", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T11:15:54.297696+00:00", "nick": "nikolaosk", "message": "ehm, name errback in the rules", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T11:16:13.837611+00:00", "nick": "nikolaosk", "message": "http://doc.scrapy.org/en/latest/topics/request-...", "links": ["http://doc.scrapy.org/en/latest/topics/request-response.html?highlight=errback"], "channel": "scrapy"},
{"date": "2014-10-28T11:16:18.463091+00:00", "nick": "nikolaosk", "message": "see here for the prototype", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T11:16:27.114270+00:00", "nick": "nikolaosk", "message": "but I am not sure you can yield items", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T11:16:48.584118+00:00", "nick": "nikolaosk", "message": "if after this it doesn't work", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T11:18:32.865004+00:00", "nick": "nikolaosk", "message": "I mean, if it can't yield items", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T11:18:46.880625+00:00", "nick": "nikolaosk", "message": "there is some trick you can do", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T11:19:25.110664+00:00", "nick": "nikolaosk", "message": "so I'd suggest to go with this, errback='parse_broken_link'", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T11:20:16.403423+00:00", "nick": "nikolaosk", "message": "and a method: def parse_broken_link(self, error):", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T11:20:30.201737+00:00", "nick": "Teodora", "message": "thanks, im looking at it", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T11:20:42.981683+00:00", "nick": "nikolaosk", "message": "now I don't know what methods and attributes this error has", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T11:21:00.158912+00:00", "nick": "nikolaosk", "message": "but you can try dropping in a shell there", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T11:21:02.207830+00:00", "nick": "nikolaosk", "message": "to inspect", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T11:21:41.911443+00:00", "nick": "Teodora", "message": "2014-10-28 13:21:20+0200 [github] DEBUG: Filtered offsite request to 'ggenov.eu': <GET http://gg 2014-10-28 13:21:20+0200 [github] DEBUG: Retrying <GET http://nosuchsiteatall23.com/&gt; (failed 1 d: address 'nosuchsiteatall23.com' not found: [Errno 11001] getaddrinfo failed. 2014-10-28 13:21:20+0200 [github] DEBUG: Retrying <GET http://nosuchsiteatall23.com/&gt; (failed 2 d: address 'nosuchsiteatall23.com' not found: [Errno 11001] getaddri", "links": ["http://gg", "http://nosuchsiteatall23.com/&amp;gt", "http://nosuchsiteatall23.com/&amp;gt"], "channel": "scrapy"},
{"date": "2014-10-28T11:24:02.643245+00:00", "nick": "nikolaosk", "message": "were these supposed to be 2 lines?", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T11:24:14.344703+00:00", "nick": "Teodora", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T11:24:14.489909+00:00", "nick": "nikolaosk", "message": "http://pastebin.com/", "links": ["http://pastebin.com/"], "channel": "scrapy"},
{"date": "2014-10-28T11:25:09.193494+00:00", "nick": "nikolaosk", "message": "you may also want to enable offsite requests", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T11:25:22.497445+00:00", "nick": "nikolaosk", "message": "if you want to crawl the web and not just a single site", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T11:26:08.015121+00:00", "nick": "Teodora", "message": "http://pastebin.com/2Pw37EmX", "links": ["http://pastebin.com/2Pw37EmX"], "channel": "scrapy"},
{"date": "2014-10-28T11:26:39.560663+00:00", "nick": "nikolaosk", "message": "so, you did define an errback", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T11:26:48.194899+00:00", "nick": "nikolaosk", "message": "and a log message shows up instead", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T11:26:56.874496+00:00", "nick": "Teodora", "message": "no no", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T11:27:00.130404+00:00", "nick": "Teodora", "message": "this is the old error", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T11:27:05.428893+00:00", "nick": "nikolaosk", "message": "ah, ok", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T11:27:05.837396+00:00", "nick": "Teodora", "message": "im about to define the errback", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T11:27:11.580658+00:00", "nick": "Teodora", "message": "im reading on how to do it", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T13:30:53.921361+00:00", "nick": "wizo", "message": "hello, I'm trying out scrapy on a test website, testfire.net.  i noticed that my crawl spider does not crawl very in depth even with DEPTH_LIMIT set to 0 in settings.py. Example, it does not appear to go go all the way with this \"survey\" at  http://testfire.net/survey_questions.aspx", "links": ["http://testfire.net", "http://testfire.net/survey_questions.aspx"], "channel": "scrapy"},
{"date": "2014-10-28T13:31:39.765947+00:00", "nick": "wizo", "message": "it only appears to follow the link inside once, and that's about it. I looked at the urls and it seemed quite normal using a href", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T13:32:43.479091+00:00", "nick": "wizo", "message": "the only rule i have is \"  Rule(LinkExtractor(), callback='parse_start_url', follow=True), \"", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T13:33:12.390509+00:00", "nick": "nikolaosk", "message": "maybe the survey needs post requests for the rests", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T13:33:14.344938+00:00", "nick": "nikolaosk", "message": "rest*", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T13:34:20.495371+00:00", "nick": "wizo", "message": "mmm not really, it's all a href", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T13:34:32.653831+00:00", "nick": "wizo", "message": "for at least 3 to 4 levels", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T13:36:12.057887+00:00", "nick": "wizo", "message": "sample url \"http://testfire.net/survey_questions.aspx?step=b", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T13:36:49.390431+00:00", "nick": "wizo", "message": "it goes step=a all the way to step=d with normal href links", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T13:37:00.972163+00:00", "nick": "wizo", "message": "but i see form scrappy output it only reaches step=a", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T13:37:04.321377+00:00", "nick": "wizo", "message": "doesn't go to b,c,d", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T13:40:05.159551+00:00", "nick": "nikolaosk", "message": "the dupfilter maybe", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T13:40:24.475552+00:00", "nick": "nikolaosk", "message": "try Request(meta={'dont_filter': True}, ...", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T13:46:25.600374+00:00", "nick": "wizo", "message": "mmm where do i put the Request object?", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T13:47:45.172834+00:00", "nick": "wizo", "message": "are you suggesting that i make a request to the step=a url directly?", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T13:47:49.294866+00:00", "nick": "nikolaosk", "message": "oh, rules", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T13:47:52.762202+00:00", "nick": "nikolaosk", "message": "the crawlspider", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T13:47:55.394729+00:00", "nick": "nikolaosk", "message": "I don't know", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T13:48:03.892351+00:00", "nick": "nikolaosk", "message": "maybe in the rules", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T13:48:13.236763+00:00", "nick": "nikolaosk", "message": "I mean, put this meta dict", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T13:48:19.004509+00:00", "nick": "wizo", "message": "oh right", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T13:48:21.153756+00:00", "nick": "nikolaosk", "message": "as a kwarg in the rules", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T13:51:49.646482+00:00", "nick": "wizo", "message": "ok lemme try that", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T13:57:00.951198+00:00", "nick": "wizo", "message": "hm I can't seem to find any documentation that demonstrates don't_filter for rules..", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T14:59:01.261547+00:00", "nick": "AndyRez", "message": "can someone explain and possibly provide a solution to why whenever I pass a url like this \"http://bryanstonproperties.co.tz/houses.html\" to scrapy's start_urls list I get a \"HTTP status code is not handled or not allowed\" error", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T15:03:52.123995+00:00", "nick": "AndyRez", "message": "anyone?", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T15:44:04.744073+00:00", "nick": "abmxer", "message": "Hi guys! I need to share between several Spiders custom codes such as a Generic Item, Wrapper to insert Item into Solr, extractors, etc.", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T15:44:37.954803+00:00", "nick": "abmxer", "message": "How do I share that code, using a python package?", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T16:03:45.530476+00:00", "nick": "rodrigo5244", "message": "abmxer, with import and extending classes?", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T16:04:46.053561+00:00", "nick": "rodrigo5244", "message": "AndyRez, You can check the log for debug messages with the error code.", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T16:05:15.474568+00:00", "nick": "AndyRez", "message": "rodrigo5244: the error code is (403)", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T16:07:49.826003+00:00", "nick": "abmxer", "message": "@rodrigo5244 yes. But I need to easily distribute that code for Spiders. I don't want to place that code directly inside Spider projects.", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T16:12:06.855739+00:00", "nick": "rodrigo5244", "message": "AndyRez, try handle_httpstatus_list = [403]", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T16:12:18.682022+00:00", "nick": "rodrigo5244", "message": "http://doc.scrapy.org/en/latest/topics/spider-m...", "links": ["http://doc.scrapy.org/en/latest/topics/spider-middleware.html#module-scrapy.contrib.spidermiddleware.httperror"], "channel": "scrapy"},
{"date": "2014-10-28T16:12:56.252154+00:00", "nick": "rodrigo5244", "message": "abmxer, you could create a package using setup.py. I am not sure how it works, but allows you to import the module from any directory.", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T16:23:24.274045+00:00", "nick": "abmxer", "message": "@rodrigo5244 Yes. Scrapy Extensions seems to be more core features, while custom code for Spiders seems to make for sense distribute via python packages.", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T18:02:17.610436+00:00", "nick": "remote", "message": "http://scrapinghub.com/pricing says \"1$ per 10k pages\" -- but in my test spider I'm only shown \"requests\"", "links": ["http://scrapinghub.com/pricing"], "channel": "scrapy"},
{"date": "2014-10-28T18:02:22.975342+00:00", "nick": "remote", "message": "can someone confirm a request = a page?", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T18:04:42.134321+00:00", "nick": "mushroomed", "message": "remote: are you using Scrapinghub dashboard?", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T18:05:03.555461+00:00", "nick": "remote", "message": "yeah", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T18:05:14.295684+00:00", "nick": "remote", "message": "Job Items (126910) Requests (461) Log (56) Stats Reports", "links": [], "channel": "scrapy"},
{"date": "2014-10-28T18:06:14.710258+00:00", "nick": "mushroomed", "message": "remote: Ok, great. So pages mean succesful requests.", "links": [], "channel": "scrapy"},
{"date": "2014-10-29T00:30:43.977447+00:00", "nick": "remote", "message": "i'm surprised i can't find this info on scrapinghub how can I make sure the data I've collected remains stored there?", "links": [], "channel": "scrapy"},
{"date": "2014-10-29T00:35:28.758005+00:00", "nick": "mushroomed", "message": "remote: it remains there", "links": [], "channel": "scrapy"},
{"date": "2014-10-29T00:38:21.138930+00:00", "nick": "mushroomed", "message": "remote: you can ask Support Team, create a ticket maybe?", "links": [], "channel": "scrapy"},
{"date": "2014-10-29T13:42:59.925360+00:00", "nick": "Hobbestigrou", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2014-10-29T13:43:32.143574+00:00", "nick": "Hobbestigrou", "message": "i have only a simple question, i would to know how i can get an option from the command line in a middleware ?", "links": [], "channel": "scrapy"},
{"date": "2014-10-29T13:54:56.464010+00:00", "nick": "Hobbestigrou", "message": "or is a bad idea ?", "links": [], "channel": "scrapy"},
{"date": "2014-10-29T14:04:53.764066+00:00", "nick": "nick123", "message": "2014-10-27 17:02:04+0100 [forum-spider] ERROR: Spider error processing <GET http://somewebsite&gt;         exceptions.UnicodeEncodeError: 'ascii' codec can't encode character u'\\xab' in position 0: ordinal not in range(128)", "links": ["http://somewebsite&gt"], "channel": "scrapy"},
{"date": "2014-10-29T14:05:09.593424+00:00", "nick": "nick123", "message": "Hi guys, I'm getting this error, but there's no line number! Any idea how I might debug this?", "links": [], "channel": "scrapy"},
{"date": "2014-10-29T14:08:56.422348+00:00", "nick": "Hobbestigrou", "message": "no answer for me ?", "links": [], "channel": "scrapy"},
{"date": "2014-10-29T14:14:43.411963+00:00", "nick": "asd_", "message": "how do you guys strip white spaces when items are in dict, or lists?", "links": [], "channel": "scrapy"},
{"date": "2014-10-29T14:41:44.512282+00:00", "nick": "AndyRez", "message": "Hi all, How do I stop my scrapy spider after I get to a particular page from pagination", "links": [], "channel": "scrapy"},
{"date": "2014-10-29T14:47:24.947207+00:00", "nick": "asd_", "message": "try ctrl + c", "links": [], "channel": "scrapy"},
{"date": "2014-10-29T14:47:59.824427+00:00", "nick": "asd_", "message": "thats when you want to manualy do it.", "links": [], "channel": "scrapy"},
{"date": "2014-10-29T14:48:20.831340+00:00", "nick": "Hobbestigrou", "message": "thanks i'm stupid", "links": [], "channel": "scrapy"},
{"date": "2014-10-29T14:50:25.515860+00:00", "nick": "asd_", "message": "how do you guys strip white spaces when items are in dict, or lists?", "links": [], "channel": "scrapy"},
{"date": "2014-10-29T16:17:02.173991+00:00", "nick": "rodrigo5244", "message": "asd_, this is how I do it: response.css('h1::text').strip()", "links": [], "channel": "scrapy"},
{"date": "2014-10-29T16:17:38.726932+00:00", "nick": "rodrigo5244", "message": "one time I had to re.sub(r' +', ' ', string) to remove from the middle.", "links": [], "channel": "scrapy"},
{"date": "2014-10-29T16:36:16.804932+00:00", "nick": "AndyRez", "message": "Hi all, How do I stop my scrapy spider after I get to a particular page from pagination programmatically?", "links": [], "channel": "scrapy"},
{"date": "2014-10-29T16:37:52.837726+00:00", "nick": "Roux_taff", "message": "AndyRez: use the signals I'd say", "links": [], "channel": "scrapy"},
{"date": "2014-10-29T16:37:57.859862+00:00", "nick": "AndyRez", "message": "I want to get pages 1 - 15 alone of a particular site.. How do I let my spider be aware of that...", "links": [], "channel": "scrapy"},
{"date": "2014-10-29T16:38:01.774241+00:00", "nick": "AndyRez", "message": "signals?", "links": [], "channel": "scrapy"},
{"date": "2014-10-29T16:38:10.005413+00:00", "nick": "AndyRez", "message": "Roux_taff: signals?", "links": [], "channel": "scrapy"},
{"date": "2014-10-29T16:38:18.020161+00:00", "nick": "Roux_taff", "message": "doc.scrapy.org/en/latest/topics/signals.html", "links": ["http://doc.scrapy.org/en/latest/topics/signals.html"], "channel": "scrapy"},
{"date": "2014-10-29T16:38:21.760636+00:00", "nick": "Roux_taff", "message": "http://doc.scrapy.org/en/latest/topics/signals....", "links": ["http://doc.scrapy.org/en/latest/topics/signals.html"], "channel": "scrapy"},
{"date": "2014-10-29T16:38:25.180156+00:00", "nick": "mushroomed", "message": "AndyRez: Create a condition and then raise CloseSpider('some message here') ?", "links": [], "channel": "scrapy"},
{"date": "2014-10-29T16:39:33.501363+00:00", "nick": "Roux_taff", "message": "in yourspider init add", "links": [], "channel": "scrapy"},
{"date": "2014-10-29T16:39:33.502218+00:00", "nick": "Roux_taff", "message": "dispatcher.connect(self.monitor, item_scraped)", "links": [], "channel": "scrapy"},
{"date": "2014-10-29T16:40:05.577724+00:00", "nick": "Roux_taff", "message": "and a function monitor where you check whether your element is the one when you want to stop, then raise closeSpider", "links": [], "channel": "scrapy"},
{"date": "2014-10-29T16:40:50.405515+00:00", "nick": "AndyRez", "message": "ok would do that, thanks for responding Roux_taff", "links": [], "channel": "scrapy"},
{"date": "2014-10-29T16:41:31.637227+00:00", "nick": "Roux_taff", "message": "but as mushroomed said, you can probably just do that at the end of your parsing function", "links": [], "channel": "scrapy"},
{"date": "2014-10-30T15:42:24.612479+00:00", "nick": "rodrigo5244", "message": "In response.headers there is 'Set-Cookie': ... is there a better way to get that information? Something like response.cookies.", "links": [], "channel": "scrapy"},
{"date": "2014-10-30T15:45:15.583864+00:00", "nick": "Roux_taff", "message": "does anybody know how to rotate scrapyd's logs properly without stopping the currently running crawls neither locking any pending jobs from being started?", "links": [], "channel": "scrapy"},
{"date": "2014-10-30T16:27:36.702592+00:00", "nick": "nyov", "message": "Roux_taff: tried logrotate?", "links": [], "channel": "scrapy"},
{"date": "2014-10-30T16:28:15.338431+00:00", "nick": "nyov", "message": "can't say I ever needed that, but I'd assume if anything knows how to handle that it would be logrotate", "links": [], "channel": "scrapy"},
{"date": "2014-10-30T16:32:13.123537+00:00", "nick": "Roux_taff", "message": "nyov: yes that's my issue", "links": [], "channel": "scrapy"},
{"date": "2014-10-30T16:32:39.009420+00:00", "nick": "Roux_taff", "message": "logrotate acts on scrapyd.log whioch makes scrapyd get stuck and not start any pending job anymore", "links": [], "channel": "scrapy"},
{"date": "2014-10-30T16:36:28.231095+00:00", "nick": "nyov", "message": "have you tried with the copytruncate option?", "links": [], "channel": "scrapy"},
{"date": "2014-10-30T16:44:47.231476+00:00", "nick": "Roux_taff", "message": "yep with and without", "links": [], "channel": "scrapy"},
{"date": "2014-10-30T16:49:49.893369+00:00", "nick": "nyov", "message": "Roux_taff: hm, then I don't know. maybe something here helps http://www.chizang.net/alex/2011/06/27/python-t...", "links": ["http://www.chizang.net/alex/2011/06/27/python-twisted-log-rotation/"], "channel": "scrapy"},
{"date": "2014-10-30T16:50:18.252921+00:00", "nick": "nyov", "message": "particularly the line `kill -SIGUSR1 To force twisted to rotate its log files`", "links": [], "channel": "scrapy"},
{"date": "2014-10-30T16:50:43.461752+00:00", "nick": "nyov", "message": "maybe it could use that after/before trying to rotate logs", "links": [], "channel": "scrapy"},
{"date": "2014-10-30T16:52:00.061118+00:00", "nick": "Roux_taff", "message": "ah great thanks, there are some options I haven't tried that make sense here thx!", "links": [], "channel": "scrapy"},
{"date": "2014-10-30T16:53:51.966563+00:00", "nick": "nyov", "message": "read first, seems the 1st part is about modding twisted and the 2nd part about using logrotate instead", "links": [], "channel": "scrapy"},
{"date": "2014-10-30T16:54:11.898236+00:00", "nick": "nyov", "message": "hope it helps, I have no clue about either way", "links": [], "channel": "scrapy"},
{"date": "2014-10-30T17:01:54.733677+00:00", "nick": "Roux_taff", "message": "yep, the nocreate option sounds like what i was missing", "links": [], "channel": "scrapy"},
{"date": "2014-10-31T04:01:54.120372+00:00", "nick": "wizo", "message": "hello, does scrapy have a function in the response object that detects the presence of a form?", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T04:53:06.573632+00:00", "nick": "rabidmadman", "message": "hello anyone here? I'm trying to parse a certain page for different sites and I have on large item that I am trying to return. My confusion is how I could get the item to be global to all parse functions", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T04:54:36.595105+00:00", "nick": "rabidmadman", "message": "i wll have multiple parse functions and rather than reinitializing the item, I'm wondering if I can just do the item initialization global within the spider class?", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T10:21:56.311878+00:00", "nick": "Morto", "message": "Howdy", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T10:22:17.857715+00:00", "nick": "Morto", "message": "Can someone help me with something", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T10:22:27.981480+00:00", "nick": "Morto", "message": "I want to repace a string if", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T10:22:50.063100+00:00", "nick": "Morto", "message": "I want to replace a string if it contains ---", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T15:41:11.100528+00:00", "nick": "fpghost84", "message": "Hi when running \"scrapy crawl MySpider\" from the command line I normally get output to screen of the items scraped, but when I run scrapy from a python script(http://doc.scrapy.org/en/latest/topics/p... I don't get those items on the fly anymore", "links": ["http://script(http://doc.scrapy.org/en/latest/topics/practices.html#run-scrapy-from-a-script)"], "channel": "scrapy"},
{"date": "2014-11-02T15:41:21.798251+00:00", "nick": "fpghost84", "message": "is there anyway I can still get this output?", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T16:24:29.476127+00:00", "nick": "toothrot", "message": "fpghost84, what do you mean? why do you want the items on stdout ?", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T16:24:48.100016+00:00", "nick": "toothrot", "message": "also, enable logging if you want logging output", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T16:30:23.457964+00:00", "nick": "fpghost84", "message": "toothrot: well having the items on stdout isn't a necessity, but when running \"scrapy crawl MySpider\" they appear there (telling me written to mongo db etc etc and the data), I just wondered how to redirect this the same when running the spider from a script....it's no big deal if it is a pain though", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T16:36:21.398134+00:00", "nick": "fpghost84", "message": "By the way, on another note, I use \"log.start(); reactor.run()\" when starting my batch of spiders via my python script, do I need to shut these down at some point?", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T16:36:43.784042+00:00", "nick": "fpghost84", "message": "Generally the script runs say 5 spiders, then does some other processing, then runs another 5 spiders and so on...", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T16:47:16.547671+00:00", "nick": "fpghost84", "message": "I know reactor.stop can do it, and I know I can listen for spider close signal", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T16:47:25.143831+00:00", "nick": "fpghost84", "message": "but what about when running multiple spiders", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T16:47:41.216756+00:00", "nick": "fpghost84", "message": "I don't want the reactor to close until each batch of spiders has ran", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T17:59:53.628315+00:00", "nick": "toothrot", "message": "fpghost84, so doing log.start() (and having logging to stdout enabled) doesn't show the item logging ?", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T18:01:11.821025+00:00", "nick": "toothrot", "message": "if you have a list of your spiders, remove them from the list as they finish, or you can check them all each time you get spider_close signal, etc", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T18:22:49.672911+00:00", "nick": "fpghost84", "message": "toothrot: yes indeed even log.start and logging to stdout doesn't show items in the same way as \"scrapy crawl..\" at cmdline does, although it seems all other output is written to stdout as usual (including my own log msgs)", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T18:26:16.210085+00:00", "nick": "fpghost84", "message": "The other strange thing is stats mailer has stopped working, when I run a spider from cmdline I get an email with errors, but when run in the script like this nothing.", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T19:12:51.305573+00:00", "nick": "rabidmadman", "message": "hey, I'm trying to parse certain web pages with different parsing functions while still keeping track of the item that they're associated with. For example, I'm parsing bands, but their description is on a separate page so I need a separate parsing function for that. I've been trying to keep track of the item by doing somethign like this: meta = {'item': item} where item is the json object that I'm yielding, but this isn't working.", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T19:13:36.554324+00:00", "nick": "rabidmadman", "message": "actually the strange part is that some of the bands just get overwritten altogether with the meta dict passed into the Request", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T19:53:07.617761+00:00", "nick": "rabidmadman", "message": "Nevermind, I think I got it, I am passing in the item as teh meta dict, and then yielding response.meta['item'] in the second parsing function and it appears to be working now", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T20:18:34.141472+00:00", "nick": "fpghost84", "message": "Does anyone know why scrapy stats does not include info like `log_count/ERROR` when the crawler is invoked from a script?", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T20:18:58.085446+00:00", "nick": "fpghost84", "message": "(yet it included `log_count` stat when ran from the cmdline)", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T20:19:18.469095+00:00", "nick": "fpghost84", "message": "I have done log.start() btw, and I see the ERROR, DEBUG, INFO messages to stdout", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T21:58:02.968998+00:00", "nick": "toothrot", "message": "fpghost84, i can't answer all that because i don't use scrapy from within a script any more, but why are you using it this way?", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:00:30.576278+00:00", "nick": "fpghost84", "message": "toothrot: I solved it by running log.start(crawler=crawler) with my crawler instance in the end", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:00:57.106166+00:00", "nick": "fpghost84", "message": "I am using the log_count to feed into my error stats mailer extension to notify me if any spiders develop problems", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:01:05.593380+00:00", "nick": "toothrot", "message": "still, i'm curious why you are using it this way", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:01:23.658472+00:00", "nick": "fpghost84", "message": "why I am using error stats mailer?", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:01:29.486031+00:00", "nick": "toothrot", "message": "yeah, my spiders don't run continuously, so i just have them email at the end of the run", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:01:38.627172+00:00", "nick": "toothrot", "message": "no, running scrapy from a script", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:03:06.519523+00:00", "nick": "fpghost84", "message": "well my spiders do runner continuously in a loop. I have around 40-50 of them, and basically want to run batches of 5 (taking say 10mins) then grab data from another source (an API) compare that data to the 5 scrapy spiders' data and then do some analysis while the data is fresh", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:03:28.060494+00:00", "nick": "fpghost84", "message": "then I do the next 5 (another 10mins) regrab the control data from the API and analyse and so on", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:03:46.689314+00:00", "nick": "fpghost84", "message": "I'm struggling now with twisted reactor though", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:04:25.954598+00:00", "nick": "fpghost84", "message": "I know how to stop is (by binding to a spiders close signal) but then I want to go and restart it after I've done my analysis", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:04:29.439501+00:00", "nick": "fpghost84", "message": "it*", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:04:37.439027+00:00", "nick": "fpghost84", "message": "which isn't possible", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:04:51.631484+00:00", "nick": "toothrot", "message": "right, you can't restart the reactor", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:05:01.906561+00:00", "nick": "fpghost84", "message": "but I don't know any other way than stopping the reactor that I can get control flow back for my analysis", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:05:39.979137+00:00", "nick": "toothrot", "message": "if you", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:06:13.058520+00:00", "nick": "toothrot", "message": "are able to pastebin any code, i might be able to make a suggestion", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:06:24.310860+00:00", "nick": "fpghost84", "message": "OK give me a min", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:07:09.856720+00:00", "nick": "toothrot", "message": "fire a Deferred when your 5 crawlers complete, another when analysis is done, which can start another 5", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:08:06.112306+00:00", "nick": "fpghost84", "message": "oh, I have never heard of 'Deferred'", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:08:14.628904+00:00", "nick": "fpghost84", "message": "that sounds promising", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:08:24.307029+00:00", "nick": "toothrot", "message": "they're used inside of scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:08:39.141153+00:00", "nick": "toothrot", "message": "some of the signals even support returning them from YOUR signal handlers", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:09:03.728169+00:00", "nick": "toothrot", "message": "really you need to know about them if you're going to work in twisted (which you will have to do to integrate with scrapy)", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:09:29.360660+00:00", "nick": "fpghost84", "message": "toothrot: OK", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:09:53.597123+00:00", "nick": "fpghost84", "message": "Here is the still very unpolished script http://paste.ubuntu.com/8794432/", "links": ["http://paste.ubuntu.com/8794432/"], "channel": "scrapy"},
{"date": "2014-11-02T22:09:57.733899+00:00", "nick": "fpghost84", "message": "you can ignore most of it", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:10:16.149546+00:00", "nick": "fpghost84", "message": "setup crawler and processBatch I guess are the pertinent bits", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:10:40.294828+00:00", "nick": "toothrot", "message": "fpghost84, is that a real pass?", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:10:47.337212+00:00", "nick": "fpghost84", "message": "erm :\\", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:11:05.866720+00:00", "nick": "toothrot", "message": "i mean, best you know you posted it :(", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:11:23.325397+00:00", "nick": "fpghost84", "message": "its only local test pass if so, so no biggie", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:11:40.700363+00:00", "nick": "fpghost84", "message": "oh yeah the postgres thing is just a dummy", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:12:02.307990+00:00", "nick": "fpghost84", "message": "so for this script processBatch", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:12:12.432601+00:00", "nick": "fpghost84", "message": "xrunner is grabbing some data from the API first", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:12:25.141618+00:00", "nick": "fpghost84", "message": "then I want to loop over a batch of scrapy spiders", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:12:40.564814+00:00", "nick": "fpghost84", "message": "then finally res = call(['./findarbs.py', ]) does the post processing", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:13:13.643484+00:00", "nick": "toothrot", "message": "so... xrunner(); run_5_spiders(); post_process(); those 3 repeat forever? or is xrunner only at the beginning?", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:13:39.246441+00:00", "nick": "fpghost84", "message": "yeah, that pattern continously, the only different is the 5 spiders will change each time", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:14:00.396354+00:00", "nick": "fpghost84", "message": "until my 50 have been exhausted", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:14:03.737346+00:00", "nick": "toothrot", "message": "the program sohuld quit when there are no spiders left?", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:14:06.948606+00:00", "nick": "fpghost84", "message": "yeah", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:14:13.238182+00:00", "nick": "fpghost84", "message": "cron will do the grander looping", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:14:31.335798+00:00", "nick": "fpghost84", "message": "(every few hours)", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:14:55.661508+00:00", "nick": "fpghost84", "message": "so it's like spiders [A,B,C,D,E]; processBatch;", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:15:07.142759+00:00", "nick": "fpghost84", "message": "spiders = [F,G,H,...]; processBatch", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:15:13.346883+00:00", "nick": "fpghost84", "message": "until the script ends", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:15:42.772218+00:00", "nick": "toothrot", "message": "and the batches are independent? they don't rely on info from the batches prior or anything?", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:15:49.636040+00:00", "nick": "fpghost84", "message": "yeah", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:15:53.699420+00:00", "nick": "fpghost84", "message": "independent", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:16:21.052380+00:00", "nick": "toothrot", "message": "i'll tell you what mine are like, and what i do", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:16:56.418551+00:00", "nick": "toothrot", "message": "i have 5-6 spiders that run in the early AM, and they all write out .json files. after they are done, a post process script runs, consuming the .json files", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:17:03.955021+00:00", "nick": "toothrot", "message": "at first i was attempting what you did", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:17:41.275653+00:00", "nick": "toothrot", "message": "i don't know if you could do something similar... just have cron run each spider in it's own process, writing out it's data. then run the process script...", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:18:52.965391+00:00", "nick": "fpghost84", "message": "I actually used to do it that way for a few months, but \"upgraded\" everything to mongo, and wanted more control of clearing dbs etc from a python script instead of my bash scripts....also it seems much faster calling spiders simultaneously like this", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:19:01.481336+00:00", "nick": "toothrot", "message": "gotcha", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:19:08.222535+00:00", "nick": "toothrot", "message": "let me take a look at your script", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:19:46.395527+00:00", "nick": "fpghost84", "message": "yeah it's a little rough and ready, but hopefully you can see what's going on :)", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:21:43.234471+00:00", "nick": "fpghost84", "message": "I think I've kinda been confusing crawler setup (and crawler.start()) with the actual start of the crawlers, which seems to be reactor.run()", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:22:52.973853+00:00", "nick": "toothrot", "message": "what version of scrapy are you on?", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:23:14.680157+00:00", "nick": "toothrot", "message": "it seems older", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:23:14.892331+00:00", "nick": "fpghost84", "message": "Scrapy 0.24.4", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:23:19.014947+00:00", "nick": "toothrot", "message": "oh, guess not", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:25:10.269102+00:00", "nick": "fpghost84", "message": "but yeah I mean you can simplify this whole script by pre_processor(); loopOver(spider=[A,B,C]); post_processor(); .............pre_processor(); loopOver(spider=[D,E,F]); post_processor();.....repeat this linearly several times", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:25:24.951855+00:00", "nick": "toothrot", "message": "they've rearranged the Crawler class in upcoming code", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:25:35.227530+00:00", "nick": "toothrot", "message": "which is probably going to break your script", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:25:39.235699+00:00", "nick": "fpghost84", "message": ":s", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:25:45.517167+00:00", "nick": "toothrot", "message": "i could be wrong", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:25:57.259315+00:00", "nick": "toothrot", "message": "but there's no .configure() or .start() on Crawler on master branch", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:26:01.830106+00:00", "nick": "fpghost84", "message": "well it's not exactly working as it is, lol", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:26:11.440756+00:00", "nick": "fpghost84", "message": "I see", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:29:55.093797+00:00", "nick": "fpghost84", "message": "what's the relationship between reactor.run() and crawler.start() here exactly?", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:30:38.895138+00:00", "nick": "fpghost84", "message": "Should I setup the crawler config first, run reactor, then loop over spiders issuing crawler start at the end maybe?", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:31:10.701186+00:00", "nick": "toothrot", "message": "i'm working on an example", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:31:16.778401+00:00", "nick": "fpghost84", "message": "thanks", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:32:13.848157+00:00", "nick": "toothrot", "message": "one question, are the batches always specific spiders?", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:32:33.623098+00:00", "nick": "toothrot", "message": "or you just want some N on each iteration?", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:33:07.582080+00:00", "nick": "fpghost84", "message": "yeah, usually specific since each individual takes different amount of time to others, so I try to group in such a way that group takes 10mins", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:52:38.798083+00:00", "nick": "toothrot", "message": "hmmm", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:52:56.345334+00:00", "nick": "fpghost84", "message": "any luck?", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:54:28.792222+00:00", "nick": "toothrot", "message": "getting log messages for stopping the spider 4 times", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:54:35.009384+00:00", "nick": "toothrot", "message": "even though only one has been run", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:55:01.061600+00:00", "nick": "fpghost84", "message": "strange", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:56:03.022652+00:00", "nick": "toothrot", "message": "actually it's 5", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:56:18.485211+00:00", "nick": "toothrot", "message": "at first i thought they were all running, but i commented out the second baTCH", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:56:29.739087+00:00", "nick": "toothrot", "message": "there are only 4 total anyhow", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:56:35.286329+00:00", "nick": "toothrot", "message": "there are only 4 total spiders anyhow", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:56:44.155831+00:00", "nick": "fpghost84", "message": "send me a pastebin if you like", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:57:19.032452+00:00", "nick": "toothrot", "message": "i'm also annoyed that stdout must be redirected/captured somehow", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:57:28.782630+00:00", "nick": "toothrot", "message": "or stderr, where ever print goes", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:57:51.673416+00:00", "nick": "fpghost84", "message": "does log.msg not work?", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:58:01.493285+00:00", "nick": "toothrot", "message": "it does..", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T22:59:02.041627+00:00", "nick": "toothrot", "message": "oops.. i see now.", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:00:52.684736+00:00", "nick": "toothrot", "message": "is your signal firing for you?", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:01:09.794708+00:00", "nick": "fpghost84", "message": "which signal sorry?", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:01:23.624868+00:00", "nick": "fpghost84", "message": "so spiderclose to kill reactor?", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:01:29.309684+00:00", "nick": "toothrot", "message": "yea", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:01:34.117685+00:00", "nick": "fpghost84", "message": "yeah I think it was working", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:01:42.515004+00:00", "nick": "toothrot", "message": "my signal is not firing it seems", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:02:52.895913+00:00", "nick": "fpghost84", "message": "yeah if I connect a given crawler with crawler.signals.connect(reactor.stop, signal=signals.spider_closed) that caused the reactor to stop", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:03:11.625635+00:00", "nick": "fpghost84", "message": "but I dont know if it all works together in the version of the script I pastebinned", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:03:24.221283+00:00", "nick": "fpghost84", "message": "(with stop and last_batch bools yada yada)", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:05:05.466115+00:00", "nick": "fpghost84", "message": "are you using deferred to do all this? I'm just reading about it and it sounds very promising if I can get my head around applying it my example", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:06:10.071628+00:00", "nick": "toothrot", "message": "yea, that's where i'm sticking, i'm connecting the signal to a handler that fires a deferred when each spider stops... that handler isn't firing (or is failing somehow)", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:09:24.602214+00:00", "nick": "toothrot", "message": "spider_closed is supposed to get (spider, reason) as the args? right?", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:09:51.801669+00:00", "nick": "fpghost84", "message": "according to the docs, yes", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:11:24.637568+00:00", "nick": "toothrot", "message": "exceptions.TypeError: test_signal() takes exactly 2 arguments (0 given)", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:12:24.012289+00:00", "nick": "fpghost84", "message": "hmm", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:19:29.320098+00:00", "nick": "fpghost84", "message": "send me over a pastebin of what you have if you like, and I'll take a look (the general ideas might be useful regardless anyway)", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:19:52.171565+00:00", "nick": "toothrot", "message": "i'd like to figure out the problem", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:20:08.294116+00:00", "nick": "fpghost84", "message": "ok sure", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:20:23.052457+00:00", "nick": "toothrot", "message": "if i can't i'll let you see what i've got", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:21:36.653496+00:00", "nick": "toothrot", "message": "the dispatch lib uses some inspect black magic apparently.", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:24:36.394686+00:00", "nick": "fpghost84", "message": "hehe", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:24:57.273135+00:00", "nick": "fpghost84", "message": "wish I could be of more assistance...", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:28:03.689059+00:00", "nick": "toothrot", "message": "i had a closure around the deferred/spider and was connecting it to the signal, but I assume it was being GC'd because the dispatch lib uses weakrefs", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:32:05.761580+00:00", "nick": "fpghost84", "message": "so you managed to resolve things?", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:34:07.032014+00:00", "nick": "toothrot", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:34:08.822153+00:00", "nick": "toothrot", "message": "one sec", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:34:14.614885+00:00", "nick": "fpghost84", "message": "awesome", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:34:33.502437+00:00", "nick": "toothrot", "message": "if you want to test this as it is, get https://github.com/scrapinghub/testspiders", "links": ["https://github.com/scrapinghub/testspiders"], "channel": "scrapy"},
{"date": "2014-11-02T23:34:48.716724+00:00", "nick": "toothrot", "message": "it uses the dummyspider from it", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:35:34.140560+00:00", "nick": "fpghost84", "message": "ok cloned", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:35:51.157592+00:00", "nick": "toothrot", "message": "https://bpaste.net/show/8b179878080e", "links": ["https://bpaste.net/show/8b179878080e"], "channel": "scrapy"},
{"date": "2014-11-02T23:36:08.402508+00:00", "nick": "toothrot", "message": "that Sigh class was trying to fiddle around the signal issue", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:36:20.896233+00:00", "nick": "toothrot", "message": "obviously the tempref on the crawler, etc isn't ideal", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:36:26.406470+00:00", "nick": "fpghost84", "message": "hehe, aptly named", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:36:41.071660+00:00", "nick": "toothrot", "message": "i'm sure i'm missing something obvious", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:36:54.760341+00:00", "nick": "toothrot", "message": "i tried using just a function, but i wasn't getting the right arguments", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:37:19.510490+00:00", "nick": "toothrot", "message": "(the dispatcher inspects the method/function when trying to decide which arguments to call your handler with it appears)", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:38:57.782697+00:00", "nick": "toothrot", "message": "one thing you have to do when using twisted is to be careful where you block... with your case it's probably not critical though since you just wait for the crawlers to finish and then do some synchronous tasks", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:40:06.025775+00:00", "nick": "fpghost84", "message": "many thanks for this, I'm just reading and trying to understand it (twisted is very new to me)", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:40:18.728852+00:00", "nick": "toothrot", "message": "well, i can say a few things about it that might help", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:40:36.713381+00:00", "nick": "fpghost84", "message": "sure, please do :)", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:40:42.541464+00:00", "nick": "toothrot", "message": "@defer.inlineCallbacks uses Deferred behind the scenes for you", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:41:03.543626+00:00", "nick": "toothrot", "message": "allows you to use the yield syntax to wait on a Deferred", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:41:23.553016+00:00", "nick": "toothrot", "message": "in this case, it waits on the DeferredList() returned from processBatch", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:41:55.248396+00:00", "nick": "toothrot", "message": "(if you want to return a value from a function decorated with @defer.inlineCallbacks, you have to use defer.returnValue(someval), but we didn't need it here )", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:42:46.914165+00:00", "nick": "toothrot", "message": "within that DeferredList is all the Deferred we created for each crawler. The DeferredList fires when all the deferreds it contains have fired...", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:43:29.044754+00:00", "nick": "fpghost84", "message": "I see, makes more sense now you've said that about the DeferredList", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:43:38.222360+00:00", "nick": "toothrot", "message": "(since scrapy doens't return any deferreds for us to use, we had to create and fire one manually with the signals when each crawler was done)", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:44:22.225921+00:00", "nick": "toothrot", "message": "if you need real help with twisted, people are helpful in #twisted", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:44:27.665855+00:00", "nick": "fpghost84", "message": "So is that what Sigh is about?", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:44:28.311283+00:00", "nick": "toothrot", "message": "i'm only an amatuer myself", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:44:36.825494+00:00", "nick": "toothrot", "message": "fpghost84, partially", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:44:52.180014+00:00", "nick": "toothrot", "message": "well", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:45:08.221075+00:00", "nick": "toothrot", "message": "i used a class because of one of the problem with the signals", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:45:22.980789+00:00", "nick": "toothrot", "message": "i wasn't getting the spider, reason args when i used a function", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:45:42.593368+00:00", "nick": "toothrot", "message": "and then assigning the _tempref thing was because it was being GC'd", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:45:57.957312+00:00", "nick": "toothrot", "message": "that can probably be fixed with some further understanding", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:46:13.500020+00:00", "nick": "fpghost84", "message": "why did you want the spider, reason args?", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:46:43.553361+00:00", "nick": "toothrot", "message": "well, i think (going by memory) when you connect that signal, it can be called from any of the spiders", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:46:44.697068+00:00", "nick": "fpghost84", "message": "it is just because the close signals returns them so you have to do something with them", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:46:58.725699+00:00", "nick": "toothrot", "message": "although that might only be when you do dispatcher.connect(...)", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:47:47.817455+00:00", "nick": "fpghost84", "message": "I guess I should know this but GC'd?", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:47:53.387687+00:00", "nick": "toothrot", "message": "garbage collected", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:47:55.553106+00:00", "nick": "fpghost84", "message": "ah", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:47:56.980077+00:00", "nick": "fpghost84", "message": "hehe", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:48:07.725517+00:00", "nick": "toothrot", "message": "there would be no references to the object", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:48:25.650291+00:00", "nick": "toothrot", "message": "one sec, let me see if what i remembered about the signals is true", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:50:09.588697+00:00", "nick": "toothrot", "message": "okay, if you connect the signal like this: dispatcher.connect(s.done, signals.spider_closed)", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:50:28.755484+00:00", "nick": "toothrot", "message": "then every spider will cause every connected handler to be called", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:50:39.263203+00:00", "nick": "fpghost84", "message": "right", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:51:16.709818+00:00", "nick": "fpghost84", "message": "but crawler.signals.connect(...) only the spider associated with crawler causes the handler to be called/", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:51:17.934710+00:00", "nick": "fpghost84", "message": "?", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:51:20.517801+00:00", "nick": "toothrot", "message": "right", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:51:29.426215+00:00", "nick": "toothrot", "message": "so we can probably change this around to remove the check", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:51:33.800007+00:00", "nick": "toothrot", "message": "comparing the spiders", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:51:39.945400+00:00", "nick": "toothrot", "message": "actually you might even be able to do...", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:53:15.229372+00:00", "nick": "toothrot", "message": "https://bpaste.net/show/e5d77add8e36", "links": ["https://bpaste.net/show/e5d77add8e36"], "channel": "scrapy"},
{"date": "2014-11-02T23:53:30.215744+00:00", "nick": "toothrot", "message": "seems to work", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:53:44.908183+00:00", "nick": "toothrot", "message": "just be careful if you start running more than one spider per crawler", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:53:53.394291+00:00", "nick": "toothrot", "message": "(can you do that anymore?)", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:54:38.465286+00:00", "nick": "fpghost84", "message": "Not sure, but I won't be doing it at any rate...", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:54:43.509426+00:00", "nick": "toothrot", "message": "you probaly want to handle spider_closed more robustly though, detect errors", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:54:47.944107+00:00", "nick": "fpghost84", "message": "could you even use a lambda func?", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:54:58.428777+00:00", "nick": "toothrot", "message": "you could but you still need to keep a reference to it", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:55:39.234544+00:00", "nick": "toothrot", "message": "maybe handle the case where the spider fails and call errback on the deferred", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:55:47.820122+00:00", "nick": "fpghost84", "message": "so if you don't keep that reference in tempref, the function foo is just garbage collected away...", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:56:06.584085+00:00", "nick": "toothrot", "message": "fpghost84, it appears so, the signals lib only keeps a weakref", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:56:34.754517+00:00", "nick": "toothrot", "message": "usually it's probably not an issue, since most people probably use methods on their spiders for the handlers (which scrapy has a reference to)", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:56:54.917727+00:00", "nick": "fpghost84", "message": "yeah, I'll look into errback, but to be honest spiders failing for non \"finished\" reasons are handled by some other bits I have like error stats mailer extension, so I think (hope) this should be good for me", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:57:11.841400+00:00", "nick": "toothrot", "message": "well, test a failure anyhow to make sure it does not hang", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:57:22.459345+00:00", "nick": "fpghost84", "message": "ok", "links": [], "channel": "scrapy"},
{"date": "2014-11-02T23:59:32.240501+00:00", "nick": "fpghost84", "message": "just out of interest does it need to be callback(True), or is that arbritrary? could it equally be callback(someOtherFunc)?", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T00:01:12.142109+00:00", "nick": "fpghost84", "message": "(I know want to do that, just wondering if that matters is some callback executes and returns)", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T00:01:17.248966+00:00", "nick": "fpghost84", "message": "know=don't*", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T00:04:50.511754+00:00", "nick": "toothrot", "message": "fpghost84, that's the result to be passed to any callbacks added to the Deferred", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T00:05:06.364021+00:00", "nick": "toothrot", "message": "in this case it doesn't matter becasue we don't use it", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T00:05:25.619785+00:00", "nick": "toothrot", "message": "it could've been None, False, 'foo', whatever", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T00:06:51.729219+00:00", "nick": "fpghost84", "message": "I see thanks", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T00:08:03.493938+00:00", "nick": "fpghost84", "message": "many thanks for all this", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T00:08:16.580754+00:00", "nick": "fpghost84", "message": "just putting my real code in to test everything", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T00:43:00.054584+00:00", "nick": "toothrot", "message": "no problem, let me know how it goes", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T00:54:49.360753+00:00", "nick": "fpghost84", "message": "toothrot: after the yield does the next batch of spiders start right away? (i.e. whilst the post-processing in your \"do something else...\" is ongoing?)", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T01:22:48.539932+00:00", "nick": "fpghost84", "message": "toothrot: I seem to end up hitting errors like [ESMTPSender,client] ERROR:", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T01:22:55.214467+00:00", "nick": "fpghost84", "message": "hmm", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T01:24:39.884453+00:00", "nick": "fpghost84", "message": "first spider runs fine, next is torrade of those messages", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T07:44:30.135116+00:00", "nick": "Zladivliba", "message": "hello scrapy !!!", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T07:44:40.313538+00:00", "nick": "Zladivliba", "message": "anywone knows why scrapy crashes with this : sgmllib.SGMLParseError: expected name token at '<!(?:\\\\[CDATA\\\\[|\\\\-\\\\-)'", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T07:44:45.810622+00:00", "nick": "Zladivliba", "message": "I have it A LOT", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:23:28.930814+00:00", "nick": "blusteal", "message": "Zladivliba bad html, probably someone put it there to stop your bot from scraping their site", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:23:54.837729+00:00", "nick": "Zladivliba", "message": "blusteal: hummm ??? ok how do I prevent scrapy from crashing ?", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:24:22.386628+00:00", "nick": "blusteal", "message": "well since that's a sgmllib file", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:24:41.320725+00:00", "nick": "blusteal", "message": "you'd have have to go look at the documentation", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:24:55.955733+00:00", "nick": "blusteal", "message": "it's just the way SGML parses the document", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:25:11.214262+00:00", "nick": "Zladivliba", "message": "ok so there's pretty much noting that I can do...", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:25:13.940176+00:00", "nick": "blusteal", "message": "you can try a different parser on the response", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:25:22.025188+00:00", "nick": "Zladivliba", "message": "ah ?", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:25:31.139714+00:00", "nick": "blusteal", "message": "if you really need the info you're going to have to work for it", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:25:43.037336+00:00", "nick": "Zladivliba", "message": "ok, no not really", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:25:53.559699+00:00", "nick": "blusteal", "message": "your using the scrapy SGML parser, i think the docs says to update to the newer parser", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:25:53.804724+00:00", "nick": "Zladivliba", "message": "I just need to manage the error actually", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:26:02.816489+00:00", "nick": "Zladivliba", "message": "ah ok ???", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:26:06.702541+00:00", "nick": "Zladivliba", "message": "I didn't know about that", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:26:09.514840+00:00", "nick": "blusteal", "message": "the lxml parser", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:26:11.078048+00:00", "nick": "Zladivliba", "message": "what's the new parser ?", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:26:12.574147+00:00", "nick": "Zladivliba", "message": "ahhhh", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:26:13.397728+00:00", "nick": "Zladivliba", "message": "ok", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:26:18.793116+00:00", "nick": "Zladivliba", "message": "yearh I've read about that", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:26:24.589984+00:00", "nick": "Zladivliba", "message": "but I coudl'nt find much information about it", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:26:40.790525+00:00", "nick": "Zladivliba", "message": "u mean I just need to replace the name of the parser ?", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:26:50.899001+00:00", "nick": "blusteal", "message": "http://doc.scrapy.org/en/latest/topics/link-ext...", "links": ["http://doc.scrapy.org/en/latest/topics/link-extractors.html"], "channel": "scrapy"},
{"date": "2014-11-03T09:27:01.725081+00:00", "nick": "blusteal", "message": "warning: SGMLParser based link extractors are unmantained and its usage is discouraged. It is recommended to migrate to LxmlLinkExtractor if you are still using SgmlLinkExtractor.", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:27:15.583744+00:00", "nick": "blusteal", "message": "so use the lxmllinkextractor", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:28:01.265649+00:00", "nick": "blusteal", "message": "if lxml still chokes on the code, you'll have to do some regex or check if that string of characters is in the response then handle it some other way", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:28:08.025192+00:00", "nick": "blusteal", "message": "skip that page or something", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:28:58.512776+00:00", "nick": "Zladivliba", "message": "ok", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:29:07.546156+00:00", "nick": "Zladivliba", "message": "if", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:29:36.590751+00:00", "nick": "Zladivliba", "message": "I replace the second line by the first line will it work the same way ? : https://gist.github.com/anonymous/7917590fbba4f...", "links": ["https://gist.github.com/anonymous/7917590fbba4f931c9fc"], "channel": "scrapy"},
{"date": "2014-11-03T09:30:42.487066+00:00", "nick": "blusteal", "message": "give it a shot", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:30:55.348978+00:00", "nick": "blusteal", "message": "it should work, i've only ever used lxml", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:31:01.287671+00:00", "nick": "blusteal", "message": "and that looks like the similar pattern", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:31:02.905712+00:00", "nick": "blusteal", "message": "hold on", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:32:04.011584+00:00", "nick": "Zladivliba", "message": "NameError: name 'LxmlLinkExtractor' is not defined", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:32:08.593986+00:00", "nick": "Zladivliba", "message": "hummm", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:32:11.101239+00:00", "nick": "blusteal", "message": "looking at some old code I have it looks similar enough", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:33:01.175960+00:00", "nick": "blusteal", "message": "rules = (Rule(LinkExtractor(allow=(),deny=('monster.com',)),callback='parse_item'),)", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:33:12.780128+00:00", "nick": "blusteal", "message": "this was code I had in a crawl spider", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:33:46.647320+00:00", "nick": "Zladivliba", "message": "ok", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:33:51.154327+00:00", "nick": "Zladivliba", "message": "how do you import it ?", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:34:11.340259+00:00", "nick": "Zladivliba", "message": "cause it's not working and the doc doesn't say (or at least what it says doesn't work...\u00e0)", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:34:51.618548+00:00", "nick": "blusteal", "message": "from scrapy.contrib.linkextractors import LinkExtractor", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:35:01.514096+00:00", "nick": "blusteal", "message": "from scrapy.contrib.spiders import CrawlSpider, Rule", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:35:21.380733+00:00", "nick": "blusteal", "message": "for the rules = (Rule())", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:36:33.360894+00:00", "nick": "Zladivliba", "message": "NameError: name 'LxmlLinkExtractor' is not defined", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:36:36.936602+00:00", "nick": "Zladivliba", "message": "here's what I get", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:36:44.527183+00:00", "nick": "Zladivliba", "message": "I added what u used", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:36:47.557052+00:00", "nick": "Zladivliba", "message": "what u said", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:37:07.403783+00:00", "nick": "Zladivliba", "message": "Rule(LxmlLinkExtractor(allow=(), allow_domains=allowed_domains , tags=('a', 'area', 'frame'), attrs=('href','src')), callback='parse_item'),", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:37:08.148831+00:00", "nick": "Zladivliba", "message": "then", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:37:14.974813+00:00", "nick": "Zladivliba", "message": "but I get errors", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:37:28.078964+00:00", "nick": "blusteal", "message": "LinkExtractor", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:37:40.087295+00:00", "nick": "blusteal", "message": "no lxml", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:37:59.240965+00:00", "nick": "blusteal", "message": "from scrapy.contrib.linkextractors import LinkExtractor", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:38:28.533596+00:00", "nick": "Zladivliba", "message": "ah ok I get it !!!", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:39:07.990754+00:00", "nick": "blusteal", "message": "good", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:39:46.686307+00:00", "nick": "Zladivliba", "message": "ok cool !", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:39:49.650961+00:00", "nick": "Zladivliba", "message": "seems to work ;)", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:40:00.415664+00:00", "nick": "Zladivliba", "message": "i'm going to try the other website that crashed originally", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:40:07.152367+00:00", "nick": "Zladivliba", "message": "thanks a LOT for ur kind help ;)", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:41:01.449673+00:00", "nick": "blusteal", "message": "no problem", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:41:06.111269+00:00", "nick": "blusteal", "message": "let us know how it works", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:44:06.862326+00:00", "nick": "Zladivliba", "message": "ok works fine now ;\u00e0", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:44:08.543399+00:00", "nick": "Zladivliba", "message": ";)", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:46:56.296874+00:00", "nick": "blusteal", "message": "great", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T09:47:00.173208+00:00", "nick": "blusteal", "message": "glad to help", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T21:02:22.422254+00:00", "nick": "rabidmadman", "message": "Hmm I'm running my spider and everything is going out to the termianl, but it's not writing to a json file", "links": [], "channel": "scrapy"},
{"date": "2014-11-03T21:02:38.904229+00:00", "nick": "rabidmadman", "message": "been trying to troubleshoot this for  a while and have no idea what's going on :(", "links": [], "channel": "scrapy"},
{"date": "2014-11-04T00:01:13.703448+00:00", "nick": "fpghost84", "message": "Anyone know how I can count the number of ERRORs logged using scrapy log when I invoke multiple spiders from a script", "links": [], "channel": "scrapy"},
{"date": "2014-11-04T00:01:58.726279+00:00", "nick": "fpghost84", "message": "(I know normally you get log_count/ERROR for a single crawer when log.start(crawler=MyCrawler), but it's not so obvious with multiple spiders running at once in same process", "links": [], "channel": "scrapy"},
{"date": "2014-11-04T06:50:10.960633+00:00", "nick": "Richie_", "message": "hi all", "links": [], "channel": "scrapy"},
{"date": "2014-11-04T06:50:31.377903+00:00", "nick": "Richie_", "message": "I'm new to scrapy - but wanted to give it a go", "links": [], "channel": "scrapy"},
{"date": "2014-11-04T06:50:44.799746+00:00", "nick": "Richie_", "message": "I'm looking to spider only a part of a website", "links": [], "channel": "scrapy"},
{"date": "2014-11-04T06:50:50.657679+00:00", "nick": "Richie_", "message": "ie a subfolder", "links": [], "channel": "scrapy"},
{"date": "2014-11-04T06:50:54.105076+00:00", "nick": "Richie_", "message": "is this possible in scrapy?", "links": [], "channel": "scrapy"},
{"date": "2014-11-04T06:52:03.982228+00:00", "nick": "Richie_", "message": "Could anybody suggest a good book for scrapy beginners?", "links": [], "channel": "scrapy"},
{"date": "2014-11-04T06:54:37.370952+00:00", "nick": "Richie_", "message": "Any idea what this means: NameError: name 'scrapy' is not defined", "links": [], "channel": "scrapy"},
{"date": "2014-11-04T07:36:23.852079+00:00", "nick": "Richie_", "message": "ok guys - got it working", "links": [], "channel": "scrapy"},
{"date": "2014-11-04T07:36:35.209216+00:00", "nick": "Richie_", "message": "anyone about to help me write a basic scraper?", "links": [], "channel": "scrapy"},
{"date": "2014-11-04T08:24:40.576495+00:00", "nick": "Research", "message": "hello everyone!", "links": [], "channel": "scrapy"},
{"date": "2014-11-04T18:01:24.272740+00:00", "nick": "leonhaynes", "message": "Is it common practice to create a scrapy project for each spider?", "links": [], "channel": "scrapy"},
{"date": "2014-11-04T18:36:07.871239+00:00", "nick": "nramirezuy", "message": "@leonhaynes no", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T00:29:40.614334+00:00", "nick": "rabidmadman", "message": "Hey, i've been at a bit of a block for a few hours now. After parsing all of my web pages, I'm struggling with figuring out when to yield my overall item", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T00:30:00.156716+00:00", "nick": "rabidmadman", "message": "i yield a Request object to every parse function, but at some point i need to yield the overall item", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T00:30:08.208242+00:00", "nick": "rabidmadman", "message": "and this needs to be within a function", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T00:33:03.485093+00:00", "nick": "toothrot", "message": "rabidmadman, what's the issue?", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T00:33:15.000452+00:00", "nick": "toothrot", "message": "you can yield Requests or Items", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T00:33:23.522054+00:00", "nick": "rabidmadman", "message": "so basically, I am parsing quite a few pages", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T00:33:33.990113+00:00", "nick": "rabidmadman", "message": "and one of hte pages is a subset of a previous item", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T00:34:16.548756+00:00", "nick": "rabidmadman", "message": "so i don't want to yield the item in the parse function that is yielding a subset of the other", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T00:39:18.025126+00:00", "nick": "rabidmadman", "message": "so i don't want to yield a request and an item in the same function because then it would terminate prematurely", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T00:39:35.949119+00:00", "nick": "toothrot", "message": "what would terminate ?", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T00:39:58.303537+00:00", "nick": "rabidmadman", "message": "so i'm basically populating band info. Each band has a release...each release has lyrics. Each come with a separate url", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T00:40:17.612308+00:00", "nick": "rabidmadman", "message": "the lyrics associated to each song", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T00:40:35.704391+00:00", "nick": "rabidmadman", "message": "so i don't want to yield the item once I set the lyrics", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T00:42:37.928579+00:00", "nick": "toothrot", "message": "you want to yield it ... ?", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T00:44:34.732175+00:00", "nick": "cornjuliox", "message": "rabidmadman: share the url?", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T00:44:35.698497+00:00", "nick": "rabidmadman", "message": "i want to yield the item once all is done yeah, but  I yield both a request object in a for loop, and then once that for loop is done, I try to yield the item. However, this fails and it terminates prematurely", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T00:44:46.526893+00:00", "nick": "rabidmadman", "message": "yeah sure", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T00:45:03.297302+00:00", "nick": "toothrot", "message": "rabidmadman, well, yielding the requests only schedules the requests", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T00:45:05.420114+00:00", "nick": "cornjuliox", "message": "and pastebin the code if you can", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T00:45:11.713771+00:00", "nick": "toothrot", "message": "so when the loop is done you've only scheduled some requests", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T00:45:20.878491+00:00", "nick": "toothrot", "message": "you don't have the replies yet.", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T00:46:39.286543+00:00", "nick": "rabidmadman", "message": "ok well here are some example of urls:", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T00:46:39.458201+00:00", "nick": "rabidmadman", "message": "band page: http://www.metal-archives.com/bands/Leeway/20277", "links": ["http://www.metal-archives.com/bands/Leeway/20277"], "channel": "scrapy"},
{"date": "2014-11-05T00:46:39.602058+00:00", "nick": "rabidmadman", "message": "release: http://www.metal-archives.com/albums/Leeway/Bor...", "links": ["http://www.metal-archives.com/albums/Leeway/Born_to_Expire/46858"], "channel": "scrapy"},
{"date": "2014-11-05T00:46:39.602194+00:00", "nick": "rabidmadman", "message": "lyrics: http://www.metal-archives.com/release/ajax-view...", "links": ["http://www.metal-archives.com/release/ajax-view-lyrics/id/353925"], "channel": "scrapy"},
{"date": "2014-11-05T00:46:59.103744+00:00", "nick": "rabidmadman", "message": "i can show the code too, one second", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T00:50:05.152542+00:00", "nick": "rabidmadman", "message": "damn pastebin/gist not auto indenting :(", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T00:50:26.644946+00:00", "nick": "cornjuliox", "message": "ouch :-p i hate it when that happens", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T00:53:55.678625+00:00", "nick": "rabidmadman", "message": "alright well this is ugly", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T00:53:56.740119+00:00", "nick": "rabidmadman", "message": "http://pastebin.com/raw.php?i=mQswkmYb", "links": ["http://pastebin.com/raw.php?i=mQswkmYb"], "channel": "scrapy"},
{"date": "2014-11-05T00:54:23.591312+00:00", "nick": "rabidmadman", "message": "that's not the entire code, but it's what i've done in recent times", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T00:54:28.190951+00:00", "nick": "cornjuliox", "message": "so basically you need the bands, their complete discographies, and the lyrics for all the songs on each?", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T00:54:32.899064+00:00", "nick": "rabidmadman", "message": "yeah", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T00:54:41.785420+00:00", "nick": "rabidmadman", "message": "and i need to use those three urls", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T00:54:50.466472+00:00", "nick": "rabidmadman", "message": "so up until this point, i was doing well", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T00:55:06.579608+00:00", "nick": "rabidmadman", "message": "but now I\"m struggling with how to approach this since lyrics are a subset of each song", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T00:55:17.757801+00:00", "nick": "cornjuliox", "message": "urls being: first to the bands page, then to each album, and then to each song lyric", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T00:55:19.403927+00:00", "nick": "rabidmadman", "message": "but they belong to a separate page", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T00:55:21.931678+00:00", "nick": "cornjuliox", "message": "how are your items set up?", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T00:55:29.032771+00:00", "nick": "rabidmadman", "message": "oh man..pretty nested", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T00:55:44.966233+00:00", "nick": "rabidmadman", "message": "i can make nother pastebin for a band", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T00:55:45.942042+00:00", "nick": "rabidmadman", "message": "one moment", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T00:56:54.017471+00:00", "nick": "cornjuliox", "message": "k", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T00:58:05.257705+00:00", "nick": "rabidmadman", "message": "just take a deep breath before you look at this", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T00:58:07.121361+00:00", "nick": "rabidmadman", "message": "http://pastebin.com/raw.php?i=mNeFDA8Q", "links": ["http://pastebin.com/raw.php?i=mNeFDA8Q"], "channel": "scrapy"},
{"date": "2014-11-05T00:59:30.726979+00:00", "nick": "cornjuliox", "message": "no, i meant your items.py file (if you have one)", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:00:31.988585+00:00", "nick": "rabidmadman", "message": "oh ok", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:01:38.026616+00:00", "nick": "cornjuliox", "message": "looking at the site, the way I'd do it is that I'd have a BandItem, an AlbumItem (representing one item in their discography), and a SongItem, since bands have multiple albums, which have multiple songs,", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:01:47.359957+00:00", "nick": "cornjuliox", "message": "let me see if i can pastebin", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:01:48.889128+00:00", "nick": "cornjuliox", "message": "hang on", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:02:42.395690+00:00", "nick": "rabidmadman", "message": "my items.py", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:02:42.841818+00:00", "nick": "rabidmadman", "message": "http://pastebin.com/raw.php?i=06zLvsZB", "links": ["http://pastebin.com/raw.php?i=06zLvsZB"], "channel": "scrapy"},
{"date": "2014-11-05T01:04:02.400448+00:00", "nick": "rabidmadman", "message": "Hmm that would have probably been the way to go..I\"ve basically created the item just by setting it to a list or dictionary", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:04:16.629215+00:00", "nick": "rabidmadman", "message": "because i watned one item per band", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:04:41.069353+00:00", "nick": "cornjuliox", "message": "this is how i'd do the items: http://pastebin.com/bGsYdC1b", "links": ["http://pastebin.com/bGsYdC1b"], "channel": "scrapy"},
{"date": "2014-11-05T01:05:08.092316+00:00", "nick": "cornjuliox", "message": "i did osmething similar in the past, i had to scrape a company directory, and get each company and all their addresses", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:05:38.517661+00:00", "nick": "cornjuliox", "message": "but with this one all the info you need are in different pages", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:07:19.819654+00:00", "nick": "rabidmadman", "message": "making separate items is much cleaner", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:07:26.101106+00:00", "nick": "rabidmadman", "message": "but i am not sure how it would solve my problem", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:09:11.223183+00:00", "nick": "cornjuliox", "message": "hang on let me think about this", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:09:51.211046+00:00", "nick": "rabidmadman", "message": "like right now i get all the way up to the lyrics", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:09:57.209600+00:00", "nick": "rabidmadman", "message": "but i odn't want to yield the item there", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:10:00.616459+00:00", "nick": "rabidmadman", "message": "or do i?", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:10:19.613555+00:00", "nick": "rabidmadman", "message": "once i get the lyrics for each track, the item is complete since the lyrics are hte last thign i parse", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:10:33.438036+00:00", "nick": "rabidmadman", "message": "but i odn't want to yield the item in my lyrics function because it still has to go to the next tracks", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:10:52.889094+00:00", "nick": "cornjuliox", "message": "ah, i noticed that you're not using xpath either", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:11:16.840030+00:00", "nick": "rabidmadman", "message": "yeah because I am lame :(", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:11:33.286925+00:00", "nick": "rabidmadman", "message": "i'm not too concerned about performance", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:12:01.723843+00:00", "nick": "rabidmadman", "message": "i already wrote a nonconcurrent 'scraper' just using pythons requests and beautifulsoup but it was going to take 7 days to run", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:12:15.379988+00:00", "nick": "rabidmadman", "message": "after spending hours upon hours parsing all of these pages with soup, i was not too keen on starting over haha", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:12:15.523556+00:00", "nick": "cornjuliox", "message": "are you searching for bands alphabetically?", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:12:38.659674+00:00", "nick": "rabidmadman", "message": "yeah", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:12:41.576426+00:00", "nick": "rabidmadman", "message": "but in this case, just Q", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:12:56.066083+00:00", "nick": "rabidmadman", "message": "there are only 166 bands in Q and i'm not trying to swarm them right now", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:14:42.188611+00:00", "nick": "cornjuliox", "message": "ok", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:15:13.000120+00:00", "nick": "rabidmadman", "message": "i can do every letter thats no problem", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:15:18.866814+00:00", "nick": "rabidmadman", "message": "already have that taken care of", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:15:29.222766+00:00", "nick": "rabidmadman", "message": "its just figuring out when/how to yield the item when its complete", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:16:01.925377+00:00", "nick": "rabidmadman", "message": "like in here for example:", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:16:02.593320+00:00", "nick": "rabidmadman", "message": "http://pastebin.com/raw.php?i=mQswkmYb", "links": ["http://pastebin.com/raw.php?i=mQswkmYb"], "channel": "scrapy"},
{"date": "2014-11-05T01:16:07.242460+00:00", "nick": "rabidmadman", "message": "in that second function", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:16:22.547495+00:00", "nick": "rabidmadman", "message": "if i yield the item outside of the foor loop (after i yield all the request objects) it won't work right?", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:18:36.430323+00:00", "nick": "cornjuliox", "message": "so you're making a dict with the discography in it, and then appending that to an item, and then passing hte item from one callback to the next via the meta dictionary", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:18:39.369468+00:00", "nick": "cornjuliox", "message": "if i'm reading this right?", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:21:31.995691+00:00", "nick": "rabidmadman", "message": "yeah", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:26:06.710363+00:00", "nick": "cornjuliox", "message": "hang on i think there's a better way to do this", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:26:37.335827+00:00", "nick": "rabidmadman", "message": "damn thanks for your help dude", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:35:18.015450+00:00", "nick": "cornjuliox", "message": "hmm this is trickier than i thought", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:35:51.443303+00:00", "nick": "rabidmadman", "message": ":[", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:36:03.771261+00:00", "nick": "rabidmadman", "message": "so close....yet so far :(", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:36:35.765369+00:00", "nick": "rabidmadman", "message": "like i literally have everything else done besides individual releases and i didn't foresee this roadblock", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:41:37.242192+00:00", "nick": "cornjuliox", "message": "ok, so i dont have any actual code, but I think the best way to do this is like this: create a class dictionary that contains band names as keys, and dictionaries containing their info as the values. that way you don't have to worry about passing the dict between callbacks", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:42:08.752102+00:00", "nick": "rabidmadman", "message": "i also just thought about making the dict global", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:42:09.454988+00:00", "nick": "cornjuliox", "message": "and then just pass the name of the band between callbacks to use as the key", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:42:19.128297+00:00", "nick": "rabidmadman", "message": "i see", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:42:22.017775+00:00", "nick": "rabidmadman", "message": "that's a good idea", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:42:31.190366+00:00", "nick": "rabidmadman", "message": "i solved the dict passing between callbacks by just making the dict global", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:42:36.556863+00:00", "nick": "rabidmadman", "message": "but going by the band is a great idea...", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:42:57.997824+00:00", "nick": "cornjuliox", "message": "so for each song you'd issue a request", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:43:17.998230+00:00", "nick": "cornjuliox", "message": "and in the callback that handles them, grab the name from response.meta", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:43:22.628289+00:00", "nick": "rabidmadman", "message": "yeah", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:43:32.688347+00:00", "nick": "rabidmadman", "message": "i will have to completely redo my code", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:43:37.695862+00:00", "nick": "rabidmadman", "message": "that is what i didn't want to hear lol", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:43:50.839398+00:00", "nick": "rabidmadman", "message": "but yeah that is the best route", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:44:09.820393+00:00", "nick": "cornjuliox", "message": "and then do self.bands[name][album][<song>]", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:44:16.614302+00:00", "nick": "cornjuliox", "message": "that's how id' do iti", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:44:25.537485+00:00", "nick": "cornjuliox", "message": "its also worth it to learn xpath", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:45:03.048976+00:00", "nick": "cornjuliox", "message": "its not just for performance, it makes things a whole lot easier to follow", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:45:46.551904+00:00", "nick": "rabidmadman", "message": "for me, it makes things harder to follow because i don't know xpath lol", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:46:15.276933+00:00", "nick": "rabidmadman", "message": "i'm kind of under pressure to finish this soon which is why i was not interested in learning xpath and reparsing all the pages", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:46:37.988558+00:00", "nick": "rabidmadman", "message": "i spent a significant amount of time parsing all the pages with soup, which is my only reason for not switching to xpath", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:47:09.659688+00:00", "nick": "cornjuliox", "message": "did your spider really run for 7 days parsing all the pages?", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:47:23.092630+00:00", "nick": "rabidmadman", "message": "no", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:47:28.177588+00:00", "nick": "rabidmadman", "message": "i closed it after one letter", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:47:42.825990+00:00", "nick": "rabidmadman", "message": "i just gave it a rough estimate based on the time for that letter", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:48:09.630371+00:00", "nick": "rabidmadman", "message": "it was outside of scrapy though..just a python script", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:48:54.020548+00:00", "nick": "rabidmadman", "message": "was going to try to learn twisted, then discovered scrapy and started over", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:49:53.470652+00:00", "nick": "cornjuliox", "message": "so you run the script on one letter at a time?", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:52:27.691012+00:00", "nick": "rabidmadman", "message": "only for testing purposes", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:52:51.305142+00:00", "nick": "cornjuliox", "message": "the way I was looking at this the spider would grab all the letters from the alphabetical index, and then yield requests for each of them. then on the index page i grab all the links to each band and yield requests for each of those", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:52:53.199765+00:00", "nick": "cornjuliox", "message": "and so on", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:53:07.040123+00:00", "nick": "rabidmadman", "message": "yeah i have that all taken care of", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:53:13.905429+00:00", "nick": "rabidmadman", "message": "my only issue is with releases", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:53:31.820507+00:00", "nick": "rabidmadman", "message": "im doing pretty much what you said", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:53:46.022143+00:00", "nick": "rabidmadman", "message": "then each band has release links..then each release has lyrics links", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:53:49.564763+00:00", "nick": "rabidmadman", "message": "for each song", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:53:57.134504+00:00", "nick": "rabidmadman", "message": "thats where the problem starts to occur for me", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:58:35.197685+00:00", "nick": "cornjuliox", "message": "yeah, nested dictionaries should take care of that. and then after the lyrics are parsed you can yield the whole item", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T01:59:27.479084+00:00", "nick": "rabidmadman", "message": "yeah so currently, for some reason it would run and only yield 13 bands", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T02:07:58.340129+00:00", "nick": "toothrot", "message": "and how are you detecting when all the releases have been crawled?", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T02:09:09.330426+00:00", "nick": "rabidmadman", "message": "i store all of them in a list", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T02:09:12.388167+00:00", "nick": "rabidmadman", "message": "all of their urls", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T02:09:16.707504+00:00", "nick": "rabidmadman", "message": "and then i yield requests for each one", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T02:34:41.856257+00:00", "nick": "toothrot", "message": "rabidmadman, that's unrelated", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T02:34:43.730394+00:00", "nick": "toothrot", "message": "for a single item", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T02:34:52.504455+00:00", "nick": "toothrot", "message": "how do you know when you have all the releases, all the lyrics.", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T02:35:18.680809+00:00", "nick": "rabidmadman", "message": "good question", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T02:37:41.756691+00:00", "nick": "toothrot", "message": "i mean, if there was only one, you could just handle it in your lyrics callback", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T02:37:57.568270+00:00", "nick": "toothrot", "message": "but you have to make each leaf of your item is fully populated", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T02:38:00.985431+00:00", "nick": "toothrot", "message": "make sure*", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T02:38:26.137138+00:00", "nick": "toothrot", "message": "i've done it a couple ways", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T02:38:57.453015+00:00", "nick": "toothrot", "message": "one is just crawl everything and then yield all the items when the crawler is done", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T02:41:53.212174+00:00", "nick": "rabidmadman", "message": "i see", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T02:41:54.857335+00:00", "nick": "toothrot", "message": "the other is wrap the all the requests in a Deferred, and use a DeferredList to know when they are done", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T02:42:14.417522+00:00", "nick": "rabidmadman", "message": "hmm don't know much about the second option", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T02:42:24.653300+00:00", "nick": "rabidmadman", "message": "cornjuliox basically recommended the first option", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T02:42:56.872545+00:00", "nick": "toothrot", "message": "yeah, it's the simplest solution, therefore probably the best", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T02:43:15.132349+00:00", "nick": "toothrot", "message": "if you don't have a long running spider where you need the items as it runs, it's the way to go IMO", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T02:46:56.774790+00:00", "nick": "rabidmadman", "message": "the first option?", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T02:46:58.639763+00:00", "nick": "rabidmadman", "message": "multiple items?", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T02:47:12.078517+00:00", "nick": "rabidmadman", "message": "but i want one item per band though", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T02:47:22.641505+00:00", "nick": "rabidmadman", "message": "so i guess set the key for my item to be another item?", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T02:47:28.288189+00:00", "nick": "rabidmadman", "message": "the value*", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T02:49:29.207839+00:00", "nick": "toothrot", "message": "eh?", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T02:49:37.701681+00:00", "nick": "toothrot", "message": "just pass the item through meta", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T02:50:12.338552+00:00", "nick": "toothrot", "message": "that's how i would do it, but creating a dict attribute on the spider an populating that works too", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T02:50:21.914265+00:00", "nick": "toothrot", "message": "either way you have to keep them in a list until the end", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T02:50:29.435002+00:00", "nick": "toothrot", "message": "or dict, whatever", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T02:50:55.994394+00:00", "nick": "rabidmadman", "message": "yeah", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T07:38:32.304446+00:00", "nick": "Research", "message": "hello, i have an error when i deploy my scrapy project, anyone can help?", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T07:39:16.784753+00:00", "nick": "Research", "message": "subprocess.CalledProcessError: Command '['/usr/bin/python', 'setup.py', 'clean', '-a', 'bdist_egg', '-d', '/tmp/scrapydeploy-HzOG1L']' returned non-zero exit status 1", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T07:39:54.612387+00:00", "nick": "Research", "message": "i installed setuptools with pip, but this can't help", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T09:45:08.649813+00:00", "nick": "ekke85_", "message": "morning, i want to write a spider that would crawl an entire site to make a archive copy of it, does anyone know of any good examples? I've started reading the documentation this morning, but any tips would be very helpful", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T11:42:08.105191+00:00", "nick": "ekke85", "message": "i want to use scrapy to make archive copies of some of my sites, at the moment i am using wget. can anyone point me in the right direction on how to do it with scrapy?", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T11:42:21.210235+00:00", "nick": "ekke85", "message": "this is what i do in wget", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T11:42:22.169489+00:00", "nick": "ekke85", "message": "wget -P . -mpck --user-agent=\"\" -e robots=off --wait 1 -E http://xxx.xxx.xxx", "links": ["http://xxx.xxx.xxx"], "channel": "scrapy"},
{"date": "2014-11-05T11:56:27.660758+00:00", "nick": "ekke85", "message": "can i even do it?", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T14:46:58.785881+00:00", "nick": "Hobbestigrou", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T14:47:48.480187+00:00", "nick": "Hobbestigrou", "message": "i have a question, maybe stupid, i would to know if it's possible to make an action just after finished visited all url, before finish to execute ?", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T14:52:35.395109+00:00", "nick": "Hobbestigrou", "message": "did you have any idea ?", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T14:54:52.861521+00:00", "nick": "nramirezuy", "message": "http://doc.scrapy.org/en/master/topics/signals....", "links": ["http://doc.scrapy.org/en/master/topics/signals.html#engine-stopped"], "channel": "scrapy"},
{"date": "2014-11-05T14:56:18.134829+00:00", "nick": "Hobbestigrou", "message": "ok yes", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T14:56:21.863979+00:00", "nick": "Hobbestigrou", "message": "or spider_closed", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T14:56:23.587360+00:00", "nick": "Hobbestigrou", "message": "thanks", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T15:03:53.994330+00:00", "nick": "Hobbestigrou", "message": "where i must store signal method", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T15:04:58.655173+00:00", "nick": "Hobbestigrou", "message": "i tried to store a method engine_stopped in my spider", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T15:05:12.132524+00:00", "nick": "Hobbestigrou", "message": "but i see nothing", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T15:12:09.030640+00:00", "nick": "scmp", "message": "Hi, how do i set the XML namespace when using ItemLoader.add_xpath? Got a valid XHTML and the xpath doesn't work.", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T15:12:11.270766+00:00", "nick": "Hobbestigrou", "message": "maybe i don't understand how i can use the signal", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T15:14:03.841253+00:00", "nick": "nramirezuy", "message": "http://doc.scrapy.org/en/master/topics/api.html...", "links": ["http://doc.scrapy.org/en/master/topics/api.html#module-scrapy.spidermanager"], "channel": "scrapy"},
{"date": "2014-11-05T15:14:18.608715+00:00", "nick": "nramirezuy", "message": "http://doc.scrapy.org/en/master/topics/api.html...", "links": ["http://doc.scrapy.org/en/master/topics/api.html#scrapy.crawler.Crawler.signals"], "channel": "scrapy"},
{"date": "2014-11-05T15:14:33.467235+00:00", "nick": "nramirezuy", "message": "and you have access to crawler via spider.crawler", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T15:15:58.403813+00:00", "nick": "nramirezuy", "message": "@scmp http://doc.scrapy.org/en/master/topics/selector...", "links": ["http://doc.scrapy.org/en/master/topics/selectors.html#scrapy.selector.Selector.register_namespace"], "channel": "scrapy"},
{"date": "2014-11-05T15:15:58.784055+00:00", "nick": "nramirezuy", "message": "build the selector first and pass it to the ItemLoader(selector=selector)", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T15:17:02.516440+00:00", "nick": "scmp", "message": "perfect, thank you", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T15:22:07.017517+00:00", "nick": "Hobbestigrou", "message": "nramirezuy, http://paste.perldancer.org/24JhXKtt8FBVT did you see any error ?", "links": ["http://paste.perldancer.org/24JhXKtt8FBVT"], "channel": "scrapy"},
{"date": "2014-11-05T15:22:47.766153+00:00", "nick": "nramirezuy", "message": "you are importing it", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T15:22:57.091172+00:00", "nick": "nramirezuy", "message": "instead to use the instantiated signal manager", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T15:24:08.482215+00:00", "nick": "Hobbestigrou", "message": "i'm not sure to undersand", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T15:24:10.889864+00:00", "nick": "nramirezuy", "message": "do the signal connection on start_requets", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T15:24:35.371558+00:00", "nick": "Hobbestigrou", "message": "understand", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T15:24:43.809281+00:00", "nick": "nramirezuy", "message": "spider.crawler.signal.connect(self.engine_stopped, signal=signals.engine_stopped)", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T15:25:18.184995+00:00", "nick": "nramirezuy", "message": "spider.crawler.signals.connect(self.engine_stopped, signal=signals.engine_stopped)", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T15:30:15.545642+00:00", "nick": "Hobbestigrou", "message": "sorry", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T15:30:33.840330+00:00", "nick": "Hobbestigrou", "message": "but i don't understand what you mean", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T15:36:29.226985+00:00", "nick": "nramirezuy", "message": "spider has an attribute called \"crawler\", \"crawler\" has an attribute called \"signal\" (SignalManager instance)", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T15:38:23.100059+00:00", "nick": "Research", "message": "anyone know how i can use scrapyd as init.d script? when i run from my /home, it's work fine, when it runs as service, scrapyd can't find my project...", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T15:44:41.673481+00:00", "nick": "Hobbestigrou", "message": "http://paste.perldancer.org/21NsukuzhMTVm", "links": ["http://paste.perldancer.org/21NsukuzhMTVm"], "channel": "scrapy"},
{"date": "2014-11-05T15:48:09.015415+00:00", "nick": "Hobbestigrou", "message": "*** AssertionError: AssertionError('Spider not bounded to any crawler',)", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T15:48:32.295761+00:00", "nick": "Hobbestigrou", "message": "when i tried to display self.crawler in my crawler", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T16:10:22.860719+00:00", "nick": "ekke85", "message": "can someone please help me, i just want to archive a site. i can do it with wget, but would much rather write a script so that i can reuse/automate the process", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T16:10:52.644188+00:00", "nick": "ekke85", "message": "if there is good examples of scraping a whole site, it would be very much appreciated", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T16:11:45.712268+00:00", "nick": "Hobbestigrou", "message": "ok thanks", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T17:22:43.045388+00:00", "nick": "Barry_", "message": "Hey", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T17:30:12.224615+00:00", "nick": "Barry_", "message": "Anyone there?", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T17:30:44.246323+00:00", "nick": "ekke85", "message": "i don't think anyone is answering questions :|", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T17:31:01.872790+00:00", "nick": "ekke85", "message": "or may be they just don't want to answer me :(", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T17:31:17.997936+00:00", "nick": "Barry_", "message": "That sucks :( I've been trying to fix this bug for 9 hours hahaha", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T17:31:22.900653+00:00", "nick": "Barry_", "message": "Might have to wait until I can afford to hire someone", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T17:51:09.322347+00:00", "nick": "asd_", "message": "ekke85, can't you just make a cron job for it?", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T17:52:00.343375+00:00", "nick": "asd_", "message": "Barry_, what problem do you have?", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T18:09:37.033996+00:00", "nick": "Barry_", "message": "Hi, sorry. Didn't receive any notifications", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T18:10:16.080343+00:00", "nick": "Barry_", "message": "http://stackoverflow.com/questions/26764126/scr... I submitted my question here", "links": ["http://stackoverflow.com/questions/26764126/scrapy-crawlspider-not-following-rules"], "channel": "scrapy"},
{"date": "2014-11-05T18:10:35.416500+00:00", "nick": "Barry_", "message": "If you wouldn't mind taking a look at it, that would be great :)", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T18:19:18.458296+00:00", "nick": "asd_", "message": "Barry_, my internet went down", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T18:21:24.147792+00:00", "nick": "Barry_", "message": "No worries :)", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T18:21:46.437564+00:00", "nick": "Barry_", "message": "Were you able to see the link?", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T18:22:10.954734+00:00", "nick": "asd_", "message": "no, can you link it again?", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T18:22:20.884871+00:00", "nick": "Barry_", "message": "Sure, here you go", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T18:22:21.381939+00:00", "nick": "Barry_", "message": "http://stackoverflow.com/questions/26764126/scr...", "links": ["http://stackoverflow.com/questions/26764126/scrapy-crawlspider-not-following-rules"], "channel": "scrapy"},
{"date": "2014-11-05T18:25:28.951512+00:00", "nick": "asd_", "message": "Barry_, so script worked when you run it from command line like scrapy crawl <spider name>?", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T18:27:20.624858+00:00", "nick": "Barry_", "message": "It doesn't work like that anymore because I edited it, but I think I can make it work from command line again", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T18:27:21.342793+00:00", "nick": "Barry_", "message": "2 secs", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T18:28:36.188568+00:00", "nick": "Barry_", "message": "Ah it's still not working", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T18:28:42.242548+00:00", "nick": "Barry_", "message": "When I run it from commandline", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T18:31:20.064156+00:00", "nick": "asd_", "message": "Barry_, doesn't scrape any links from command line?", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T18:31:42.047244+00:00", "nick": "Barry_", "message": "No links at all", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T18:32:35.523702+00:00", "nick": "Barry_", "message": "http://i.imgur.com/gT7fJ5F.png", "links": ["http://i.imgur.com/gT7fJ5F.png"], "channel": "scrapy"},
{"date": "2014-11-05T18:32:42.613274+00:00", "nick": "asd_", "message": "instead of print 'parsed' you could use: self.log('Parsed from: %s' % response.url)", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T18:34:00.782967+00:00", "nick": "Barry_", "message": "Thanks, I didn't know that", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T18:34:01.826683+00:00", "nick": "Barry_", "message": "Changed it", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T18:34:12.641587+00:00", "nick": "Barry_", "message": "Still not saying it's parsing anything", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T18:34:16.137023+00:00", "nick": "asd_", "message": "Well its not mistake or anything", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T18:34:21.288637+00:00", "nick": "asd_", "message": "to use print", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T18:34:54.971660+00:00", "nick": "Barry_", "message": "I get you", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T18:35:02.528943+00:00", "nick": "Barry_", "message": "Just a good idea to log things instead of print :)", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T18:36:45.514240+00:00", "nick": "asd_", "message": "Did you write this all alone?", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T18:36:55.870172+00:00", "nick": "Barry_", "message": "Yeah", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T18:37:00.828493+00:00", "nick": "Barry_", "message": "sorry if it's poorly written, never used scrapy before", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T18:41:11.801760+00:00", "nick": "asd_", "message": "so start url is http://freelancedirectory.org/ and then you want to extract all hrefs that are in there and then all href that are in those hrefs right?", "links": ["http://freelancedirectory.org/"], "channel": "scrapy"},
{"date": "2014-11-05T18:42:59.207582+00:00", "nick": "Barry_", "message": "Yeah, basically I want to extract all the hrefs, save them then follow the internal ones and extract from there", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T18:43:04.018633+00:00", "nick": "Barry_", "message": "untilt he deph limit is reached", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T18:43:09.850387+00:00", "nick": "Barry_", "message": "depth*", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T18:44:47.168504+00:00", "nick": "asd_", "message": "I think there is simpler way to write that kind of crawler", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T18:44:48.814486+00:00", "nick": "asd_", "message": "http://doc.scrapy.org/en/latest/topics/spiders....", "links": ["http://doc.scrapy.org/en/latest/topics/spiders.html#crawlspider-example"], "channel": "scrapy"},
{"date": "2014-11-05T18:45:17.355049+00:00", "nick": "asd_", "message": "start with something like this", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T18:46:37.123996+00:00", "nick": "asd_", "message": "I gotta run sorry its just too complicated script...", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T18:47:28.733361+00:00", "nick": "Barry_", "message": "Fair enough, thanks for your help! Appreciate it", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T18:50:23.639136+00:00", "nick": "NewToScrapy", "message": "To get to the page I need to scrape there is a button that needs to be pressed and this javascript generates the url: onclick=\"return methodName();\"  Is it possible to get access to this url in scrapy?", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T20:09:54.713229+00:00", "nick": "kaido", "message": "hello. i have problem with python scrapy framework. I am extracting images from a webshop http://www.oxygenboutique.com/620-Mid-Rise-Supe..., all images  are extracted but only one image is in full size all other images are small, how can i extract image links with full images size for all images ?", "links": ["http://www.oxygenboutique.com/620-Mid-Rise-Super-Skinny-Stocking-Jean-Mystery.aspx"], "channel": "scrapy"},
{"date": "2014-11-05T21:14:18.372362+00:00", "nick": "AndyRez", "message": "hi all, what is the best way to run scrapy spider on a site daily to get only new and/or updated items", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T21:29:50.508351+00:00", "nick": "AndyRez", "message": "Hi all, is there a way I can make a spider sleep after ithas finished crawling a site? The reason for this because since scrapy doesn't scrape duplicate link, it would make sense for it to sleep and then resume after a while so it can scrape only new items on a website", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T21:31:38.656669+00:00", "nick": "AndyRez", "message": "anyone?", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T21:45:44.610726+00:00", "nick": "cornjuliox", "message": "is there a way to check what pipelines are enabled on scrapyd?", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T22:04:33.095177+00:00", "nick": "ekke85", "message": "hello", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T22:07:16.192699+00:00", "nick": "ekke85", "message": "can someone help me with a login", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T22:07:26.130638+00:00", "nick": "ekke85", "message": "i am trying to login to my router", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T22:07:33.687621+00:00", "nick": "ekke85", "message": "i am very new to scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T22:07:46.583662+00:00", "nick": "ekke85", "message": "this is my spider and also the html from the router", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T22:07:54.120912+00:00", "nick": "ekke85", "message": "http://pastebin.com/Wanwgqgm", "links": ["http://pastebin.com/Wanwgqgm"], "channel": "scrapy"},
{"date": "2014-11-05T22:08:11.355999+00:00", "nick": "ekke85", "message": "i keep getting AuthFail", "links": [], "channel": "scrapy"},
{"date": "2014-11-05T22:09:55.615352+00:00", "nick": "ekke85", "message": "the name field keeps changing", "links": [], "channel": "scrapy"},
{"date": "2014-11-06T03:15:36.172311+00:00", "nick": "tightflks", "message": "Hi", "links": [], "channel": "scrapy"},
{"date": "2014-11-06T03:15:39.263923+00:00", "nick": "tightflks", "message": "anyone in here?", "links": [], "channel": "scrapy"},
{"date": "2014-11-06T03:23:53.637547+00:00", "nick": "tightflks", "message": "is it possible to scrape with multiple logins?", "links": [], "channel": "scrapy"},
{"date": "2014-11-06T03:56:47.714192+00:00", "nick": "Gymgle", "message": "Hi, I want to download the APK files from website, the APK URLs is saved into items, how could I start to download the APKs?", "links": [], "channel": "scrapy"},
{"date": "2014-11-06T03:58:50.412972+00:00", "nick": "Gymgle", "message": "Should I use the DownloaderMiddleware?", "links": [], "channel": "scrapy"},
{"date": "2014-11-06T07:12:29.398288+00:00", "nick": "Research", "message": "Hi! how i can use scrapyd as init.d script? when i run 'scrapyd' from terminal, it's work fine when i in /home, where located my project, when it runs as service, scrapyd can't find my project...", "links": [], "channel": "scrapy"},
{"date": "2014-11-06T11:52:36.134191+00:00", "nick": "adders_", "message": "hey all", "links": [], "channel": "scrapy"},
{"date": "2014-11-06T14:02:54.968337+00:00", "nick": "rodrigo5244", "message": "hey", "links": [], "channel": "scrapy"},
{"date": "2014-11-06T21:16:55.261913+00:00", "nick": "jumanji", "message": "hi everyone", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T01:24:28.521970+00:00", "nick": "atrus6", "message": "I'm trying to deploy a spider I made in portia to scrapyd, but scrapyd keeps throwing the error  \"No module named slybot.spidermanager\" What am I doing wrong?", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T02:07:17.851792+00:00", "nick": "atrus6", "message": "I'm trying to deploy a spider I made in portia to scrapyd, but scrapyd keeps throwing the error  \"No module named slybot.spidermanager\" What am I doing wrong?", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T11:13:35.204572+00:00", "nick": "AndyRez", "message": "Can someone help with a fix on my code... I followed the tutorial on https://scrapy.readthedocs.org/en/latest/topics...", "links": ["https://scrapy.readthedocs.org/en/latest/topics/request-response.html#passing-additional-data-to-callback-functions"], "channel": "scrapy"},
{"date": "2014-11-07T11:13:48.120091+00:00", "nick": "AndyRez", "message": "but my spider doesn't scrape, it just crawls...", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T11:13:51.548345+00:00", "nick": "AndyRez", "message": "here is my code..", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T11:15:51.586682+00:00", "nick": "AndyRez", "message": "https://bpaste.net/show/6cb8296abb77", "links": ["https://bpaste.net/show/6cb8296abb77"], "channel": "scrapy"},
{"date": "2014-11-07T11:38:25.642503+00:00", "nick": "AndyRez", "message": "anyone?", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T11:59:03.353246+00:00", "nick": "asd", "message": "In which file in scrapy project should i put in built in e-mail notifier.", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T12:14:33.770749+00:00", "nick": "AndyRez", "message": "can someone please explain why I am getting a exceptions.KeyError in my code", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T12:16:53.163868+00:00", "nick": "AndyRez", "message": "https://bpaste.net/show/78d69e913d0a", "links": ["https://bpaste.net/show/78d69e913d0a"], "channel": "scrapy"},
{"date": "2014-11-07T12:17:06.190326+00:00", "nick": "AndyRez", "message": "can someone please explain why I am getting a KeyError?", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T12:45:39.402958+00:00", "nick": "rodrigo5244", "message": "AndyRez, what line is giving you that error?", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T12:45:49.573901+00:00", "nick": "AndyRez", "message": "line 88", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T12:45:59.122889+00:00", "nick": "AndyRez", "message": "rodrigo5244: line 88", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T12:46:23.641372+00:00", "nick": "rodrigo5244", "message": "asd, there is an e-mail notifier that comes with scrapy if you don't what to use that it depends on when you want to send the e-mail.", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T12:47:10.660152+00:00", "nick": "rodrigo5244", "message": "AndyRez, change data = response.request.meta['data'] to data = response.meta['data']", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T12:47:56.880125+00:00", "nick": "AndyRez", "message": "rodrigo5244: I did that already and I still get the same KeyError", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T12:49:08.315275+00:00", "nick": "AndyRez", "message": "File \"/rez/Projects/scrapes/immobinvest/immobinvest.py\", line 88, in parse_page1", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T12:49:08.626251+00:00", "nick": "AndyRez", "message": "    data = response.meta['data']", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T12:49:08.771740+00:00", "nick": "AndyRez", "message": "exceptions.KeyError: 'data'", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T12:50:54.352109+00:00", "nick": "rodrigo5244", "message": "AndyRez: in 75 change yield data to yield request.", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T12:51:28.692439+00:00", "nick": "asd", "message": "rodrigo5244, i want to send it when scraping is finished", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T12:51:57.720113+00:00", "nick": "asd", "message": "so in which file in scrapy project should i drop this  code in? http://pastebin.com/Py4Jakps", "links": ["http://pastebin.com/Py4Jakps"], "channel": "scrapy"},
{"date": "2014-11-07T12:53:08.259414+00:00", "nick": "AndyRez", "message": "rodrigo5244: It still gives the same KeyError after changing yield data to yield request", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T12:54:47.641271+00:00", "nick": "rodrigo5244", "message": "asd: In your spider you could add this:", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T12:54:50.900314+00:00", "nick": "rodrigo5244", "message": "asd:     def from_crawler(cls, crawler):", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T12:54:51.135337+00:00", "nick": "rodrigo5244", "message": "spider = cls()", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T12:54:51.281074+00:00", "nick": "rodrigo5244", "message": "crawler.signals.connect(spider.spider_closed, signals.spider_closed)", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T12:54:51.281151+00:00", "nick": "rodrigo5244", "message": "return spider", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T12:54:51.282841+00:00", "nick": "rodrigo5244", "message": "def spider_closed(self, spider):", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T12:54:51.922729+00:00", "nick": "rodrigo5244", "message": "#try here", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T12:55:17.140025+00:00", "nick": "rodrigo5244", "message": "asd: This allows you to execute code at the end of the process.", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T12:55:31.872767+00:00", "nick": "rodrigo5244", "message": "asd: There may be other easier ways of doing that.", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T12:56:11.418101+00:00", "nick": "rodrigo5244", "message": "AndyRez: using yield request and data = response.meta['data'] ?", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T12:58:15.678341+00:00", "nick": "AndyRez", "message": "rodrigo5244: yes. Do I create another Rule for page_parse?", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T13:00:15.794093+00:00", "nick": "rodrigo5244", "message": "AndyRez: In line 58 I think you want to start with parse_page, not parse_page1", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T13:00:32.009071+00:00", "nick": "asd", "message": "rodrigo5244, thanks", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T13:01:01.590843+00:00", "nick": "rodrigo5244", "message": "AndyRez: If you don't pass through parse_page before parse_page1 you should get an error.", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T13:01:07.092234+00:00", "nick": "rodrigo5244", "message": "asd: You are welcome.", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T13:02:42.172383+00:00", "nick": "AndyRez", "message": "rodrigo5244: It crwals with no error now, but it doesn't scrape any data..", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T13:05:48.031125+00:00", "nick": "rodrigo5244", "message": "AndyRez: request = Request(response.url, callback=self.parse_page1)", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T13:06:01.638975+00:00", "nick": "rodrigo5244", "message": "AndyRez: This means you are getting the same page again.", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T13:06:24.972190+00:00", "nick": "AndyRez", "message": "rodrigo5244: yeah, and it isn't getting the right values if I specify it that way..", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T13:06:44.384750+00:00", "nick": "AndyRez", "message": "I am trying to get the values \"Disponsible\" from here http://www.immobinvest.com/agence/produit.php?i...", "links": ["http://www.immobinvest.com/agence/produit.php?id=6&amp=&lng=fr"], "channel": "scrapy"},
{"date": "2014-11-07T13:07:05.863843+00:00", "nick": "AndyRez", "message": "and then after getting that I want to get the values from voir details link", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T13:07:25.555790+00:00", "nick": "AndyRez", "message": "for each listings...", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T13:09:38.202667+00:00", "nick": "rodrigo5244", "message": "AndyRez: I can see a form \"Recherche Rapide\" in \"http://www.immobinvest.com/agence/\" do you want to try all the combinations of that form?", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T13:10:42.991096+00:00", "nick": "AndyRez", "message": "rodrigo5244: no, sorry. I already have a Rule getting all the links from the left hand side..", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T13:12:25.290400+00:00", "nick": "AndyRez", "message": "rodrigo5244: But for each of the listings on each link on the left hand side, I need to get the values \"DISPONSIBLE\" before I visit the \"voir detail\" link to get other values I need.", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T13:12:44.613170+00:00", "nick": "rodrigo5244", "message": "AndyRez: I can't see anything wrong right away. I will have to look with more care, I will have some time in 40 minutes, than I can run the code and look at it with more attention.", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T13:13:49.937315+00:00", "nick": "AndyRez", "message": "rodrigo5244: okay, I would keep trying to fix it too, but I am here and when you are checking it, you cal walk me through as well..", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T13:14:25.645096+00:00", "nick": "rodrigo5244", "message": "AndyRez: Sounds good", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T14:02:20.809454+00:00", "nick": "rodrigo5244", "message": "AndyRez: Ok gonna check your code now.", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T14:02:31.738619+00:00", "nick": "AndyRez", "message": "rodrigo5244: ok, cool. I am here.. :)", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T14:02:43.212728+00:00", "nick": "rodrigo5244", "message": "AndyRez: Wait something came up.", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T14:02:54.589263+00:00", "nick": "AndyRez", "message": "rodrigo5244: okay", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T14:19:06.807527+00:00", "nick": "rodrigo5244", "message": "AndyRez: I am back", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T14:19:19.810632+00:00", "nick": "AndyRez", "message": "rodrigo5244: :)", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T14:19:37.369523+00:00", "nick": "rodrigo5244", "message": "AndyRez: Can you paste the code again for me?", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T14:19:45.351262+00:00", "nick": "AndyRez", "message": "ok, please 1 min", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T14:21:01.978097+00:00", "nick": "AndyRez", "message": "rodrigo5244: https://bpaste.net/show/9a3b4fd233d3", "links": ["https://bpaste.net/show/9a3b4fd233d3"], "channel": "scrapy"},
{"date": "2014-11-07T14:29:02.697024+00:00", "nick": "rodrigo5244", "message": "AndyRez: the crawl went to the right page.", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T14:29:27.760615+00:00", "nick": "rodrigo5244", "message": "data['Availability'] is empty, but data['Name'] has stuff in it", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T14:29:42.700903+00:00", "nick": "AndyRez", "message": "rodrigo5244: oh...", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T14:29:55.061814+00:00", "nick": "AndyRez", "message": "maybe I need to fix the xpath of that..", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T14:30:12.721868+00:00", "nick": "AndyRez", "message": "rodrigo5244: but how did you view that?", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T14:31:58.266182+00:00", "nick": "rodrigo5244", "message": "AndyRez I am using open_in_browser(response) and inspect_response(response)", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T14:32:22.906891+00:00", "nick": "rodrigo5244", "message": "if you merge parse_page and parse_page1 into a single functions it works better.", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T14:32:34.223509+00:00", "nick": "rodrigo5244", "message": "There is no point of requesting the same page twice.", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T14:32:43.750227+00:00", "nick": "AndyRez", "message": "rodrigo5244: How do I merge it?", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T14:32:47.616545+00:00", "nick": "rodrigo5244", "message": "let me try this pastebin to share code with you.", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T14:33:03.166263+00:00", "nick": "AndyRez", "message": "rodrigo5244: okay cool. Waiting to see the edited code..", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T14:33:45.481923+00:00", "nick": "rodrigo5244", "message": "https://bpaste.net/show/839f41d5b876", "links": ["https://bpaste.net/show/839f41d5b876"], "channel": "scrapy"},
{"date": "2014-11-07T14:33:49.967409+00:00", "nick": "rodrigo5244", "message": "That is just the function", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T14:33:56.078995+00:00", "nick": "rodrigo5244", "message": "I just put the two together.", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T14:34:31.319582+00:00", "nick": "rodrigo5244", "message": "does that makes sense to you?", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T14:34:41.201883+00:00", "nick": "AndyRez", "message": "rodrigo5244: Let me check the output here.. Please hold...", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T14:42:31.324743+00:00", "nick": "AndyRez", "message": "rodrigo5244: cool, but availability can't be found on those URL's only on the page before the url it crwals, hence my reason for wanting to get the value first before getting the other values..", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T14:42:38.588525+00:00", "nick": "AndyRez", "message": "rodrigo5244: for example..", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T14:42:53.153554+00:00", "nick": "AndyRez", "message": "rodrigo5244: this link shows listings, http://www.immobinvest.com/agence/produit.php?i...", "links": ["http://www.immobinvest.com/agence/produit.php?id=6&amp=&lng=fr"], "channel": "scrapy"},
{"date": "2014-11-07T14:43:40.328438+00:00", "nick": "AndyRez", "message": "rodrigo5244: but if I click on the first link, I wouldn't see any part which states that it is available, for example on the first listing page..", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T14:44:21.307577+00:00", "nick": "AndyRez", "message": "rodrigo5244: http://www.immobinvest.com/agence/detail-produi...  <==== I can't see disponsible on that link, but on the listings page where others listings are lined up..", "links": ["http://www.immobinvest.com/agence/detail-produit.php?ref=618&amp=&lng=fr"], "channel": "scrapy"},
{"date": "2014-11-07T14:44:44.813591+00:00", "nick": "AndyRez", "message": "rodrigo5244: do you understand my challenge?", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T14:48:47.287819+00:00", "nick": "AndyRez", "message": "rodrigo5244: basically I am trying to get data's from two different pages... https://scrapy.readthedocs.org/en/latest/topics...", "links": ["https://scrapy.readthedocs.org/en/latest/topics/request-response.html#passing-additional-data-to-callback-functions"], "channel": "scrapy"},
{"date": "2014-11-07T14:50:16.531074+00:00", "nick": "rodrigo5244", "message": "AndyRez: I see", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T14:52:41.485284+00:00", "nick": "AndyRez", "message": "rodrigo5244: yeah, thats what I am trying to achieve..", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T14:54:16.266306+00:00", "nick": "AndyRez", "message": "rodrigo5244: is it possible?", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T14:54:23.971849+00:00", "nick": "rodrigo5244", "message": "AndyRez: https://bpaste.net/show/981b51b6ffb4", "links": ["https://bpaste.net/show/981b51b6ffb4"], "channel": "scrapy"},
{"date": "2014-11-07T14:54:34.635165+00:00", "nick": "rodrigo5244", "message": "AndyRez: My focus time is up", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T14:54:49.719412+00:00", "nick": "rodrigo5244", "message": "AndyRez: You should build on the code I sent you.", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T14:55:13.573497+00:00", "nick": "AndyRez", "message": "rodrigo5244: okay, would look at it. Thanks for your time. I appreciate the effort.", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T14:55:23.946493+00:00", "nick": "rodrigo5244", "message": "AndyRez: Basically you need to go to the page where the data is, save the date, then figure out the next url and go to the next parser.", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T14:55:35.999930+00:00", "nick": "rodrigo5244", "message": "save the data*", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T14:56:29.885184+00:00", "nick": "AndyRez", "message": "rodrigo5244: yeah that was what I was trying to do. I got the xpath's right but parsing the url to parse_page2 was where I was having issues", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T14:57:15.355552+00:00", "nick": "rodrigo5244", "message": "AndyRez: You have to extract the link, I think that part is working, but in my code I am missing the left part of the url.", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T14:57:50.744244+00:00", "nick": "rodrigo5244", "message": "If you do this url = 'http://...' + url and add the missing part of the url that should work.", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T14:57:51.592223+00:00", "nick": "AndyRez", "message": "rodrigo5244: you mean the details link where I am to get the rest of the data for parse_page1?", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T14:57:57.784351+00:00", "nick": "rodrigo5244", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T14:58:14.774156+00:00", "nick": "AndyRez", "message": "can I pass it as an xpath? to parse_page?", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T14:58:33.419440+00:00", "nick": "AndyRez", "message": "and then when it yields it access the link in parse_page1?", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T14:59:17.787831+00:00", "nick": "rodrigo5244", "message": "in parse_page you need to get the url based on the response. with that url you can go to parse_page1, if in the next page there is a link with more details you can repeat the process.", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T15:00:30.375424+00:00", "nick": "rodrigo5244", "message": "AndyRez: Does that make sense?", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T15:00:49.721378+00:00", "nick": "AndyRez", "message": "I am checking it.. please hold..", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T15:03:21.984246+00:00", "nick": "AndyRez", "message": "rodrigo5244: exceptions.ValueError: Missing scheme in request url: %3Ca%20href=%22detail-produit.php?ref=233&amp;lng=fr%22%3E%20%C2%A0voir%20d%C3%A9tail%20%C2%A0%20%3C/a%3E", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T15:03:31.825157+00:00", "nick": "AndyRez", "message": "I don't understand what that means", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T15:11:06.978723+00:00", "nick": "AndyRez", "message": "rodrigo5244: I did this url = \"http://www.immobinvest.com/agence/\" + ''.join(response.xpath('//div[@id=&...", "links": ["mailto:''.join(response.xpath('//div[@id=\"reserver\"]/a/@href').extract()[0])"], "channel": "scrapy"},
{"date": "2014-11-07T15:11:13.865229+00:00", "nick": "AndyRez", "message": "rodrigo5244: in line 73", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T15:11:44.978749+00:00", "nick": "AndyRez", "message": "but it gets all the values of data['Availability'] = response.xpath('//div[@class=\"slogonsp&... and puts that in all the links", "links": ["mailto:response.xpath('//div[@class=\"slogonsp\"]/text()').extract()"], "channel": "scrapy"},
{"date": "2014-11-07T15:11:53.610059+00:00", "nick": "AndyRez", "message": "insteead of the specific 1 for the link...", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T15:12:16.974721+00:00", "nick": "rodrigo5244", "message": "AndyRez: Maybe there is a way to use the link extractor you made  before to get that url", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T15:13:08.690972+00:00", "nick": "rodrigo5244", "message": "AndyRez change response to sel, I think if you use response your are going to get all the links, but you only need the link of that particular place.", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T15:13:36.470140+00:00", "nick": "AndyRez", "message": "rodrigo5244: yeah, hence my reason for not puting url as part of the function parse_page earlier, but I do not know how to go about doing that..", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T15:16:29.804623+00:00", "nick": "AndyRez", "message": "rodrigo5244: looking at the code, isn't that what yield is supposed to do?", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T15:19:21.861378+00:00", "nick": "rodrigo5244", "message": "yield is going to download a new page and pass it to the callback function.", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T15:24:44.697355+00:00", "nick": "AndyRez", "message": "rodrigo5244: yeah, but since we have a for statement, what is making it spilt all the values of data['Availability'] = response.xpath('//div[@class=\"slogonsp&... to each link?", "links": ["mailto:response.xpath('//div[@class=\"slogonsp\"]/text()').extract()"], "channel": "scrapy"},
{"date": "2014-11-07T15:34:24.904229+00:00", "nick": "bahamas", "message": "the advantage of yield over return is that you can use multiple times in a function", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T15:34:40.918409+00:00", "nick": "bahamas", "message": "all the values will then be available when the functions is called", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T15:39:01.969454+00:00", "nick": "bahamas", "message": "AndyRez: on line 69 you do for sel in ..., but you don't use that sel anywhere", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T15:44:21.243627+00:00", "nick": "AndyRez", "message": "bahamas: sel is supposed to be the counter for each in response.xpath('//table[@class=\"mb_loadingd\"]'):", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T15:45:31.264252+00:00", "nick": "AndyRez", "message": "right?", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T15:46:03.098674+00:00", "nick": "AndyRez", "message": ":S", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T15:46:13.135168+00:00", "nick": "AndyRez", "message": "confused much....", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T15:47:55.156588+00:00", "nick": "bahamas", "message": "AndyRez: counter? do you know what 'for a in foo' means?", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T15:48:31.899442+00:00", "nick": "bahamas", "message": "it means, take each element in 'foo' and give it the name 'a', so you can address it with the name 'a'", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T15:54:15.256367+00:00", "nick": "AndyRez", "message": "bahamas: oh.., but how do i address it with each value it assigns to sel in this case? I keep getting object does not support item assignment..", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T15:58:29.435300+00:00", "nick": "bahamas", "message": "AndyRez: you probably want sel.xpath in the loop", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T15:58:51.513878+00:00", "nick": "bahamas", "message": "I assume response.xpath returns a list of selectors, so sel is each selector in path", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T15:59:28.984990+00:00", "nick": "bahamas", "message": "and you don't want to start with '//' the xpath in sel.xpath.", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T16:01:35.428451+00:00", "nick": "AndyRez", "message": "bahamas: please can you edit the code to show me this? Its confusing with just words.... :(", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T16:01:59.288854+00:00", "nick": "AndyRez", "message": "using sel in scrapy shell says its depreciated, so I use response instead...", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T16:27:38.389466+00:00", "nick": "seni", "message": "Hi!", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T18:18:00.907434+00:00", "nick": "AndyRez", "message": "bahamas: I got it..", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T18:18:25.339528+00:00", "nick": "AndyRez", "message": "rodrigo5244: I got it but not all values are correct..", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T18:18:34.226513+00:00", "nick": "AndyRez", "message": "trying to figure out why that is the case", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T18:20:32.311978+00:00", "nick": "rodrigo5244", "message": "AndyRez: Ok, but I am out of time.", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T18:20:45.585202+00:00", "nick": "rodrigo5244", "message": "AndyRez: Try using pdb", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T18:21:04.937898+00:00", "nick": "AndyRez", "message": "rodrigo5244: its okay, i would try to get it to work perfectly..hopefully...", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T18:22:02.827339+00:00", "nick": "rodrigo5244", "message": "AndyRez: Good luck.", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T18:23:09.050441+00:00", "nick": "AndyRez", "message": "rodrigo5244:  thanks", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T20:00:52.371059+00:00", "nick": "naveen", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T20:01:57.107305+00:00", "nick": "naveen", "message": "i am trying to download multiple images from a web-page through scrapy in python but only able to get only one image", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T20:02:03.727704+00:00", "nick": "naveen", "message": "why?", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T20:02:24.656364+00:00", "nick": "naveen", "message": "my code is this:", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T20:02:25.859025+00:00", "nick": "naveen", "message": "def parse(self, response):         item = DmozItem()         image_urls = response.xpath('//div[@class=\"overhid...         item['image_urls'] = [ x for x in image_urls]         yield item", "links": ["mailto:response.xpath('//div[@class=\"overhid\"]//img/@src').extract()"], "channel": "scrapy"},
{"date": "2014-11-07T20:05:14.386334+00:00", "nick": "naveen", "message": "anybody there?", "links": [], "channel": "scrapy"},
{"date": "2014-11-07T21:04:35.678704+00:00", "nick": "ItsScrapyTime", "message": "If data is loaded and displayed via javascript, is it impossible to scrape this data with scrappy?", "links": [], "channel": "scrapy"},
{"date": "2014-11-08T01:20:00.070193+00:00", "nick": "rabidmadman", "message": "Hey, so say you yield a request object in a  for loop that populates an item by another parse function. In that parse function you yield another request object that requires a different url and this also is within a for loop. Finally, you get to the last function that populates one aspect of an item. Since this last parse function is basically nested within the previous function (which is then nested in one other function), I am ver", "links": [], "channel": "scrapy"},
{"date": "2014-11-08T01:20:54.223382+00:00", "nick": "rabidmadman", "message": "would i simply yield the item outside of the first calling function?", "links": [], "channel": "scrapy"},
{"date": "2014-11-08T01:23:22.632462+00:00", "nick": "rabidmadman", "message": "so imagine this:", "links": [], "channel": "scrapy"},
{"date": "2014-11-08T01:23:22.778153+00:00", "nick": "rabidmadman", "message": "def parse 0:", "links": [], "channel": "scrapy"},
{"date": "2014-11-08T01:23:22.778224+00:00", "nick": "rabidmadman", "message": "For whatever in x:", "links": [], "channel": "scrapy"},
{"date": "2014-11-08T01:23:22.779651+00:00", "nick": "rabidmadman", "message": "yield Request(url,callback=parse1,meta={'item': item})", "links": [], "channel": "scrapy"},
{"date": "2014-11-08T01:23:22.780844+00:00", "nick": "rabidmadman", "message": "def parse 1:", "links": [], "channel": "scrapy"},
{"date": "2014-11-08T01:23:22.925525+00:00", "nick": "rabidmadman", "message": "##using item from previous function", "links": [], "channel": "scrapy"},
{"date": "2014-11-08T01:23:22.925595+00:00", "nick": "rabidmadman", "message": "for whateve rin whatever", "links": [], "channel": "scrapy"},
{"date": "2014-11-08T01:23:23.859439+00:00", "nick": "rabidmadman", "message": "populate item", "links": [], "channel": "scrapy"},
{"date": "2014-11-08T01:23:23.859510+00:00", "nick": "rabidmadman", "message": "yield Request(url,callback=parse2)", "links": [], "channel": "scrapy"},
{"date": "2014-11-08T01:23:24.868091+00:00", "nick": "rabidmadman", "message": "def parse2:", "links": [], "channel": "scrapy"},
{"date": "2014-11-08T01:23:24.868256+00:00", "nick": "rabidmadman", "message": "i##using item from previous function", "links": [], "channel": "scrapy"},
{"date": "2014-11-08T01:23:25.993677+00:00", "nick": "rabidmadman", "message": "##populate part of item here", "links": [], "channel": "scrapy"},
{"date": "2014-11-08T01:23:50.112112+00:00", "nick": "rabidmadman", "message": "when the item is done, i want to yield it, but it's not done until parse2 is done", "links": [], "channel": "scrapy"},
{"date": "2014-11-08T03:44:17.088649+00:00", "nick": "toothrot", "message": "rabidmadman, yield it in parse2 then, what's the issue?", "links": [], "channel": "scrapy"},
{"date": "2014-11-08T03:44:57.073461+00:00", "nick": "toothrot", "message": "rabidmadman, if you're talking about your band site still, i think you need to cover the case where you have multiple parse1/parse2 results for the same item", "links": [], "channel": "scrapy"},
{"date": "2014-11-08T05:33:13.222634+00:00", "nick": "rabidmadman", "message": "thanks toothrot I really need to refactor my code", "links": [], "channel": "scrapy"},
{"date": "2014-11-08T06:09:28.947522+00:00", "nick": "scr_ape", "message": "Hello, everyone", "links": [], "channel": "scrapy"},
{"date": "2014-11-08T06:09:55.868236+00:00", "nick": "scr_ape", "message": "I had a question regarding start_url in scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-11-08T06:10:39.092847+00:00", "nick": "scr_ape", "message": "does the scraper only crawls the url from the domain defined in start_url?", "links": [], "channel": "scrapy"},
{"date": "2014-11-08T23:20:01.025681+00:00", "nick": "jumanji", "message": "hi guys", "links": [], "channel": "scrapy"},
{"date": "2014-11-08T23:21:14.568302+00:00", "nick": "jumanji", "message": "anyone know how you can create a generic crawler", "links": [], "channel": "scrapy"},
{"date": "2014-11-08T23:28:34.297213+00:00", "nick": "jumanji", "message": "http://stackoverflow.com/questions/26823228/scr...", "links": ["http://stackoverflow.com/questions/26823228/scrapy-how-to-create-a-generic-crawler-that-you-can-pass-arguments-to"], "channel": "scrapy"},
{"date": "2014-11-08T23:35:37.207545+00:00", "nick": "rabidmadman", "message": "I'm a bit confused about what's happening behind the scenes when a Request object is yielded", "links": [], "channel": "scrapy"},
{"date": "2014-11-08T23:35:44.053448+00:00", "nick": "rabidmadman", "message": "vs. an item object", "links": [], "channel": "scrapy"},
{"date": "2014-11-08T23:36:22.326445+00:00", "nick": "jumanji", "message": "is there a call back function to run javascript on a page scrapy is currently on?", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T08:31:01.072671+00:00", "nick": "rabidmadman", "message": "so a major issue that iw as having was that i was sending a request to hte same url twice...and i didn't realize it", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T08:31:12.905634+00:00", "nick": "rabidmadman", "message": "spent 2 days staring at my code not realizing why....:)", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T19:45:02.029502+00:00", "nick": "rabidmadman", "message": "how does returning from a callback work? LIke instead of yielding an item, I'd like to return to the for loop from which it was called in", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T19:45:12.376319+00:00", "nick": "rabidmadman", "message": "i tried doing this, but now i have 1000's of items that shoudln't be there", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T19:45:39.797175+00:00", "nick": "rabidmadman", "message": "also if i modify an item in a callback function and return it, i want to preserve all changes done", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T19:49:13.108156+00:00", "nick": "rabidmadman", "message": "i was thinking that instead of yielding a request object to another functoin, maybe i could just store the request in a variable and do all my parsing within the same function?", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T19:49:28.738962+00:00", "nick": "rabidmadman", "message": "(using the new response body from that request", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:18:49.636202+00:00", "nick": "bezirg", "message": "hi, question: I have setup 100 concurrent requests: this means I have 100 spider threads. If a spider thread err's (mostly inside the parse method), this means that this thread will be killed and then I will be left with 99 spider threads, 98, 97 and so on?", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:37:14.571537+00:00", "nick": "toothrot", "message": "100 spider threads?", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:38:21.328262+00:00", "nick": "toothrot", "message": "bezirg, ?", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:38:41.345129+00:00", "nick": "bezirg", "message": "toothrot: that is not correct?", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:38:50.225240+00:00", "nick": "toothrot", "message": "scrapy doesn't use threads", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:38:59.891755+00:00", "nick": "bezirg", "message": "well I mean network requests", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:39:07.335644+00:00", "nick": "bezirg", "message": "async threads", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:39:45.709590+00:00", "nick": "toothrot", "message": "and surely you mean you have 100 requests scheduld, not 100 requests going at the same time?", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:40:30.415874+00:00", "nick": "toothrot", "message": "default concurrent request setting is 16, iirc", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:41:05.175413+00:00", "nick": "bezirg", "message": "toothrot: ok", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:41:06.940178+00:00", "nick": "toothrot", "message": "anyhow, if your parse method fails, that doesn't affect how many requests scrapy will process at once going forward... it'll start on the next ones", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:41:15.576326+00:00", "nick": "toothrot", "message": "if there are any", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:41:23.442418+00:00", "nick": "toothrot", "message": "what are you concerned about?", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:41:26.700178+00:00", "nick": "bezirg", "message": "but if 16 requests err with exception in parse", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:41:35.916343+00:00", "nick": "bezirg", "message": "that means I am left with no spiders running?", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:41:56.800904+00:00", "nick": "toothrot", "message": "no, the next 16 requests will be started, if there are any", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:42:13.893969+00:00", "nick": "toothrot", "message": "now if you're relying on new requests coming from your parse method, then there wouldn't be any new ones to start", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:42:42.384028+00:00", "nick": "bezirg", "message": "toothrot: ok I getit, ty very much!", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:43:10.490071+00:00", "nick": "toothrot", "message": "1 spider handles multiple requests at the same time, so the spider continues to run", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:43:22.294830+00:00", "nick": "toothrot", "message": "(it's not a spider per request, typically)", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:43:35.279231+00:00", "nick": "bezirg", "message": "last question: if I switch to bread-first scheduler, would that have any effect in performance, potentially?", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:43:50.720489+00:00", "nick": "bezirg", "message": "bcs I am crawling a lot of internal links", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:44:08.101882+00:00", "nick": "bezirg", "message": "i have a huge seed and I follow only internal links from this seed", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:46:17.768244+00:00", "nick": "toothrot", "message": "it might use more memory depending on the pages, but it shouldn't affect anything else", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:47:25.391821+00:00", "nick": "toothrot", "message": "not really sure about that though", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:47:33.382130+00:00", "nick": "bezirg", "message": "ok I will try  nonetheless", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:48:06.103289+00:00", "nick": "toothrot", "message": "i don't think it'll be an issue in most cases", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:48:07.450250+00:00", "nick": "bezirg", "message": "in my logs: i see an info where INFO: Crawled N pages", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:48:14.160667+00:00", "nick": "bezirg", "message": "this N is rly decreasing... why is that?", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:48:27.641569+00:00", "nick": "toothrot", "message": "isn't that a pages per second ?", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:48:27.844253+00:00", "nick": "bezirg", "message": "it goes from 149.. 57.. 18..", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:48:35.474645+00:00", "nick": "bezirg", "message": "pages per min", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:48:59.060437+00:00", "nick": "toothrot", "message": "not sure, are you using the cache?", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:49:08.065592+00:00", "nick": "bezirg", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:49:51.609664+00:00", "nick": "toothrot", "message": "well, if only part of it's cached, those are crawled instantly and aren't throttled by the settings", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:50:01.247765+00:00", "nick": "toothrot", "message": "it'll run through the first part quickly, and then start doing real requests", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:50:09.696644+00:00", "nick": "toothrot", "message": "it'll run through the first part [which is cached] quickly, and then start doing real requests", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:50:16.389358+00:00", "nick": "bezirg", "message": "aha", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:50:22.508290+00:00", "nick": "toothrot", "message": "that's just a guess though", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:50:24.862242+00:00", "nick": "bezirg", "message": "ok", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:50:41.054543+00:00", "nick": "bezirg", "message": "well do u think 13 pages per min is \"normal\" behaviour?", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:50:56.545979+00:00", "nick": "bezirg", "message": "on a 4-core, 100mbps, small ping conn", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:52:29.789624+00:00", "nick": "toothrot", "message": "i use: CONCURRENT_REQUESTS = 2", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:52:29.935063+00:00", "nick": "toothrot", "message": "DOWNLOAD_DELAY = 3.5", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:52:35.166238+00:00", "nick": "toothrot", "message": "on most of mine, to be nice", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:52:50.280813+00:00", "nick": "toothrot", "message": "and get around that 14-15 rate", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:52:56.336661+00:00", "nick": "toothrot", "message": "so it depends on the settings", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:53:02.136440+00:00", "nick": "bezirg", "message": "aha", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:53:08.588729+00:00", "nick": "toothrot", "message": "depends on whether the server can actually handle the requests quickly", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:53:58.726718+00:00", "nick": "toothrot", "message": "i'd say it souhld be going faster if you are using the default settings", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:54:22.310280+00:00", "nick": "bezirg", "message": "but it doesn't", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:54:24.606952+00:00", "nick": "toothrot", "message": "but like mentioned, if the server is doing a poor job of handling concurrent requests that's going to be a deciding factor", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:54:49.187820+00:00", "nick": "bezirg", "message": "it feels like scrapy starts fast but then the scrapy process does not use even 2% of my cpu", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:54:53.365777+00:00", "nick": "toothrot", "message": "you could try a becnhmark tool to see if it's a problem outside of scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:55:02.352470+00:00", "nick": "bezirg", "message": "it feels like it is sleeping on the DOWNLOAD_DELAY", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:55:09.007685+00:00", "nick": "bezirg", "message": "which is like 30secs for me", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:55:22.365797+00:00", "nick": "toothrot", "message": "oh, you set the download delay to 30 secs?", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:55:22.849612+00:00", "nick": "bezirg", "message": "toothrot: a yes good idea", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:55:26.891069+00:00", "nick": "bezirg", "message": "toothrot: yes", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:55:28.840233+00:00", "nick": "toothrot", "message": "well", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:55:34.788781+00:00", "nick": "toothrot", "message": "it's slightly randomized", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:55:42.257380+00:00", "nick": "bezirg", "message": "toothrot: I am brainwashed by other crawlers and want to be polite", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:55:45.704333+00:00", "nick": "toothrot", "message": "so 16 concurrent requests, each per 30 secs", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:56:13.323235+00:00", "nick": "bezirg", "message": "y that is why I was thinking to switch to bfs", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:56:26.076223+00:00", "nick": "bezirg", "message": "breadth first scheduler", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:56:42.336162+00:00", "nick": "bezirg", "message": "actually I have 100 concurrent requests", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:56:45.652475+00:00", "nick": "toothrot", "message": "oh", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:56:55.126929+00:00", "nick": "bezirg", "message": "and now I am at 4pages/min", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:56:57.471723+00:00", "nick": "bezirg", "message": "and going down", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:57:04.090032+00:00", "nick": "toothrot", "message": "if the server can handle it, it should be going faster", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:57:22.286758+00:00", "nick": "bezirg", "message": "ok I will try with bfs", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:57:23.229734+00:00", "nick": "toothrot", "message": "let me look at the docs a bit more about download_delay", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:57:31.122274+00:00", "nick": "toothrot", "message": "that probably won't make a difference", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:57:35.672186+00:00", "nick": "bezirg", "message": "ok", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:57:37.520348+00:00", "nick": "toothrot", "message": "if it's the settings", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:57:44.903481+00:00", "nick": "toothrot", "message": "(or the server)", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:58:01.413915+00:00", "nick": "toothrot", "message": "also, there's CONCURRENT_REQUESTS_PER_DOMAIN", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:58:04.779786+00:00", "nick": "toothrot", "message": "which is default 8", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:58:11.338738+00:00", "nick": "toothrot", "message": "so you may be running into that", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:59:11.220291+00:00", "nick": "toothrot", "message": "DOWNLOAD_DELAY", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:59:11.221713+00:00", "nick": "toothrot", "message": "Default: 0", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:59:11.222842+00:00", "nick": "toothrot", "message": "The amount of time (in secs) that the downloader should wait before downloading consecutive pages", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T22:59:18.162556+00:00", "nick": "toothrot", "message": "from the same website", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:00:07.055256+00:00", "nick": "toothrot", "message": "sounds like that's applied across all requests", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:00:11.441517+00:00", "nick": "toothrot", "message": "but you could test it", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:00:17.632491+00:00", "nick": "bezirg", "message": "i don't understand the difference betweeen CONCURRENT_REQUESTS and CONCURRENT_REQUESTS_PER_DOMAIN", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:00:34.659823+00:00", "nick": "bezirg", "message": "if I am polite, how there can be concurrent_requests_per_domain?", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:00:37.282037+00:00", "nick": "bezirg", "message": "that buffles me", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:00:38.415193+00:00", "nick": "toothrot", "message": "one is overall across all domains, the other for a specific domain", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:01:15.046599+00:00", "nick": "toothrot", "message": "i think with your current setup you will never get more than 2 items per minute from each domain", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:01:22.034559+00:00", "nick": "bezirg", "message": "y u r right", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:01:36.483956+00:00", "nick": "bezirg", "message": "that's what I want", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:02:01.056449+00:00", "nick": "toothrot", "message": "you want it slow? and it is slow. so...", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:02:39.607382+00:00", "nick": "toothrot", "message": "or, what are you saying you want?", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:02:55.143009+00:00", "nick": "toothrot", "message": "how many reqs per sec do you want to each domain? (sounds like there's only 1 perhaps?)", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:03:17.245765+00:00", "nick": "bezirg", "message": "i want 1 concurrent request per domain", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:03:26.258842+00:00", "nick": "bezirg", "message": "but many concurrent requests", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:03:41.215941+00:00", "nick": "toothrot", "message": "how many domains are involved the crawl?", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:03:51.385644+00:00", "nick": "bezirg", "message": "if I cannot hit a domain bcs of politeness, instead of waiting 30secs, I want to fetch non-stalled domains", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:04:00.881357+00:00", "nick": "bezirg", "message": "15000 domains", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:04:59.998199+00:00", "nick": "toothrot", "message": "so yes, changing to a BFS sohuld help this case", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:05:06.337955+00:00", "nick": "toothrot", "message": "i was wrong to say it would not", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:05:19.743621+00:00", "nick": "toothrot", "message": "not you're jammed up with all requests on one (or a few) domains all waiting 30 secs", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:05:30.907346+00:00", "nick": "toothrot", "message": "it could still happen, but it should be better", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:06:44.497155+00:00", "nick": "toothrot", "message": "(or maybe the scheduler is smart enough in that case... not sure)", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:07:16.738045+00:00", "nick": "bezirg", "message": "toothrot: thx I will try BFS", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:07:33.599501+00:00", "nick": "bezirg", "message": "should I keep concurrent_request = 100 and make concurrent_requests_per_domain=1 ?", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:07:53.424881+00:00", "nick": "bezirg", "message": "does per_domain=1 make sense in my case, u think?", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:08:08.088581+00:00", "nick": "toothrot", "message": "depends on what you want, it's nice the sites for sure.", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:13:33.731034+00:00", "nick": "rabidmadman", "message": "hey toothrot im having a weird issue where I'm returning from a callback function without issues. But the function being returned to is also called from another callback function. At this poitn python throws the error about having a return within a yield", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:13:56.507448+00:00", "nick": "rabidmadman", "message": "i basically want to return the state of an item to the prevoius function that it was called in", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:14:11.219518+00:00", "nick": "rabidmadman", "message": "and can't seem to easily figure this out", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:14:47.353723+00:00", "nick": "rabidmadman", "message": "greatly appreciate all of your help", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:17:34.181086+00:00", "nick": "toothrot", "message": "if you're talking about a previous callback you can't do that", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:18:35.431903+00:00", "nick": "toothrot", "message": "the previous callback functions have already returned", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:19:28.877780+00:00", "nick": "rabidmadman", "message": "i guess i don't fully understand what's goign on with the request objects", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:19:46.259868+00:00", "nick": "toothrot", "message": "what exactly are you trying to accomplish?", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:19:52.732959+00:00", "nick": "rabidmadman", "message": "basically exactly what we discussed before haha", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:20:24.329750+00:00", "nick": "rabidmadman", "message": "one function yields a request object and then that function yields another request object", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:20:47.230277+00:00", "nick": "rabidmadman", "message": "but i don't want the item to be yielded once the 3rd function is done", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:21:09.365293+00:00", "nick": "toothrot", "message": "so are you now deciding you don't want to wait until the end of the crawl ?", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:22:46.105457+00:00", "nick": "rabidmadman", "message": "function 1 yields a request object to function 2. Function 2 yields a request object to function 3. I want function 3 to complete what i'ts doing, then go back to function 2 and have that complete what it's doing (go back and forth) and then yield the item in function 1", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:23:49.000697+00:00", "nick": "rabidmadman", "message": "functoin 2 is called within a foor loop of function 1. Function 3 is called within a for loop of function 2", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:27:53.181314+00:00", "nick": "rabidmadman", "message": "ideally, I'd have liked to return the state of the item in the deepest nested function to the previous one, then do the same for the next one and so forth", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:31:15.787986+00:00", "nick": "rabidmadman", "message": "i was thinking  i could just set the Request object to a variable and do all of my parsing within the same function", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:31:44.509097+00:00", "nick": "rabidmadman", "message": "so instead of specifying a callback, just do the parsing within the same function", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:33:08.461057+00:00", "nick": "rabidmadman", "message": "but this then leaves me with the default parse function callback so it's no good", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:39:19.231799+00:00", "nick": "bezirg", "message": "toothrot: I changed to BFS scheduler. I manually checked the sites crawled and indeed they are in breadth-first search. However I have similar speed as DFS. I think DOWNLOAD_DELAY is just a sleeping timer for a concurrent request handler", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:39:33.895339+00:00", "nick": "bezirg", "message": "toothrot: it does not apply well in BFS", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:48:18.878193+00:00", "nick": "toothrot", "message": "bezirg, so the requests are coming from multiple domains?", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:48:27.690399+00:00", "nick": "bezirg", "message": "toothrot: y", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:48:51.024369+00:00", "nick": "bezirg", "message": "the seed has unique domains. for each unique domain, I want to crawl its internal links", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:48:55.030613+00:00", "nick": "bezirg", "message": "up to some depth", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:51:05.333803+00:00", "nick": "bezirg", "message": "i moved to BFS +  download_delay=2", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:51:17.871042+00:00", "nick": "bezirg", "message": "now I can sustain 90pages/min", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:52:21.627321+00:00", "nick": "toothrot", "message": "what version os scrapy are you on?", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:52:53.430734+00:00", "nick": "toothrot", "message": "er, of", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:53:15.999021+00:00", "nick": "bezirg", "message": "latest", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:53:22.402371+00:00", "nick": "bezirg", "message": "0.24.4", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:56:10.927998+00:00", "nick": "toothrot", "message": "it seems like it shouldn't be the case according to the docs, however without know exactly what's sitting in the scheduler's queue it's hard to say", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:57:42.618798+00:00", "nick": "bezirg", "message": "toothrot: do u think I should contact the mailing list or add it to the bug tracker?", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:58:08.544232+00:00", "nick": "toothrot", "message": "you could try the mail list... i'm tempted to test it out, but i don't have time", "links": [], "channel": "scrapy"},
{"date": "2014-11-09T23:59:50.490342+00:00", "nick": "bezirg", "message": "toothrot: ok, thx", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T00:00:09.198279+00:00", "nick": "toothrot", "message": "is it possible the scheduler is backed up with requests from the same domain ?", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T00:03:14.889937+00:00", "nick": "bezirg", "message": "y", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T00:03:32.432907+00:00", "nick": "bezirg", "message": "internal links", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T00:03:56.476239+00:00", "nick": "bezirg", "message": "domain.com and i yield in parse 10 urls like domain.com/sth", "links": ["http://domain.com", "http://domain.com/sth"], "channel": "scrapy"},
{"date": "2014-11-10T00:05:07.209973+00:00", "nick": "bezirg", "message": "toothrot: also another question, DEPTH=1 setting meens the seed (start_urls) + 1 level deep from there?", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T00:05:27.529443+00:00", "nick": "bezirg", "message": "bcs I was used to DEPTH=1 means only crawl the seed (start_urls)", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T00:05:37.730430+00:00", "nick": "bezirg", "message": "in other crawler software", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T00:05:56.284432+00:00", "nick": "toothrot", "message": "DEPTH_LIMIT ?", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T00:06:11.204339+00:00", "nick": "bezirg", "message": "y", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T00:07:05.780217+00:00", "nick": "toothrot", "message": "not familiar with it, but that sounds reasonable", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T00:07:17.454298+00:00", "nick": "toothrot", "message": "it's what i would think", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T00:07:20.631698+00:00", "nick": "toothrot", "message": "(if you only want to crawl the start_urls, just don't use a CrawlSpider and don't yield any requests, i guess)", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T00:09:02.669840+00:00", "nick": "bezirg", "message": "toothrot: u r right, but in debugging, it would be nice if you could only crawl the seed", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T00:09:28.473305+00:00", "nick": "bezirg", "message": "toothrot: i don't know how u can do it in scrapy, since setting DEPTH_LIMIT=0 makes it unlimited", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T00:09:35.376869+00:00", "nick": "toothrot", "message": "true", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T00:10:00.741545+00:00", "nick": "toothrot", "message": "what does -1 do ?", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T00:11:18.433756+00:00", "nick": "toothrot", "message": "might work since the special case is for 0", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T00:11:27.601274+00:00", "nick": "toothrot", "message": "or it might reject the start_urls too", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T00:13:27.920982+00:00", "nick": "bezirg", "message": "didn't try it", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T00:13:29.091816+00:00", "nick": "bezirg", "message": "I will", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T00:41:01.691009+00:00", "nick": "bezirg", "message": "toothrot: it seems like BFS is not respected", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T00:42:10.837274+00:00", "nick": "bezirg", "message": "toothrot: I am hitting the same website (a depth 1 page following by a depth 2 page with no politeness) without even finishing the start_seed", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T00:42:31.470494+00:00", "nick": "bezirg", "message": "I thought by using BFS, I would finish the whole start_urls seed and then move on to the next depth", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T00:42:35.112373+00:00", "nick": "bezirg", "message": "that is not the case", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T02:16:54.425596+00:00", "nick": "rabidmadman", "message": "yeah i still have no clue how to re-implement this", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T02:17:03.512300+00:00", "nick": "rabidmadman", "message": "gonna be a long night", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T02:56:43.114719+00:00", "nick": "rabidmadman", "message": "hmm im thinking about makign a custom pipeline that returns an item only when it's 'complete'", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:15:18.992051+00:00", "nick": "toothrot", "message": "rabidmadman, \"functoin 2 is called within a foor loop of function 1. Function 3 is called within a for loop of function 2\", but these aren't really functions called within each other, are they?", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:15:26.436482+00:00", "nick": "toothrot", "message": "they're callbacks for Requests", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:18:21.837563+00:00", "nick": "rabidmadman", "message": "yeah i wasn't wording it properly", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:18:35.269302+00:00", "nick": "toothrot", "message": "i still think the simplest solution is to assemble the items after the crawl is done, why did you decide against that?", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:18:56.812595+00:00", "nick": "rabidmadman", "message": "no idea how to do that", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:19:16.045709+00:00", "nick": "toothrot", "message": "each band has a indentifier, right ?", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:19:19.403212+00:00", "nick": "toothrot", "message": "identifier?", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:19:26.004842+00:00", "nick": "rabidmadman", "message": "build multiple global items and then attach them to each other?", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:19:33.974146+00:00", "nick": "rabidmadman", "message": "yeah each band has a unique identifier", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:19:44.761921+00:00", "nick": "toothrot", "message": "well, it would'n't be global, it would be an attribute on the spider probably", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:20:00.246151+00:00", "nick": "rabidmadman", "message": "yeah true thats what i meant", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:20:03.874581+00:00", "nick": "toothrot", "message": "create a dict self.band_items = {} on the spider instance", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:20:15.237652+00:00", "nick": "toothrot", "message": "for each Request, pass that identifer in meta", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:20:38.214117+00:00", "nick": "rabidmadman", "message": "right so that is sort of what i was trying to do", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:20:51.691554+00:00", "nick": "toothrot", "message": "and at the end of each Request.. self.band_items.setdefault(band_id, {})['parse1_results']", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:20:59.791837+00:00", "nick": "toothrot", "message": "self.band_items.setdefault(band_id, {})['parse2_results']", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:21:21.427713+00:00", "nick": "toothrot", "message": "self.band_items.setdefault(band_id, {})['parse3_results'] = 'whatever'", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:21:41.698838+00:00", "nick": "rabidmadman", "message": "Ah i see", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:21:49.799473+00:00", "nick": "toothrot", "message": "what you're talking about before can be accomplished by using twisted's DeferredList, or simlilar solution", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:22:11.296946+00:00", "nick": "toothrot", "message": "but it will require knowledge of twisted and care to not block scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:22:23.448206+00:00", "nick": "rabidmadman", "message": "so i will definitely have to redfine my item right?", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:22:29.430502+00:00", "nick": "toothrot", "message": "why?", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:22:50.570067+00:00", "nick": "rabidmadman", "message": "what exactly would i be yielding?", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:23:14.800106+00:00", "nick": "rabidmadman", "message": "because if i am just setting values to the spider atribute, im not yielding any items", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:23:37.844250+00:00", "nick": "toothrot", "message": "one sec, let me test something", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:25:06.441661+00:00", "nick": "rabidmadman", "message": "based on what you described, my overall result will be stored in the class atribute", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:25:52.485017+00:00", "nick": "rabidmadman", "message": "so it sounds like im not yielding any scrapy items at all", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:28:23.236470+00:00", "nick": "toothrot", "message": "that's a good point, i believe there's a way that you can yield items at the end of a run", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:28:30.681923+00:00", "nick": "toothrot", "message": "i'm just not discovering/remembering it", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:28:59.186979+00:00", "nick": "rabidmadman", "message": "with my implementation i think i could get it to work somehow", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:29:01.516994+00:00", "nick": "toothrot", "message": "your idea to pass the item through meta, and then use your item pipeline to do a is_item_complete(item) kind of thing would be simple too", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:29:19.594773+00:00", "nick": "rabidmadman", "message": "yeah im passing the item through the meta", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:29:38.731678+00:00", "nick": "rabidmadman", "message": "its just that  if i yield every time i get lyrics, i'll have 10000's of objects", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:29:41.024745+00:00", "nick": "rabidmadman", "message": "instead of 100's", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:31:13.687135+00:00", "nick": "toothrot", "message": "that's why you can use the item pipeline to NOT yield the item when it's not complete", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:31:14.884236+00:00", "nick": "toothrot", "message": "http://doc.scrapy.org/en/latest/topics/item-pip...", "links": ["http://doc.scrapy.org/en/latest/topics/item-pipeline.html#process_item"], "channel": "scrapy"},
{"date": "2014-11-10T04:33:08.657201+00:00", "nick": "kaido", "message": "hello", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:33:32.181795+00:00", "nick": "toothrot", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:34:34.842009+00:00", "nick": "kaido", "message": "i am heaving some serious issues with scrapping all data from this page http://www.oxygenboutique.com, i cant get all product, for example under dresses.asp i get 21 items but there are around 80 items", "links": ["http://www.oxygenboutique.com"], "channel": "scrapy"},
{"date": "2014-11-10T04:35:03.783074+00:00", "nick": "kaido", "message": "http://pastebin.com/xHZtSKQ7", "links": ["http://pastebin.com/xHZtSKQ7"], "channel": "scrapy"},
{"date": "2014-11-10T04:35:10.541509+00:00", "nick": "kaido", "message": "this is my code", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:36:00.513632+00:00", "nick": "kaido", "message": "anyone have any recommendation of what I'm doing wrong ? :)", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:43:43.063009+00:00", "nick": "toothrot", "message": "rabidmadman, also, you can yield Items using self.engine.scraper fron spider_close", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:43:56.001799+00:00", "nick": "toothrot", "message": "so it still is possible to do it at the end", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:44:05.649518+00:00", "nick": "toothrot", "message": "although i think your current idea is just fine", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:44:30.747373+00:00", "nick": "rabidmadman", "message": "ok toothrot I greatly greatly apprciate your help", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:44:38.966966+00:00", "nick": "rabidmadman", "message": "i hope i can figure this out", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:44:59.647804+00:00", "nick": "toothrot", "message": "do you have your item pipeline operating ?", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:46:01.321119+00:00", "nick": "toothrot", "message": "should be simple: def process_item(self, item): if not item_has_all_stuff(item): raise DropItem; else return item", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:47:51.852197+00:00", "nick": "toothrot", "message": "kaido, debug your xpath, and make sure nothing is being added through AJAX", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:49:59.848538+00:00", "nick": "toothrot", "message": "also, this code is not correct: `callback = self.parse_page(response)`", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:50:17.727089+00:00", "nick": "toothrot", "message": "you want to pass the method, but you're calling the handler yourself", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:50:37.777266+00:00", "nick": "toothrot", "message": "which appears to return None implicitly", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:50:50.176498+00:00", "nick": "rabidmadman", "message": "toothrot would i just be adding items to a list?", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:51:07.126995+00:00", "nick": "toothrot", "message": "rabidmadman, with the item pipeline method, you don't need to", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:51:09.704107+00:00", "nick": "rabidmadman", "message": "also an item isn't done until all of its releases have been processed", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:51:11.499367+00:00", "nick": "toothrot", "message": "what do you want to do with the data ?", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:51:23.075114+00:00", "nick": "rabidmadman", "message": "what i want to do conceptually, is yield the item once every release has been processed", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:51:44.383074+00:00", "nick": "toothrot", "message": "okayif that's the case you have no way of knowning how many relases are included, so your pipeline method won't work.", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:51:48.575898+00:00", "nick": "toothrot", "message": "releases", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:51:56.894264+00:00", "nick": "toothrot", "message": "unless you process all releases in one request ?", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:51:59.607650+00:00", "nick": "rabidmadman", "message": "i'm basically building a sister site to the metal archives and using their data for testing purposes", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:52:06.169575+00:00", "nick": "rabidmadman", "message": "yeah see that would be ideal", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:52:11.422523+00:00", "nick": "rabidmadman", "message": "but each release has a unique url", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:52:27.268511+00:00", "nick": "rabidmadman", "message": "i was trying to do that but request objects require a parse call back", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:52:30.041771+00:00", "nick": "toothrot", "message": "whether it's ideal is irrelevant, if the site does not support it, it's not viable", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:52:48.043888+00:00", "nick": "rabidmadman", "message": "yeah i know", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:53:14.348889+00:00", "nick": "toothrot", "message": "although, if you know the # of releases ahead of item", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:53:26.895638+00:00", "nick": "toothrot", "message": "you could still check in the item pipeline whether they are all complete", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:53:43.722894+00:00", "nick": "toothrot", "message": "you'll be checking lots of items again and again though", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:54:42.154151+00:00", "nick": "toothrot", "message": "i still think the easiest thing to do would be to do all this at the end", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:54:56.710313+00:00", "nick": "toothrot", "message": "you don't HAVE to use items either if you jsut want to save the data", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:55:05.810686+00:00", "nick": "rabidmadman", "message": "ah yeah", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:55:10.889915+00:00", "nick": "toothrot", "message": "just store it all, and then process it in a spider_closed signal handler", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:55:49.483740+00:00", "nick": "rabidmadman", "message": "without rewriting my code your suggestion of checking whether they are all complete is the best one", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:55:59.680237+00:00", "nick": "rabidmadman", "message": "i have to figure out how to implement that", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:56:10.270690+00:00", "nick": "rabidmadman", "message": "i know the # of releases from the page", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T04:56:44.890357+00:00", "nick": "toothrot", "message": "sure, so store that, and then as each item is possibly going to be completed check thAT # of relaseses is completely filled out", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:00:25.142387+00:00", "nick": "kaido", "message": "toothrot: i get data back, but i dont know how to extract all dresses.aspx products, i get only 21 results returned", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:01:31.741669+00:00", "nick": "toothrot", "message": "kaido, you need to fix the mistake i pointed out first of all", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:02:53.902194+00:00", "nick": "rabidmadman", "message": "and this would be in the pipelines script?", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:03:34.416030+00:00", "nick": "toothrot", "message": "rabidmadman, you can do it there", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:03:52.824615+00:00", "nick": "toothrot", "message": "it's kind of weird, becasue you're yielding the same item over and over, hoping it's complete", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:04:51.699466+00:00", "nick": "toothrot", "message": "anyways, good luck to you both, gotta get some sleep", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:05:50.109911+00:00", "nick": "toothrot", "message": "rabidmadman, i really think it's easier to store everything and then just json.dump() it all when your spider is done", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:06:41.397401+00:00", "nick": "toothrot", "message": "it sohuld be a massive change, you just have to store the base item in that self.band_items, and then populate it through each parse method (which gets it through meta)", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:06:52.447030+00:00", "nick": "toothrot", "message": "it shouldn't be a massive change", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:06:56.915683+00:00", "nick": "toothrot", "message": "rather..", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:06:58.325099+00:00", "nick": "rabidmadman", "message": "so", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:07:08.422720+00:00", "nick": "rabidmadman", "message": "i would just be yielding the requests right?", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:07:20.259672+00:00", "nick": "rabidmadman", "message": "and once the overall dictionary is populated, dump it?", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:07:21.460588+00:00", "nick": "toothrot", "message": "you wouldn't yield any items, just build them up", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:07:50.550893+00:00", "nick": "rabidmadman", "message": "so my dictionary would be a list of band objects simlar to the items right?", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:07:58.448343+00:00", "nick": "rabidmadman", "message": "i mean a list of dictionaries*", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:08:02.820755+00:00", "nick": "toothrot", "message": "not a list, no", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:08:18.669337+00:00", "nick": "rabidmadman", "message": "seems like that's what scrapy dumps at the end, a list of items", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:09:00.672999+00:00", "nick": "toothrot", "message": "a dict of something like {'band_id1': {name: whatever', 'releases': [{...}, {...}]}, 'band_id2': {...} }", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:09:29.000546+00:00", "nick": "toothrot", "message": "you could even dump a json file per band, whatever you feel like would be easier to process later", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:09:39.643970+00:00", "nick": "toothrot", "message": "actually it could be a list", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:09:49.544447+00:00", "nick": "toothrot", "message": "if you pass the item through meta you don't need to store them by id", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:10:01.242951+00:00", "nick": "toothrot", "message": "as long as you have a reference to the band item", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:10:35.303714+00:00", "nick": "toothrot", "message": "just append the empty item/dict to the list ONCE before you kick off all the other requests", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:10:42.443498+00:00", "nick": "rabidmadman", "message": "i see", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:11:17.910240+00:00", "nick": "toothrot", "message": "well, it wouldn't be empty at that point i guess, but would only have the data from the band page.. releases/lyrics to be added in later parse methods", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:11:29.139135+00:00", "nick": "rabidmadman", "message": "so basically just do not yield any items", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:11:37.004344+00:00", "nick": "rabidmadman", "message": "just dump the overall dict that i build", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:11:45.089839+00:00", "nick": "toothrot", "message": "or list", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:11:49.668865+00:00", "nick": "rabidmadman", "message": "list of dicts yeah", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:11:51.989769+00:00", "nick": "toothrot", "message": "yea", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:12:06.428507+00:00", "nick": "rabidmadman", "message": "that's essentially what the spider gives when you output json", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:12:43.828311+00:00", "nick": "rabidmadman", "message": "ok i guess i will thnk about this approach..it won't use the nifty item/json export feature of scrpay though", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:12:58.722267+00:00", "nick": "toothrot", "message": "well, you can still do that if you want", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:13:08.181346+00:00", "nick": "toothrot", "message": "instead of dumping the JSON, just yield all the items at the end", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:13:31.363699+00:00", "nick": "toothrot", "message": "(it would an internal method though)", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:13:34.221748+00:00", "nick": "toothrot", "message": "would use", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:14:03.095534+00:00", "nick": "toothrot", "message": "self.crawler.engine.scraper._process_spidermw_output(item, None, None, self) # None, None are req, resp", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:14:47.186953+00:00", "nick": "toothrot", "message": "i don't use the JSON export so i don't know what it gives you over just using json.dump", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:15:32.086665+00:00", "nick": "toothrot", "message": "in fact i have my own pipeline that does json output", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:15:59.980955+00:00", "nick": "rabidmadman", "message": "i see", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:16:01.438332+00:00", "nick": "toothrot", "message": "(it adds crawled_on, and places the files in a directory from settings)", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:16:26.310164+00:00", "nick": "toothrot", "message": "it's about 7 lines, so not a big deal", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:16:35.540982+00:00", "nick": "rabidmadman", "message": "yeah thats pretty cool", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:16:54.570761+00:00", "nick": "toothrot", "message": "it'll be easy either way i guess i'm saying", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:17:03.920567+00:00", "nick": "rabidmadman", "message": "dumping json isn't a big deal", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:17:10.276614+00:00", "nick": "rabidmadman", "message": "its just yielding the right items righ tnow", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:17:18.683356+00:00", "nick": "rabidmadman", "message": "either that or doing what you said with a new dict", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:17:35.495314+00:00", "nick": "toothrot", "message": "oh, so you got the item check working ?", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:17:39.547762+00:00", "nick": "rabidmadman", "message": "id like to figure out how to yield an item only if it has all of the releases", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:17:42.261297+00:00", "nick": "rabidmadman", "message": "no i haven't coded yet", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:17:45.290097+00:00", "nick": "rabidmadman", "message": "just thinking", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:18:10.784260+00:00", "nick": "toothrot", "message": "like i said, you can do this with twisted, it's just not worth it IMO", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:18:12.397193+00:00", "nick": "rabidmadman", "message": "i'm not too familiar with pipelines so this would be an additional class that checks the items release count right?", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:18:23.446665+00:00", "nick": "rabidmadman", "message": "no i think checking an items release count would be the easiest thing to do", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:18:45.119539+00:00", "nick": "toothrot", "message": "yeah, right you just need to implement one method, process_item", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:18:52.995696+00:00", "nick": "toothrot", "message": "and then you add that class to your settings", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:18:55.954499+00:00", "nick": "rabidmadman", "message": "so when i do yield item in my most inner function, it simply won't yield unless it has the right release count?", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:19:13.372008+00:00", "nick": "toothrot", "message": "you yield it whether or not it's complete", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:19:23.769424+00:00", "nick": "toothrot", "message": "or you can check it then", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:19:29.864354+00:00", "nick": "toothrot", "message": "it doesn't really matter", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:19:55.388327+00:00", "nick": "toothrot", "message": "check it before you try to yield it and forget the extra pipeline class", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:20:22.376703+00:00", "nick": "toothrot", "message": "either way it'll be `if is_item_complete(item): yield item`", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:21:02.484286+00:00", "nick": "toothrot", "message": "dang, it's too late, good luck", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:21:50.578701+00:00", "nick": "rabidmadman", "message": "thanks man", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:22:20.840661+00:00", "nick": "rabidmadman", "message": "so i would call the if is_item complete method?", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:22:26.993188+00:00", "nick": "rabidmadman", "message": "and if so, yield the item?", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:22:31.072296+00:00", "nick": "rabidmadman", "message": "that makes sense to me", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:23:55.734068+00:00", "nick": "toothrot", "message": "that would be one way of doing it that seems simple enough", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:24:18.354070+00:00", "nick": "toothrot", "message": "(you'll have to do it for each release, but who cares if it makes it easy)", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:24:22.153398+00:00", "nick": "rabidmadman", "message": "so just implement a method that checks it", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:24:26.228422+00:00", "nick": "rabidmadman", "message": "no need for pipeline?", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:24:32.457652+00:00", "nick": "toothrot", "message": "right", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:24:36.446654+00:00", "nick": "rabidmadman", "message": "ah ok", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:24:43.544802+00:00", "nick": "rabidmadman", "message": "i don't want to hold you anymore", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:24:52.166347+00:00", "nick": "rabidmadman", "message": "i don't really understand the pipelines", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:25:07.464095+00:00", "nick": "rabidmadman", "message": "because the docs make it seem that pipelines are necessary for any modifications to the yielding process", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:25:26.623696+00:00", "nick": "toothrot", "message": "they aren't it's just a way to oragnize and possibly re-use code", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:25:50.350546+00:00", "nick": "toothrot", "message": "eg my pipeline that adds the crawled_on date and then dumps the json i used in multiple crawlers", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:25:56.267065+00:00", "nick": "toothrot", "message": "i just add it into my settings", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:26:08.431202+00:00", "nick": "rabidmadman", "message": "i see", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:26:28.471701+00:00", "nick": "rabidmadman", "message": "so if i put this method in teh pipelines script", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:26:34.504071+00:00", "nick": "rabidmadman", "message": "how would i access it in the spider?", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:26:39.464187+00:00", "nick": "toothrot", "message": "i thin it makes more sense in the spider really", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:26:43.546879+00:00", "nick": "rabidmadman", "message": "yeah same here", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:26:53.988446+00:00", "nick": "rabidmadman", "message": "but how do you call item pipelines within a spider?", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:27:01.473247+00:00", "nick": "toothrot", "message": "you just yield the item", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:27:07.462005+00:00", "nick": "toothrot", "message": "scrapy calls proces_item for you", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:27:17.977533+00:00", "nick": "toothrot", "message": "you add your pipeline class to your settings", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:27:20.713277+00:00", "nick": "rabidmadman", "message": "oh ok, so this is why i feel the pipeline makes more sense now hahah", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:27:24.412213+00:00", "nick": "toothrot", "message": "scrapy instatietes it", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:27:39.840702+00:00", "nick": "toothrot", "message": "and when it see an item, calls process_item for each pipeline class", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:27:45.070783+00:00", "nick": "rabidmadman", "message": "ok awesome", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:27:49.326116+00:00", "nick": "rabidmadman", "message": "i think i will do that", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:27:58.785679+00:00", "nick": "rabidmadman", "message": "i greatly appreciate your help dude, you are awesome", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:28:00.771056+00:00", "nick": "toothrot", "message": "each process_item can either change the item, raise DropItem", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:28:27.461853+00:00", "nick": "rabidmadman", "message": "right so i used the docs to make a duplicate pipline but I was goign to check for duplicates in an overall list..wouldve been a huge mess", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:28:44.445807+00:00", "nick": "toothrot", "message": "the only thing about the pipeline is that you'd be yielding the same item multiple times, hoping it was complete", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:28:56.476401+00:00", "nick": "rabidmadman", "message": "right but the pipeline wouldn't process it if it wasn't", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:28:58.314324+00:00", "nick": "rabidmadman", "message": "so thats good", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:28:59.397756+00:00", "nick": "toothrot", "message": "(rasing DropItem when it wasn't)", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:29:13.725160+00:00", "nick": "rabidmadman", "message": "so the item wouldn't retain its changes?", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:29:31.323885+00:00", "nick": "toothrot", "message": "it would.", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:29:53.117798+00:00", "nick": "toothrot", "message": "just because the pipeline drops the item and stops processing doesn't mean then item goes away", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:30:02.388371+00:00", "nick": "toothrot", "message": "you'd have a referece you would yield again", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:30:13.587359+00:00", "nick": "rabidmadman", "message": "ok", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:30:20.125650+00:00", "nick": "rabidmadman", "message": "so this should work", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:30:33.068977+00:00", "nick": "rabidmadman", "message": "or i could implement a method for the spider that does the same thing", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:30:35.856690+00:00", "nick": "toothrot", "message": "like i said, do the check where ever you'd like", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:30:53.518612+00:00", "nick": "rabidmadman", "message": "so the benefit of doing it in the spider is that theres less processing going on", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:30:54.622135+00:00", "nick": "toothrot", "message": "i think it make smore sense on the spider, then you're only yielding each item once when it's done", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:30:58.615944+00:00", "nick": "toothrot", "message": "not really", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:30:58.972646+00:00", "nick": "rabidmadman", "message": "just a simple if check", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:31:04.696092+00:00", "nick": "toothrot", "message": "it just happens somewhere else", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:31:07.595715+00:00", "nick": "rabidmadman", "message": "ah ok", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:31:18.045328+00:00", "nick": "rabidmadman", "message": "ill do it in the pipeline", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:31:18.959012+00:00", "nick": "toothrot", "message": "maybe slightly less, since it keeps it out of scrapy's item processing", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:31:23.378226+00:00", "nick": "toothrot", "message": "hehehe.", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:31:26.212699+00:00", "nick": "rabidmadman", "message": "yeah", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:31:30.265904+00:00", "nick": "toothrot", "message": "it really doesn't matter", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:31:33.192217+00:00", "nick": "rabidmadman", "message": "well thanks man, ill let you go", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:31:35.860435+00:00", "nick": "toothrot", "message": "later", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T05:31:38.217330+00:00", "nick": "rabidmadman", "message": "good night", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T10:42:22.687399+00:00", "nick": "fpghost84", "message": "Hi, I have an image that displays but then if I set width 100% it disappears, why is this?", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T10:42:47.070163+00:00", "nick": "fpghost84", "message": "I can still see the border of the img itself just not the picture", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T10:43:00.269587+00:00", "nick": "fpghost84", "message": "(I'm embedding from an external gif btw)", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T12:35:26.612960+00:00", "nick": "asd__", "message": "Did anyone worked with built in email solution that scrapy has?", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T12:39:38.452282+00:00", "nick": "nikolaosk", "message": "asd__: do you mean the stats mailer?", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T12:43:52.785632+00:00", "nick": "asd__", "message": "nikolaosk, i think so", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T12:44:31.065481+00:00", "nick": "asd__", "message": "i got it to work with smtplib library, but not with mailer.send(to=[\"someone@example.com\"], subject=\"Some subject\", body=\"Some body\", cc=[\"another@example.com\"])", "links": ["mailto:mailer.send(to=[\"someone@example.com\"]", "mailto:cc=[\"another@example.com\"]"], "channel": "scrapy"},
{"date": "2014-11-10T12:45:58.437826+00:00", "nick": "nikolaosk", "message": "you got IT to work?", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T12:46:14.105184+00:00", "nick": "nikolaosk", "message": "I don't get it", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T12:46:24.024406+00:00", "nick": "nikolaosk", "message": "are you implementing your own solution, the example above", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T12:46:35.923111+00:00", "nick": "nikolaosk", "message": "and because it doesn't work you want to try the builtin", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T12:47:06.158808+00:00", "nick": "asd__", "message": "yeah", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T12:47:35.459667+00:00", "nick": "nikolaosk", "message": "ok, to use the stats mailer all you need is to set a setting", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T12:47:55.463417+00:00", "nick": "nikolaosk", "message": "STATS_RCPTS I think", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T12:48:01.268811+00:00", "nick": "nikolaosk", "message": "look up its source", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T12:48:13.180917+00:00", "nick": "nikolaosk", "message": "the rest are irrelevant to scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T12:50:08.648611+00:00", "nick": "asd__", "message": "mailer = MailSender.from_settings(settings), something like this?", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T12:56:45.719971+00:00", "nick": "nikolaosk", "message": "I don't understand at all", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T12:59:11.265006+00:00", "nick": "asd__", "message": "I think i got it", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T16:41:42.571737+00:00", "nick": "asd__", "message": "nikolaosk, you are still here?", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T17:35:07.738894+00:00", "nick": "asd__", "message": "did anyone worked with MailSender? i can't get it to work properly", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T19:46:17.308813+00:00", "nick": "asd", "message": "did anyone worked with MailSender? i can't get it to work properly", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T20:12:49.308338+00:00", "nick": "Jo777", "message": "Hello, I want to store a taglist in an item, what is a good way to to so...", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T20:13:20.996061+00:00", "nick": "Jo777", "message": "So I have the item product, and each product has multiple tags. I want to store them in an item...", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T20:13:25.395895+00:00", "nick": "Jo777", "message": "How can I do this...", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T20:50:05.394549+00:00", "nick": "rabidmadman", "message": "you could set the product = to an empty dictionary and just set its values or you can set it to a list and append values", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T20:52:07.439492+00:00", "nick": "rabidmadman", "message": "item['product'[ = {'tags': []} or something", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T21:21:11.008030+00:00", "nick": "Jo777", "message": "@rabidmadman: I use an item loader and just have to do nothing :) My tags are a list when I just use add_xpath and take Identity() as output processor", "links": [], "channel": "scrapy"},
{"date": "2014-11-10T21:21:16.382827+00:00", "nick": "Jo777", "message": "Thank you", "links": [], "channel": "scrapy"},
{"date": "2014-11-11T00:24:54.334545+00:00", "nick": "toothrot", "message": "rabidmadman, success?", "links": [], "channel": "scrapy"},
{"date": "2014-11-11T02:16:06.276015+00:00", "nick": "rabidmadman", "message": "toothrot", "links": [], "channel": "scrapy"},
{"date": "2014-11-11T02:16:20.862706+00:00", "nick": "rabidmadman", "message": "still no success, although i lowered my scraped item count to 400 isntead of in the 1000's", "links": [], "channel": "scrapy"},
{"date": "2014-11-11T02:17:45.861734+00:00", "nick": "rabidmadman", "message": "i added a lyrics count field to my item and am passing a parsed_lyrics variable to the lyrics callback and i am basically checking to see if the parsed lyrics equals the total lyric count. In addition to this check, I'm asloc hecking if the amount of releases in my release list equal to the total releases that the band has", "links": [], "channel": "scrapy"},
{"date": "2014-11-11T02:18:06.282018+00:00", "nick": "rabidmadman", "message": "and only the ndo i yield the item", "links": [], "channel": "scrapy"},
{"date": "2014-11-11T02:18:28.019307+00:00", "nick": "rabidmadman", "message": "i'm starting to think this is somethign that should be done in the pipeline though", "links": [], "channel": "scrapy"},
{"date": "2014-11-11T04:09:38.999469+00:00", "nick": "toothrot", "message": "rabidmadman, why? that part of it can be made into one if/else before the yield... it's not likely to make a huge difference.", "links": [], "channel": "scrapy"},
{"date": "2014-11-11T04:27:24.866619+00:00", "nick": "rabidmadman", "message": "well here's the thing", "links": [], "channel": "scrapy"},
{"date": "2014-11-11T04:27:41.781118+00:00", "nick": "rabidmadman", "message": "i didn't initialize the item as a class attribute", "links": [], "channel": "scrapy"},
{"date": "2014-11-11T04:27:49.776612+00:00", "nick": "rabidmadman", "message": "it's initialized in the first parse function", "links": [], "channel": "scrapy"},
{"date": "2014-11-11T04:28:48.245130+00:00", "nick": "rabidmadman", "message": "so throughotu the for loop, is the state of the item preserved during times in which it isn't yielded?", "links": [], "channel": "scrapy"},
{"date": "2014-11-11T04:32:47.544505+00:00", "nick": "rabidmadman", "message": "like her'es the gist of what i'm doing:", "links": [], "channel": "scrapy"},
{"date": "2014-11-11T04:32:47.690605+00:00", "nick": "rabidmadman", "message": "", "links": [], "channel": "scrapy"},
{"date": "2014-11-11T04:32:47.690704+00:00", "nick": "rabidmadman", "message": "item['lyrics_count'] = whatever", "links": [], "channel": "scrapy"},
{"date": "2014-11-11T04:32:47.692129+00:00", "nick": "rabidmadman", "message": "for x in whatever:", "links": [], "channel": "scrapy"},
{"date": "2014-11-11T04:32:47.693554+00:00", "nick": "rabidmadman", "message": "parse_lyrics + = 1", "links": [], "channel": "scrapy"},
{"date": "2014-11-11T04:32:47.867292+00:00", "nick": "rabidmadman", "message": "yield Request(url,call_back=parse_song_lyrics,meta={'num_lyrics'=parse_lyrics)", "links": [], "channel": "scrapy"},
{"date": "2014-11-11T04:32:47.867363+00:00", "nick": "rabidmadman", "message": "def parse_song_lyrics:", "links": [], "channel": "scrapy"},
{"date": "2014-11-11T04:32:48.834910+00:00", "nick": "rabidmadman", "message": "//parsing for lyrics", "links": [], "channel": "scrapy"},
{"date": "2014-11-11T04:32:48.980191+00:00", "nick": "rabidmadman", "message": "lyrics_count = response.meta['num_lyrics']", "links": [], "channel": "scrapy"},
{"date": "2014-11-11T04:32:49.854534+00:00", "nick": "rabidmadman", "message": "if len(releases) == num_releases:", "links": [], "channel": "scrapy"},
{"date": "2014-11-11T04:32:49.999864+00:00", "nick": "rabidmadman", "message": "if num_lyrics = lyrics_count:", "links": [], "channel": "scrapy"},
{"date": "2014-11-11T04:32:50.835636+00:00", "nick": "rabidmadman", "message": "yield item", "links": [], "channel": "scrapy"},
{"date": "2014-11-11T04:34:15.592500+00:00", "nick": "rabidmadman", "message": "i calculate how many lyrics available each release has and i store it as a value in my item..and then everytime i call the parse_lyrics function to yield an item, i increment a count that i compare to the total lyrics_count...also it's possible for an item to have no lyrics at all..in which case the request for the lyrics isn't yielded. In this case, I yield the item  using similar if checks", "links": [], "channel": "scrapy"},
{"date": "2014-11-11T05:34:11.318214+00:00", "nick": "AndyRez", "message": "hi all,", "links": [], "channel": "scrapy"},
{"date": "2014-11-11T09:05:17.783764+00:00", "nick": "kakashi_", "message": "hi, is it possible to access crawler.stat in pipeline.py", "links": [], "channel": "scrapy"},
{"date": "2014-11-11T11:13:41.636975+00:00", "nick": "ckesselh", "message": "hi everybody", "links": [], "channel": "scrapy"},
{"date": "2014-11-11T11:16:16.177978+00:00", "nick": "ckesselh", "message": "question for the scrapy experts: i\u2019m trying to sequentially run one spider after another, but only start with spider 2 if the result of spider 1 somehow \u201cmatches my expectation\u201d. I realize that a lot has changed in that part of the API in GIT master, but would there be a way to make this work in 0.24, too?", "links": [], "channel": "scrapy"},
{"date": "2014-11-11T11:17:18.293677+00:00", "nick": "ckesselh", "message": "i\u2019ve seen that all the *.start() methods already return a twisted deferred, but I don\u2019t know twisted well enough to understand how I could put them together to produce the effect I\u2019ve outlined before", "links": [], "channel": "scrapy"},
{"date": "2014-11-11T11:47:38.809001+00:00", "nick": "asd", "message": "can somebody help me out with this question? http://stackoverflow.com/questions/26852868/ema...", "links": ["http://stackoverflow.com/questions/26852868/emailing-when-scrapy-project-is-finished"], "channel": "scrapy"},
{"date": "2014-11-11T12:27:17.416595+00:00", "nick": "flag", "message": "hi all", "links": [], "channel": "scrapy"},
{"date": "2014-11-11T12:28:24.115734+00:00", "nick": "flag", "message": "how can I use complex selectors when declaring an Item Loader? For example, its sometimes not enough just to have one XPath expression like in the examples: l.add_xpath('price', '//p[@id=\"price\"]')", "links": [], "channel": "scrapy"},
{"date": "2014-11-11T12:29:36.480249+00:00", "nick": "flag", "message": "maybe my price is something like: respose.xpath('//div[@id=\"price\"]...", "links": ["mailto:respose.xpath('//div[@id=\"price\"]').css('.someclass').xpath('.//p/text()').extract()"], "channel": "scrapy"},
{"date": "2014-11-11T12:29:39.588398+00:00", "nick": "flag", "message": "or even more comples", "links": [], "channel": "scrapy"},
{"date": "2014-11-11T12:29:42.520144+00:00", "nick": "flag", "message": "complex*", "links": [], "channel": "scrapy"},
{"date": "2014-11-11T12:30:36.588493+00:00", "nick": "flag", "message": "is there are way to concatenate when creating a loader?", "links": [], "channel": "scrapy"},
{"date": "2014-11-11T15:10:12.623616+00:00", "nick": "flag", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2014-11-11T15:10:45.769840+00:00", "nick": "flag", "message": "how about that concatenation of XPath and CSS selectors when creating Item Loaders? Is that possible?", "links": [], "channel": "scrapy"},
{"date": "2014-11-11T16:43:09.078069+00:00", "nick": "rodrigo5244", "message": "flag: Don't know if you can use an Item Loader for that. You can always fill up an item as a dictionary if that does not work.", "links": [], "channel": "scrapy"},
{"date": "2014-11-11T17:38:36.608056+00:00", "nick": "kaido", "message": "hello", "links": [], "channel": "scrapy"},
{"date": "2014-11-11T17:45:31.798268+00:00", "nick": "kaido", "message": "I have a quetion, how can i write a scrapy rule for the methods get_all_items() and parse_all_items(), and use a rule for crawling. http://pastebin.com/Nyj3VMni. tnx in advice", "links": ["http://pastebin.com/Nyj3VMni"], "channel": "scrapy"},
{"date": "2014-11-11T18:15:38.667499+00:00", "nick": "cr7", "message": "Hey!", "links": [], "channel": "scrapy"},
{"date": "2014-11-11T18:16:38.545982+00:00", "nick": "cr7", "message": "Can anyone help me with selecting an option from a dropdown list from a webpage using scrapy?", "links": [], "channel": "scrapy"},
{"date": "2014-11-11T18:28:24.181343+00:00", "nick": "rodrigo5244", "message": "cr7: What is the page?", "links": [], "channel": "scrapy"},
{"date": "2014-11-12T00:21:35.211660+00:00", "nick": "toothrot", "message": "rabidmadman, you're incrementing the lyrics counter at the wrong place.", "links": [], "channel": "scrapy"},
{"date": "2014-11-12T00:22:06.939077+00:00", "nick": "toothrot", "message": "o, not here", "links": [], "channel": "scrapy"},
{"date": "2014-11-12T08:06:32.781256+00:00", "nick": "philip_", "message": "hi all", "links": [], "channel": "scrapy"},
{"date": "2014-11-12T08:07:25.222265+00:00", "nick": "philip_", "message": "does defining the input_processor in the Field constructor make sense when not actually using any Item Loader? I just want to apply the unicode.strip, but it doesn't seem to work...", "links": [], "channel": "scrapy"},
{"date": "2014-11-12T09:13:15.147746+00:00", "nick": "nikolaosk", "message": "no", "links": [], "channel": "scrapy"},
{"date": "2014-11-12T09:13:52.081969+00:00", "nick": "nikolaosk", "message": "on it's own it means nothing", "links": [], "channel": "scrapy"},
{"date": "2014-11-12T10:30:53.511642+00:00", "nick": "philip__", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2014-11-12T10:30:55.591023+00:00", "nick": "philip__", "message": "anyone here?", "links": [], "channel": "scrapy"},
{"date": "2014-11-12T15:04:00.898124+00:00", "nick": "AndyRez", "message": "Hi all, Please can someone tell me why I am getting this error \"TypeError: __init__() got an unexpected keyword argument 'process_value'\".. When trying to use the process_value function in LinkExtractor", "links": [], "channel": "scrapy"},
{"date": "2014-11-12T15:35:14.440001+00:00", "nick": "AndyRez", "message": "anyone?", "links": [], "channel": "scrapy"},
{"date": "2014-11-12T15:35:44.722010+00:00", "nick": "AndyRez", "message": "Hi all, Please can someone tell me why I am getting this error \"TypeError: __init__() got an unexpected keyword argument 'process_value'\".. When trying to use the process_value function in LinkExtractor", "links": [], "channel": "scrapy"},
{"date": "2014-11-12T15:37:00.924103+00:00", "nick": "AndyRez", "message": "my code : https://bpaste.net/show/e8b49f7a81cd", "links": ["https://bpaste.net/show/e8b49f7a81cd"], "channel": "scrapy"},
{"date": "2014-11-12T17:50:46.441223+00:00", "nick": "asd", "message": "Hey guys how do you run more than one scrapy at a time in terminal?", "links": [], "channel": "scrapy"},
{"date": "2014-11-12T17:51:01.228465+00:00", "nick": "asd", "message": "i meant spider instead of scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-11-12T20:37:00.036704+00:00", "nick": "abdrraf", "message": "hello", "links": [], "channel": "scrapy"},
{"date": "2014-11-13T04:06:09.307274+00:00", "nick": "ragnarock", "message": "Hi I am new to opensource ,How can I contribute to this project..??", "links": [], "channel": "scrapy"},
{"date": "2014-11-13T06:28:43.220061+00:00", "nick": "dssdds", "message": "i want to copy a complete website is that possible? any examples", "links": [], "channel": "scrapy"},
{"date": "2014-11-13T12:25:59.038809+00:00", "nick": "Streward", "message": "hello scrapy users :)", "links": [], "channel": "scrapy"},
{"date": "2014-11-13T12:26:26.161128+00:00", "nick": "Streward", "message": "i sit on my first scrapy project. the site i woul like to \"scrap\" is epages online shop.", "links": [], "channel": "scrapy"},
{"date": "2014-11-13T12:26:55.524633+00:00", "nick": "Streward", "message": "here is no api available, so i would like to take scrapy to set states on the orders . for example paid and sent", "links": [], "channel": "scrapy"},
{"date": "2014-11-13T12:27:41.345115+00:00", "nick": "Streward", "message": "i was able now to login into the shop with formrequest, read out wether the login was successful and redirect per request to the orders overview site", "links": [], "channel": "scrapy"},
{"date": "2014-11-13T12:28:02.669944+00:00", "nick": "Streward", "message": "also i am able to loop over the tr's in the overview table and request to detail order site", "links": [], "channel": "scrapy"},
{"date": "2014-11-13T12:28:20.914817+00:00", "nick": "Streward", "message": "now i would like to set checkbox on states.", "links": [], "channel": "scrapy"},
{"date": "2014-11-13T12:28:29.224067+00:00", "nick": "Streward", "message": "my first example is paid", "links": [], "channel": "scrapy"},
{"date": "2014-11-13T12:28:45.254047+00:00", "nick": "Streward", "message": "because it is in a form, i tried it with formrequest also", "links": [], "channel": "scrapy"},
{"date": "2014-11-13T12:28:57.398544+00:00", "nick": "Streward", "message": "but unfortunately it seems not to work", "links": [], "channel": "scrapy"},
{"date": "2014-11-13T12:31:08.508543+00:00", "nick": "Streward", "message": "here is my first spidercode", "links": [], "channel": "scrapy"},
{"date": "2014-11-13T12:31:09.457253+00:00", "nick": "Streward", "message": "http://pastebin.com/69hx57QJ", "links": ["http://pastebin.com/69hx57QJ"], "channel": "scrapy"},
{"date": "2014-11-13T12:31:26.408769+00:00", "nick": "Streward", "message": "maybe anyone could have a look, to know, what could be wrong", "links": [], "channel": "scrapy"},
{"date": "2014-11-13T12:31:38.237990+00:00", "nick": "Streward", "message": "in the log, there is no failure or something else", "links": [], "channel": "scrapy"},
{"date": "2014-11-13T12:33:00.371024+00:00", "nick": "Streward", "message": "http://pastebin.com/iVgxQEjx", "links": ["http://pastebin.com/iVgxQEjx"], "channel": "scrapy"},
{"date": "2014-11-13T12:33:05.273381+00:00", "nick": "Streward", "message": "that would be the log", "links": [], "channel": "scrapy"},
{"date": "2014-11-13T18:54:44.853351+00:00", "nick": "jphipps", "message": "hi.  I am trying to implement a custom redirect middleware. I added the DOWNLOADER_MIDDLEWARE dict to my project scrapy.cnf but when I try to run, I get", "links": [], "channel": "scrapy"},
{"date": "2014-11-13T18:54:46.155961+00:00", "nick": "jphipps", "message": "ConfigParser.ParsingError: File contains parsing errors: C:\\Scrapy\\tutorial\\scrapy.cfg", "links": [], "channel": "scrapy"},
{"date": "2014-11-13T18:54:46.302891+00:00", "nick": "jphipps", "message": "[line 16]: '}'", "links": [], "channel": "scrapy"},
{"date": "2014-11-13T18:55:07.338228+00:00", "nick": "jphipps", "message": "Would someone tell me what I am doing wrong?", "links": [], "channel": "scrapy"},
{"date": "2014-11-13T18:55:53.646297+00:00", "nick": "jphipps", "message": "This is what I've added to my scrapy.cfg", "links": [], "channel": "scrapy"},
{"date": "2014-11-13T18:55:54.726514+00:00", "nick": "jphipps", "message": "DOWNLOADER_MIDDLEWARES = {", "links": [], "channel": "scrapy"},
{"date": "2014-11-13T18:55:54.726616+00:00", "nick": "jphipps", "message": "'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': None,", "links": [], "channel": "scrapy"},
{"date": "2014-11-13T18:55:54.728733+00:00", "nick": "jphipps", "message": "'tutorial.middlewares.CustomRedirectMiddleware': 100,", "links": [], "channel": "scrapy"},
{"date": "2014-11-13T18:55:54.729514+00:00", "nick": "jphipps", "message": "}", "links": [], "channel": "scrapy"},
{"date": "2014-11-13T18:58:40.810868+00:00", "nick": "jphipps", "message": "Nevermind.  I'm an idiot. =P", "links": [], "channel": "scrapy"},
{"date": "2014-11-15T21:49:04.620225+00:00", "nick": "seni_", "message": "Hi!", "links": [], "channel": "scrapy"},
{"date": "2014-11-15T21:49:21.681332+00:00", "nick": "seni_", "message": "I'm trying to set a new stat value from pipeline.", "links": [], "channel": "scrapy"},
{"date": "2014-11-15T21:49:35.216604+00:00", "nick": "seni_", "message": "can you give some quick hint on how to do it?", "links": [], "channel": "scrapy"},
{"date": "2014-11-15T21:52:05.830825+00:00", "nick": "seni_", "message": "I tried spider.crawler.stats.set_value('my_new_stat', 'test_value') in my pipeline process_item method, but it doesn't output my new stat in the scraping log", "links": [], "channel": "scrapy"},
{"date": "2014-11-15T21:52:34.990685+00:00", "nick": "seni_", "message": "Nevermind, it did work.", "links": [], "channel": "scrapy"},
{"date": "2014-11-15T22:07:40.529547+00:00", "nick": "rabidmadman", "message": "toothrot, i'm so close...getting 157/166 items right now", "links": [], "channel": "scrapy"},
{"date": "2014-11-15T22:07:55.640915+00:00", "nick": "rabidmadman", "message": "no idea where my logic is going wrong haha", "links": [], "channel": "scrapy"},
{"date": "2014-11-15T22:08:20.573920+00:00", "nick": "rabidmadman", "message": "for some strange reason, it won't yield 9 items that should be yielded", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T02:44:12.545425+00:00", "nick": "toothrot", "message": "rabidmadman, you around?", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T19:59:34.000037+00:00", "nick": "asd_", "message": "Hey guys did anyone used  MailSender in past?", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T20:12:39.945360+00:00", "nick": "rabidmadman", "message": "toothrot, you there by any chance?", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T21:27:35.650726+00:00", "nick": "toothrot", "message": "rabidmadman, yes", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T21:27:54.794153+00:00", "nick": "toothrot", "message": "asd_, the mail sender in scrapy? i've used it for years.", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T21:28:22.064918+00:00", "nick": "toothrot", "message": "or are you talking about something else?", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T21:28:38.724035+00:00", "nick": "rabidmadman", "message": "hey man", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T21:29:12.047551+00:00", "nick": "toothrot", "message": "still having trouble?", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T21:29:32.639910+00:00", "nick": "rabidmadman", "message": "yeah haha..but close", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T21:29:39.924487+00:00", "nick": "rabidmadman", "message": "157/166 items scraped", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T21:31:19.473468+00:00", "nick": "toothrot", "message": "can you post the code?", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T21:31:26.701098+00:00", "nick": "toothrot", "message": "(msg if needed)", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T21:40:15.788394+00:00", "nick": "rabidmadman", "message": "ill pm you", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T21:40:24.129983+00:00", "nick": "rabidmadman", "message": "actually no need", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T21:40:30.657940+00:00", "nick": "rabidmadman", "message": "ill just make a gist of the last two functions", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T21:40:35.280260+00:00", "nick": "rabidmadman", "message": "here it is", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T21:40:38.520183+00:00", "nick": "rabidmadman", "message": "https://gist.githubusercontent.com/T-G-P/065e72...", "links": ["https://gist.githubusercontent.com/T-G-P/065e72ceb4c7a382aef7/raw/364de48b2d19ba2ffba9e209bb8d4796816ab340/thanks"], "channel": "scrapy"},
{"date": "2014-11-16T21:41:15.373364+00:00", "nick": "rabidmadman", "message": "excuse poor tabbing...the formatting got destroyed when pasting into the gist so I manually adjusted the tabs so imay have missed some", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T21:41:33.953993+00:00", "nick": "rabidmadman", "message": "but anyway the most important parts are at the end of those two functions", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T21:41:37.342844+00:00", "nick": "rabidmadman", "message": "right as i am yielding", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T21:41:49.548478+00:00", "nick": "rabidmadman", "message": "you can probably ignore the bulk of whats going on in the first one", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T21:43:41.026943+00:00", "nick": "rabidmadman", "message": "i already see some wrong indentation in that gist but that's because i did it on the fly, my bad", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T21:46:09.011866+00:00", "nick": "rabidmadman", "message": "so to sum up, what im doing:", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T21:46:09.073758+00:00", "nick": "rabidmadman", "message": "1) if there are lyrics, take note of the total lyrics count for the release", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T21:46:09.126598+00:00", "nick": "rabidmadman", "message": "*Take note of the total release count for that band", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T21:46:09.126691+00:00", "nick": "rabidmadman", "message": "* Take note of the total track count for that release", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T21:46:09.128185+00:00", "nick": "rabidmadman", "message": "*Check if the release count is = to the total release count so far", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T21:46:09.769219+00:00", "nick": "rabidmadman", "message": "*Check if the track count is = to the final track for each release", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T21:46:09.769317+00:00", "nick": "rabidmadman", "message": "*if the number of parsed lyrics is = to the lyrics count, and all of these conditions are met, yield the item", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T21:46:10.803244+00:00", "nick": "rabidmadman", "message": "2) No lyrics at all,", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T21:46:10.803318+00:00", "nick": "rabidmadman", "message": "Follow all of the conditions above besides the parsed lyrics count, and then yield the item.", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T21:50:45.051093+00:00", "nick": "toothrot", "message": "passing the indexes through meta shouldn't be done IMO", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T21:50:55.857875+00:00", "nick": "toothrot", "message": "also, you shouldn't be trying to yield items from both of those callbacks", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T21:51:10.846320+00:00", "nick": "toothrot", "message": "oh wait", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T21:51:20.504095+00:00", "nick": "toothrot", "message": "that's only if no lyrics", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T21:51:22.056228+00:00", "nick": "toothrot", "message": "my mistake", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T21:53:04.283233+00:00", "nick": "rabidmadman", "message": "yeah", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T21:53:19.547505+00:00", "nick": "rabidmadman", "message": "there's a lot of awful code there, i know", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T21:53:22.729383+00:00", "nick": "rabidmadman", "message": "im doing a lot of stupid stuff", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T21:53:47.590321+00:00", "nick": "rabidmadman", "message": "but passing the indicies through the meta was the quickiest, hackiest solution", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T21:53:55.133641+00:00", "nick": "rabidmadman", "message": "quickest*", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T21:54:13.262793+00:00", "nick": "rabidmadman", "message": "but somewhere/somehow my logic is failng for a total of 9 bands", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T21:54:48.233061+00:00", "nick": "rabidmadman", "message": "so i think at this point, im better off running the spider and finding out exactly which bands were parsed, then compare with the total bands for Q", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T21:55:00.900992+00:00", "nick": "toothrot", "message": "i wouldn't", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T21:55:11.253638+00:00", "nick": "toothrot", "message": "hold on a sec", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T21:55:36.580503+00:00", "nick": "rabidmadman", "message": "it's really strange that it would fail for only 9 bands", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T21:57:39.402459+00:00", "nick": "toothrot", "message": "why do you chain all of these release parse methods, rather than scheduling them all from parse_band ?", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T21:58:47.706313+00:00", "nick": "rabidmadman", "message": "not sure how I'd do that", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T21:58:59.998564+00:00", "nick": "toothrot", "message": "doesn't matter really", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T21:59:08.795402+00:00", "nick": "toothrot", "message": "we sohuld eliminate all this manual counting", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T21:59:10.065174+00:00", "nick": "rabidmadman", "message": "i definitely would like to refactor this", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:01:39.033400+00:00", "nick": "toothrot", "message": "are the lyrics on a page for each song or for each release?", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:02:01.114955+00:00", "nick": "rabidmadman", "message": "each song has a lyrics url associated to it", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:02:24.585648+00:00", "nick": "rabidmadman", "message": "http://www.metal-archives.com/albums/Leeway/Bor...", "links": ["http://www.metal-archives.com/albums/Leeway/Born_to_Expire/46858"], "channel": "scrapy"},
{"date": "2014-11-16T22:02:29.498475+00:00", "nick": "toothrot", "message": "some songs don't have lyrics, right?", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:03:06.135106+00:00", "nick": "rabidmadman", "message": "yea", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:03:11.346846+00:00", "nick": "rabidmadman", "message": "so I count how many lyrics tags there are", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:03:15.847399+00:00", "nick": "rabidmadman", "message": "here's an example of a lyrics link", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:03:16.308174+00:00", "nick": "rabidmadman", "message": "http://www.metal-archives.com/release/ajax-view...", "links": ["http://www.metal-archives.com/release/ajax-view-lyrics/id/353925"], "channel": "scrapy"},
{"date": "2014-11-16T22:04:59.157831+00:00", "nick": "toothrot", "message": "one sec, let me clone this", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:05:42.146969+00:00", "nick": "rabidmadman", "message": "the whole repo?", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:05:53.023600+00:00", "nick": "toothrot", "message": "i found it, yea", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:05:55.820447+00:00", "nick": "rabidmadman", "message": "ok", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:06:11.351611+00:00", "nick": "toothrot", "message": "i'm going to run it on the frist 6-7 bands and have that cached so i can test my changes", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:06:27.159184+00:00", "nick": "rabidmadman", "message": "run it for all of Q", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:06:29.670446+00:00", "nick": "rabidmadman", "message": "it's 166 bands", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:06:36.592736+00:00", "nick": "rabidmadman", "message": "it will take about 3 minutes", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:06:37.957559+00:00", "nick": "toothrot", "message": "how long does it take?", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:06:39.524802+00:00", "nick": "toothrot", "message": "ok", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:06:51.385162+00:00", "nick": "toothrot", "message": "do you have caching enabled already in your settings?", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:06:55.206837+00:00", "nick": "rabidmadman", "message": "no", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:07:04.985669+00:00", "nick": "rabidmadman", "message": "i disabled throttling as well just for testing purposes", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:07:09.398118+00:00", "nick": "rabidmadman", "message": "so i can have full speed", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:07:15.624010+00:00", "nick": "toothrot", "message": "you should, since it'll run faster and be nicer", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:07:24.846307+00:00", "nick": "rabidmadman", "message": "ok ill definitely do that", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:07:39.632185+00:00", "nick": "toothrot", "message": "it's real easy, just a setting change", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:07:47.486321+00:00", "nick": "rabidmadman", "message": "ok ill definitely enable cachcing", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:08:08.247692+00:00", "nick": "rabidmadman", "message": "once i ultimately run this for everything, I will set a reasonable delay per request", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:08:52.512786+00:00", "nick": "toothrot", "message": "right, but this is good for testing since it won't be hitting the server again, and it runs much faster", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:10:02.513984+00:00", "nick": "rabidmadman", "message": "oh", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:10:14.743609+00:00", "nick": "rabidmadman", "message": "good call then", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:10:56.200630+00:00", "nick": "rabidmadman", "message": "but yeah ultimately, I will definitely schedule the other parsing functions within parse band", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:11:49.326687+00:00", "nick": "rabidmadman", "message": "i didn't know what you meant before, but I was planning on  just making all the urls part of the class and yielding request objects for them", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:12:06.239670+00:00", "nick": "toothrot", "message": "i'll show you", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:12:14.563811+00:00", "nick": "rabidmadman", "message": "but i wanted to get this bug sorted out first", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:12:27.699478+00:00", "nick": "toothrot", "message": "right", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:12:40.591616+00:00", "nick": "toothrot", "message": "is Q the smallest ?", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:14:50.286157+00:00", "nick": "rabidmadman", "message": "yep", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:14:56.634090+00:00", "nick": "rabidmadman", "message": "main reason for choosing Q", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:22:54.518692+00:00", "nick": "toothrot", "message": "do you have the pastebin link i gave you initially ?", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:23:34.745012+00:00", "nick": "rabidmadman", "message": "which one?", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:24:04.090467+00:00", "nick": "rabidmadman", "message": "the one where you crawled the json urls?", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:24:08.211359+00:00", "nick": "toothrot", "message": "yea", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:24:22.935537+00:00", "nick": "rabidmadman", "message": "i don't have it, but i tested my version and it produced the same results", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:24:34.405664+00:00", "nick": "rabidmadman", "message": "only need one start url", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:24:36.249973+00:00", "nick": "toothrot", "message": "it gives a duplicate url...", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:25:03.187599+00:00", "nick": "rabidmadman", "message": "oh it parses the same url twice?", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:25:12.590661+00:00", "nick": "toothrot", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:25:23.137773+00:00", "nick": "rabidmadman", "message": "one second", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:26:07.822718+00:00", "nick": "toothrot", "message": "it doesn't matter, i'm working around it", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:26:27.008199+00:00", "nick": "rabidmadman", "message": "here you go", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:26:27.344220+00:00", "nick": "rabidmadman", "message": "https://github.com/T-G-P/xTr33m/commit/51a549e4...", "links": ["https://github.com/T-G-P/xTr33m/commit/51a549e43d36a2b609296f4b75d7f655fd83ece1"], "channel": "scrapy"},
{"date": "2014-11-16T22:27:07.434611+00:00", "nick": "rabidmadman", "message": "what you gave me in the pastbin is there", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:28:18.008901+00:00", "nick": "rabidmadman", "message": "yeah is ee what you mean, it just sends a request to the same url twice", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:28:29.246599+00:00", "nick": "rabidmadman", "message": "i didn't think it was a big deal at the time because its not like that url was being parsed", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:29:18.911756+00:00", "nick": "toothrot", "message": "yea, it's not really a big deal", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:29:19.975547+00:00", "nick": "rabidmadman", "message": "just the first url for each letter is sent to the parse_json function", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:29:34.936516+00:00", "nick": "rabidmadman", "message": "and then a request is sent from that url to parse the url", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:29:48.631597+00:00", "nick": "toothrot", "message": "do these print statements actually work for you ?", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:29:57.301619+00:00", "nick": "toothrot", "message": "scrapy is swallowing them here.", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:30:06.767597+00:00", "nick": "rabidmadman", "message": "so here's another issue", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:30:15.513823+00:00", "nick": "rabidmadman", "message": "i run all my tests locally", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:30:33.830600+00:00", "nick": "rabidmadman", "message": "and ive noticed different results on my server...", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:30:34.741744+00:00", "nick": "rabidmadman", "message": "so uhhh", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:30:36.378535+00:00", "nick": "rabidmadman", "message": ":(", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:30:43.312701+00:00", "nick": "rabidmadman", "message": "somethign to do with the parser?", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:30:54.901554+00:00", "nick": "rabidmadman", "message": "it's a conflict with soup/scrapy would be my geuss", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:31:16.905163+00:00", "nick": "rabidmadman", "message": "actually after i changed some stuff, i get the same output on my server, so everything works", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:31:29.319708+00:00", "nick": "rabidmadman", "message": "you need to install beautifulsoup if you don't have it", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:33:35.032072+00:00", "nick": "toothrot", "message": "i installed it", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:34:02.798874+00:00", "nick": "rabidmadman", "message": "no idea why it won't work, but like i said ive had issues with performance between my server and locally until I chagned some stuff", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:34:30.150977+00:00", "nick": "rabidmadman", "message": "for some reason some soup functionality doesn't work with the response body on my server, so i worked around it and did things a ltitle differently", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:36:11.078097+00:00", "nick": "toothrot", "message": "hrm", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:36:21.065319+00:00", "nick": "toothrot", "message": "for some reason Bs3 got installed through pip", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:37:49.930718+00:00", "nick": "rabidmadman", "message": "man i really appreciate this, but don't worry about it. You've been a tremendous help as is", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:38:05.127699+00:00", "nick": "toothrot", "message": "i've got it running now", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:38:06.655178+00:00", "nick": "rabidmadman", "message": "i don't want you to be dealing with the soup stuff since you'll never even use it", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:38:08.122680+00:00", "nick": "rabidmadman", "message": "oh ok cool", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:38:14.896794+00:00", "nick": "toothrot", "message": "i just installed the wrong version", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:38:16.409183+00:00", "nick": "toothrot", "message": "no biggie", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:38:19.695422+00:00", "nick": "rabidmadman", "message": "oh ok cool", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:38:36.950044+00:00", "nick": "rabidmadman", "message": "should take 3 minutes", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:40:25.167276+00:00", "nick": "toothrot", "message": "wonder if these guys notice how popluar the Q bands have been lately :P", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:43:58.432135+00:00", "nick": "rabidmadman", "message": "lol", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:44:39.452129+00:00", "nick": "rabidmadman", "message": "when I start running this for all the letters, i dont' know what im gonna do", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:44:44.239246+00:00", "nick": "rabidmadman", "message": "i think ill just do it one at a time", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:45:05.348597+00:00", "nick": "rabidmadman", "message": "and for the really large bands, i'll set the delay to be pretty heavy", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:52:06.176914+00:00", "nick": "rabidmadman", "message": "it's also not a very intelligent scraper because the user agents are completely random for each request...it's obviously a bot lol", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:52:20.971674+00:00", "nick": "toothrot", "message": "right,  i disabled that", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:52:26.504175+00:00", "nick": "toothrot", "message": "and just set one", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:52:32.798074+00:00", "nick": "rabidmadman", "message": "i see", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:52:34.712995+00:00", "nick": "rabidmadman", "message": "still running?", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:52:42.038138+00:00", "nick": "toothrot", "message": "no", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:52:45.172023+00:00", "nick": "rabidmadman", "message": "ok good haha", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:52:48.665319+00:00", "nick": "rabidmadman", "message": "thought my figure was way off", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:52:52.264681+00:00", "nick": "rabidmadman", "message": "did you get 166?", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:52:56.231357+00:00", "nick": "toothrot", "message": "i'm narrowing down on one of the missing ones", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:53:03.198777+00:00", "nick": "rabidmadman", "message": "what's your number?", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:53:09.965550+00:00", "nick": "rabidmadman", "message": "yeah i was gonna do that too...was gonna just look at the 9 missing bands", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:53:17.462813+00:00", "nick": "toothrot", "message": "i found one", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:53:23.444400+00:00", "nick": "toothrot", "message": "lxml is way better than bs4", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:53:32.346225+00:00", "nick": "toothrot", "message": "you might consider that for the future", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:53:39.973182+00:00", "nick": "rabidmadman", "message": "ok", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:54:05.809711+00:00", "nick": "rabidmadman", "message": "well i've evidently made questionable decisions thats for sure", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:54:17.109678+00:00", "nick": "rabidmadman", "message": "i didn't want to start from scratch with xpath so I used what i knew", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:54:32.416757+00:00", "nick": "rabidmadman", "message": "but i read that lxml is just a different parser to use right?", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:54:50.808658+00:00", "nick": "toothrot", "message": "lxml has similar css select option too", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:55:08.689316+00:00", "nick": "toothrot", "message": "all these unicode warnings.. sheesh", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:56:20.773208+00:00", "nick": "rabidmadman", "message": "oh yeah so that comes next", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:56:27.763003+00:00", "nick": "rabidmadman", "message": "i have absolutely no clue how to resolve all the bad characters", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:56:41.722623+00:00", "nick": "rabidmadman", "message": "tried using all kinds of built in python functionality/libraries", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:57:34.175554+00:00", "nick": "rabidmadman", "message": "when testing i even printed some of my changes and they printed the correct encoding, but when going into the dictionary, everything is just utf-8", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:57:41.494468+00:00", "nick": "toothrot", "message": "well, the first missing item is because there are no releases", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:57:42.502658+00:00", "nick": "toothrot", "message": ":(", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:58:10.773166+00:00", "nick": "rabidmadman", "message": "yeah i figured that was an extra check, but i set hte default release count to be zero", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:58:22.351735+00:00", "nick": "rabidmadman", "message": "i figured that most likely it was all th bands with no releases", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:59:37.465135+00:00", "nick": "toothrot", "message": "lxml is also much faster", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T22:59:56.942397+00:00", "nick": "rabidmadman", "message": "lxml is a dom parser right?", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:00:01.949296+00:00", "nick": "rabidmadman", "message": "or is that a parsing library", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:00:07.451570+00:00", "nick": "toothrot", "message": "it's an xml/html parser, yea", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:00:19.541371+00:00", "nick": "rabidmadman", "message": "what does scrapy use?", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:00:28.629574+00:00", "nick": "toothrot", "message": "lxml", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:00:33.590386+00:00", "nick": "rabidmadman", "message": "yeah i know i figured that was the way to go", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:00:35.221375+00:00", "nick": "rabidmadman", "message": "lol", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:00:47.115650+00:00", "nick": "toothrot", "message": "you don't have to use scrapy's classes", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:00:50.912692+00:00", "nick": "toothrot", "message": "i usually use lxml directly", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:00:56.665164+00:00", "nick": "rabidmadman", "message": "i know im not doing the brightest thing, but after spending days parsing all those pages with soup, i wasn't too eager to start over", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:01:06.094120+00:00", "nick": "toothrot", "message": "i'm not suggesting that either", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:01:08.740847+00:00", "nick": "rabidmadman", "message": "that stuff wasn't very fun", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:01:22.843368+00:00", "nick": "rabidmadman", "message": "curious to find hte performance difference", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:01:23.993791+00:00", "nick": "toothrot", "message": "'item_scraped_count': 163, when i yield if release_count == 0", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:01:32.454250+00:00", "nick": "rabidmadman", "message": "what was it prior to that?", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:01:34.735423+00:00", "nick": "rabidmadman", "message": "157?", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:01:43.485658+00:00", "nick": "toothrot", "message": "same as you had i think", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:01:49.353953+00:00", "nick": "rabidmadman", "message": "ok cool...3 bands to go", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:01:50.726865+00:00", "nick": "rabidmadman", "message": "lol...", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:02:16.449887+00:00", "nick": "rabidmadman", "message": "so if the release count is zero, just yield the item", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:02:25.905995+00:00", "nick": "rabidmadman", "message": "ill add that in right now", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:03:51.401982+00:00", "nick": "toothrot", "message": "i'm getting some 500 status codes too", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:04:09.249400+00:00", "nick": "rabidmadman", "message": "oh no...", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:04:24.416300+00:00", "nick": "rabidmadman", "message": "so i'm sending some bad urls?", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:04:43.610480+00:00", "nick": "rabidmadman", "message": "excelllent", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:08:10.853722+00:00", "nick": "toothrot", "message": "it's one of the lyric links", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:10:07.964834+00:00", "nick": "rabidmadman", "message": "ok cool", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:10:16.341122+00:00", "nick": "rabidmadman", "message": "my item scraped", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:10:20.184906+00:00", "nick": "rabidmadman", "message": "is 165!!!", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:10:21.629972+00:00", "nick": "rabidmadman", "message": "lol", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:10:28.944749+00:00", "nick": "rabidmadman", "message": "no idea why yours is 163...", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:10:58.564472+00:00", "nick": "rabidmadman", "message": "what are you using to debug this?", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:11:18.552880+00:00", "nick": "toothrot", "message": "maybe i got an error on some that you didn't.", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:11:31.582283+00:00", "nick": "rabidmadman", "message": "did you change anything?", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:11:44.895922+00:00", "nick": "rabidmadman", "message": "seems bizarre that we'd get different results using the same code", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:12:07.132813+00:00", "nick": "rabidmadman", "message": "now i literally just need to figure out which band wasn't parsed", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:13:51.575698+00:00", "nick": "toothrot", "message": "check The Quill", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:13:56.810740+00:00", "nick": "toothrot", "message": "that was the one giving me an error.", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:15:02.632792+00:00", "nick": "rabidmadman", "message": "ok", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:15:55.499261+00:00", "nick": "rabidmadman", "message": "how were you debugging this?", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:16:02.917161+00:00", "nick": "rabidmadman", "message": "i want to run it again because i didn't output anything", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:16:07.426212+00:00", "nick": "toothrot", "message": "ok, apparently the link is probably okay", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:16:08.099302+00:00", "nick": "rabidmadman", "message": "also i wan tto enable caching", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:16:19.762897+00:00", "nick": "toothrot", "message": "you'll have to make some changes to enavble caching", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:16:30.530721+00:00", "nick": "rabidmadman", "message": "can i log anything that throws any status code outside of 200?", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:16:32.994509+00:00", "nick": "toothrot", "message": "because the cache busting arg on the first urls", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:16:42.313940+00:00", "nick": "rabidmadman", "message": "ah ok", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:17:10.401521+00:00", "nick": "rabidmadman", "message": "the problem is definitely seeming like a bad url now that it's down to one band for me", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:17:17.488259+00:00", "nick": "toothrot", "message": "it's already logged but you're print so much other stuff it's probaly not noticed", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:17:21.881707+00:00", "nick": "toothrot", "message": "look in your stats at the end", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:17:28.659128+00:00", "nick": "toothrot", "message": "it'll tell you how many 500s", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:17:52.502227+00:00", "nick": "toothrot", "message": "the link was actually goof", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:17:54.790833+00:00", "nick": "toothrot", "message": "the link was actually good'", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:17:59.859844+00:00", "nick": "rabidmadman", "message": "{'downloader/request_bytes': 1470447,", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:17:59.955295+00:00", "nick": "rabidmadman", "message": " 'downloader/request_count': 3537,", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:17:59.955367+00:00", "nick": "rabidmadman", "message": " 'downloader/request_method_count/GET': 3537,", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:18:00.007966+00:00", "nick": "rabidmadman", "message": " 'downloader/response_bytes': 6492206,", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:18:00.008059+00:00", "nick": "rabidmadman", "message": " 'downloader/response_count': 3537,", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:18:00.773252+00:00", "nick": "rabidmadman", "message": " 'downloader/response_status_count/200': 3537,", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:18:00.825846+00:00", "nick": "rabidmadman", "message": " 'dupefilter/filtered': 1,", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:18:01.755587+00:00", "nick": "rabidmadman", "message": " 'finish_reason': 'finished',", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:18:01.808746+00:00", "nick": "rabidmadman", "message": " 'finish_time': datetime.datetime(2014, 11, 16, 23, 6, 14, 382414),", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:18:02.689543+00:00", "nick": "rabidmadman", "message": " 'item_scraped_count': 165,", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:18:02.742232+00:00", "nick": "rabidmadman", "message": " 'log_count/DEBUG': 7242,", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:18:03.836605+00:00", "nick": "rabidmadman", "message": " 'log_count/INFO': 9,", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:18:03.892573+00:00", "nick": "rabidmadman", "message": " 'request_depth_max': 12,", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:18:04.788978+00:00", "nick": "rabidmadman", "message": " 'response_received_count': 3537,", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:18:22.153197+00:00", "nick": "toothrot", "message": "okay.. no 500s for you then", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:18:27.861527+00:00", "nick": "toothrot", "message": "so there must be one other problem still", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:18:50.982603+00:00", "nick": "rabidmadman", "message": "grrr", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:19:06.334168+00:00", "nick": "rabidmadman", "message": "ill have to just run this and log every single band", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:19:15.021341+00:00", "nick": "toothrot", "message": "hold on", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:19:33.394903+00:00", "nick": "rabidmadman", "message": "also why does any type of IO prior to yielding mess up results?", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:19:45.107025+00:00", "nick": "toothrot", "message": "what do you mean?", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:19:48.317863+00:00", "nick": "rabidmadman", "message": "like if i write to a text file, prior to yielding the spider terminates prematurely", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:20:03.478021+00:00", "nick": "rabidmadman", "message": "i just wanted to log all the band names in a text file", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:20:13.286806+00:00", "nick": "toothrot", "message": "i've never encountered that, and i have frequestly dumped out html to text files", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:20:37.021608+00:00", "nick": "rabidmadman", "message": "ok i will try again", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:20:44.352494+00:00", "nick": "rabidmadman", "message": "it straight up just terminates after 16 bands", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:20:57.012277+00:00", "nick": "toothrot", "message": "i found it", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:21:11.681691+00:00", "nick": "rabidmadman", "message": "?", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:22:15.117028+00:00", "nick": "toothrot", "message": "it's one of the Queensr\u00ffche", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:22:44.492664+00:00", "nick": "rabidmadman", "message": "i see", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:22:53.727122+00:00", "nick": "toothrot", "message": "the one with a bunch of relases", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:22:54.312781+00:00", "nick": "rabidmadman", "message": "is it because of the duplicate name?", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:22:55.950126+00:00", "nick": "toothrot", "message": "releases", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:23:04.033658+00:00", "nick": "rabidmadman", "message": "ahyeah", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:23:13.168428+00:00", "nick": "toothrot", "message": "do you key by band name ?", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:23:24.831211+00:00", "nick": "toothrot", "message": "don't think so", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:26:17.354063+00:00", "nick": "rabidmadman", "message": "no i forgot that i resolved that already", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:27:13.152168+00:00", "nick": "rabidmadman", "message": "do you have your json dump?", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:27:18.422884+00:00", "nick": "rabidmadman", "message": "can you tell me the ID of the queensryche that you have", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:27:38.321049+00:00", "nick": "rabidmadman", "message": "there's an ID field", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:29:19.361444+00:00", "nick": "rabidmadman", "message": "there are only two", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:29:26.385503+00:00", "nick": "toothrot", "message": "there's a race condition in the way you are tracking lyrics", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:30:26.554006+00:00", "nick": "rabidmadman", "message": "wow", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:32:33.650384+00:00", "nick": "rabidmadman", "message": "i figured that would be impossible using scrapy lol", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:32:43.566225+00:00", "nick": "toothrot", "message": "basically if the request are not processed in the order you're yielding them, you can yield the item before it's ready.", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:32:53.061107+00:00", "nick": "toothrot", "message": "what do you mean?", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:34:09.059191+00:00", "nick": "rabidmadman", "message": "i haven't written much parallel code before but I figured I wouldn't have to worry about out of order execution with scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:34:37.463795+00:00", "nick": "toothrot", "message": "scrapy doesn't control the order the server replies in (or how quickly, etc)", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:34:44.146439+00:00", "nick": "rabidmadman", "message": "ok", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:36:30.231136+00:00", "nick": "toothrot", "message": "the problem is around `parsed_lyrics += 1` and then passing that through meta", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:36:42.280020+00:00", "nick": "toothrot", "message": "imagine the last lyric reply comes first...", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:37:15.856675+00:00", "nick": "rabidmadman", "message": "yeah", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:37:41.025059+00:00", "nick": "rabidmadman", "message": "wait why would that effect that though?", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:37:57.229423+00:00", "nick": "rabidmadman", "message": "i would increment that by one", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:38:15.953677+00:00", "nick": "rabidmadman", "message": "maybe I should have a lyrics_count associated to the item?", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:38:30.951411+00:00", "nick": "rabidmadman", "message": "and increment that items lyrics count value?", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:39:15.419048+00:00", "nick": "toothrot", "message": "you're incrementing the counter before you get the reply", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:40:14.424392+00:00", "nick": "toothrot", "message": "also, you can vastly simplfy the meta dict", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:40:23.184489+00:00", "nick": "toothrot", "message": "by just passing the release itself", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:40:33.356736+00:00", "nick": "toothrot", "message": "i'll show you...", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:40:57.528746+00:00", "nick": "rabidmadman", "message": "sorry man, I'm embarassed of my shitty code :(", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:41:33.258634+00:00", "nick": "rabidmadman", "message": "i definitely will refactor it quite a bit", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:41:35.001261+00:00", "nick": "toothrot", "message": "i wouldn't want anyone to see any of my early code", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:41:47.118779+00:00", "nick": "rabidmadman", "message": "lol", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:41:50.538057+00:00", "nick": "toothrot", "message": "if you recognize it has flaws, you're okay", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:41:56.668710+00:00", "nick": "toothrot", "message": "because you can improve from there", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:42:16.858125+00:00", "nick": "rabidmadman", "message": "appreciate that", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:44:34.966610+00:00", "nick": "toothrot", "message": "one sec, i'll show you how'd i'd start on this", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:44:47.736861+00:00", "nick": "rabidmadman", "message": "so to resolve this, i was gonna go with my original solution and just increment the release lyrics count if the release", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:45:02.895126+00:00", "nick": "rabidmadman", "message": "this way nothing is being incremented until i'm dealing with the lyrics body", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:45:48.355551+00:00", "nick": "toothrot", "message": "hold on a sec", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:45:55.009013+00:00", "nick": "rabidmadman", "message": "ok", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:55:31.520400+00:00", "nick": "toothrot", "message": "rabidmadman, question", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:55:59.901482+00:00", "nick": "rabidmadman", "message": "sure", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:56:22.096506+00:00", "nick": "toothrot", "message": "why do you have each release as a dict within a dict ?", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:56:49.020990+00:00", "nick": "toothrot", "message": "how you have {release_name: {...}, parsed:0}", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:57:20.590095+00:00", "nick": "rabidmadman", "message": "i thoguht it was a list of dicts", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:57:32.877047+00:00", "nick": "rabidmadman", "message": "yeah that represents one release", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:57:35.045409+00:00", "nick": "rabidmadman", "message": "and i have a list of those", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:57:52.016461+00:00", "nick": "toothrot", "message": "you do have a list of dicts, but it's nested for a reaosn i don't understand", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:58:15.768168+00:00", "nick": "rabidmadman", "message": "lets just say that there is no reason", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:58:21.522586+00:00", "nick": "rabidmadman", "message": "or one that isn't very good", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:58:23.982476+00:00", "nick": "rabidmadman", "message": "because i dont' have a good reason", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:58:39.098046+00:00", "nick": "toothrot", "message": "okay, well, i can change it in my example, i justed wanted to make sure :P", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:58:59.942073+00:00", "nick": "rabidmadman", "message": "i think it's because i liked having the 'release_name' key", "links": [], "channel": "scrapy"},
{"date": "2014-11-16T23:59:13.121099+00:00", "nick": "rabidmadman", "message": "and i wanted that key to just be outside of the release", "links": [], "channel": "scrapy"},
{"date": "2014-11-17T00:00:04.754118+00:00", "nick": "rabidmadman", "message": "parsed is a flag/hacky way that i made progress", "links": [], "channel": "scrapy"},
{"date": "2014-11-17T00:00:09.657006+00:00", "nick": "toothrot", "message": "'release_name': 'Some release name' is great", "links": [], "channel": "scrapy"},
{"date": "2014-11-17T00:00:15.079215+00:00", "nick": "toothrot", "message": "but you have in some sections:", "links": [], "channel": "scrapy"},
{"date": "2014-11-17T00:00:30.863011+00:00", "nick": "toothrot", "message": "{'Some release name': {...} }", "links": [], "channel": "scrapy"},
{"date": "2014-11-17T00:00:48.507665+00:00", "nick": "rabidmadman", "message": "yeah i recognize that it's not good", "links": [], "channel": "scrapy"},
{"date": "2014-11-17T00:01:03.233541+00:00", "nick": "rabidmadman", "message": "it wasn't like that originally", "links": [], "channel": "scrapy"},
{"date": "2014-11-17T00:01:10.498489+00:00", "nick": "toothrot", "message": "i just wanted to make sure you weren't relying on that for some external reason", "links": [], "channel": "scrapy"},
{"date": "2014-11-17T00:01:16.712710+00:00", "nick": "rabidmadman", "message": "no", "links": [], "channel": "scrapy"},
{"date": "2014-11-17T00:01:43.925437+00:00", "nick": "rabidmadman", "message": "i mean i probably am  after that because everything depends on that structure", "links": [], "channel": "scrapy"},
{"date": "2014-11-17T00:03:17.283184+00:00", "nick": "toothrot", "message": "okay, well i'll run with it, if you decide that change isn't good you can revert that part.", "links": [], "channel": "scrapy"},
{"date": "2014-11-17T00:04:31.026389+00:00", "nick": "rabidmadman", "message": "don't go crazy man no need to refactor my code for me lol", "links": [], "channel": "scrapy"},
{"date": "2014-11-17T00:04:40.805731+00:00", "nick": "rabidmadman", "message": "i greatly appreciate your advice/help though", "links": [], "channel": "scrapy"},
{"date": "2014-11-17T00:04:50.390190+00:00", "nick": "rabidmadman", "message": "i definitely do not like the way im doing stuff", "links": [], "channel": "scrapy"},
{"date": "2014-11-17T00:09:25.890929+00:00", "nick": "toothrot", "message": "no trouble, almost done", "links": [], "channel": "scrapy"},
{"date": "2014-11-17T00:17:22.481523+00:00", "nick": "rabidmadman", "message": "ran it again and got this", "links": [], "channel": "scrapy"},
{"date": "2014-11-17T00:17:22.545248+00:00", "nick": "rabidmadman", "message": "'downloader/response_status_count/200': 3537,", "links": [], "channel": "scrapy"},
{"date": "2014-11-17T00:17:22.545350+00:00", "nick": "rabidmadman", "message": " 'downloader/response_status_count/500': 3,", "links": [], "channel": "scrapy"},
{"date": "2014-11-17T00:17:22.597468+00:00", "nick": "rabidmadman", "message": " 'dupefilter/filtered': 1,", "links": [], "channel": "scrapy"},
{"date": "2014-11-17T00:17:36.535164+00:00", "nick": "rabidmadman", "message": "but scraped item count was 165", "links": [], "channel": "scrapy"},
{"date": "2014-11-17T04:57:35.784867+00:00", "nick": "n00b", "message": "hello", "links": [], "channel": "scrapy"},
{"date": "2014-11-17T04:57:45.153335+00:00", "nick": "n00b", "message": "i need a bit of help", "links": [], "channel": "scrapy"},
{"date": "2014-11-17T05:01:40.943733+00:00", "nick": "Guest45691", "message": "hello", "links": [], "channel": "scrapy"},
{"date": "2014-11-17T05:03:17.490437+00:00", "nick": "m0rpheu5", "message": "hello", "links": [], "channel": "scrapy"},
{"date": "2014-11-17T05:03:34.614110+00:00", "nick": "m0rpheu5", "message": "why scrapy blog is down ?", "links": [], "channel": "scrapy"},
{"date": "2014-11-17T05:12:22.969276+00:00", "nick": "rak_", "message": "how to scrape ajax websites ??", "links": [], "channel": "scrapy"},
{"date": "2014-11-17T20:36:47.352418+00:00", "nick": "webmaven_work", "message": "This might be of interest: https://github.com/aosabook/500lines/tree/maste...", "links": ["https://github.com/aosabook/500lines/tree/master/crawler"], "channel": "scrapy"},
{"date": "2014-11-17T20:42:35.492593+00:00", "nick": "vurk", "message": "hullo, everyone -- just discovered scrapy, looks awesome! debugging a pipeline problem, if anyone has a second", "links": [], "channel": "scrapy"},
{"date": "2014-11-17T20:42:50.853891+00:00", "nick": "vurk", "message": "basically, looks like __init__ gets called, but process_item never does", "links": [], "channel": "scrapy"},
{"date": "2014-11-17T20:45:17.208303+00:00", "nick": "jsjc", "message": "hi! Is there a way of sending from one parsefunction to another the current repsonse?? lets say I want to parse some stuff that I have in another parse function. could I yield self.otherparse(response) ??", "links": [], "channel": "scrapy"},
{"date": "2014-11-18T16:26:55.461108+00:00", "nick": "asd_", "message": "hey guys anybody used mailSender before? Anyway here's code that i got so far https://bpaste.net/show/11195dee8f95, am i missing something?", "links": ["https://bpaste.net/show/11195dee8f95"], "channel": "scrapy"},
{"date": "2014-11-18T16:27:40.264086+00:00", "nick": "onlythebestfakes", "message": "anybody has worked on www.onlythebestfakes.com???", "links": ["http://www.onlythebestfakes.com?%3F%3F="], "channel": "scrapy"},
{"date": "2014-11-18T16:27:48.523718+00:00", "nick": "onlythebestfakes", "message": "anybody has worked on www.onlythebestfakes.com ???", "links": ["http://www.onlythebestfakes.com"], "channel": "scrapy"},
{"date": "2014-11-18T16:28:13.426195+00:00", "nick": "onlythebestfakes", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2014-11-18T16:28:15.032062+00:00", "nick": "onlythebestfakes", "message": "anybody has worked on www.onlythebestfakes.com ???", "links": ["http://www.onlythebestfakes.com"], "channel": "scrapy"},
{"date": "2014-11-18T16:52:48.744363+00:00", "nick": "onlythebestfakes", "message": "anybody has worked on www.onlythebestfakes.com ???", "links": ["http://www.onlythebestfakes.com"], "channel": "scrapy"},
{"date": "2014-11-19T07:35:26.175245+00:00", "nick": "rabidmadman", "message": "any ideas what would cause scrapy to segfault?", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T08:14:10.689054+00:00", "nick": "ChrisP", "message": "Greetings, I am looking for a good tutorial introduction to scrapy for scraping multiple pages using 0.24. The material I seem to be finding on youtube appears out of date and I am a bit lost with the base tutorial. Any assistance would be appreciated.", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T08:20:37.564392+00:00", "nick": "rak_", "message": "hello", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T08:21:22.727002+00:00", "nick": "rak_", "message": "can i get any possible help scraping an ajax website ?", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T08:40:03.244861+00:00", "nick": "rak_", "message": "hello ??", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T08:40:25.845894+00:00", "nick": "rak_", "message": "anybody gonna reply in here ?", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T08:57:09.572152+00:00", "nick": "rabidmadman", "message": "It's 4am over here, i have never done ajax crawling but there is a config to enable ajax crawling", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T08:57:24.230230+00:00", "nick": "ChrisP", "message": "Greetings. I am sure this is a very basic question that I cannot seem to find data for. If I am scraping and the data pulls a page with an & (rather than &amp; in it, will that stop the pagination? If so (that seems to be my experience) how do I correct that? Thank you.", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T08:57:24.343717+00:00", "nick": "rabidmadman", "message": "AJAX_CRAWL_ENABLED", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T08:58:04.593628+00:00", "nick": "rabidmadman", "message": "not sure what your problem is", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T08:58:39.227599+00:00", "nick": "rabidmadman", "message": "what's wrong with an & in the url", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T08:59:43.847531+00:00", "nick": "ChrisP", "message": "Scrapy doesn't seem to properly load the page. If I have an & in the starting url, it will not run properly (which is fine since I can replace it with &amp;), but for urls I extract, can I correct that?", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T09:03:11.106076+00:00", "nick": "rabidmadman", "message": "ah yeah", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T09:03:36.982758+00:00", "nick": "rabidmadman", "message": "before you yield the request for the next url, modify the &'s", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T09:04:00.811784+00:00", "nick": "rabidmadman", "message": "basic string manipulation in python would do the trick", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T09:04:16.803804+00:00", "nick": "ChrisP", "message": "Would I put that in the rule that is extracting it somehow?", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T09:04:31.875992+00:00", "nick": "rabidmadman", "message": "don't have much experience using those", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T09:05:24.105916+00:00", "nick": "rabidmadman", "message": "so you're programmatically extracting links right?", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T09:05:28.737768+00:00", "nick": "rabidmadman", "message": "buildling a list of urls?", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T09:05:38.614445+00:00", "nick": "rabidmadman", "message": "seems all you need to do is modify all the &'s in the urls", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T09:06:12.749768+00:00", "nick": "rabidmadman", "message": "like if you're extracting all the links from a page and then trying to parse them all, prior to yielding the request for those links, you can modify the urls", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T09:06:15.350224+00:00", "nick": "ChrisP", "message": "So the problem is that its automatically pulling the links via a crawlspider, rather than by my manual input.", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T09:06:41.553497+00:00", "nick": "ChrisP", "message": "Since I am using a crawlspider, they are not yielded to my understanding.", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T09:06:51.813733+00:00", "nick": "ChrisP", "message": "So I'm not sure where that modification shoul take place", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T09:06:53.174208+00:00", "nick": "rabidmadman", "message": "what i would do instead is write a simple thing that just gets all the links that you're looking to parse", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T09:07:24.151565+00:00", "nick": "rabidmadman", "message": "i mean that's what i would do, I\"m not that experienced with scrapy, but I've managed to scrape 1000's of pages without using all of scrapy's functoinality lol", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T09:08:07.529486+00:00", "nick": "rabidmadman", "message": "so i'd get all the links, modify them on the fly, then send requests to the newly modified link", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T09:08:10.432571+00:00", "nick": "ChrisP", "message": "Yeah, I'm pretending to teach myself atm with variable amounts of success", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T09:09:03.084337+00:00", "nick": "rabidmadman", "message": "so to get all the links, you can do a simple list comprehension and select all the anchor tags and then get all the href's", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T09:09:20.760733+00:00", "nick": "rabidmadman", "message": "then at every link, you can just swap out the & on the fly", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T09:11:59.956601+00:00", "nick": "ChrisP", "message": "I guess I just need to figure out how to code that in python in the middle of the rule.", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T09:16:50.248635+00:00", "nick": "rabidmadman", "message": "yeah im not sure how the link extractors work", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T09:17:22.417740+00:00", "nick": "rabidmadman", "message": "never used it before, just wrote my own link extractor for any page(s) i needed to parse", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T09:17:33.663042+00:00", "nick": "rabidmadman", "message": "probably over complicated things but i'm stubborn", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T09:18:18.358381+00:00", "nick": "rabidmadman", "message": "i started learning scrapy 2 weeks ago and was hopiong to read the entire docs front to back but wound up just the bare minimum necessary functionality that I needed to accomplish what i wanted ot do", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T09:22:22.024834+00:00", "nick": "ChrisP", "message": "Fair enough, I'm sure I'll figure it out eventually.", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T17:39:08.594371+00:00", "nick": "asd", "message": "is there any way to delete white space when using scrapy shell?", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T18:35:31.718415+00:00", "nick": "rabidmadman", "message": "anyway to easily debug a segfault in scrapy?", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T18:39:28.176728+00:00", "nick": "samtc", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T18:53:18.576594+00:00", "nick": "SirSkitzo", "message": "I'm scraping numbers from directory pages and every so often the xpath changes. What's the best way to compare the results of two different xpaths and return the one that grabs the phone number?", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T18:54:59.480542+00:00", "nick": "rabidmadman", "message": "might need a regex to validate the phone number", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T18:55:35.855697+00:00", "nick": "rabidmadman", "message": "if the value found is valid, yield it or whatever you are doing otherwise ignore it", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T18:56:51.487088+00:00", "nick": "rabidmadman", "message": "there's a library for parsing phone numbers too", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T18:56:51.989113+00:00", "nick": "rabidmadman", "message": "https://github.com/daviddrysdale/python-phonenu...", "links": ["https://github.com/daviddrysdale/python-phonenumbers"], "channel": "scrapy"},
{"date": "2014-11-19T19:21:26.099493+00:00", "nick": "SirSkitzo", "message": "Nice, thank you", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T19:22:21.025183+00:00", "nick": "rabidmadman", "message": "no problem dude", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T19:24:00.764308+00:00", "nick": "_ingsoc", "message": "How good is Scrapy with login pages? I have no idea what the backend of the login page is.", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T19:25:13.171700+00:00", "nick": "SirSkitzo", "message": "Would this be a case where an Item Loader should be used? I've been reading about them but I'm still not clear about it", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T19:29:54.524639+00:00", "nick": "rabidmadman", "message": "never used an item loader", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T19:30:11.262894+00:00", "nick": "rabidmadman", "message": "ive gotten by without using a lot of the awesome scrapy functionality lol", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T19:30:40.361105+00:00", "nick": "rabidmadman", "message": "my suggestion in this case will be bad coding practice, but you can do everything you need within your spider", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T19:31:13.706418+00:00", "nick": "rabidmadman", "message": "there are some docs for pages that require form submission and stuff never dealt with it", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T19:41:52.140049+00:00", "nick": "asd", "message": "rabidmadman, do you have expirience with mailSender?", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T19:46:39.458421+00:00", "nick": "rabidmadman", "message": "no sorry", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T20:10:47.020212+00:00", "nick": "rodrigo5244", "message": "asd, I have used mailsender before, In one particular case I had to use python's mail stuff because scrapy was not working.", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T20:11:05.511865+00:00", "nick": "rodrigo5244", "message": "asd, I am leaving the room now, sorry", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T20:11:34.916695+00:00", "nick": "rodrigo5244", "message": "mailsender worked fine with gmail, but it didn't with a local mail service.", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T21:45:24.000346+00:00", "nick": "bosnjak", "message": "hi all", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T21:48:07.660103+00:00", "nick": "bosnjak", "message": "after yielding a FormRequest from a Rule callback, will it be processed by the Rules again? Because it doesn't seem to do so for me.", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T22:05:58.649448+00:00", "nick": "SirSkitzo", "message": "bosnjak, code always helps", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T23:20:10.078811+00:00", "nick": "ndb", "message": "Hi!", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T23:21:46.273067+00:00", "nick": "ndb", "message": "on >= 0.24.4, i'm having some problems with Twisted>=10.0.0 DistributionNotFound", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T23:21:58.797220+00:00", "nick": "ndb", "message": "is there some way to bypass it, on setup.py maybe?", "links": [], "channel": "scrapy"},
{"date": "2014-11-19T23:38:50.287876+00:00", "nick": "SirSkitzo", "message": "how often does scrapy save when using -t csv option?", "links": [], "channel": "scrapy"},
{"date": "2014-11-20T02:11:12.559772+00:00", "nick": "toothrot", "message": "ndb, did you figure it out?", "links": [], "channel": "scrapy"},
{"date": "2014-11-20T10:27:07.987374+00:00", "nick": "asd_", "message": "what should i do if i have multipe values inside 1 item? it returns just the first value in list", "links": [], "channel": "scrapy"},
{"date": "2014-11-20T10:27:59.717295+00:00", "nick": "iliana_", "message": "go over the list with a loop", "links": [], "channel": "scrapy"},
{"date": "2014-11-20T10:29:06.309766+00:00", "nick": "asd_", "message": "i can't because it's generator so return value won't work", "links": [], "channel": "scrapy"},
{"date": "2014-11-20T10:37:14.957128+00:00", "nick": "asd_", "message": "in shell it prints list but when i do feed export it lists just first item", "links": [], "channel": "scrapy"},
{"date": "2014-11-20T12:16:26.607265+00:00", "nick": "dpn`", "message": "asd_, items = [1,2,3]; while items: yield items.pop()", "links": [], "channel": "scrapy"},
{"date": "2014-11-20T12:19:32.527964+00:00", "nick": "asd_", "message": "dpn`, This is the error that i'm getting:  ERROR: Spider must return Request, BaseItem or None", "links": [], "channel": "scrapy"},
{"date": "2014-11-20T12:20:06.489449+00:00", "nick": "dpn`", "message": "asd_, are you yielding items? (not the integers i used in the example)", "links": [], "channel": "scrapy"},
{"date": "2014-11-20T12:21:26.957290+00:00", "nick": "asd_", "message": "dpn`, yea i'm yielding items anyway this is code that i got https://bpaste.net/show/64ae2b942421", "links": ["https://bpaste.net/show/64ae2b942421"], "channel": "scrapy"},
{"date": "2014-11-20T12:22:19.362593+00:00", "nick": "asd_", "message": "friends is where i got multiple values but only get one when scraping", "links": [], "channel": "scrapy"},
{"date": "2014-11-20T12:23:11.630868+00:00", "nick": "dpn`", "message": "ah i misread your original question", "links": [], "channel": "scrapy"},
{"date": "2014-11-20T12:23:31.885716+00:00", "nick": "dpn`", "message": "so sometimes you want the list, othertimes you want a single value?", "links": [], "channel": "scrapy"},
{"date": "2014-11-20T12:24:26.311233+00:00", "nick": "asd_", "message": "well in this case i want list, should i use item loader instead of selector maybe?", "links": [], "channel": "scrapy"},
{"date": "2014-11-20T12:24:50.278869+00:00", "nick": "asd_", "message": "somewhere i read that item loader by default can extract list of values in item", "links": [], "channel": "scrapy"},
{"date": "2014-11-20T12:26:26.892727+00:00", "nick": "dpn`", "message": "hmm not sure I understand the question well enough to answer meaningfully", "links": [], "channel": "scrapy"},
{"date": "2014-11-20T12:26:36.349147+00:00", "nick": "dpn`", "message": "i think my brain is already in bed", "links": [], "channel": "scrapy"},
{"date": "2014-11-20T12:28:04.194147+00:00", "nick": "asd_", "message": "well basically i want multiple values in one items to be extracted", "links": [], "channel": "scrapy"},
{"date": "2014-11-20T12:28:14.782756+00:00", "nick": "asd_", "message": "in one item*", "links": [], "channel": "scrapy"},
{"date": "2014-11-20T12:30:30.030595+00:00", "nick": "iliana_", "message": "kinda sounds like u want a list to be extracted to another list", "links": [], "channel": "scrapy"},
{"date": "2014-11-20T12:30:55.871134+00:00", "nick": "asd_", "message": "yep", "links": [], "channel": "scrapy"},
{"date": "2014-11-20T13:27:36.090069+00:00", "nick": "asd_", "message": "i got it to work with item loaders finally", "links": [], "channel": "scrapy"},
{"date": "2014-11-20T18:44:12.833313+00:00", "nick": "Jo777", "message": "Hey guys, I try to use the spider.state function as described in http://doc.scrapy.org/en/0.16/topics/jobs.html, but I get the error 'state' is not defined", "links": ["http://doc.scrapy.org/en/0.16/topics/jobs.html"], "channel": "scrapy"},
{"date": "2014-11-20T18:44:26.781144+00:00", "nick": "Jo777", "message": "Do you have an idea what could be wrong?", "links": [], "channel": "scrapy"},
{"date": "2014-11-20T18:46:35.635756+00:00", "nick": "Jo777", "message": "I try to use it in the __ini__() function of a CrawlSpider derived class", "links": [], "channel": "scrapy"},
{"date": "2014-11-20T18:46:46.424478+00:00", "nick": "Jo777", "message": "Is it possible, that this is too early?", "links": [], "channel": "scrapy"},
{"date": "2014-11-21T15:31:26.974204+00:00", "nick": "vurk", "message": "Hi!, According to scrapy docs: http://doc.scrapy.org/en/latest/topics/request-..., the request's 'meta' field sholud contain a key 'bindaddress' with a value of the IP address for destination server, right?", "links": ["http://doc.scrapy.org/en/latest/topics/request-response.html#topics-request-meta"], "channel": "scrapy"},
{"date": "2014-11-21T15:31:49.019321+00:00", "nick": "vurk", "message": "In my trials, there is no 'bindaddress' key", "links": [], "channel": "scrapy"},
{"date": "2014-11-22T13:17:12.591847+00:00", "nick": "asd_", "message": "Somebody knows how can i scrape this value? http://i.imgur.com/ltuVfTQ.png", "links": ["http://i.imgur.com/ltuVfTQ.png"], "channel": "scrapy"},
{"date": "2014-11-22T19:30:05.053430+00:00", "nick": "se_", "message": "my scrapy program doesn't extract links", "links": [], "channel": "scrapy"},
{"date": "2014-11-22T19:30:59.342220+00:00", "nick": "se_", "message": "http://pastebin.com/SwUTaVVk the code", "links": ["http://pastebin.com/SwUTaVVk"], "channel": "scrapy"},
{"date": "2014-11-23T14:28:47.535936+00:00", "nick": "ndb", "message": "hi! is there some way to run the spiders for each psql row for example.. like users", "links": [], "channel": "scrapy"},
{"date": "2014-11-23T14:29:12.598009+00:00", "nick": "ndb", "message": "i'm passing the row content as an argument on a for python script", "links": [], "channel": "scrapy"},
{"date": "2014-11-23T14:30:01.682140+00:00", "nick": "ndb", "message": "http://doc.scrapy.org/en/latest/topics/practice...", "links": ["http://doc.scrapy.org/en/latest/topics/practices.html"], "channel": "scrapy"},
{"date": "2014-11-23T14:30:02.365571+00:00", "nick": "ndb", "message": ":P", "links": [], "channel": "scrapy"},
{"date": "2014-11-23T17:12:06.054120+00:00", "nick": "medecau", "message": "Hi!", "links": [], "channel": "scrapy"},
{"date": "2014-11-23T17:13:51.892583+00:00", "nick": "medecau", "message": "I have been considering some libs for automated content extraction and was wondering if there is any work or exploration done in regards to automated extraction of forum posts.", "links": [], "channel": "scrapy"},
{"date": "2014-11-23T17:14:29.444468+00:00", "nick": "medecau", "message": "I\u2019m happy to have some literature pointers or example code. It\u2019s something I am exploring and want to get a head start", "links": [], "channel": "scrapy"},
{"date": "2014-11-25T07:35:12.982158+00:00", "nick": "Ton", "message": "Hi! i just want to add item to pipeline and do some other stuff in my proc, i don`t want to make a return from proc right now... what i have to do? yield (items.append(il.load_item())) ?", "links": [], "channel": "scrapy"},
{"date": "2014-11-25T16:40:39.495113+00:00", "nick": "ninetynine", "message": "Hi, I am in a need of advice with Scrapy. I am trying to write spider, which would extract links from all pages on domain (without visiting them twice) and on some of them, perform item extraction. The latter works for me (with Rule), but it stops after getting first item and never explores rest of the web. My code: http://pastebin.com/GZMQyu91", "links": ["http://pastebin.com/GZMQyu91"], "channel": "scrapy"},
{"date": "2014-11-25T22:23:41.556191+00:00", "nick": "eval", "message": "hi everyone", "links": [], "channel": "scrapy"},
{"date": "2014-11-25T22:24:04.893952+00:00", "nick": "eval", "message": "having an issue where I don't want scrapy Requests to encode the URL, specifically I want url's with commas to stay as commas", "links": [], "channel": "scrapy"},
{"date": "2014-11-25T22:24:14.054058+00:00", "nick": "eval", "message": "http://stackoverflow.com/questions/24884011/how... also had the issue but I was not able to use the answer", "links": ["http://stackoverflow.com/questions/24884011/how-to-prevent-scrapy-from-url-encoding-request-urls"], "channel": "scrapy"},
{"date": "2014-11-25T22:24:15.957009+00:00", "nick": "eval", "message": "any ideas?", "links": [], "channel": "scrapy"},
{"date": "2014-11-26T11:48:01.014208+00:00", "nick": "cornjuliox", "message": "this might not be exactly the right place to ask, but yeah - does anyone know people/companies hiring remote scrapy devs? I just got laid off today and its by far my strongest skill. i'd like to keep using it", "links": [], "channel": "scrapy"},
{"date": "2014-11-26T16:37:59.211296+00:00", "nick": "eric___", "message": "hi is anyone around?", "links": [], "channel": "scrapy"},
{"date": "2014-11-26T16:38:56.365595+00:00", "nick": "eric___", "message": "trying to scrape: http://list.jd.com/list.html?cat=6728,6745,11881 and the URL has commas in it, but they are being converted to % symbols when I yield Request(url) and it is throwing errors.", "links": ["http://list.jd.com/list.html?cat=6728%2C6745%2C11881"], "channel": "scrapy"},
{"date": "2014-11-26T16:39:32.903069+00:00", "nick": "eric___", "message": "Much like this: http://stackoverflow.com/questions/24884011/how...", "links": ["http://stackoverflow.com/questions/24884011/how-to-prevent-scrapy-from-url-encoding-request-urls"], "channel": "scrapy"},
{"date": "2014-11-26T16:39:42.891288+00:00", "nick": "eric___", "message": "but the fixes do not work and I dn't want to mess up the base python packages with changes", "links": [], "channel": "scrapy"},
{"date": "2014-11-26T16:40:00.180995+00:00", "nick": "eric___", "message": "is there a middleware I could write to make sure the URL remains as is and run through the safe_url_encoding?", "links": [], "channel": "scrapy"},
{"date": "2014-11-27T00:44:32.161857+00:00", "nick": "blusteal", "message": "hey guys", "links": [], "channel": "scrapy"},
{"date": "2014-11-27T00:44:50.781205+00:00", "nick": "blusteal", "message": "I was wondering, with python 3.4 out with asyncio being a part of the standard library", "links": [], "channel": "scrapy"},
{"date": "2014-11-27T00:45:28.492346+00:00", "nick": "blusteal", "message": "wouldn't it be quite easy to replace the twisted framework parts in scrapy with asyncio, reducing the overhead, bringing scrapy up to python 3.4 compliant, etc, etc....", "links": [], "channel": "scrapy"},
{"date": "2014-11-27T00:45:31.401874+00:00", "nick": "blusteal", "message": "?", "links": [], "channel": "scrapy"},
{"date": "2014-11-27T00:45:47.578256+00:00", "nick": "blusteal", "message": "i'd like to hear your thoughts on that idea", "links": [], "channel": "scrapy"},
{"date": "2014-11-27T03:14:28.351042+00:00", "nick": "dpn`", "message": "blusteal, why would you want to do that?", "links": [], "channel": "scrapy"},
{"date": "2014-11-27T03:14:41.235737+00:00", "nick": "dpn`", "message": "ahh 3.4 compliance.. right", "links": [], "channel": "scrapy"},
{"date": "2014-11-27T03:14:54.130935+00:00", "nick": "dpn`", "message": "it would have to be pluggable - because most of the world still uses 2.X", "links": [], "channel": "scrapy"},
{"date": "2014-11-27T03:15:19.170684+00:00", "nick": "dpn`", "message": "frankly short of stuff running on 3.4 I've not heard a compelling reason to drop twisted anywhere yet", "links": [], "channel": "scrapy"},
{"date": "2014-11-27T18:19:35.150030+00:00", "nick": "asd_", "message": "so i have a xpath like this: //h1/span/text(), is it possible to extract just first 5 elements so i could get a year", "links": [], "channel": "scrapy"},
{"date": "2014-11-27T18:19:49.190054+00:00", "nick": "asd_", "message": "btw i use ItemLoader", "links": [], "channel": "scrapy"},
{"date": "2014-11-27T18:20:46.509759+00:00", "nick": "asd_", "message": "sorry i meant first 5 letters, i tried l.add_xpath('year', '//h1/span/text()')[0:5] but i get error", "links": [], "channel": "scrapy"},
{"date": "2014-11-28T06:02:26.970332+00:00", "nick": "Fandekasp", "message": "hi guys. How would you handle multiprocessing within scrapy? Until now, I've been wrapping scrapy in a bash script, and doing multiprocessing there, running a scraper per category of the website to crawl. But that's neither efficient nor scalable. Your advices would be much appreciated!", "links": [], "channel": "scrapy"},
{"date": "2014-11-28T06:07:30.142815+00:00", "nick": "Fandekasp", "message": "In the  docs http://doc.scrapy.org/en/latest/topics/broad-cr..., I can see a CONCURRENT_REQUESTS setting that might fit", "links": ["http://doc.scrapy.org/en/latest/topics/broad-crawls.html"], "channel": "scrapy"},
{"date": "2014-11-29T20:01:29.390223+00:00", "nick": "nomadist", "message": "Hi!", "links": [], "channel": "scrapy"},
{"date": "2014-11-30T01:51:25.917909+00:00", "nick": "cornjuliox", "message": "so i'm looking for scrapy work. would it be against the rules to post on the mailing list looking for work?", "links": [], "channel": "scrapy"},
{"date": "2014-11-30T12:50:54.955327+00:00", "nick": "nomadist", "message": "HiQ", "links": [], "channel": "scrapy"},
{"date": "2014-11-30T12:50:58.816495+00:00", "nick": "nomadist", "message": "Hi!", "links": [], "channel": "scrapy"},
{"date": "2014-11-30T12:51:10.399838+00:00", "nick": "nomadist", "message": "can itemloaders be used with nested selectors?", "links": [], "channel": "scrapy"},
{"date": "2014-11-30T12:52:41.919609+00:00", "nick": "nomadist", "message": "moreover, if I am populating an Item() instance from different pages, using requests.meta then does it make sense to use itemloaders?", "links": [], "channel": "scrapy"},
{"date": "2014-11-30T12:53:02.118620+00:00", "nick": "nomadist", "message": "sorry of this question is stupid as I'm a noob", "links": [], "channel": "scrapy"},
{"date": "2014-11-30T12:53:30.502098+00:00", "nick": "nomadist", "message": "s/requests.meta/Requests().meta/", "links": [], "channel": "scrapy"},
{"date": "2014-12-01T09:43:20.256416+00:00", "nick": "nomadist", "message": "Hi!", "links": [], "channel": "scrapy"},
{"date": "2014-12-01T20:14:40.498162+00:00", "nick": "remote", "message": "I ported a spider to scraping hub a few weeks back and looking at the docs again it's still a mistery to me how I'm supposed to be able to query the data they aquired", "links": [], "channel": "scrapy"},
{"date": "2014-12-01T20:15:06.029122+00:00", "nick": "remote", "message": "can I see everything the spider stored?", "links": [], "channel": "scrapy"},
{"date": "2014-12-01T20:16:01.365132+00:00", "nick": "remote", "message": "what I'm seeing is jobs that have items", "links": [], "channel": "scrapy"},
{"date": "2014-12-01T20:24:47.902727+00:00", "nick": "nramirezuy", "message": "@remote can you tell me how are you trying to query?", "links": [], "channel": "scrapy"},
{"date": "2014-12-01T20:47:33.319031+00:00", "nick": "vurk", "message": "hey all - is there any builtin webservice that runs when a spider is not active? I'd like to launch a preset one via a json call", "links": [], "channel": "scrapy"},
{"date": "2014-12-01T20:50:13.247525+00:00", "nick": "vurk", "message": "Oh. Scrapyd. :)", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T04:29:52.252009+00:00", "nick": "sckir", "message": "hmm i'm having a problem with using two pipelines in scrapy by some reason", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T04:29:55.130951+00:00", "nick": "sckir", "message": "ITEM_PIPELINES = {'mm.pipelines.pipe1' : 100, 'mm.piplines.pipe2' : 200}", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T04:30:55.472012+00:00", "nick": "sckir", "message": "if i only run one of them there is no problem, but when i want to use both (starting with pipe1) the spider seems to get stuck in __init__(self) in pipe1", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T04:31:11.546791+00:00", "nick": "sckir", "message": "and never reaches parse_item()", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T04:31:19.725565+00:00", "nick": "sckir", "message": "process_item(), sorry :P", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T04:31:31.990893+00:00", "nick": "sckir", "message": "both functions are returning item when they are done", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T04:32:38.271143+00:00", "nick": "sckir", "message": "also tried to return item in process_item() in both classes (pipe1 and pipe2) but it's still getting stuck in pipe1 __init__ by some reason", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T04:32:57.263941+00:00", "nick": "sckir", "message": "do you have any idea why this is happening?", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T05:01:43.138870+00:00", "nick": "dpn`", "message": "sckir, can you pinpoint the line it's getting stuck on?", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T05:17:27.880811+00:00", "nick": "sckir", "message": "dpn`, sorry it doesn't get stuck in __init__() but it returns from it and then i can't trace the flow anymore", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T05:17:51.428194+00:00", "nick": "sckir", "message": "so if i print a message in the end of __init__() i can see it", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T05:18:20.166736+00:00", "nick": "sckir", "message": "but if i print a message in the beginning of process_item() it is never called", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T05:25:43.947283+00:00", "nick": "dpn`", "message": "sckir, are you returning items the right way etc from the pipelines?", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T05:30:12.505217+00:00", "nick": "sckir", "message": "dpn`, i think so, if i clear pipe1 and pipe2 classes from everything but __init__() and process_item(self, item, spider): return item", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T05:30:15.540259+00:00", "nick": "sckir", "message": "i still have the same problem", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T05:32:22.829002+00:00", "nick": "dpn`", "message": "process_item is never called or it's called once?", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T05:54:05.062311+00:00", "nick": "sckir", "message": "dpn`, never called", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T05:55:19.581825+00:00", "nick": "dpn`", "message": "sckir, hmm not sure sorry - hopefully id you idle a bit someone else will be able to help", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T06:02:40.458301+00:00", "nick": "sckir", "message": "dpn`, ok thanks", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T19:16:06.601523+00:00", "nick": "nomadist", "message": "Hi!", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T19:16:42.402197+00:00", "nick": "nomadist", "message": "anyone here?", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T19:19:06.283036+00:00", "nick": "nomadist", "message": "is this a dev irc or can common questions be asked here as well?", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T19:38:16.817767+00:00", "nick": "nramirezuy", "message": "idk what is a common question? do you mean like a chat? xD", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T19:50:48.290979+00:00", "nick": "bosnjak", "message": "hi nomadist", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:04:14.406989+00:00", "nick": "nomadist", "message": "hi, i mean a silly question.. a list of mine is getting mungled up somewhere inside the itemloader and I can't understand why", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:05:08.267484+00:00", "nick": "nomadist", "message": "ive posted it in SO now", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:05:51.923137+00:00", "nick": "SoFLy", "message": "what do you mean nomadist", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:05:57.669703+00:00", "nick": "SoFLy", "message": "post the SO url or care to expand?", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:06:06.344439+00:00", "nick": "nomadist", "message": "https://stackoverflow.com/questions/27257374/sc...", "links": ["https://stackoverflow.com/questions/27257374/scrapy-list-is-getting-mungled-up-somewhere-in-the-item-loader-pipeline"], "channel": "scrapy"},
{"date": "2014-12-02T20:06:10.434891+00:00", "nick": "SoFLy", "message": "if you just ask the question it'll go unanswered (worst) or someone will help (best)", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:06:54.659256+00:00", "nick": "nomadist", "message": "SoFLy,  heh. I hope its the latter. I've been banging my head for a while trying to understand this behaviour.", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:08:15.006332+00:00", "nick": "SoFLy", "message": "have you instantiated the item loader?", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:09:16.122759+00:00", "nick": "nomadist", "message": "yup, there a bunch of input and output processors defined and they work as expected. I can also do `item_list_out = TakeFirst()` and only the first element of the list is gotten", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:10:00.205765+00:00", "nick": "nomadist", "message": "and on the parse side, I've done a `print item_list` to check that the extraction is all right as well", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:12:24.191539+00:00", "nick": "SoFLy", "message": "and you're sure your output parser code isn't doing anything to have it appear 3 times instead of once?", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:12:26.430128+00:00", "nick": "SoFLy", "message": "(the set)", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:12:36.444975+00:00", "nick": "SoFLy", "message": "or your input parser", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:13:21.626023+00:00", "nick": "nramirezuy", "message": "'//some/tag()' should be './/some/tag()'", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:13:47.272143+00:00", "nick": "SoFLy", "message": "ooo man", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:13:55.374337+00:00", "nick": "SoFLy", "message": "that's a great catch", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:13:56.619321+00:00", "nick": "nomadist", "message": "nramirezuy, I am searching globally", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:13:56.723295+00:00", "nick": "nramirezuy", "message": "did you tried printing the item_list?", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:14:08.259650+00:00", "nick": "nomadist", "message": "oh", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:14:11.170117+00:00", "nick": "nomadist", "message": "let me check", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:15:05.172476+00:00", "nick": "nomadist", "message": "no, that isn't the case. This is the extraction line: response.css('.nav-buttons.column').xpa...", "links": ["mailto:response.css('.nav-buttons.column').xpath('.//a/img/@alt').extract()"], "channel": "scrapy"},
{"date": "2014-12-02T20:15:51.247700+00:00", "nick": "nramirezuy", "message": "if you print the output of that, is it right?", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:17:17.294055+00:00", "nick": "nomadist", "message": "yes. The funny thing is that, as I go down the json file, the `item_list` keeps getting bigger and bigger.. eventually spanning multiple lines while I go down", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:17:49.775393+00:00", "nick": "nramirezuy", "message": "are you using a custom item loader? with default_item_class defined?", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:18:42.517856+00:00", "nick": "nomadist", "message": "its pretty short so Ill put it here.. class MyLoader(ItemLoader):", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:18:42.902314+00:00", "nick": "nomadist", "message": "default_output_processor = TakeFirst()", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:18:42.924774+00:00", "nick": "nomadist", "message": "default_input_processor = MapCompose(unicode.strip)", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:18:42.924825+00:00", "nick": "nomadist", "message": "item_list_in = Identity()", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:18:42.947155+00:00", "nick": "nomadist", "message": "item_list_out = Identity()", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:19:25.522170+00:00", "nick": "nramirezuy", "message": "so you are passing item as an argument?", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:19:45.808184+00:00", "nick": "nramirezuy", "message": "when you instantiate the itemloader", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:21:15.247494+00:00", "nick": "nomadist", "message": "yes, loader = MyLoader(MyItem, <response xpath>)", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:21:30.117813+00:00", "nick": "nramirezuy", "message": "MyItem is a class or a instance?", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:21:58.315325+00:00", "nick": "nomadist", "message": "instance, sorry, should've been MyItem()..", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:22:43.440893+00:00", "nick": "nomadist", "message": "but this happens in the first parse method, where I'm just yielding the request object.. the item_list gets parsed in a callback where the loader is retreived from the meta attribute of request/response", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:23:27.995425+00:00", "nick": "nramirezuy", "message": "so you are passing the instance of item loader ?", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:23:48.423598+00:00", "nick": "nramirezuy", "message": "one instance for the whole crawl?", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:24:12.455650+00:00", "nick": "nomadist", "message": "no, for each link", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:25:34.587294+00:00", "nick": "nramirezuy", "message": "then why you move the loader in the meta, I don't quite get it :(", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:25:49.371344+00:00", "nick": "nramirezuy", "message": "can you upload the spider code to gist?", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:26:23.823455+00:00", "nick": "nomadist", "message": "I'm doing it", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:26:41.368160+00:00", "nick": "nramirezuy", "message": "nramirezuy my acc if you want it private", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:28:31.709906+00:00", "nick": "SoFLy", "message": "yeah I was going to ask for the ItemLoader code, i'm a tad confused by the architecture as you've described it too", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:33:51.485713+00:00", "nick": "nomadist", "message": "hey, I've abstracted the relevant parts of my code here--> http://pastebin.com/5dbWZ8B2", "links": ["http://pastebin.com/5dbWZ8B2"], "channel": "scrapy"},
{"date": "2014-12-02T20:35:25.280552+00:00", "nick": "nomadist", "message": "oops, on line 47 it should be print item_list.. (tried to change the variable names forgot about that one)", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:36:11.591117+00:00", "nick": "SoFLy", "message": "not that this is actually the cause (just doing a first readthrough)", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:36:21.608369+00:00", "nick": "SoFLy", "message": "but on line 30 you're instantiating the loader inside of a loop", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:36:28.298378+00:00", "nick": "SoFLy", "message": "so it's re-instantiating each iteration, no?", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:36:28.669929+00:00", "nick": "nomadist", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:36:35.875166+00:00", "nick": "SoFLy", "message": "is that behavior you're looking for?", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:37:09.383220+00:00", "nick": "nomadist", "message": "yes, each ul, contains a link that I follow", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:38:28.959077+00:00", "nick": "nomadist", "message": "its a page with a list of links, some info is available on the listing-page, I take that and follow each page's link to get additional info from parse_more", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:42:57.647683+00:00", "nick": "nomadist", "message": "I am pretty sure its somewhere in the itemloader... as I'm printing it out right there and its fine", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:43:37.607282+00:00", "nick": "nomadist", "message": "if I do away with itemloaders and yield raw MyItem() instances.. its fine", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:44:15.538981+00:00", "nick": "nomadist", "message": "but I don't understand this behaviour- Identity() on input and output should leave the list untouched", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:48:10.590469+00:00", "nick": "nramirezuy", "message": "I'm pretty sure it has something to do with passing the loader instance in the meta; can we do a test?", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T20:50:45.012933+00:00", "nick": "nramirezuy", "message": "http://pastebin.com/2eiDd4pr", "links": ["http://pastebin.com/2eiDd4pr"], "channel": "scrapy"},
{"date": "2014-12-02T21:08:36.611448+00:00", "nick": "nomadist", "message": "sorry got disconnected", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T21:09:06.962747+00:00", "nick": "nomadist", "message": "nramirezuy, thanks a lot. and when you return please please explain your wizardry", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T21:09:21.511998+00:00", "nick": "nramirezuy", "message": "it workd?", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T21:09:27.701130+00:00", "nick": "nomadist", "message": "yup", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T21:09:43.323006+00:00", "nick": "nomadist", "message": "the only changes were in line 34 and line 40, right?", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T21:09:45.080347+00:00", "nick": "nramirezuy", "message": "i think is something with shared state, I'm not sure what it is", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T21:09:53.368886+00:00", "nick": "nramirezuy", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T21:10:00.325599+00:00", "nick": "nramirezuy", "message": "instead of moving the loader instance", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T21:10:04.066048+00:00", "nick": "nramirezuy", "message": "you move the item", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T21:10:14.423458+00:00", "nick": "nomadist", "message": "so a dict instead of the loader object- which must have had some state shared", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T21:10:25.503788+00:00", "nick": "nomadist", "message": "between the two parse methods?", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T21:11:14.142613+00:00", "nick": "nramirezuy", "message": "I believe so, I'm created a test spider. To figure out what it is exactly", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T21:14:11.178189+00:00", "nick": "nomadist", "message": "I'll investigate too. There is some other piece of code which is failing after this change in the second parse method.", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T21:15:28.523507+00:00", "nick": "nramirezuy", "message": "what is it?", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T21:15:42.863767+00:00", "nick": "nomadist", "message": "its loader.get_output_value('field_name')", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T21:15:54.343429+00:00", "nick": "nomadist", "message": "i just put a pdb.set_trace", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T21:16:12.778955+00:00", "nick": "nramirezuy", "message": "try doing", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T21:16:13.030443+00:00", "nick": "nramirezuy", "message": "loader.add_value(None, item)", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T21:16:30.468466+00:00", "nick": "nramirezuy", "message": "it will load the item in the loader", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T21:16:34.833618+00:00", "nick": "nomadist", "message": "and I can confirm that the loader object is returning None for a field, when I can clearly see that it has a non-None value using loader.load_item", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T21:16:43.923414+00:00", "nick": "nramirezuy", "message": "you can also", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T21:16:47.985851+00:00", "nick": "nramirezuy", "message": "access the item directly", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T21:16:54.540873+00:00", "nick": "nramirezuy", "message": "which I think is a lot better", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T21:18:42.398440+00:00", "nick": "nomadist", "message": "suppose loader.load_item() gives `{'item1': 'value1', 'item_list':['a', 'b', 'c']}`, and if I do loader.get_output_value('item1') I get None, even though its value is 'value1'", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T21:19:04.691072+00:00", "nick": "nomadist", "message": "earlier code gave me the expected output, i.e 'value1'", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T21:19:29.787388+00:00", "nick": "nomadist", "message": "loader.add_value is only for adding values to fields, right?", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T21:20:21.958580+00:00", "nick": "nramirezuy", "message": "that is because the item1 key is on the item, not on the loader instance", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T21:21:45.865934+00:00", "nick": "nomadist", "message": "so how can I get the item1 key's value? using add_value?", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T21:22:03.298469+00:00", "nick": "nramirezuy", "message": "I mean, dont do that there is no need", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T21:22:11.819637+00:00", "nick": "nramirezuy", "message": "you can just use item['item1']", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T21:22:22.618076+00:00", "nick": "nramirezuy", "message": "or item.get('item1')", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T21:23:26.452344+00:00", "nick": "nomadist", "message": "earlier, if loader.get_output_value('country') == \"SomeCountry\":", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T21:23:26.875528+00:00", "nick": "nomadist", "message": "loader.add_value('some_field', something_country_specific)", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T21:23:47.509516+00:00", "nick": "nomadist", "message": "and I haven't instantiated an item", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T21:24:15.694512+00:00", "nick": "nramirezuy", "message": "country is filled on the previous callback?", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T21:24:26.079444+00:00", "nick": "nomadist", "message": "no in the second one (parse_more)", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T21:24:45.401409+00:00", "nick": "nramirezuy", "message": "then it should be available via get_output_value", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T21:25:59.949911+00:00", "nick": "nomadist", "message": "that's what's strange.. I'm in pdb right now and can inspect loader", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T21:26:14.577229+00:00", "nick": "nomadist", "message": "loader.load_item()", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T21:26:14.906438+00:00", "nick": "nomadist", "message": "{'city': u'Lille',", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T21:26:14.933707+00:00", "nick": "nomadist", "message": "'country': u'France', etc etc...", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T21:26:33.691889+00:00", "nick": "nomadist", "message": "(Pdb) type(loader.get_output_value('country'))", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T21:26:33.714185+00:00", "nick": "nomadist", "message": "<type 'NoneType'>", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T21:27:18.970704+00:00", "nick": "nramirezuy", "message": "idk what to say, pastebin ? xD", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T21:29:46.214104+00:00", "nick": "nomadist", "message": "http://pastebin.com/6ewnC6pJ", "links": ["http://pastebin.com/6ewnC6pJ"], "channel": "scrapy"},
{"date": "2014-12-02T21:31:56.889980+00:00", "nick": "nomadist", "message": "this was working before (when I was passing the loader as an object between the parse methods)", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T21:37:56.691892+00:00", "nick": "nramirezuy", "message": "weird", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T21:39:01.464550+00:00", "nick": "nomadist", "message": "I guess the only workaround is to then do something like, `item = loader.load_item()`, then perform the check using item as a dictionary?", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T21:39:26.500215+00:00", "nick": "nramirezuy", "message": "I think so", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T21:40:21.424792+00:00", "nick": "nomadist", "message": "duck typing to the rescue I guess..", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T21:40:34.015913+00:00", "nick": "nomadist", "message": "thankfully item quacks like a duck", "links": [], "channel": "scrapy"},
{"date": "2014-12-02T21:41:19.136146+00:00", "nick": "nomadist", "message": "anyhow, thanks nramirezuy. I don't exactly understand how this problem was corrected but it was.", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:26:43.798881+00:00", "nick": "nomadist", "message": "hi!", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:32:27.904526+00:00", "nick": "nomadist", "message": "I cloned the scrapy repo from github.. in a virtualenv when I type \"python setup.py install\" it fails with the following message:", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:32:54.003086+00:00", "nick": "nomadist", "message": "(venv)[scrapy]$ python setup.py install", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:32:54.293206+00:00", "nick": "nomadist", "message": "Traceback (most recent call last):", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:32:54.320575+00:00", "nick": "nomadist", "message": "File \"setup.py\", line 5, in <module>", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:32:54.343056+00:00", "nick": "nomadist", "message": "with open(join(dirname(__file__), 'scrapy/VERSION'), 'rb') as f:", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:32:54.343129+00:00", "nick": "nomadist", "message": "IOError: [Errno 2] No such file or directory: 'scrapy/VERSION'", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:33:18.159260+00:00", "nick": "nomadist", "message": "I couldn't find anything in the docs http://doc.scrapy.org/en/latest/intro/install.h...", "links": ["http://doc.scrapy.org/en/latest/intro/install.html#ubuntu-9-10-or-above"], "channel": "scrapy"},
{"date": "2014-12-03T12:36:28.000087+00:00", "nick": "nikolaosk", "message": "nomadist: virtualenv installs pip", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:36:32.158028+00:00", "nick": "nikolaosk", "message": "inside it", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:37:29.884853+00:00", "nick": "nikolaosk", "message": "you can use it to install directly from the cloned repo", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:37:51.680877+00:00", "nick": "nikolaosk", "message": "I think it's the -e flag if I am not mistaken", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:38:06.404275+00:00", "nick": "nikolaosk", "message": "pip install -e directory_of_clone/", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:38:12.310274+00:00", "nick": "nikolaosk", "message": "or something", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:38:15.984306+00:00", "nick": "nomadist", "message": "nikolaosk, pip install . from inside the repo gives me Command python setup.py egg_info failed with error code 1 in /tmp/pip-k_jT3e-build", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:38:16.305014+00:00", "nick": "nomadist", "message": "Storing debug log for failure in /home/myname/.pip/pip.log", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:38:17.020398+00:00", "nick": "nikolaosk", "message": "see pip help install", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:39:07.761348+00:00", "nick": "nikolaosk", "message": "try pip list | grep setuptools", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:39:16.230029+00:00", "nick": "nikolaosk", "message": "which version?", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:39:55.252542+00:00", "nick": "nomadist", "message": "Scrapy 0.14.4", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:40:34.528572+00:00", "nick": "nomadist", "message": "setuptools (2.1)", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:41:02.864455+00:00", "nick": "nikolaosk", "message": "how come really?", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:41:27.852555+00:00", "nick": "nomadist", "message": "sorry, I think the scrapy version is my system-wide version as virtualenv defaults to no-site-packages and I haven't installed scrapy inside here", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:41:51.000453+00:00", "nick": "nomadist", "message": "pip freeze", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:41:51.322786+00:00", "nick": "nomadist", "message": "Twisted==14.0.2", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:41:51.345307+00:00", "nick": "nomadist", "message": "argparse==1.2.1", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:41:51.345368+00:00", "nick": "nomadist", "message": "cffi==0.8.6", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:41:51.346792+00:00", "nick": "nomadist", "message": "cryptography==0.6.1", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:41:51.568832+00:00", "nick": "nomadist", "message": "cssselect==0.9.1", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:41:53.504832+00:00", "nick": "nomadist", "message": "lxml==3.4.1", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:41:55.490871+00:00", "nick": "nomadist", "message": "pyOpenSSL==0.14", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:41:57.494340+00:00", "nick": "nomadist", "message": "pycparser==2.10", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:41:59.496989+00:00", "nick": "nomadist", "message": "queuelib==1.2.2", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:42:01.515871+00:00", "nick": "nomadist", "message": "six==1.8.0", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:42:03.518156+00:00", "nick": "nomadist", "message": "w3lib==1.10.0", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:42:05.505528+00:00", "nick": "nomadist", "message": "wsgiref==0.1.2", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:42:07.548167+00:00", "nick": "nomadist", "message": "zope.interface==4.1.1", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:43:07.504604+00:00", "nick": "nikolaosk", "message": "better pastebin these things", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:43:17.492310+00:00", "nick": "nikolaosk", "message": "pip list -o will list just obsolete packages", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:43:32.997613+00:00", "nick": "nikolaosk", "message": "like Scrapy .14 and setuptools 2.1", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:43:40.716876+00:00", "nick": "nomadist", "message": "yeah, sorry. After posting I realized Im needlessly populating it here.", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:43:41.750044+00:00", "nick": "nikolaosk", "message": "very obsolete", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:43:56.154794+00:00", "nick": "nikolaosk", "message": "but really", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:44:03.868901+00:00", "nick": "nikolaosk", "message": "how come you ended up with scrapy .14", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:44:31.268309+00:00", "nick": "nomadist", "message": "argparse setuptools and pip are outdated", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:44:47.795700+00:00", "nick": "nomadist", "message": "nikolaosk, oh I used the ubuntu repositories sometime back to install it", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:46:32.090762+00:00", "nick": "nikolaosk", "message": "then re-read this section http://doc.scrapy.org/en/latest/intro/install.h...", "links": ["http://doc.scrapy.org/en/latest/intro/install.html#ubuntu-9-10-or-above"], "channel": "scrapy"},
{"date": "2014-12-03T12:46:32.843561+00:00", "nick": "nomadist", "message": "nikolaosk, I updated the outdated packages. I still get the error.", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:46:53.835426+00:00", "nick": "nikolaosk", "message": "I'd suggest", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:46:59.336917+00:00", "nick": "nikolaosk", "message": "remove the system scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:47:02.333413+00:00", "nick": "nikolaosk", "message": "with apt-get remove", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:47:16.921140+00:00", "nick": "nikolaosk", "message": "and install only in the virtual env", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:47:27.371816+00:00", "nick": "nikolaosk", "message": "install with pip install --upgrade", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:47:37.479238+00:00", "nick": "nikolaosk", "message": "and even -e if you want the repo", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:47:48.764967+00:00", "nick": "nikolaosk", "message": "but first upgrade setuptools", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:47:53.834371+00:00", "nick": "nikolaosk", "message": "you have an old version", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:49:47.910560+00:00", "nick": "nomadist", "message": "I have setuptools (7.0)", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:49:48.227730+00:00", "nick": "nomadist", "message": "now, and I removed the system-wide scrapy. Still getting the same error", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:50:32.044510+00:00", "nick": "nomadist", "message": "there's only outdated: argparse (Current: 1.2.1 Latest: 1.2.2), but I think that's a python3 version", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:50:50.257907+00:00", "nick": "nikolaosk", "message": "ignore it", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:51:10.660071+00:00", "nick": "nikolaosk", "message": "same error when running: python setup.py", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:51:13.916729+00:00", "nick": "nikolaosk", "message": "is it?", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:51:43.576288+00:00", "nick": "nikolaosk", "message": "sorry, python setup.py install", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:53:14.091055+00:00", "nick": "nomadist", "message": "just a sec, recloning in a fresh virtualenv", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:56:53.548043+00:00", "nick": "nomadist", "message": "nikolaosk, thanks a lot. It worked!", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T12:59:58.955842+00:00", "nick": "nikolaosk", "message": "super, have fun :)", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T13:13:13.369738+00:00", "nick": "nomadist", "message": "(got disc) one more silly question: import scrapy works as expected, but the cli client, i.e  `scrapy crawl somespider` throws \"bash: /usr/bin/scrapy: No such file or directory\"", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T16:49:43.153845+00:00", "nick": "justonequestion", "message": "Hello there,", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T16:50:38.721637+00:00", "nick": "nramirezuy", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T16:51:07.515886+00:00", "nick": "justonequestion", "message": "I'd like to change a setting defined in settings.py in the spider __init__ method, I tried using get_project_settings.set(), but in my item pipeline module, the setting used is still the one in the settings.py file", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T16:51:36.142655+00:00", "nick": "justonequestion", "message": "basically, I'm trying to override the ELASTICSEARCH_INDEX settings so I can specify it in the command line for the scrapy-elasticsearch item pipeline", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T16:51:40.309009+00:00", "nick": "justonequestion", "message": "any clues?", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T16:53:16.846903+00:00", "nick": "Digenis", "message": "if you definine it in the command line", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T16:53:22.996433+00:00", "nick": "justonequestion", "message": "I tried using", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T16:53:24.204817+00:00", "nick": "justonequestion", "message": "scrapy crawl RobotsSpider -s \"ELASTICSEARCH_INDEX=asdasd\"", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T16:53:27.055471+00:00", "nick": "Digenis", "message": "why again in the spider?", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T16:53:34.275422+00:00", "nick": "justonequestion", "message": "but it's just not using it, it goes back to the project default", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T16:53:44.615322+00:00", "nick": "Digenis", "message": "it shouldn't", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T16:53:47.688086+00:00", "nick": "Digenis", "message": "check for typos", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T16:54:34.859682+00:00", "nick": "justonequestion", "message": "nope, no typos in the settings name", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T16:54:57.327352+00:00", "nick": "justonequestion", "message": "it looks like scrapy is not paying attention to the settings sent via -s in the command line", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T16:55:25.041328+00:00", "nick": "nramirezuy", "message": "is the  mdw fault", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T16:55:26.080622+00:00", "nick": "nramirezuy", "message": "https://github.com/knockrentals/scrapy-elastics...", "links": ["https://github.com/knockrentals/scrapy-elasticsearch/blob/master/scrapyelasticsearch/scrapyelasticsearch.py#L21"], "channel": "scrapy"},
{"date": "2014-12-03T16:56:01.898792+00:00", "nick": "justonequestion", "message": "oh, I see", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T16:56:04.432812+00:00", "nick": "nramirezuy", "message": "You have to extend the mdw pulling the settings from the crawler", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T16:56:14.818070+00:00", "nick": "nramirezuy", "message": "like on scrapy.contrib", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T16:56:16.562035+00:00", "nick": "justonequestion", "message": "what would be the right way to access settings from the ItemPipeline", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T16:56:34.480985+00:00", "nick": "justonequestion", "message": "so I can send a PR to the scrapy-elasticsearch project", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T16:56:53.418061+00:00", "nick": "nramirezuy", "message": "https://github.com/scrapy/scrapy/blob/master/sc...", "links": ["https://github.com/scrapy/scrapy/blob/master/scrapy/contrib/downloadermiddleware/retry.py#L49"], "channel": "scrapy"},
{"date": "2014-12-03T16:58:03.349853+00:00", "nick": "justonequestion", "message": "I see", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T16:58:11.767013+00:00", "nick": "justonequestion", "message": "is from_crawler called in an item pipeline then?", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T16:58:32.508001+00:00", "nick": "nramirezuy", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T16:58:55.324695+00:00", "nick": "justonequestion", "message": "or can I use from_settings as in https://github.com/scrapy/scrapy/blob/master/sc... ?", "links": ["https://github.com/scrapy/scrapy/blob/master/scrapy/contrib/pipeline/images.py#L45"], "channel": "scrapy"},
{"date": "2014-12-03T16:59:43.473207+00:00", "nick": "nramirezuy", "message": "from_settings is deprecated", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T17:00:02.204546+00:00", "nick": "nramirezuy", "message": "it is on deprecation process :P", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T17:00:06.990377+00:00", "nick": "justonequestion", "message": "thanks mate!", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T17:00:16.473955+00:00", "nick": "justonequestion", "message": "I'll try it out now and let you know how that went", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T17:02:14.172288+00:00", "nick": "Digenis", "message": "how after all this time with the from_crawler() confusion", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T17:02:36.510599+00:00", "nick": "Digenis", "message": "the crawler object didn't end up as an argument to __init__()", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T17:02:47.953513+00:00", "nick": "Digenis", "message": "the crawler instance I mean", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T17:11:54.396095+00:00", "nick": "nramirezuy", "message": "because is easier to instantiate components without the crawler instance", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T17:12:50.261040+00:00", "nick": "Digenis", "message": "I think you mean less costly but I can't guess in what", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T17:14:34.634310+00:00", "nick": "Digenis", "message": "is it start up time?", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T17:15:59.849212+00:00", "nick": "nramirezuy", "message": "I mean for test purposes", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T17:17:45.865699+00:00", "nick": "Digenis", "message": "oh, yes", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T17:17:47.538718+00:00", "nick": "Digenis", "message": "purity", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T17:18:06.488302+00:00", "nick": "Digenis", "message": "very good reason", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T17:19:28.574021+00:00", "nick": "ayoub", "message": "hello", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T17:19:35.914898+00:00", "nick": "ayoub", "message": "can we scrapy an java aplication ?", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T17:20:45.243027+00:00", "nick": "Digenis", "message": "\"as a\" java app?", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T17:20:57.696357+00:00", "nick": "Digenis", "message": "never heard of this", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T17:21:26.279243+00:00", "nick": "Digenis", "message": "do you have jython in mind?", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T17:23:26.914341+00:00", "nick": "nramirezuy", "message": "ayoub, can you give us some insight on it? what do you want to do?", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T17:33:44.385766+00:00", "nick": "justonequestion", "message": "@nramirezuy That did the trick, thanks I sent a PR to the scrapy-elasticsearch repo. Thanks vecino from across the pond, enjoy a mate for me ;)", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T18:56:13.246303+00:00", "nick": "etudor", "message": "hi, what python version do i need to install for scrapy?", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T18:56:33.765809+00:00", "nick": "etudor", "message": "i have tried with virtualenv but i get errors", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T18:56:39.648621+00:00", "nick": "etudor", "message": "i am on a windows machine", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T19:01:10.400599+00:00", "nick": "etudor", "message": "so much activity here", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T19:01:22.475785+00:00", "nick": "etudor", "message": ":-)", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T19:08:30.105261+00:00", "nick": "ayoub", "message": "nramirezuy, in fact i have an java application", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T19:08:50.877628+00:00", "nick": "ayoub", "message": "and i have many action on it", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T19:09:02.213313+00:00", "nick": "ayoub", "message": "i want automatisation this action", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T19:09:28.327711+00:00", "nick": "ayoub", "message": "but is a gui interface so only way is a scrapy but how scrap an java gui interface? ?", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T19:10:16.314021+00:00", "nick": "nramirezuy", "message": "are you trying to crawl a java app?", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T19:10:57.690221+00:00", "nick": "nramirezuy", "message": "@etudor python 2.7 ; you might be missing some dependency", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T19:11:15.946506+00:00", "nick": "etudor", "message": "i have 2.7.3", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T19:11:39.698890+00:00", "nick": "etudor", "message": "work with this version to?", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T19:13:02.592133+00:00", "nick": "nramirezuy", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T19:18:40.358706+00:00", "nick": "etudor", "message": "ok, thank you", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T19:19:33.786833+00:00", "nick": "etudor", "message": "when i hit python --version i get this: Python 2.7.8", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T19:20:00.125360+00:00", "nick": "nramirezuy", "message": "thats fine too", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T19:20:29.002062+00:00", "nick": "etudor", "message": "but when i hit scrapy startproject laptop i get this: Traceback (most recent call last):", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T19:20:29.106044+00:00", "nick": "etudor", "message": "File \"C:\\Python34\\Scripts\\scrapy-script.py\", line 5, in <module>", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T19:20:53.901487+00:00", "nick": "etudor", "message": "i don't know why it uses python34", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T19:22:24.890839+00:00", "nick": "nramirezuy", "message": "seems to be installed under python 3.4", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T19:23:55.657343+00:00", "nick": "ayoub", "message": "can we scrap a java application", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T19:23:56.661955+00:00", "nick": "ayoub", "message": "?", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T19:24:23.738482+00:00", "nick": "etudor", "message": "how can i install it under python 2.7", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T19:26:21.961535+00:00", "nick": "nramirezuy", "message": "@etudor  you have to use the correspondent setup tools I suppose; I don't use windows for dev at all :(", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T19:27:02.415401+00:00", "nick": "etudor", "message": "ok", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T19:27:05.917703+00:00", "nick": "etudor", "message": "thank you", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T19:27:59.759770+00:00", "nick": "nramirezuy", "message": "@ayoub that depends on the app if it use http; but I don't think it use it. Generally java apps use encrypted client-sever communication.", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T19:28:51.576388+00:00", "nick": "ayoub", "message": "ok thanks", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T22:32:30.319871+00:00", "nick": "nomadist", "message": "hi! I have a question about output processor MapCompose", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T22:32:54.340897+00:00", "nick": "nomadist", "message": "From the docs \"The results of these function calls (one for each element) are concatenated to construct a new iterable, which is then used to apply the second function, and so on, until the last function is applied to each value of the list of values collected so far. The output values of the last function are concatenated together to produce the output of this processor\"", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T22:33:53.243246+00:00", "nick": "nomadist", "message": "I'm wondering whether `proc = MapCompose(returns_dict, TakeFirst())` will work, where returns_dict returns a dictionary", "links": [], "channel": "scrapy"},
{"date": "2014-12-03T22:35:01.829931+00:00", "nick": "nomadist", "message": "and I assume that the dictionary would be made iterable by doing list(<dict value>), where <dict value> can be retreived via TakeFirst()", "links": [], "channel": "scrapy"},
{"date": "2014-12-05T18:31:24.408198+00:00", "nick": "SirSkitzo", "message": "Are there any good tutorials for scraping a site that generates mailto: links using javascript? With javascript disabled it will only show N/A", "links": [], "channel": "scrapy"},
{"date": "2014-12-05T18:50:27.822320+00:00", "nick": "Lucax", "message": "hello, is there any gui for scrapy?", "links": [], "channel": "scrapy"},
{"date": "2014-12-05T18:50:37.813741+00:00", "nick": "Lucax", "message": "would be fun", "links": [], "channel": "scrapy"},
{"date": "2014-12-05T18:53:46.825286+00:00", "nick": "SirSkitzo", "message": "Never used it, but I found this: http://blog.scrapinghub.com/2014/04/01/announci...", "links": ["http://blog.scrapinghub.com/2014/04/01/announcing-portia/"], "channel": "scrapy"},
{"date": "2014-12-05T19:38:06.266530+00:00", "nick": "nomadist", "message": "hi!", "links": [], "channel": "scrapy"},
{"date": "2014-12-05T19:38:32.742418+00:00", "nick": "nomadist", "message": "I am using selenium to handle some ajaxy page..", "links": [], "channel": "scrapy"},
{"date": "2014-12-05T19:39:12.402511+00:00", "nick": "nomadist", "message": "for uniformity, I would want to convert the driver.page_source into a scrapy.http.response.html.HtmlResponse object", "links": [], "channel": "scrapy"},
{"date": "2014-12-05T19:39:34.247432+00:00", "nick": "nomadist", "message": "so I can use the convenient .css and xpath selectors instead of resorting lxml", "links": [], "channel": "scrapy"},
{"date": "2014-12-05T19:40:05.725254+00:00", "nick": "nomadist", "message": "just wanted to know if this is an ok practice or if there are any drawbacks/gotchas for this approach", "links": [], "channel": "scrapy"},
{"date": "2014-12-05T21:58:05.615086+00:00", "nick": "SirSkitzo", "message": "Are there any good tutorials for scraping a site that generates mailto: links using javascript? With javascript disabled it will only show N/A", "links": [], "channel": "scrapy"},
{"date": "2014-12-05T22:37:34.722446+00:00", "nick": "SirSkitzo", "message": "How do I activate a middleware for use with scrapy shell?", "links": [], "channel": "scrapy"},
{"date": "2014-12-06T02:17:31.900784+00:00", "nick": "SirSkitzo", "message": "Are there any good tutorials for rendering JavaScript elements through Scrapy?", "links": [], "channel": "scrapy"},
{"date": "2014-12-06T02:33:59.572686+00:00", "nick": "SirSkitzo", "message": "What's the best way to have Scrapy render JavaScript on Windows?", "links": [], "channel": "scrapy"},
{"date": "2014-12-06T04:48:04.946715+00:00", "nick": "SirSkitzo", "message": "What's the best way to have Scrapy render JavaScript on Windows?", "links": [], "channel": "scrapy"},
{"date": "2014-12-06T20:11:44.242292+00:00", "nick": "brati", "message": "Trying to fix a bug in scrapy. How are tests executed in current stable version? DOcs say bin/runtests.sh, but there is no runtests.sh file anymore anywhere in scrapy source.", "links": [], "channel": "scrapy"},
{"date": "2014-12-07T11:43:18.201115+00:00", "nick": "nomadist", "message": "hi!", "links": [], "channel": "scrapy"},
{"date": "2014-12-07T11:43:51.154807+00:00", "nick": "nomadist", "message": "Is there an alternative pattern to dont_filter=True in the Request object?", "links": [], "channel": "scrapy"},
{"date": "2014-12-07T11:44:19.627883+00:00", "nick": "nomadist", "message": "my question here: http://stackoverflow.com/questions/27341803/mul...", "links": ["http://stackoverflow.com/questions/27341803/multi-level-parsing-pattern-to-avoid-duplicates"], "channel": "scrapy"},
{"date": "2014-12-07T12:09:49.491305+00:00", "nick": "brati", "message": "I usually also do like Kev's answer says", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T15:00:19.942108+00:00", "nick": "scmp", "message": "Hello, possible bug in 0.24.2 with LinkExtractor(process_value) http://paste.debian.net/135476/", "links": ["http://paste.debian.net/135476/"], "channel": "scrapy"},
{"date": "2014-12-08T15:00:30.412516+00:00", "nick": "scmp", "message": "not all url's have the xxxx attached", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T15:01:11.065748+00:00", "nick": "scmp", "message": "no time to fill a bug, but hope someone picks it up. thx!", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T15:03:39.942243+00:00", "nick": "highgainer", "message": "Hi", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:18:15.393895+00:00", "nick": "chocko01", "message": "hello", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:18:50.391395+00:00", "nick": "nramirezuy", "message": "scmp: its not a bug on LinkExtractor, the page has fragment links that are getting cleaned by canonicalize", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:18:53.469805+00:00", "nick": "nramirezuy", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:22:39.152952+00:00", "nick": "chocko01", "message": "I have set up a spider ( I am new to scrapy but not to scraping world) and I am hesitating between the different ways I have to export my scraped data.", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:24:03.625509+00:00", "nick": "nramirezuy", "message": "how you done it before?", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:24:04.879378+00:00", "nick": "chocko01", "message": "The different parse* methods yields requests between them and also 3 different type scraped object (Item)", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:25:20.099256+00:00", "nick": "nramirezuy", "message": "and you want different files for each item type?", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:25:24.517104+00:00", "nick": "chocko01", "message": "nramirezuy: with urllib2, cookiejar and lxml, a somewhat bad framework, much happier now with srapy", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:25:42.338542+00:00", "nick": "nramirezuy", "message": "I mean data export", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:26:23.381584+00:00", "nick": "chocko01", "message": "i would like to export my data to perform data analysys with pandas and create statistics", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:27:20.008231+00:00", "nick": "chocko01", "message": "the idea is that I am going to launch the spider every day and that the data will eventually change. I want to be able to detect theses changes.", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:28:20.935452+00:00", "nick": "chocko01", "message": "I have defined 3 different Item", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:28:49.758873+00:00", "nick": "chocko01", "message": "in only one spider", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:29:38.029280+00:00", "nick": "chocko01", "message": "so that when i call \"scrapy crawl myspider -o out.jon\" , the 3 items and entangled", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:30:13.307683+00:00", "nick": "chocko01", "message": "and if they have the same field names, there is no way to distingish them", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:30:56.629455+00:00", "nick": "nramirezuy", "message": "you can create a field with the item type", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:30:58.708751+00:00", "nick": "chocko01", "message": "I know it is possible to write a pipeline", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:31:14.727571+00:00", "nick": "chocko01", "message": "is it the only way ?", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:32:05.642481+00:00", "nick": "nramirezuy", "message": "you can create a file per item type", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:32:25.481455+00:00", "nick": "nramirezuy", "message": "but is more work", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:33:11.146123+00:00", "nick": "chocko01", "message": "actually, I do no know if I want to write the results in a file", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:33:46.526097+00:00", "nick": "chocko01", "message": "I will eventually write it in a database, mongodb or a relational one", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:34:02.717597+00:00", "nick": "chocko01", "message": "do you know the best pratices about that ?", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:35:37.972860+00:00", "nick": "brati", "message": "chocko01: I usually write a pipeline for mongodb support. I guess there are already some open source pipelines for mongo out there", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:38:31.280245+00:00", "nick": "chocko01", "message": "brati: yeah I found them too.", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:39:19.141855+00:00", "nick": "brati", "message": "and then with mongo you could use a custom key for item uniqueness. E.g. if you say an item is unique if all fields are equal, you could create a sha-hash or similar from all fields", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:39:34.894250+00:00", "nick": "brati", "message": "The custom key can be assigned with _id=key I think", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:39:43.731631+00:00", "nick": "chocko01", "message": "but I would like to knwo the best practices between exporting in csv/json or directly in a database", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:40:24.397543+00:00", "nick": "brati", "message": "mmhh depends. files are bad if they grow large and you need something like online-duplicate finding", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:40:38.256948+00:00", "nick": "brati", "message": "If you only need a log of all findings (duplicate or not) for data analysis, files are ok", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:40:56.350891+00:00", "nick": "brati", "message": "(duplicate in the sense of \"oh, I saw this yesterday alreadt)", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:41:43.927953+00:00", "nick": "chocko01", "message": "yeah I need indexing", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:41:59.819091+00:00", "nick": "chocko01", "message": "actually the data is going to be in a db in the end", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:42:19.014054+00:00", "nick": "chocko01", "message": "I am going tu use pandas", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:45:11.430193+00:00", "nick": "chocko01", "message": "what if i use mongodb to store all the scraped item each day (each day = one collection) and I export the collection in pandas, build the models, reports, and finally store the linked models", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:45:14.639508+00:00", "nick": "chocko01", "message": "?", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:46:39.003694+00:00", "nick": "chocko01", "message": "I know it a bit out of the scope of scrapy project, but I do know where I could ask anyway", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:47:07.760667+00:00", "nick": "chocko01", "message": "I know it is a bit out of the scope of the scrapy project, but I do not know where I could ask anyway", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:47:47.742521+00:00", "nick": "highgainer", "message": "how would you implement an intial search request before scraping?", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:47:55.270876+00:00", "nick": "highgainer", "message": "FORM POST submit", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:48:49.403619+00:00", "nick": "brati", "message": "chocko01: sounds reasonable. I mean mongo is only a document storage and one could say your one-day-scrape is one document if your applications see this useful. There are not so many best practices for doc-stores I think, because the format of the db depends highly on application (not like RDBMS where we have normal forms)", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:49:08.188997+00:00", "nick": "chocko01", "message": "highgainer: I am sorry, I did not understand your question", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:49:20.930453+00:00", "nick": "brati", "message": "chocko01: I think it was a new question", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:49:52.708373+00:00", "nick": "highgainer", "message": "i have a page wher ei have to submit search before results(that i want to scrape= appear", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:49:59.655110+00:00", "nick": "highgainer", "message": "it uses asp session too", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:50:08.173651+00:00", "nick": "highgainer", "message": "so i need to keep cookies during requests", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:50:10.903560+00:00", "nick": "brati", "message": "highgainer: there is some example somewhere in the docs, don't know where atm.. I think they implement a method \"login()\" and just call it either in __init__ or somewhere", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:50:22.518100+00:00", "nick": "highgainer", "message": "well actually its not a login", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:50:36.305167+00:00", "nick": "highgainer", "message": "i have to crawl all search results from AA to ZZ", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:50:39.166948+00:00", "nick": "brati", "message": "highgainer: http://www.sitemaps.org/schemas/sitemap/0.9", "links": ["http://www.sitemaps.org/schemas/sitemap/0.9"], "channel": "scrapy"},
{"date": "2014-12-08T16:50:46.324420+00:00", "nick": "brati", "message": "No, but the pattern is similar", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:50:53.619435+00:00", "nick": "brati", "message": "Both are requests right before scraping", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:51:29.912898+00:00", "nick": "chocko01", "message": "brati: ok thanks   that's pretty much what I thought", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:51:38.290599+00:00", "nick": "brati", "message": "Cookies are managed by cookiemiddleware, I think. So that should be no problem", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:53:32.270258+00:00", "nick": "chocko01", "message": "brati: I am also looking for information concerning what is the best way to ierate every day with the scrapted data. I googled a bit and found something called ETL", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:54:42.302186+00:00", "nick": "chocko01", "message": "it seems there is some python frameworks that do that : bubbles, pygrametl, Python-ETL", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:55:26.839157+00:00", "nick": "highgainer", "message": "how would i loop over AA to ZZ as search string", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:55:31.634085+00:00", "nick": "highgainer", "message": "in main parse function?", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:55:39.028129+00:00", "nick": "chocko01", "message": "do you kown anything about them ?", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:56:02.418030+00:00", "nick": "brati", "message": "chocko01: never dealt with warehousing, but this ETL is interesting for me too, as I'm doing such stuff at company now :)", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:56:06.665596+00:00", "nick": "chocko01", "message": "highgainer: use the iter module", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:57:06.667490+00:00", "nick": "highgainer", "message": "python iter module?", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T16:57:27.669215+00:00", "nick": "highgainer", "message": "and should i just use spider as base class oder crawl?", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T17:01:49.503337+00:00", "nick": "chocko01", "message": "highgainer: import itertools ; import string ; [''.join(tok) for tok in itertools.product(string.uppercase, repeat=2)]", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T17:01:58.094053+00:00", "nick": "highgainer", "message": "thx will tr<y", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T17:03:55.348314+00:00", "nick": "highgainer", "message": "perfect", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T17:03:56.054606+00:00", "nick": "highgainer", "message": "thx", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T17:07:12.805617+00:00", "nick": "chocko01", "message": "brati: I know I could write the code myself to plug the scraped items to the analysis backend, but I am tired of doing it nad I would like to \"industrialize\" it abit like when I do it with scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T17:09:08.518503+00:00", "nick": "highgainer", "message": "so i will ise start request as an init?", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T17:09:31.659865+00:00", "nick": "highgainer", "message": "hmm", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T17:09:34.422109+00:00", "nick": "highgainer", "message": "i am confused ;)", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T17:09:56.019440+00:00", "nick": "highgainer", "message": "where shoudl i put the iration coee form aa to zz?  mainclass?  parse()?", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T17:21:53.078267+00:00", "nick": "PythonasaurusRex", "message": "Anybody home?", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T17:31:45.286796+00:00", "nick": "highgainer", "message": "yeah", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T17:42:02.942537+00:00", "nick": "highgainer", "message": "for search_string in [''.join(tok) for tok in itertools.product(string.uppercase, repeat=2)]:", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T17:42:03.236559+00:00", "nick": "highgainer", "message": "yield FormRequest.from_response(response, formname='aspnetForm', formdata={'':''}, callback=self.parse_pages)", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T17:42:09.175118+00:00", "nick": "highgainer", "message": "sry", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T17:59:26.128850+00:00", "nick": "highgainer", "message": "any idea why i am getting this errpor?", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T17:59:35.091415+00:00", "nick": "highgainer", "message": "exceptions.TypeError: argument of type 'NoneType' is not iterable", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T17:59:42.872092+00:00", "nick": "highgainer", "message": "File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/abstract.py\", line 522, in isIPv6Address", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T17:59:43.115911+00:00", "nick": "highgainer", "message": "    if '%' in addr:", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T18:49:53.527485+00:00", "nick": "highgainer", "message": "how i do see the post request scrapy did?", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T18:49:59.719127+00:00", "nick": "highgainer", "message": "i think i am missing something", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T22:30:03.078453+00:00", "nick": "nomadist", "message": "hi!", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T22:31:10.523408+00:00", "nick": "nomadist", "message": "I want to test a parse methods using the inbuilt contracts.. the problem is that it loads some information from the response received from another parse method, so I get a key error", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T22:32:21.922421+00:00", "nick": "nomadist", "message": "I was thinking of maybe overriding ScrapesContract to preprocess the request and load some dummy values in request.meta, not sure if that is good practice though", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T22:46:18.541424+00:00", "nick": "samei", "message": "hi every one", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T22:46:57.493500+00:00", "nick": "samei", "message": "how r u", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T23:02:09.854835+00:00", "nick": "nomadist", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2014-12-08T23:46:18.547314+00:00", "nick": "chocko01", "message": "hello", "links": [], "channel": "scrapy"},
{"date": "2014-12-09T02:18:53.441123+00:00", "nick": "nomadist", "message": "I've asked a question here: http://stackoverflow.com/questions/27368342/how...", "links": ["http://stackoverflow.com/questions/27368342/how-to-add-attributes-to-a-request-in-a-scrapy-contract"], "channel": "scrapy"},
{"date": "2014-12-09T12:43:35.272398+00:00", "nick": "wisemanSSZ", "message": "Good day?", "links": [], "channel": "scrapy"},
{"date": "2014-12-09T12:44:03.922121+00:00", "nick": "wisemanSSZ", "message": "Is it suitable to ask questions concerning use of Scrapy here?", "links": [], "channel": "scrapy"},
{"date": "2014-12-09T20:49:43.993629+00:00", "nick": "eren", "message": "hello", "links": [], "channel": "scrapy"},
{"date": "2014-12-09T20:50:06.826622+00:00", "nick": "eren", "message": "the website I'm scraping generates the pagination with javascript, so I cannot extract the link via XPath", "links": [], "channel": "scrapy"},
{"date": "2014-12-09T20:50:29.167950+00:00", "nick": "eren", "message": "however, it has tags for current page and maximum page number, I can make use of this data to visit the next page", "links": [], "channel": "scrapy"},
{"date": "2014-12-09T20:50:46.238359+00:00", "nick": "eren", "message": "is it possible to tell scrapy, on each request, what page it should next crawl?", "links": [], "channel": "scrapy"},
{"date": "2014-12-09T20:51:16.228443+00:00", "nick": "eren", "message": "I will just get the current page number, increment by 1, and instruct scrapy to visit \"URL?p=NUMBER\" until the last page is reached", "links": [], "channel": "scrapy"},
{"date": "2014-12-09T20:57:52.020396+00:00", "nick": "brati", "message": "eren: yes, just return a new Request() object from your parse method", "links": [], "channel": "scrapy"},
{"date": "2014-12-09T21:00:25.981153+00:00", "nick": "eren", "message": "brati: my parse method yields Item currently", "links": [], "channel": "scrapy"},
{"date": "2014-12-09T21:00:40.907921+00:00", "nick": "brati", "message": "eren: no problem, you can mix both :)", "links": [], "channel": "scrapy"},
{"date": "2014-12-09T21:00:45.091002+00:00", "nick": "eren", "message": "oh", "links": [], "channel": "scrapy"},
{"date": "2014-12-09T21:00:52.913745+00:00", "nick": "eren", "message": "great, I'm trying :0", "links": [], "channel": "scrapy"},
{"date": "2014-12-09T21:00:54.296076+00:00", "nick": "eren", "message": ":)", "links": [], "channel": "scrapy"},
{"date": "2014-12-09T21:02:11.191460+00:00", "nick": "eren", "message": "huh, Python < 3.3 do not allow return inside a generator", "links": [], "channel": "scrapy"},
{"date": "2014-12-09T21:03:30.189939+00:00", "nick": "eren", "message": "does that mean that I should update to Python 3.4 for this feature?", "links": [], "channel": "scrapy"},
{"date": "2014-12-09T21:05:36.338220+00:00", "nick": "brati", "message": "eren: this will be allowed in future/is already?", "links": [], "channel": "scrapy"},
{"date": "2014-12-09T21:05:46.551317+00:00", "nick": "brati", "message": "Nah, you should not return Request, buy also yield it", "links": [], "channel": "scrapy"},
{"date": "2014-12-09T21:06:28.937519+00:00", "nick": "eren", "message": "brati: I'm using python 2.7, I guess return statement is not possible if 'yield' is used before", "links": [], "channel": "scrapy"},
{"date": "2014-12-09T21:08:28.575073+00:00", "nick": "brati", "message": "eren: yes and it would not be the thing you want in python 3.3 either. Just read it, it's just some minor changement from return resulting in StopIteration() which will now be StopIteration(return-Value).", "links": [], "channel": "scrapy"},
{"date": "2014-12-09T21:08:35.962129+00:00", "nick": "brati", "message": "You want to use yield Request(...)", "links": [], "channel": "scrapy"},
{"date": "2014-12-09T21:09:14.277595+00:00", "nick": "eren", "message": "oh right, it seems to work", "links": [], "channel": "scrapy"},
{"date": "2014-12-09T21:09:23.508564+00:00", "nick": "eren", "message": "it's strange how this thing works, though", "links": [], "channel": "scrapy"},
{"date": "2014-12-09T21:09:34.512436+00:00", "nick": "eren", "message": "i just used another yield for Request object", "links": [], "channel": "scrapy"},
{"date": "2014-12-09T21:09:42.367739+00:00", "nick": "eren", "message": "before, I have for loop which gets items from the page", "links": [], "channel": "scrapy"},
{"date": "2014-12-09T21:10:22.788006+00:00", "nick": "brati", "message": "Consider it like a sorting machine at a pipeline (not scrapy pipelines, but factory pipeline ;). Items come along the pipeline and then there is a man standing and sorting the items. \"Item\" goes left, \"Request\" goes right", "links": [], "channel": "scrapy"},
{"date": "2014-12-09T21:10:49.997053+00:00", "nick": "brati", "message": "So each element is sorted to the right direction for further handling (request to request new websites, items to the item pipeline)", "links": [], "channel": "scrapy"},
{"date": "2014-12-09T21:12:08.253108+00:00", "nick": "eren", "message": "great", "links": [], "channel": "scrapy"},
{"date": "2014-12-09T21:12:31.498289+00:00", "nick": "eren", "message": "so each time parse is called, my for loop returns item, which is recognized and processed", "links": [], "channel": "scrapy"},
{"date": "2014-12-09T21:12:40.319678+00:00", "nick": "eren", "message": "at the end of the parse, we have a Request instance", "links": [], "channel": "scrapy"},
{"date": "2014-12-09T21:13:00.502180+00:00", "nick": "brati", "message": "You can also yield the request instance earlier", "links": [], "channel": "scrapy"},
{"date": "2014-12-09T21:13:00.696538+00:00", "nick": "eren", "message": "it is processed differently and the next crawl occurs", "links": [], "channel": "scrapy"},
{"date": "2014-12-09T21:13:11.126694+00:00", "nick": "brati", "message": "apart from that, yes.", "links": [], "channel": "scrapy"},
{"date": "2014-12-09T21:13:31.279010+00:00", "nick": "eren", "message": "I'm now reading testspider code to make the crawler accept console parameters", "links": [], "channel": "scrapy"},
{"date": "2014-12-09T21:13:45.859289+00:00", "nick": "eren", "message": "I'm only interested in crawling a specific page, which will be got from a user", "links": [], "channel": "scrapy"},
{"date": "2014-12-09T21:19:08.784506+00:00", "nick": "eren", "message": "why does .extract() always return a list? I am targeting specific XPath and I get the data", "links": [], "channel": "scrapy"},
{"date": "2014-12-09T21:19:19.592261+00:00", "nick": "eren", "message": "I need to use [0] to get it in clean form", "links": [], "channel": "scrapy"},
{"date": "2014-12-09T21:19:23.688809+00:00", "nick": "eren", "message": "am I doing it wrong?", "links": [], "channel": "scrapy"},
{"date": "2014-12-09T21:28:56.725812+00:00", "nick": "brati", "message": "eren: no, that's right (I'll be away in a few seconds again, just went back to write a mail). It always returns a list, because the XPath always could have multiple results", "links": [], "channel": "scrapy"},
{"date": "2014-12-09T21:30:29.583204+00:00", "nick": "brati", "message": "btw it's also interchangeable if you use [0]->extract() or ->extract()[0]. The first just extracts all and then gets the first element. The second gets the first element and then extracts the result.", "links": [], "channel": "scrapy"},
{"date": "2014-12-09T22:17:51.306290+00:00", "nick": "eren", "message": "brati: thank you!", "links": [], "channel": "scrapy"},
{"date": "2014-12-09T22:18:05.580634+00:00", "nick": "eren", "message": "I've finished the base scraper, it runs as expected", "links": [], "channel": "scrapy"},
{"date": "2014-12-09T22:18:25.947761+00:00", "nick": "eren", "message": "now I need to set up SQLAlchemy to write the items in PostgreSQL, or whatever db is configured", "links": [], "channel": "scrapy"},
{"date": "2014-12-09T22:19:00.796705+00:00", "nick": "eren", "message": "I will also need to implement uplicate filter using the database", "links": [], "channel": "scrapy"},
{"date": "2014-12-09T22:19:34.313633+00:00", "nick": "eren", "message": "SQLAlchemy sounds like an overengineering solution but I don't want to use plain sql queries with only sqlite :)", "links": [], "channel": "scrapy"},
{"date": "2014-12-10T12:20:34.767299+00:00", "nick": "Solopher", "message": "Hi", "links": [], "channel": "scrapy"},
{"date": "2014-12-10T12:21:18.424488+00:00", "nick": "rodrigo5244", "message": "Hi", "links": [], "channel": "scrapy"},
{"date": "2014-12-10T16:18:39.875545+00:00", "nick": "anapitupulu", "message": "Hi!", "links": [], "channel": "scrapy"},
{"date": "2014-12-10T16:18:58.584993+00:00", "nick": "anapitupulu", "message": "I have an issue launching scrapyd", "links": [], "channel": "scrapy"},
{"date": "2014-12-10T16:19:12.769171+00:00", "nick": "anapitupulu", "message": "I installed in ubuntu using apt-get", "links": [], "channel": "scrapy"},
{"date": "2014-12-10T16:19:36.985450+00:00", "nick": "anapitupulu", "message": "However, when I launched \"scrapyd\" (without doing any change to the configuration)", "links": [], "channel": "scrapy"},
{"date": "2014-12-10T16:19:49.479183+00:00", "nick": "anapitupulu", "message": "I got: sqlite3.OperationalError: unable to open database file", "links": [], "channel": "scrapy"},
{"date": "2014-12-10T16:20:33.743266+00:00", "nick": "anapitupulu", "message": "I googled it but the other cases were about deploying the egg (not booting up the scrapyd)", "links": [], "channel": "scrapy"},
{"date": "2014-12-10T16:20:46.367688+00:00", "nick": "anapitupulu", "message": "(other cases with the same error message)", "links": [], "channel": "scrapy"},
{"date": "2014-12-10T16:21:11.127187+00:00", "nick": "anapitupulu", "message": "has anyone seen this before? thanks", "links": [], "channel": "scrapy"},
{"date": "2014-12-10T17:23:45.682016+00:00", "nick": "lobius", "message": "Greetings. The \"final code\" of the tutorial fails. Any ideas? The big change is importing and library called the \"DmozItem\", which the code does not like. As I am a beginner to Scrapy, I want to know: what factors would lead to this? Was there a failure in the project start?", "links": [], "channel": "scrapy"},
{"date": "2014-12-10T17:36:55.242176+00:00", "nick": "lobius", "message": "https://bpaste.net/show/23c7c2691424", "links": ["https://bpaste.net/show/23c7c2691424"], "channel": "scrapy"},
{"date": "2014-12-10T17:47:11.709402+00:00", "nick": "hunter", "message": "Hi! I'm a first year undergraduate student from IIT Kharagpur, India. I want to participate in Google Summer of code with scrapy. I find it interesting and I'm willing to learn it and then start contributing. I'm looking for some guidance.", "links": [], "channel": "scrapy"},
{"date": "2014-12-10T18:11:10.994282+00:00", "nick": "samtc", "message": "hunter: you could have a look at https://github.com/scrapy/scrapy/issues", "links": ["https://github.com/scrapy/scrapy/issues"], "channel": "scrapy"},
{"date": "2014-12-10T18:12:43.407018+00:00", "nick": "hunter", "message": "samtc: I'll first have to learn scrapy. Issues have terms of scrapy.", "links": [], "channel": "scrapy"},
{"date": "2014-12-10T21:03:39.972658+00:00", "nick": "brati", "message": "eren: where you the one asking for extract() returning a list yesterday?", "links": [], "channel": "scrapy"},
{"date": "2014-12-10T21:05:33.349997+00:00", "nick": "brati", "message": "just found a discussion on the scrapy issues where there is an interesting basic python solution: (sel.xpath(\"//*[@name='Exactly Once']\").extract() or [None])[0] (from https://github.com/scrapy/scrapy/issues/568#iss... )", "links": ["https://github.com/scrapy/scrapy/issues/568#issuecomment-36232338"], "channel": "scrapy"},
{"date": "2014-12-10T21:06:37.723527+00:00", "nick": "brati", "message": "but actually as they introduced ItemLoaders (which this dicussion was mainly about), you would rather use item loaders with the appropriate classes. Don't know the names, but there is also one for TakeFirst", "links": [], "channel": "scrapy"},
{"date": "2014-12-10T21:07:44.729262+00:00", "nick": "brati", "message": "but in case you're interested, you can read through the whole discussion thread. It's actually exactly that question: \"why extract()[0] all the time\"", "links": [], "channel": "scrapy"},
{"date": "2014-12-11T02:38:11.726782+00:00", "nick": "nomadist", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2014-12-11T02:39:22.500190+00:00", "nick": "nomadist", "message": "On a website I am noting a pattern that its OK for about 200 crawls and after that every request makes a 111 connection error", "links": [], "channel": "scrapy"},
{"date": "2014-12-11T02:40:13.392758+00:00", "nick": "nomadist", "message": "So i used the malicious trick of rotating user agents and proxies (from the free list at hide-my-ass) and its just ridculously slow now", "links": [], "channel": "scrapy"},
{"date": "2014-12-11T02:40:22.446793+00:00", "nick": "nomadist", "message": "like 0-1 crawls/min", "links": [], "channel": "scrapy"},
{"date": "2014-12-11T02:40:55.098940+00:00", "nick": "nomadist", "message": "I've used autothrottline and set a download_delay of 0.25 seconds.. but it doesn't help", "links": [], "channel": "scrapy"},
{"date": "2014-12-11T02:42:03.717453+00:00", "nick": "nomadist", "message": "any ideas?", "links": [], "channel": "scrapy"},
{"date": "2014-12-11T02:59:15.562192+00:00", "nick": "dpn`", "message": "nomadist, easiest thing would be to use crawlera", "links": [], "channel": "scrapy"},
{"date": "2014-12-11T03:05:28.856407+00:00", "nick": "nomadist", "message": "dpn`, thanks. Ill look into that.", "links": [], "channel": "scrapy"},
{"date": "2014-12-11T10:20:26.062782+00:00", "nick": "NNN", "message": "Anybody here?", "links": [], "channel": "scrapy"},
{"date": "2014-12-11T10:21:15.544261+00:00", "nick": "NNN", "message": "need help... http://stackoverflow.com/questions/27420005/con...", "links": ["http://stackoverflow.com/questions/27420005/connect-to-mq-inside-a-scrapy-spider-or-how-to-create-clientcreator-inside-scrap"], "channel": "scrapy"},
{"date": "2014-12-11T10:38:13.275870+00:00", "nick": "NNN", "message": "http://stackoverflow.com/questions/27420005/con...", "links": ["http://stackoverflow.com/questions/27420005/connect-to-mq-inside-a-scrapy-spider-or-how-to-create-clientcreator-inside-scrap"], "channel": "scrapy"},
{"date": "2014-12-11T12:47:06.407371+00:00", "nick": "eren", "message": "brati: thanks for your interest. I wasn't the one asking on the list. I will take a look at the thread to understand why .extract()[0] all the time", "links": [], "channel": "scrapy"},
{"date": "2014-12-11T13:04:21.421872+00:00", "nick": "bosnjak", "message": "join #zwave", "links": [], "channel": "scrapy"},
{"date": "2014-12-11T14:22:04.895793+00:00", "nick": "NNN", "message": "http://stackoverflow.com/questions/27420005/con...", "links": ["http://stackoverflow.com/questions/27420005/connect-to-mq-inside-a-scrapy-spider-or-how-to-create-clientcreator-inside-scrap"], "channel": "scrapy"},
{"date": "2014-12-11T14:38:22.971730+00:00", "nick": "NNN", "message": "http://stackoverflow.com/questions/27420005/con...", "links": ["http://stackoverflow.com/questions/27420005/connect-to-mq-inside-a-scrapy-spider-or-how-to-create-clientcreator-inside-scrap"], "channel": "scrapy"},
{"date": "2014-12-12T07:52:48.553054+00:00", "nick": "iamgiri", "message": "Hi!", "links": [], "channel": "scrapy"},
{"date": "2014-12-12T07:54:20.665835+00:00", "nick": "iamgiri", "message": "I have 1 million urls in my database. I want t scrape data and update the rows. Can you guys give me some resources where i can learn about update rows using scrapy?", "links": [], "channel": "scrapy"},
{"date": "2014-12-13T10:50:29.070030+00:00", "nick": "tayo", "message": "I am a newbie in scrappy", "links": [], "channel": "scrapy"},
{"date": "2014-12-13T10:50:58.588352+00:00", "nick": "unkn0wn_ng", "message": "Hello Room I am a newbie in scrapy and python as a whole", "links": [], "channel": "scrapy"},
{"date": "2014-12-13T10:51:01.976155+00:00", "nick": "unkn0wn_ng", "message": "I need a mentor", "links": [], "channel": "scrapy"},
{"date": "2014-12-13T10:51:04.931570+00:00", "nick": "unkn0wn_ng", "message": "That can guide me", "links": [], "channel": "scrapy"},
{"date": "2014-12-13T17:09:37.749453+00:00", "nick": "lobius", "message": "Anybody home?", "links": [], "channel": "scrapy"},
{"date": "2014-12-13T20:23:42.057738+00:00", "nick": "flyingtriangle", "message": "anyone know how to make scrapy's linkextractor include form action URLs?", "links": [], "channel": "scrapy"},
{"date": "2014-12-13T20:23:56.853512+00:00", "nick": "flyingtriangle", "message": "I want it to crawl and scrape as many urls as it can", "links": [], "channel": "scrapy"},
{"date": "2014-12-13T20:24:06.825812+00:00", "nick": "flyingtriangle", "message": "and it doesn't seem to crawl form action links by default", "links": [], "channel": "scrapy"},
{"date": "2014-12-13T21:40:37.979389+00:00", "nick": "nomadist", "message": "hi!", "links": [], "channel": "scrapy"},
{"date": "2014-12-14T02:36:25.787322+00:00", "nick": "baqer", "message": "Hello everyone", "links": [], "channel": "scrapy"},
{"date": "2014-12-14T02:36:50.889436+00:00", "nick": "baqer", "message": "Does someone knows how I can run one crawler simultaneously to gain speed?", "links": [], "channel": "scrapy"},
{"date": "2014-12-14T02:37:20.333340+00:00", "nick": "baqer", "message": "I have a pretty big website that I want to crawl frequently and it took a week until one crawl step finishes", "links": [], "channel": "scrapy"},
{"date": "2014-12-14T02:37:37.411473+00:00", "nick": "baqer", "message": "I know about scrapyd but it seems it is only useful to run the crawler at the background", "links": [], "channel": "scrapy"},
{"date": "2014-12-14T02:45:46.122596+00:00", "nick": "toothrot", "message": "'one crawler simultaneously' ?", "links": [], "channel": "scrapy"},
{"date": "2014-12-14T12:15:21.939170+00:00", "nick": "nomadist", "message": "hi!", "links": [], "channel": "scrapy"},
{"date": "2014-12-14T17:09:13.084022+00:00", "nick": "nomadist", "message": "hi!", "links": [], "channel": "scrapy"},
{"date": "2014-12-14T18:14:22.153516+00:00", "nick": "nomadist", "message": "where can I find the score (0-1000) of the default middlewares?", "links": [], "channel": "scrapy"},
{"date": "2014-12-14T18:16:12.070202+00:00", "nick": "brati", "message": "nomadist: currently with 56k, but I guess on https://github.com/scrapy/scrapy -> subfolder scrapy -> settings.py if there is one", "links": ["https://github.com/scrapy/scrapy"], "channel": "scrapy"},
{"date": "2014-12-14T18:22:17.908004+00:00", "nick": "brati", "message": "nomadist: scrapy/settings/default_settings.py there it is", "links": [], "channel": "scrapy"},
{"date": "2014-12-14T18:44:25.953350+00:00", "nick": "Llamageddon", "message": "Hey!", "links": [], "channel": "scrapy"},
{"date": "2014-12-14T18:44:58.861853+00:00", "nick": "Llamageddon", "message": "Is it in any way possible to navigate JS-powered sites with scrapy?", "links": [], "channel": "scrapy"},
{"date": "2014-12-14T18:45:12.763732+00:00", "nick": "Llamageddon", "message": "Say, a list that uses JS to load pages other than the first", "links": [], "channel": "scrapy"},
{"date": "2014-12-14T19:19:37.853437+00:00", "nick": "Llamageddon", "message": "Ok, nevermind that problem, I circumvented it", "links": [], "channel": "scrapy"},
{"date": "2014-12-14T19:19:43.031929+00:00", "nick": "Llamageddon", "message": "Now I need a way for scrapy to login into steam", "links": [], "channel": "scrapy"},
{"date": "2014-12-14T19:19:56.506856+00:00", "nick": "Llamageddon", "message": "And I have zero idea about how logins and sessions and cookies work :(", "links": [], "channel": "scrapy"},
{"date": "2014-12-14T19:31:13.501661+00:00", "nick": "brati", "message": "scrapy already handles cookies. I don't knowabout steam, but on the scrapy docs page for spiders, there is an example to login a spider before scraping", "links": [], "channel": "scrapy"},
{"date": "2014-12-14T19:31:57.015017+00:00", "nick": "Llamageddon", "message": "brati: I know that Steam needs you to get a confirmation code via email and type it out. I don't know if it would be possible to scrape some cookies/whatever from firefox or opera and use that or not.", "links": [], "channel": "scrapy"},
{"date": "2014-12-14T19:49:14.185591+00:00", "nick": "lobius", "message": "Is anybody here?", "links": [], "channel": "scrapy"},
{"date": "2014-12-14T19:50:48.180301+00:00", "nick": "Llamageddon", "message": "lobius: I'm here but I'm incapable of helping, sorry", "links": [], "channel": "scrapy"},
{"date": "2014-12-14T20:13:03.296460+00:00", "nick": "lobius", "message": "Thanks for answering. Nobody's home but I'm a decent hacker. I will figure it out.", "links": [], "channel": "scrapy"},
{"date": "2014-12-14T20:44:47.003074+00:00", "nick": "nomadist", "message": "lobius, the easiest but inefficient solution is to use a selenium, preferrably with a headless webdriver with phanomtjs", "links": [], "channel": "scrapy"},
{"date": "2014-12-14T20:45:03.880271+00:00", "nick": "nomadist", "message": "other options are splash or scrapy-js", "links": [], "channel": "scrapy"},
{"date": "2014-12-14T20:45:22.817220+00:00", "nick": "nomadist", "message": "but I find it best if you are able to simulate any ajax request and handle the data directly", "links": [], "channel": "scrapy"},
{"date": "2014-12-14T20:45:47.100501+00:00", "nick": "nomadist", "message": "in cases where there's js voodoo happening, I simply prefer selenium", "links": [], "channel": "scrapy"},
{"date": "2014-12-14T21:00:25.943735+00:00", "nick": "lobius", "message": "Thank you, Sir. I suspect you answered an earlier visit. You are correct. I am bypassing Ajax entirely.", "links": [], "channel": "scrapy"},
{"date": "2014-12-14T21:02:29.761516+00:00", "nick": "lobius", "message": "The flow of my pages will be login, target form and then start scraping. Do I need 2 separate spiders or can I do it with one?", "links": [], "channel": "scrapy"},
{"date": "2014-12-14T21:06:08.346393+00:00", "nick": "lobius", "message": "nomadist: forgot to mention your name in my response.", "links": [], "channel": "scrapy"},
{"date": "2014-12-14T21:44:57.857164+00:00", "nick": "nomadist", "message": "lobius, no. one can be enough.", "links": [], "channel": "scrapy"},
{"date": "2014-12-14T21:45:46.178730+00:00", "nick": "nomadist", "message": "you yield a FormRequest in your parse method", "links": [], "channel": "scrapy"},
{"date": "2014-12-14T21:57:06.254179+00:00", "nick": "toothrot", "message": "yes, logging in is usually easy with a FormRequest, scrapy does a good job with hidden fields and cookies", "links": [], "channel": "scrapy"},
{"date": "2014-12-15T14:35:21.397351+00:00", "nick": "Mihai86", "message": "hi !", "links": [], "channel": "scrapy"},
{"date": "2014-12-15T14:35:30.564448+00:00", "nick": "Mihai86", "message": "Anybody here ?", "links": [], "channel": "scrapy"},
{"date": "2014-12-15T14:36:11.070364+00:00", "nick": "Mihai86", "message": "Anyone use portia ? (https://github.com/scrapinghub/portia/)", "links": ["https://github.com/scrapinghub/portia/"], "channel": "scrapy"},
{"date": "2014-12-15T14:37:55.106107+00:00", "nick": "Mihai86", "message": "I'm having some issues with it and I can't figure out the cause. It's probably my fault, being my first attempt but i need to talk to someone with a bit of experience.", "links": [], "channel": "scrapy"},
{"date": "2014-12-15T14:40:45.584039+00:00", "nick": "Mihai86", "message": "hello ?", "links": [], "channel": "scrapy"},
{"date": "2014-12-16T00:18:11.380834+00:00", "nick": "cocoaflyer", "message": "Interested in (and struggling to write) some input processors but first I have a question about item loaders and adding xpaths... If i was selecting data without a loader, I would take selector with an xpath, extract the value(s) then grab the first item (i.e. response.xpath('//div).extract()[0]) but when I'm adding an xpath to an item loader, how am I able to grab only the first value?... when setting TakeFirst() as the processor for a", "links": [], "channel": "scrapy"},
{"date": "2014-12-16T00:18:11.491928+00:00", "nick": "cocoaflyer", "message": "field, it still results in a list", "links": [], "channel": "scrapy"},
{"date": "2014-12-16T10:54:46.573725+00:00", "nick": "r00s", "message": "is there a common way of exporting *typed* json? i need to distinguish between multiple item classes that have the same fields", "links": [], "channel": "scrapy"},
{"date": "2014-12-16T10:55:47.185411+00:00", "nick": "r00s", "message": "if possible, i'd like to not use the other data interchange format that already has this feature built in...", "links": [], "channel": "scrapy"},
{"date": "2014-12-16T10:56:16.140095+00:00", "nick": "r00s", "message": "setting a default value for items of a class would also be okay", "links": [], "channel": "scrapy"},
{"date": "2014-12-16T10:56:29.831368+00:00", "nick": "r00s", "message": "*for fields of class instances", "links": [], "channel": "scrapy"},
{"date": "2014-12-16T10:57:26.481767+00:00", "nick": "r00s", "message": "hmkay, apparently i can do scrapy.Field(default=\"my_type\")", "links": [], "channel": "scrapy"},
{"date": "2014-12-16T21:51:00.997344+00:00", "nick": "kertska", "message": "Hi guys! Is there an easy way to extract offsite domains from my spider? Because I noticed that scrapy keeps log of them.", "links": [], "channel": "scrapy"},
{"date": "2014-12-16T21:51:43.828475+00:00", "nick": "nramirezuy", "message": "extract offsite domains? what do you mean?", "links": [], "channel": "scrapy"},
{"date": "2014-12-16T21:53:15.529539+00:00", "nick": "kertska", "message": "i mean, i just created a spider, which crawls within certain domain, and I want to extract all the links which point to another sites", "links": [], "channel": "scrapy"},
{"date": "2014-12-16T21:54:11.274827+00:00", "nick": "nramirezuy", "message": "you have to store them", "links": [], "channel": "scrapy"},
{"date": "2014-12-16T21:54:15.138078+00:00", "nick": "nramirezuy", "message": "on an item", "links": [], "channel": "scrapy"},
{"date": "2014-12-16T21:54:25.437509+00:00", "nick": "nramirezuy", "message": "what you can do is use linkextractors", "links": [], "channel": "scrapy"},
{"date": "2014-12-16T21:54:32.870252+00:00", "nick": "nramirezuy", "message": "with deny_domain", "links": [], "channel": "scrapy"},
{"date": "2014-12-16T22:01:58.120552+00:00", "nick": "kertska", "message": "Ok, i will look into that, thanks", "links": [], "channel": "scrapy"},
{"date": "2014-12-17T04:50:06.754522+00:00", "nick": "mushroomed", "message": "Hi all", "links": [], "channel": "scrapy"},
{"date": "2014-12-17T04:51:36.453488+00:00", "nick": "mushroomed", "message": "I want my Spider to start from 'X' where it's content has A,B,C,D,E,F,G,H links that I want to follow and finally only scrape data in those A,B,C,D,E,F,G,H pages", "links": [], "channel": "scrapy"},
{"date": "2014-12-17T09:54:48.204687+00:00", "nick": "nomadist", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2014-12-17T13:05:53.202150+00:00", "nick": "allen", "message": "a", "links": [], "channel": "scrapy"},
{"date": "2014-12-17T16:54:49.276605+00:00", "nick": "scmp", "message": "Hi, any ideas how i could get DeltaFetch to mark a page as \"visited\" without producing an item?", "links": [], "channel": "scrapy"},
{"date": "2014-12-17T16:54:52.533419+00:00", "nick": "scmp", "message": "https://github.com/scrapinghub/scrapylib/blob/m...", "links": ["https://github.com/scrapinghub/scrapylib/blob/master/scrapylib/deltafetch.py#L68"], "channel": "scrapy"},
{"date": "2014-12-17T16:55:35.399124+00:00", "nick": "scmp", "message": "I got the case where i don't create items on some pages and would like to avoid to visit them again at the next run as well.", "links": [], "channel": "scrapy"},
{"date": "2014-12-17T16:56:02.400765+00:00", "nick": "nramirezuy", "message": "extend and use some other class", "links": [], "channel": "scrapy"},
{"date": "2014-12-17T16:56:18.762947+00:00", "nick": "nramirezuy", "message": "but don't yield it", "links": [], "channel": "scrapy"},
{"date": "2014-12-17T17:20:08.133561+00:00", "nick": "anomaly_the", "message": "clear", "links": [], "channel": "scrapy"},
{"date": "2014-12-17T17:30:59.033326+00:00", "nick": "scmp", "message": "thx", "links": [], "channel": "scrapy"},
{"date": "2014-12-17T20:58:41.380977+00:00", "nick": "lobius1523", "message": "I'm looking at http://doc.scrapy.org/en/latest/topics/request-...", "links": ["http://doc.scrapy.org/en/latest/topics/request-response.html#topics-request-response-ref-request-userlogin"], "channel": "scrapy"},
{"date": "2014-12-17T20:59:30.481895+00:00", "nick": "lobius1523", "message": "It looks like I will need to add this as a standalone spider. I heard that did not have to do that but I don't understand where I go after using the parse method.", "links": [], "channel": "scrapy"},
{"date": "2014-12-17T21:00:05.029107+00:00", "nick": "lobius1523", "message": "If there is there anyone who can help me with that logic, I would appreciate it.", "links": [], "channel": "scrapy"},
{"date": "2014-12-17T21:39:43.002161+00:00", "nick": "Jiar", "message": "hello", "links": [], "channel": "scrapy"},
{"date": "2014-12-17T21:39:54.298089+00:00", "nick": "Jiar", "message": "i have a question", "links": [], "channel": "scrapy"},
{"date": "2014-12-17T21:40:22.834092+00:00", "nick": "Jiar", "message": "is it possible to make the spider start at link 2000", "links": [], "channel": "scrapy"},
{"date": "2014-12-17T21:40:37.714641+00:00", "nick": "Jiar", "message": "like skip the first 2000 links in the start_urls[] ?", "links": [], "channel": "scrapy"},
{"date": "2014-12-17T21:49:54.627165+00:00", "nick": "Jiar", "message": "guys?", "links": [], "channel": "scrapy"},
{"date": "2014-12-17T22:00:13.777082+00:00", "nick": "Jiar", "message": "pls respond", "links": [], "channel": "scrapy"},
{"date": "2014-12-17T22:55:21.520545+00:00", "nick": "Jiar", "message": "pls respond ;_;", "links": [], "channel": "scrapy"},
{"date": "2014-12-18T17:10:34.332713+00:00", "nick": "DanielW", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2014-12-18T17:24:17.711215+00:00", "nick": "nramirezuy", "message": "what you can do is use a different slot for retried requests", "links": [], "channel": "scrapy"},
{"date": "2014-12-18T17:41:09.398884+00:00", "nick": "nramirezuy", "message": "downloader slot", "links": [], "channel": "scrapy"},
{"date": "2014-12-18T18:11:57.336145+00:00", "nick": "DanielW", "message": "the thing is, after like 100 or 200 requests  (it differs) i am getting 403s (speedbump block in http header) even with a download delay of 5.  so i want the spider to stop for 60 seconds (have do play with that) after a 403 and then try again", "links": [], "channel": "scrapy"},
{"date": "2014-12-18T18:17:25.619813+00:00", "nick": "nramirezuy", "message": "but I would try using that mdw first", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T01:35:48.353241+00:00", "nick": "SirSkitzo", "message": "If anyone has some spare time, would you mind helping me with the issue described in this reddit post? http://www.reddit.com/r/Python/comments/2pqrwb/...", "links": ["http://www.reddit.com/r/Python/comments/2pqrwb/scrapy_yp_spider_stops_scraping_after_100/"], "channel": "scrapy"},
{"date": "2014-12-19T01:40:20.969720+00:00", "nick": "SirSkitzo", "message": "Oh, and Hi!", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T11:18:10.996257+00:00", "nick": "ducsuus", "message": "Hi!", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T11:29:27.051526+00:00", "nick": "ducsuus", "message": "When you specify a callback for a http request, is there a way to pass in custom arguments to that callback?", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T11:46:04.774492+00:00", "nick": "tomwardill", "message": "ducsuus: http://doc.scrapy.org/en/latest/topics/request-... will do what you want", "links": ["http://doc.scrapy.org/en/latest/topics/request-response.html#scrapy.http.Request.meta"], "channel": "scrapy"},
{"date": "2014-12-19T11:46:21.854454+00:00", "nick": "tomwardill", "message": "populate the 'meta' property of the Request, it'll be available as 'response.meta' in your callback", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T11:46:34.490994+00:00", "nick": "ducsuus", "message": "Ooh", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T11:46:37.374961+00:00", "nick": "ducsuus", "message": "Thankyou!", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T11:48:00.359244+00:00", "nick": "ducsuus", "message": "tomwardill, is there any official way I am meant to change this (using the repace() function?) or should I just set it directly? Are there any \"protocols\" I should use when storing data in this?>", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T11:49:03.591130+00:00", "nick": "tomwardill", "message": "you're best to set it when you create the Request object, in it's initialisation: yield Request(url, callback=self.a_callback, meta={'foo': 'bar'}) or similar", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T11:49:45.233208+00:00", "nick": "tomwardill", "message": "it's passed around with every Request, so try not to make it too big, and it's generally best to use just strings or similar, not objects in the dict", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T11:52:37.726632+00:00", "nick": "ducsuus", "message": "All I need to do is submit a dictonary containing two strings: {\"data1\":\"data1\", \"data2\":\"data2\"}", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T11:52:48.946935+00:00", "nick": "tomwardill", "message": "should be fine then :)", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T11:53:42.150236+00:00", "nick": "ducsuus", "message": ":D", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T11:54:03.332518+00:00", "nick": "ducsuus", "message": "Just to be clear: This meta data is not sumbited in the POST requestion at all?", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T11:54:18.989119+00:00", "nick": "tomwardill", "message": "nope, it's just held by scrapy locally", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T11:55:02.947472+00:00", "nick": "ducsuus", "message": "OK, sounds good :)", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T12:05:09.735759+00:00", "nick": "ducsuus", "message": "Hmm", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T12:05:18.203777+00:00", "nick": "ducsuus", "message": "I am having a problem installing Scrapy", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T12:05:49.377631+00:00", "nick": "ducsuus", "message": "I am greeted with the error \"Command /usr/bin/python3 -c \"import setuptools, tokenize;__file__='/tmp/pip_build_root/lxml/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\\r\\n', '\\n'), __file__, 'exec'))\" install --record /tmp/pip-82fjnpcg-record/install-record.txt --single-version-externally-managed --compile failed with error code 1 in /tmp/pip_build_root/lxml", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T12:05:49.489355+00:00", "nick": "ducsuus", "message": "Storing debug log for failure in /root/.pip/pip.log", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T12:05:49.514804+00:00", "nick": "ducsuus", "message": "\"", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T12:06:34.139526+00:00", "nick": "ducsuus", "message": "And a similar error when using pip install (instead of pip3 install)", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T12:11:20.890598+00:00", "nick": "ducsuus", "message": "HMm", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T12:11:32.243215+00:00", "nick": "ducsuus", "message": "Is Scrapy ONLY Python 2.7?", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T12:11:42.250736+00:00", "nick": "ducsuus", "message": "It would be nice if it was Python 3 compatible...", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T12:11:49.744314+00:00", "nick": "tomwardill", "message": "python 2.7 only at the minute", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T12:11:55.367839+00:00", "nick": "ducsuus", "message": "Crap", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T12:11:58.262845+00:00", "nick": "ducsuus", "message": "Hmm", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T12:11:59.253412+00:00", "nick": "ducsuus", "message": "Well", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T12:12:02.062034+00:00", "nick": "ducsuus", "message": "That's bad :P", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T12:12:28.264001+00:00", "nick": "ducsuus", "message": "Is there a command line way to check if Scrapy is installed?", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T12:17:53.691847+00:00", "nick": "ducsuus", "message": "Scrapy doesn't appear to be being installed :S", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T12:17:57.675465+00:00", "nick": "ducsuus", "message": "Even on Python 2", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T13:25:12.560576+00:00", "nick": "ducsuus", "message": "Would anyone know where I can find a *simple* example on how to download a page with Scrapy (preferably with a POST request)?", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T13:25:22.404003+00:00", "nick": "ducsuus", "message": "I am finding it really hard to find a non-overcomplicated example", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T14:21:54.412001+00:00", "nick": "danielw_", "message": "ducsuus: just yield or return the request in the start_requests method of your spider (i don't know of any example)", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T14:22:20.417135+00:00", "nick": "ducsuus", "message": "danielw_, I don't know how to do that :p", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T14:25:38.435462+00:00", "nick": "danielw_", "message": "ducsuus: http://doc.scrapy.org/en/0.22/intro/tutorial.html have you read that?", "links": ["http://doc.scrapy.org/en/0.22/intro/tutorial.html"], "channel": "scrapy"},
{"date": "2014-12-19T14:26:12.007067+00:00", "nick": "ducsuus", "message": "Yes", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T14:26:17.770577+00:00", "nick": "ducsuus", "message": "Not helpful at all really :S", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T14:26:26.708928+00:00", "nick": "ducsuus", "message": "In fact it confused me", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T14:26:31.887076+00:00", "nick": "danielw_", "message": "mhh", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T14:27:09.289696+00:00", "nick": "ducsuus", "message": "I don't want to learn how to create a crawler, I just want to learn how to download one single page, as raw HTML", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T14:28:07.298002+00:00", "nick": "danielw_", "message": "ducsuus: all you want is to download one single html page? then you don't need scrapy (look at requests for example)", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T14:28:31.281486+00:00", "nick": "ducsuus", "message": "danielw_: I need a non-blocking library", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T14:28:52.313757+00:00", "nick": "ducsuus", "message": "In fact I need to download thousands of pages, but I can't build a program that does that if I don't know how to download a single page", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T14:29:52.989802+00:00", "nick": "danielw_", "message": "the DomzSpider in the tutorial does just that, download 2 pages and save them in a file", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T14:30:37.404118+00:00", "nick": "ducsuus", "message": "Domzspider?", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T14:30:44.879309+00:00", "nick": "ducsuus", "message": "Oh", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T14:30:47.924023+00:00", "nick": "ducsuus", "message": "Dmozspider sorry", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T14:31:21.420124+00:00", "nick": "ducsuus", "message": "But what is this \"response.url.split(\"/\")[-2]\"?", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T14:31:24.989286+00:00", "nick": "danielw_", "message": "but it is (from my experience) quite complicated to integrate scrapy into another program.  if you want to do that, then i think scrapy may not be the right tool", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T14:31:59.685528+00:00", "nick": "ducsuus", "message": "Yeah - I just need to be able to download pages and receive a callback when it is done (and the ability to submit extra data to that callback", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T14:32:12.410491+00:00", "nick": "danielw_", "message": "response is an object which represents the http response.   response.url a property with the url (a string)", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T14:32:58.715292+00:00", "nick": "danielw_", "message": "response.body is webpage itself", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T14:34:07.628892+00:00", "nick": "danielw_", "message": "instead of putting the urls in \"start_urls\" you can implement the start_requests method  which should yield or return a list of Request objects (with meta data if needed)  those meta data will be in the call back method in response.meta", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T14:35:19.425081+00:00", "nick": "danielw_", "message": "also the callback itself can yield new Request objects", "links": [], "channel": "scrapy"},
{"date": "2014-12-19T14:39:45.105450+00:00", "nick": "danielw_", "message": "i hate it to wake up to an email reporting that one of my nightly crawls did fail :(", "links": [], "channel": "scrapy"},
{"date": "2014-12-20T10:13:49.710992+00:00", "nick": "Padawan", "message": "What if i want grab information of a webapp hat have a very strong session mechanism.", "links": [], "channel": "scrapy"},
{"date": "2014-12-20T10:13:54.484351+00:00", "nick": "Padawan", "message": "what should i do", "links": [], "channel": "scrapy"},
{"date": "2014-12-20T10:59:13.365863+00:00", "nick": "highbit045", "message": "Hello", "links": [], "channel": "scrapy"},
{"date": "2014-12-20T11:00:19.333308+00:00", "nick": "highbit045", "message": "I am a developer with decent experience in Python 2.7. I am just starting out with Scrapy and intend to contribute to it soon.", "links": [], "channel": "scrapy"},
{"date": "2014-12-20T11:00:58.080145+00:00", "nick": "highbit045", "message": "I need some guidance to do the same", "links": [], "channel": "scrapy"},
{"date": "2014-12-21T02:39:37.714763+00:00", "nick": "GomJabbar_", "message": "Hello", "links": [], "channel": "scrapy"},
{"date": "2014-12-21T02:39:42.386905+00:00", "nick": "GomJabbar_", "message": "Is anyone in here?", "links": [], "channel": "scrapy"},
{"date": "2014-12-21T02:40:36.595979+00:00", "nick": "GomJabbar_", "message": "question. Can anyone help me install scrapy into a virtualenv?", "links": [], "channel": "scrapy"},
{"date": "2014-12-21T10:06:45.362245+00:00", "nick": "nomad", "message": "hi!", "links": [], "channel": "scrapy"},
{"date": "2014-12-21T10:06:49.505901+00:00", "nick": "nomad", "message": "anyone here?", "links": [], "channel": "scrapy"},
{"date": "2014-12-21T10:30:48.816681+00:00", "nick": "nomad", "message": "hi deepy", "links": [], "channel": "scrapy"},
{"date": "2014-12-21T10:31:49.050647+00:00", "nick": "deepy", "message": "hello", "links": [], "channel": "scrapy"},
{"date": "2014-12-21T18:09:56.648259+00:00", "nick": "ndb", "message": "hiall!", "links": [], "channel": "scrapy"},
{"date": "2014-12-21T18:10:23.242800+00:00", "nick": "ndb", "message": "is there some easy way to see if the page changed before scraping it enterily", "links": [], "channel": "scrapy"},
{"date": "2014-12-21T18:12:43.649605+00:00", "nick": "toothrot", "message": "ndb, what do you mean?", "links": [], "channel": "scrapy"},
{"date": "2014-12-21T18:16:05.983184+00:00", "nick": "ndb", "message": "toothrot: i have a link with pagination and the stuff is updated in times, before going through the pages I wanna se if the first one have changed", "links": [], "channel": "scrapy"},
{"date": "2014-12-21T18:17:22.445657+00:00", "nick": "toothrot", "message": "well you'll have to store the first page, or data from the first page to compare to", "links": [], "channel": "scrapy"},
{"date": "2014-12-21T18:17:45.328716+00:00", "nick": "ndb", "message": "yea, it is an option", "links": [], "channel": "scrapy"},
{"date": "2014-12-21T18:17:48.921385+00:00", "nick": "toothrot", "message": "headers might help here, but probably not", "links": [], "channel": "scrapy"},
{"date": "2014-12-21T18:18:06.228068+00:00", "nick": "toothrot", "message": "since headers you'd need usually aren't going to be set for dynamic content like that", "links": [], "channel": "scrapy"},
{"date": "2014-12-22T03:10:43.649913+00:00", "nick": "SirSkitzo", "message": "Anyone know how I can select a unicode lateral with restrict_xpath? Here is what I have so far, and the error it is throwing: http://pastebin.com/PrPzGa2f", "links": ["http://pastebin.com/PrPzGa2f"], "channel": "scrapy"},
{"date": "2014-12-22T11:57:08.221053+00:00", "nick": "tanmaysahay", "message": "Hi, my name is Tanmay. I'm an undergrad sophomore from IIIT Hyderabad, India. I've been writing python code for over a year now and am interested in web scraping. What you guys are doing at Scrapy has really caught my attention and I'd love contribute! I've read http://doc.scrapy.org/en/latest/contributing.html but I'd like some advice from you guys how I should get started. Thanks a lot! :)", "links": ["http://doc.scrapy.org/en/latest/contributing.html"], "channel": "scrapy"},
{"date": "2014-12-22T17:58:04.926521+00:00", "nick": "nramirezuy", "message": "restrict_xpaths expects an iterable", "links": [], "channel": "scrapy"},
{"date": "2014-12-22T19:55:58.213791+00:00", "nick": "alejandrozf", "message": "Hi! want to use scrapy on wxp 32-bits, but getting many errors with dependencies, would you give a hand?", "links": [], "channel": "scrapy"},
{"date": "2014-12-22T19:56:56.627979+00:00", "nick": "nramirezuy", "message": "not a win user :(", "links": [], "channel": "scrapy"},
{"date": "2014-12-22T20:02:17.804540+00:00", "nick": "alejandrozf", "message": "nramirezuy: I'm \"obbligato\" :(", "links": [], "channel": "scrapy"},
{"date": "2014-12-22T20:03:26.008436+00:00", "nick": "nramirezuy", "message": "post your issue; maybe we can find something. Or I seen it before", "links": [], "channel": "scrapy"},
{"date": "2014-12-22T20:08:52.350607+00:00", "nick": "alejandrozf", "message": "pip doesn't work for me, my connection is too slow, so I tried to find an scrapy binarie for my \"horrible\" OS", "links": [], "channel": "scrapy"},
{"date": "2014-12-22T20:10:25.182630+00:00", "nick": "alejandrozf", "message": "i mean binary,,, sorry for poor english", "links": [], "channel": "scrapy"},
{"date": "2014-12-22T20:11:07.156413+00:00", "nick": "nramirezuy", "message": "how about downloading the source from git ?", "links": [], "channel": "scrapy"},
{"date": "2014-12-22T20:15:01.718323+00:00", "nick": "alejandrozf", "message": "failed with dependencies, i have installed especially for cryptography", "links": [], "channel": "scrapy"},
{"date": "2014-12-22T20:15:17.360259+00:00", "nick": "alejandrozf", "message": "... and pyOpenssl", "links": [], "channel": "scrapy"},
{"date": "2014-12-22T20:15:45.704876+00:00", "nick": "alejandrozf", "message": "I mean have installed some, but cryptography and PyOpenSSl give me trobles", "links": [], "channel": "scrapy"},
{"date": "2014-12-22T20:15:49.545895+00:00", "nick": "alejandrozf", "message": "*troubles", "links": [], "channel": "scrapy"},
{"date": "2014-12-22T20:15:51.741150+00:00", "nick": "alejandrozf", "message": "uff", "links": [], "channel": "scrapy"},
{"date": "2014-12-22T20:16:30.617169+00:00", "nick": "nramirezuy", "message": "what kind of troubles ?", "links": [], "channel": "scrapy"},
{"date": "2014-12-22T20:18:16.436407+00:00", "nick": "alejandrozf", "message": "with C api", "links": [], "channel": "scrapy"},
{"date": "2014-12-22T20:19:19.367117+00:00", "nick": "alejandrozf", "message": "where I could put the traceback?", "links": [], "channel": "scrapy"},
{"date": "2014-12-22T20:19:23.525767+00:00", "nick": "alejandrozf", "message": "linkcode?", "links": [], "channel": "scrapy"},
{"date": "2014-12-22T20:19:35.008619+00:00", "nick": "alejandrozf", "message": "linkode?", "links": [], "channel": "scrapy"},
{"date": "2014-12-22T20:24:39.939354+00:00", "nick": "nramirezuy", "message": "yes link your traceback", "links": [], "channel": "scrapy"},
{"date": "2014-12-22T20:31:50.714070+00:00", "nick": "alejandrozf", "message": "http://linkode.org/9JaGKIKUKQY2HE8uHpLTy4", "links": ["http://linkode.org/9JaGKIKUKQY2HE8uHpLTy4"], "channel": "scrapy"},
{"date": "2014-12-22T20:38:26.178024+00:00", "nick": "nramirezuy", "message": "did you follow the doc ? https://cryptography.io/en/latest/installation/...", "links": ["https://cryptography.io/en/latest/installation/#on-windows"], "channel": "scrapy"},
{"date": "2014-12-22T20:40:01.924496+00:00", "nick": "alejandrozf", "message": "no, I will follow now, thanks! i will tell you if success...", "links": [], "channel": "scrapy"},
{"date": "2014-12-22T21:06:07.424213+00:00", "nick": "lobius1523", "message": "To deal with 2 forms\u0085 Parse the first one and call back to the 2nd form\u0085 The first is login, the 2nd is search\u0085 After parsing the 2nd form, call back to parsing the details of the search. Is this correct logic for Scrapy?", "links": [], "channel": "scrapy"},
{"date": "2014-12-22T21:06:50.283810+00:00", "nick": "nramirezuy", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2014-12-22T21:43:26.592565+00:00", "nick": "lobius1523", "message": "Thank you for your response. Another question: the 2nd form leads to a stupid Ajax paging set up\u0085 Can Scrapy ignore that or is it something I must consider?", "links": [], "channel": "scrapy"},
{"date": "2014-12-23T00:58:18.009683+00:00", "nick": "cocoaflyer", "message": "Question about pipelines vs custom processors... I'm scraping data that may be in the format 1,234 or 1234 or N/A. I've written a custom processor that provides consistent formatting. I also want to delete the entry from the item if the value is N/A. Is it best to do all formatting / validation in a pipeline vs a custom processor?", "links": [], "channel": "scrapy"},
{"date": "2014-12-23T01:18:54.449194+00:00", "nick": "nomadist", "message": "hi!", "links": [], "channel": "scrapy"},
{"date": "2014-12-23T08:29:10.849685+00:00", "nick": "nomadist", "message": "hi!", "links": [], "channel": "scrapy"},
{"date": "2014-12-23T08:29:38.455721+00:00", "nick": "nomadist", "message": "I added the following lines in scrapy/__init__.py", "links": [], "channel": "scrapy"},
{"date": "2014-12-23T08:29:49.807380+00:00", "nick": "nomadist", "message": "from twisted.internet import gtk2reactor; gtk2reactor.install()", "links": [], "channel": "scrapy"},
{"date": "2014-12-23T08:30:11.215088+00:00", "nick": "nomadist", "message": "on importing scrapy, I am getting ImportError: No module named gobject", "links": [], "channel": "scrapy"},
{"date": "2014-12-23T08:30:26.428675+00:00", "nick": "nomadist", "message": "I am on ubuntu, and python-gobject is already installed", "links": [], "channel": "scrapy"},
{"date": "2014-12-23T08:30:31.100711+00:00", "nick": "nomadist", "message": "anyone have a clue?", "links": [], "channel": "scrapy"},
{"date": "2014-12-23T15:54:39.958696+00:00", "nick": "lobius1523", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2014-12-24T17:01:55.009210+00:00", "nick": "madb", "message": "Is anyone in here familiar with doing custom log handling within downloader middleware? When I actually add the file handler to my logger (a single line), I start seeing duplicate items in my item pipeline.", "links": [], "channel": "scrapy"},
{"date": "2014-12-24T18:27:08.730581+00:00", "nick": "havij", "message": "is there a way to use django models in scrapy pipelines ? I can't use dajngo models standalone :(", "links": [], "channel": "scrapy"},
{"date": "2014-12-24T18:28:18.851451+00:00", "nick": "havij", "message": "or I must write pure sql query indide pipelines ?", "links": [], "channel": "scrapy"},
{"date": "2014-12-24T18:28:27.161028+00:00", "nick": "havij", "message": "inside*", "links": [], "channel": "scrapy"},
{"date": "2014-12-24T18:43:36.775450+00:00", "nick": "ikeboy", "message": "hey, I'm new to scrapy. How can I run a script for every url that scrapy hits?", "links": [], "channel": "scrapy"},
{"date": "2014-12-25T10:34:05.336525+00:00", "nick": "nomadist", "message": "hi!", "links": [], "channel": "scrapy"},
{"date": "2014-12-25T10:34:15.524363+00:00", "nick": "nomadist", "message": "I am trying to run scrapyjs on a vps", "links": [], "channel": "scrapy"},
{"date": "2014-12-25T10:34:56.362836+00:00", "nick": "nomadist", "message": "nvm", "links": [], "channel": "scrapy"},
{"date": "2014-12-26T00:56:36.957277+00:00", "nick": "mcspud", "message": "hi guys, feel like I'm derping here.  Trying to install an older version of python (2.7.x) into a new Virtualenv.  I've read the documentation and by default it is cloning 3.4.1 into it.  Is there a way to override the base python package that is replicated across?", "links": [], "channel": "scrapy"},
{"date": "2014-12-26T04:49:10.004287+00:00", "nick": "rabidmadman", "message": "what would be the best way to yield a certain amount of objects at a time but not lose track while scraping?", "links": [], "channel": "scrapy"},
{"date": "2014-12-26T16:41:05.440316+00:00", "nick": "lobius", "message": "How do I add something to the query string if I am posting? The page I am working with has some truly stupid design decisions. One of them is to have everyone variable query to determine whether or not a user is browsing results or at the main page.", "links": [], "channel": "scrapy"},
{"date": "2014-12-26T16:53:28.891070+00:00", "nick": "lobius", "message": "Figured it out.", "links": [], "channel": "scrapy"},
{"date": "2014-12-26T22:28:53.542995+00:00", "nick": "lobius", "message": "Clearly, I'm doing something wrong. When I go to extract data, I get only", "links": [], "channel": "scrapy"},
{"date": "2014-12-26T22:28:53.653671+00:00", "nick": "lobius", "message": "newlines and return characters", "links": [], "channel": "scrapy"},
{"date": "2014-12-26T23:18:58.740190+00:00", "nick": "toothrot", "message": "depends on the html and xpath", "links": [], "channel": "scrapy"},
{"date": "2014-12-27T20:25:19.451103+00:00", "nick": "eren", "message": "I want to use LxmlLinkExtractor as a python script, not inside a crawler", "links": [], "channel": "scrapy"},
{"date": "2014-12-27T20:25:40.314202+00:00", "nick": "eren", "message": "I've looked at request and response classes but they just wrap the data", "links": [], "channel": "scrapy"},
{"date": "2014-12-27T20:25:53.710038+00:00", "nick": "eren", "message": "when I create a request instance, there is no code for connecting adn getting the data", "links": [], "channel": "scrapy"},
{"date": "2014-12-27T20:26:04.732256+00:00", "nick": "eren", "message": "how can I use request/response and link extractors as a script?", "links": [], "channel": "scrapy"},
{"date": "2014-12-27T20:26:30.466296+00:00", "nick": "eren", "message": "I just want to extract relevant links within a python script (i.e: in if __name__ == '__main__')", "links": [], "channel": "scrapy"},
{"date": "2014-12-28T03:03:00.161613+00:00", "nick": "rabidmadman", "message": "anyone here?", "links": [], "channel": "scrapy"},
{"date": "2014-12-28T03:03:12.124653+00:00", "nick": "rabidmadman", "message": "What's the best way to debug a spider that yields several identical items?", "links": [], "channel": "scrapy"},
{"date": "2014-12-28T03:40:48.085059+00:00", "nick": "rabidmadman", "message": "im using a duplicates pipeline, was wondering if thats the best approach", "links": [], "channel": "scrapy"},
{"date": "2014-12-28T03:47:00.435653+00:00", "nick": "rabidmadman", "message": "i just don't understand why duplicates are being yielded in the first place, thus needing the duplicate pipeline", "links": [], "channel": "scrapy"},
{"date": "2014-12-28T11:05:10.054940+00:00", "nick": "worllffad", "message": "Hi! What is important when looking for vps which will handle crawling 24/7? Big amount of ram?", "links": [], "channel": "scrapy"},
{"date": "2014-12-28T17:43:53.932526+00:00", "nick": "nomadist", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2014-12-28T17:44:11.028704+00:00", "nick": "nomadist", "message": "does anyone here use selenium with scrapy?", "links": [], "channel": "scrapy"},
{"date": "2014-12-28T17:44:33.976457+00:00", "nick": "nomadist", "message": "is it always error-prone.. with the webdriver simply failing to instantiate on many cases?", "links": [], "channel": "scrapy"},
{"date": "2014-12-28T18:15:08.853218+00:00", "nick": "coltim", "message": "hi - I'm running a bunch of scrapers through portia and noticed that each spider starts 8 processes; is this normal/expected or is there something I can do to make it only run one process per spider?", "links": [], "channel": "scrapy"},
{"date": "2014-12-28T18:40:53.238662+00:00", "nick": "rabidmadman", "message": "So I added dupe filtering and now my scraper has been running for 8 hours and still has not finished, while without dupe filtering it finishes in 20 minutes", "links": [], "channel": "scrapy"},
{"date": "2014-12-28T18:41:08.090666+00:00", "nick": "rabidmadman", "message": "So i don't think the duplicate filtering pipeline is the solution for me haha", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T01:39:04.344820+00:00", "nick": "italomaia_", "message": "hello folks", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T01:39:11.909930+00:00", "nick": "italomaia_", "message": "does scrapy has a helper function to fake a response?", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T15:22:20.768975+00:00", "nick": "nomadist", "message": "hi!", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T15:22:57.439995+00:00", "nick": "nomadist", "message": "when using scrapyjs.. after crawling about half the pages, scrapy stops with the simple message \"Killed\"", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T15:23:30.396236+00:00", "nick": "nomadist", "message": "no other information in the logs, no dictionary containing information about number of exceptions, items scraped, etc", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T15:23:34.940607+00:00", "nick": "nomadist", "message": "anyone would have a clue?", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T15:25:40.755565+00:00", "nick": "nomadist", "message": "btw, I'm doing a focussed crawl of a small set of urls from a list (not letting it run wild on the internet or anything)", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T16:10:58.444025+00:00", "nick": "maia", "message": "hello folks", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T16:11:27.925230+00:00", "nick": "maia", "message": "does anyone use scrapy for ajax crawling? how does it go?", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T16:11:41.443199+00:00", "nick": "nramirezuy", "message": "?", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T16:12:01.848857+00:00", "nick": "nramirezuy", "message": "emulate the request; get a response :D", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T16:12:57.722766+00:00", "nick": "maia", "message": "What I mean is, I have a service which spits out all the data for me", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T16:13:14.724183+00:00", "nick": "maia", "message": "I just have to change the query string", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T16:14:50.903398+00:00", "nick": "maia", "message": "I was wondering if scrapy has anything specific to deal with this scenario. Anyway; I'll just use a regular spider with request, then", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T16:15:48.803511+00:00", "nick": "nramirezuy", "message": "a function to change the query of a url ?", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T16:17:04.982157+00:00", "nick": "maia", "message": "nope. A spider to craw the url itself", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T16:17:47.003061+00:00", "nick": "maia", "message": "here's what I'll do: grab the first url; read the json; change the query; yield another request with the new query, but same path", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T16:26:56.262701+00:00", "nick": "maia", "message": "maybe a response.parse_json() would be nice xP\u00e7", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T16:26:57.463789+00:00", "nick": "maia", "message": "xP~", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T16:27:23.875444+00:00", "nick": "nramirezuy", "message": "nope we dont have that :(", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T16:30:06.889089+00:00", "nick": "nramirezuy", "message": "I guess it can be a new response class", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T16:40:58.095320+00:00", "nick": "maia", "message": "so far, so good, here; is there a way to improve spider feedback info besides logging?", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T16:41:15.462105+00:00", "nick": "maia", "message": "I have a spider that is ending prematurely and I don't know why", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T16:41:55.099876+00:00", "nick": "maia", "message": "it would be nice to see how many matches a LinkExtractor finds per page", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T16:57:46.002523+00:00", "nick": "nramirezuy", "message": "you can use stats", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T16:57:58.017215+00:00", "nick": "nramirezuy", "message": "those are printed at the end", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T17:02:07.145153+00:00", "nick": "maia", "message": "I need more info. Probably subclass Rule and see how it goes.", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T20:44:46.283491+00:00", "nick": "CthUlhUzzz", "message": "Hi, guys! How I can auth my spider (and recieve 'correct' cookie's) for all my future requests?", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T20:51:10.810052+00:00", "nick": "maia", "message": "did you look at this http://doc.scrapy.org/en/0.24/topics/downloader...", "links": ["http://doc.scrapy.org/en/0.24/topics/downloader-middleware.html?highlight=auth#module-scrapy.contrib.downloadermiddleware.httpauth"], "channel": "scrapy"},
{"date": "2014-12-29T20:51:11.400061+00:00", "nick": "maia", "message": "?", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T20:51:47.875648+00:00", "nick": "maia", "message": "question: can multiple rules scrape the same url in the same page?", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T20:52:10.821845+00:00", "nick": "maia", "message": "if I have multiple rules set in my spider, will all be run against the current page?", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T20:56:01.692298+00:00", "nick": "CthUlhUzzz", "message": "I have a login page, it returns a cookie, and I have to make requests to other pages using these cookies.", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T20:56:34.928942+00:00", "nick": "CthUlhUzzz", "message": "Sorry for my english", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T21:00:54.742387+00:00", "nick": "CthUlhUzzz", "message": "Authentication is performed through a form", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T21:52:06.933186+00:00", "nick": "maia", "message": "hello", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T21:52:21.347846+00:00", "nick": "maia", "message": "folks, how rules work? I have a site I'm scrapping and I have two rules for it", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T21:52:38.985691+00:00", "nick": "maia", "message": "one with follow = True and another with a callback", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T21:53:24.589368+00:00", "nick": "maia", "message": "if I use both, my callback function is never called", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T21:53:38.509815+00:00", "nick": "maia", "message": "both with the same \"allow\" regex", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T21:53:57.186412+00:00", "nick": "maia", "message": "this /departamento[/\\?]", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T21:55:59.445291+00:00", "nick": "brati", "message": "two times the same regexp in my eyes does not work. The first which meets the condition will be called, the others not", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T21:56:06.529203+00:00", "nick": "brati", "message": "So only one of both will be called", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T21:56:36.120910+00:00", "nick": "brati", "message": "Does it work using Rule(allow=..., callback=function, follow=True)? Otherwise you'll have to select URLs manually in callback and yield Requests", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T21:58:54.435384+00:00", "nick": "maia", "message": "here is the full rules http://dpaste.com/1VZYN57", "links": ["http://dpaste.com/1VZYN57"], "channel": "scrapy"},
{"date": "2014-12-29T21:59:07.335180+00:00", "nick": "maia", "message": "I'll try, brati", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T22:06:11.148803+00:00", "nick": "maia", "message": "brati: doesn't work", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T22:06:44.273860+00:00", "nick": "maia", "message": "I didn't know that the first rule to match is the only one called", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T22:07:31.329357+00:00", "nick": "brati", "message": "mh then maybe the only option is to extract URLs yourself?... But I guess there must be a function which you could call. Maybe you can find the function being called, when usually follow=True is active in the source? https://github.com/scrapy/scrapy", "links": ["https://github.com/scrapy/scrapy"], "channel": "scrapy"},
{"date": "2014-12-29T22:07:56.393265+00:00", "nick": "maia", "message": "follow=true doesn't work if there is a callback", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T22:08:07.915689+00:00", "nick": "maia", "message": "they both are maped to the same instance attribute", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T22:10:52.613668+00:00", "nick": "brati", "message": "from how I read the code, it should work. if you set follow=True explicitely, callback doesn't matter. Only if you do not set follow explicitely, it is defaulted to False if callback is given", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T22:11:07.995524+00:00", "nick": "brati", "message": "https://github.com/scrapy/scrapy/blob/master/sc...", "links": ["https://github.com/scrapy/scrapy/blob/master/scrapy/contrib/spiders/crawl.py#L25"], "channel": "scrapy"},
{"date": "2014-12-29T22:11:38.294088+00:00", "nick": "maia", "message": "my mistake", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T22:12:44.708609+00:00", "nick": "brati", "message": "np, hope it helps", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T22:18:00.011468+00:00", "nick": "brati", "message": "VERFICTES SCHEISSDRECKS HURENVIRTUALBOX", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T22:26:55.453624+00:00", "nick": "maia", "message": "seems to be working brati", "links": [], "channel": "scrapy"},
{"date": "2014-12-29T22:26:56.973068+00:00", "nick": "maia", "message": "= ]", "links": [], "channel": "scrapy"},
{"date": "2014-12-30T07:03:47.754118+00:00", "nick": "t_p", "message": "how to run scrapy from source code", "links": [], "channel": "scrapy"},
{"date": "2014-12-31T17:21:28.324637+00:00", "nick": "GaryOak_", "message": "Hello", "links": [], "channel": "scrapy"},
{"date": "2014-12-31T17:23:59.871428+00:00", "nick": "nramirezuy", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2014-12-31T17:33:25.659187+00:00", "nick": "GaryOak_", "message": "I was gonna ask about contracts, but you are the same person that closed my issue from yesterday :P", "links": [], "channel": "scrapy"},
{"date": "2014-12-31T17:38:22.447349+00:00", "nick": "nramirezuy", "message": ":D", "links": [], "channel": "scrapy"},
{"date": "2014-12-31T17:38:34.170934+00:00", "nick": "nramirezuy", "message": "I don't really use contracts :(", "links": [], "channel": "scrapy"},
{"date": "2014-12-31T17:38:48.232159+00:00", "nick": "GaryOak_", "message": "lol that's what you said in the issue", "links": [], "channel": "scrapy"},
{"date": "2014-12-31T17:41:25.339430+00:00", "nick": "nramirezuy", "message": "I guess you can use a spider argument", "links": [], "channel": "scrapy"},
{"date": "2014-12-31T17:41:35.413426+00:00", "nick": "nramirezuy", "message": "and format the string", "links": [], "channel": "scrapy"},
{"date": "2014-12-31T17:41:37.012494+00:00", "nick": "nramirezuy", "message": "with it", "links": [], "channel": "scrapy"},
{"date": "2014-12-31T17:44:32.988338+00:00", "nick": "GaryOak_", "message": "I guess the arguments are just a bad way of fixing another issue I'm having", "links": [], "channel": "scrapy"},
{"date": "2014-12-31T17:45:07.544142+00:00", "nick": "GaryOak_", "message": "I'm trying to scrape the same site but in two different modes, with the same spider", "links": [], "channel": "scrapy"},
{"date": "2014-12-31T17:49:48.411696+00:00", "nick": "nramirezuy", "message": "modes? like a dev and a prod ? or different items and stuff ?", "links": [], "channel": "scrapy"},
{"date": "2014-12-31T17:50:18.055368+00:00", "nick": "GaryOak_", "message": "so like a search and a details", "links": [], "channel": "scrapy"},
{"date": "2014-12-31T17:50:42.162431+00:00", "nick": "GaryOak_", "message": "one scrapes results from a search, and one gets the details for the items searched", "links": [], "channel": "scrapy"},
{"date": "2014-12-31T17:54:19.688903+00:00", "nick": "nramirezuy", "message": "3 spiders might be what you are looking for", "links": [], "channel": "scrapy"},
{"date": "2014-12-31T17:54:34.203423+00:00", "nick": "nramirezuy", "message": "a base, a search and a details", "links": [], "channel": "scrapy"},
{"date": "2014-12-31T17:54:47.507666+00:00", "nick": "nramirezuy", "message": "base can be without name", "links": [], "channel": "scrapy"},
{"date": "2014-12-31T17:55:23.359580+00:00", "nick": "GaryOak_", "message": "I'm going to have to make a bunch of them for a few different sites, so I could just subclass them", "links": [], "channel": "scrapy"},
{"date": "2014-12-31T17:58:32.139484+00:00", "nick": "GaryOak_", "message": "Having a base, search, and details subclassed, hmmm", "links": [], "channel": "scrapy"},
{"date": "2014-12-31T17:58:40.646681+00:00", "nick": "GaryOak_", "message": "I need to think about this structure some more", "links": [], "channel": "scrapy"},
{"date": "2015-01-02T13:42:54.013790+00:00", "nick": "ahop", "message": "Hi!", "links": [], "channel": "scrapy"},
{"date": "2015-01-02T13:43:34.334550+00:00", "nick": "ahop", "message": "Would you have an idea for this #scrapy related question : http://stackoverflow.com/questions/27742542/a-w... ?", "links": ["http://stackoverflow.com/questions/27742542/a-web-crawler-in-a-self-contained-python-file"], "channel": "scrapy"},
{"date": "2015-01-02T13:48:12.863207+00:00", "nick": "ahop", "message": "any idea devonmeunier__ ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-02T13:53:02.374280+00:00", "nick": "devonmeunier__", "message": "come again?", "links": [], "channel": "scrapy"},
{"date": "2015-01-02T14:03:55.032813+00:00", "nick": "ahop", "message": "devonmeunier__ yeah :)", "links": [], "channel": "scrapy"},
{"date": "2015-01-02T14:04:32.310669+00:00", "nick": "ahop", "message": "are you here?", "links": [], "channel": "scrapy"},
{"date": "2015-01-02T14:05:50.780699+00:00", "nick": "ahop", "message": "#/usr/bin/scrapy is :   from scrapy.cmdline import execute  /// execute()", "links": [], "channel": "scrapy"},
{"date": "2015-01-02T14:05:55.918354+00:00", "nick": "ahop", "message": "what does this do ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-02T14:06:07.317751+00:00", "nick": "ahop", "message": "I would like to call scrapy directly in a python file like:", "links": [], "channel": "scrapy"},
{"date": "2015-01-02T14:06:16.696462+00:00", "nick": "ahop", "message": "python mycrawler.py  rather than   \"scrapy ...\"", "links": [], "channel": "scrapy"},
{"date": "2015-01-02T14:06:20.550995+00:00", "nick": "ahop", "message": "how to do that?", "links": [], "channel": "scrapy"},
{"date": "2015-01-02T14:13:22.453565+00:00", "nick": "devonmeunier__", "message": "Sorry, I don't have any experience with that", "links": [], "channel": "scrapy"},
{"date": "2015-01-02T14:13:59.539186+00:00", "nick": "nramirezuy", "message": "http://doc.scrapy.org/en/latest/topics/practice...", "links": ["http://doc.scrapy.org/en/latest/topics/practices.html"], "channel": "scrapy"},
{"date": "2015-01-02T14:13:59.879305+00:00", "nick": "nramirezuy", "message": "But I don't recomend", "links": [], "channel": "scrapy"},
{"date": "2015-01-02T21:53:49.496738+00:00", "nick": "GaryOak_", "message": "Hello, does anyone know a good way to compare all the items returned and then output them", "links": [], "channel": "scrapy"},
{"date": "2015-01-02T21:54:33.680147+00:00", "nick": "GaryOak_", "message": "I'm trying to select one element by finding the best metaphone match with a keyword argument and then output that item from the result set", "links": [], "channel": "scrapy"},
{"date": "2015-01-03T06:27:21.712298+00:00", "nick": "rokson", "message": "hey, how to get response.body just before spider is initialized?", "links": [], "channel": "scrapy"},
{"date": "2015-01-04T13:59:45.303581+00:00", "nick": "AndyRez", "message": "hi all", "links": [], "channel": "scrapy"},
{"date": "2015-01-04T14:00:12.529187+00:00", "nick": "AndyRez", "message": "I am having a hard time figuring out my environment variable/python path/django/scrapy setup...", "links": [], "channel": "scrapy"},
{"date": "2015-01-04T14:00:31.379498+00:00", "nick": "AndyRez", "message": "following the link here http://doc.scrapy.org/en/latest/topics/djangoit...", "links": ["http://doc.scrapy.org/en/latest/topics/djangoitem.html"], "channel": "scrapy"},
{"date": "2015-01-04T14:01:07.123106+00:00", "nick": "AndyRez", "message": "I think I have everything set to my best of knowledge, but I am having an issue with the PYTHONPATH thing... At least there where I think the problem is from..", "links": [], "channel": "scrapy"},
{"date": "2015-01-04T14:01:19.173607+00:00", "nick": "AndyRez", "message": "Can anyone assist in fixing this thanks", "links": [], "channel": "scrapy"},
{"date": "2015-01-04T15:22:25.064261+00:00", "nick": "Andyrez", "message": "Hi anyone available?", "links": [], "channel": "scrapy"},
{"date": "2015-01-04T23:06:05.590442+00:00", "nick": "skinkie", "message": "Hi, how can I have a trace from scrapy similar to tcpdump? I am looking to debug a HTTPS scrape.", "links": [], "channel": "scrapy"},
{"date": "2015-01-04T23:52:04.206533+00:00", "nick": "mathandpizza", "message": "Hi", "links": [], "channel": "scrapy"},
{"date": "2015-01-04T23:52:48.237147+00:00", "nick": "mathandpizza", "message": "im new to scrapy and was interested in asking some high level questions about a crawler i wanted to make", "links": [], "channel": "scrapy"},
{"date": "2015-01-05T00:00:44.574168+00:00", "nick": "mathandpizza", "message": "this is the website i want to crawl https://www.wegmans.com/webapp/wcs/stores/servl...", "links": ["https://www.wegmans.com/webapp/wcs/stores/servlet/ProductListView?forwardto=ProductListView&amp=&value=0&amp=&langId=-1&amp=&storeId=10052&amp=&property=VIEWRECS&amp=&URL=RedirectView&amp=&catalogId=10002&amp=&N=207"], "channel": "scrapy"},
{"date": "2015-01-05T00:02:06.909233+00:00", "nick": "mathandpizza", "message": "first i would like to goto each of ~30 departments, then in each department goto each product webpage, then on each product webpage grab some information to generate an \u201citem\u201d", "links": [], "channel": "scrapy"},
{"date": "2015-01-05T00:03:08.350003+00:00", "nick": "mathandpizza", "message": "will i need multiple spiders for this?, departmentSpider, productSpider, itemSpider for instance", "links": [], "channel": "scrapy"},
{"date": "2015-01-05T00:27:48.463949+00:00", "nick": "mathandpizza", "message": "or do i just start with that webpage and make requests using department callback function that makes requests using product callback function etc\u2026?", "links": [], "channel": "scrapy"},
{"date": "2015-01-05T19:22:54.251028+00:00", "nick": "rak__", "message": "hello", "links": [], "channel": "scrapy"},
{"date": "2015-01-05T19:23:29.527458+00:00", "nick": "rak__", "message": "anyone active in here ??", "links": [], "channel": "scrapy"},
{"date": "2015-01-05T19:33:51.416509+00:00", "nick": "nramirezuy", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2015-01-05T19:58:02.533354+00:00", "nick": "rak__", "message": "hello again", "links": [], "channel": "scrapy"},
{"date": "2015-01-05T19:59:04.789121+00:00", "nick": "rak__", "message": "need some help with xpath", "links": [], "channel": "scrapy"},
{"date": "2015-01-05T20:05:29.732639+00:00", "nick": "rak__", "message": "anyone?", "links": [], "channel": "scrapy"},
{"date": "2015-01-05T20:06:44.277833+00:00", "nick": "BManojlovic", "message": "didn't touch it for a while", "links": [], "channel": "scrapy"},
{"date": "2015-01-05T20:06:48.227219+00:00", "nick": "BManojlovic", "message": "waht is your issue", "links": [], "channel": "scrapy"},
{"date": "2015-01-05T20:07:19.292878+00:00", "nick": "BManojlovic", "message": "and for good help wtih xpath use firebug and its xpath representation", "links": [], "channel": "scrapy"},
{"date": "2015-01-05T20:22:14.835456+00:00", "nick": "rak__", "message": "thanks for answering ! i got it resolved on stackoverflow", "links": [], "channel": "scrapy"},
{"date": "2015-01-05T20:22:36.154380+00:00", "nick": "rak__", "message": "i was missing a child node", "links": [], "channel": "scrapy"},
{"date": "2015-01-06T10:14:44.380163+00:00", "nick": "Gopi", "message": "Hello Eveyone", "links": [], "channel": "scrapy"},
{"date": "2015-01-06T10:15:15.586143+00:00", "nick": "Gopi", "message": "I am trying to make scrapy scrape sites with AJAX, any pointers in that direction would be really helpful", "links": [], "channel": "scrapy"},
{"date": "2015-01-06T10:15:47.542114+00:00", "nick": "Gopi", "message": "I am planning on using selenium + scrapy..But I understand its heavy ..is there any other way of going about it ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-06T12:21:06.494116+00:00", "nick": "gigo1980", "message": "Hi ;) is there an elastic search implemantion alredy there ? or does i have to reimport my grabed items ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-06T16:00:23.781690+00:00", "nick": "JamesS_", "message": "Any tips on debugging calls to the FormRequest.from_response helper method?", "links": [], "channel": "scrapy"},
{"date": "2015-01-07T01:29:18.764935+00:00", "nick": "VooDooNOFX", "message": "How can I access the settings for a running spider from within the my spider class?", "links": [], "channel": "scrapy"},
{"date": "2015-01-07T22:08:46.662079+00:00", "nick": "torik", "message": "Hi there! Does Scrapy offer the functionality to verify that my assumptions on the structure of the raw data to be scraped are still correct?", "links": [], "channel": "scrapy"},
{"date": "2015-01-07T22:10:35.949281+00:00", "nick": "torik", "message": "For instance, I create a scraper for Amazon.com on Monday based on the structure of their product page. On Tuesday, Amazon developers change their code so that the structure of the HTML is different. Thus, the assumptions I made on Monday are no longer valid. Does Scrapy offer a way to enumerate and link my assumptions, and tell me when those assumptions are no longer accurate?", "links": ["http://Amazon.com"], "channel": "scrapy"},
{"date": "2015-01-07T22:11:50.324650+00:00", "nick": "torik", "message": "I can make integration tests do accomplish this. However, I'd like to see if Scrapy offers some functionality before I start writing integration tests using one of the many testing suites available.", "links": [], "channel": "scrapy"},
{"date": "2015-01-07T22:13:02.951327+00:00", "nick": "DanielW", "message": "torik: http://doc.scrapy.org/en/0.22/topics/contracts.... have a look at spider contracts. but i never tried them", "links": ["http://doc.scrapy.org/en/0.22/topics/contracts.html"], "channel": "scrapy"},
{"date": "2015-01-07T22:15:37.146542+00:00", "nick": "torik", "message": "Will do. Thank you DanielW.", "links": [], "channel": "scrapy"},
{"date": "2015-01-08T13:26:59.031623+00:00", "nick": "aaearon", "message": "following the scrapy tutorial but getting caught up on running `scrapy crawl dmoz` http://pastebin.com/UeDr0Jum", "links": ["http://pastebin.com/UeDr0Jum"], "channel": "scrapy"},
{"date": "2015-01-08T13:27:12.384516+00:00", "nick": "aaearon", "message": "ive verified that for the spider, name = \"dmoz\"", "links": [], "channel": "scrapy"},
{"date": "2015-01-08T13:30:06.341520+00:00", "nick": "aaearon", "message": "spider isnt listed when i do `scrapy list` either", "links": [], "channel": "scrapy"},
{"date": "2015-01-08T13:45:26.515094+00:00", "nick": "aaearon", "message": "not sure if my virtualenv is messed up? im pretty sure i have the required dependencies http://pastebin.com/YWmv7Uhp", "links": ["http://pastebin.com/YWmv7Uhp"], "channel": "scrapy"},
{"date": "2015-01-08T13:57:27.055712+00:00", "nick": "aaearon", "message": "using BaseSpider doesnt make a difference either", "links": [], "channel": "scrapy"},
{"date": "2015-01-08T15:23:28.394437+00:00", "nick": "cipher__", "message": "Can anyone suggest a tutorial (pdf?) to get started?", "links": [], "channel": "scrapy"},
{"date": "2015-01-08T15:34:38.513229+00:00", "nick": "aaearon", "message": "cipher__  the scrapy site has a tutorial", "links": [], "channel": "scrapy"},
{"date": "2015-01-08T19:20:28.619013+00:00", "nick": "barraponto", "message": "has 0.25 been released?", "links": [], "channel": "scrapy"},
{"date": "2015-01-08T19:24:48.721215+00:00", "nick": "brati", "message": "Does Scrapy release the odd numbers? I only saw 0.22, 0.24 and so", "links": [], "channel": "scrapy"},
{"date": "2015-01-08T21:23:22.041577+00:00", "nick": "AndyRez", "message": "hi all,", "links": [], "channel": "scrapy"},
{"date": "2015-01-08T21:24:20.073786+00:00", "nick": "AndyRez", "message": "I would like to know if there is a way to store scraped data in a different folder than the current folder by running \"scrapy crawl spider-name -o item.csv -t csv\"", "links": [], "channel": "scrapy"},
{"date": "2015-01-08T21:37:07.548464+00:00", "nick": "DanielW", "message": "?", "links": [], "channel": "scrapy"},
{"date": "2015-01-08T21:37:47.423405+00:00", "nick": "DanielW", "message": "just use a different path instead of item.csv", "links": [], "channel": "scrapy"},
{"date": "2015-01-08T21:38:05.693903+00:00", "nick": "DanielW", "message": "/home/test/nice/path/for/outout/item.csv ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-08T21:38:13.561073+00:00", "nick": "DanielW", "message": "p", "links": [], "channel": "scrapy"},
{"date": "2015-01-08T21:50:26.666423+00:00", "nick": "barraponto", "message": "AndyRez: see above", "links": [], "channel": "scrapy"},
{"date": "2015-01-08T21:50:52.665188+00:00", "nick": "AndyRez", "message": "oh, thanks DanielW", "links": [], "channel": "scrapy"},
{"date": "2015-01-08T21:51:27.793094+00:00", "nick": "AndyRez", "message": "thanks barraponto for making me see the alert. Was carried away trying to do a research on which VPS to run a scrapyd server on..", "links": [], "channel": "scrapy"},
{"date": "2015-01-08T21:51:44.088571+00:00", "nick": "barraponto", "message": "AndyRez: please give feedback on that research ;)", "links": [], "channel": "scrapy"},
{"date": "2015-01-08T21:51:52.230310+00:00", "nick": "AndyRez", "message": "speaking of server services (VPS)", "links": [], "channel": "scrapy"},
{"date": "2015-01-08T21:52:04.448358+00:00", "nick": "AndyRez", "message": "lol. I was just going to ask which is better...", "links": [], "channel": "scrapy"},
{"date": "2015-01-08T21:52:11.009822+00:00", "nick": "AndyRez", "message": "I am thinking Linode...", "links": [], "channel": "scrapy"},
{"date": "2015-01-08T21:52:36.574968+00:00", "nick": "AndyRez", "message": "instead of Digital Ocean (DO)..", "links": [], "channel": "scrapy"},
{"date": "2015-01-08T21:55:04.306907+00:00", "nick": "DanielW", "message": "it must be nice to live in the usa when one is searching for servers.   there are so many options", "links": [], "channel": "scrapy"},
{"date": "2015-01-08T21:55:19.141174+00:00", "nick": "barraponto", "message": "do is enough for me, but i like ec2 too", "links": [], "channel": "scrapy"},
{"date": "2015-01-08T21:56:45.325607+00:00", "nick": "DanielW", "message": "i am using jiffybox (tried ec2, but they didn't have servers in germany at that time and it is quite complicated to use)", "links": [], "channel": "scrapy"},
{"date": "2015-01-08T21:58:25.345045+00:00", "nick": "AndyRez", "message": "Can I possibly run a scrapyd server on 512MB? I am thinking a minimum of 1GB would suffice.", "links": [], "channel": "scrapy"},
{"date": "2015-01-08T22:00:09.544033+00:00", "nick": "DanielW", "message": "it depends on what you want to do with", "links": [], "channel": "scrapy"},
{"date": "2015-01-08T22:00:35.066280+00:00", "nick": "AndyRez", "message": "basically just daily scrapes but not large....", "links": [], "channel": "scrapy"},
{"date": "2015-01-08T22:00:41.574047+00:00", "nick": "DanielW", "message": "i am using it on a virtual server with 2 GB ram. but  don't need that much. one GB is been used by a redis server", "links": [], "channel": "scrapy"},
{"date": "2015-01-08T22:02:09.898327+00:00", "nick": "DanielW", "message": "i believe it would even run with 250 mb in my case or maybe less.", "links": [], "channel": "scrapy"},
{"date": "2015-01-08T22:02:41.493229+00:00", "nick": "DanielW", "message": "just try it", "links": [], "channel": "scrapy"},
{"date": "2015-01-08T22:04:25.793870+00:00", "nick": "AndyRez", "message": "okay, I could run it on 512 and create a different droplet for the other 512 ram.", "links": [], "channel": "scrapy"},
{"date": "2015-01-08T22:04:39.605165+00:00", "nick": "AndyRez", "message": "I would go ahead with DO then..", "links": [], "channel": "scrapy"},
{"date": "2015-01-08T22:06:42.360269+00:00", "nick": "AndyRez", "message": "Later good people", "links": [], "channel": "scrapy"},
{"date": "2015-01-09T08:11:22.286880+00:00", "nick": "deltaskelta", "message": "hey all, working my way through the docs and I can't seem to get scrapy to actually follow any links and scrape them after the first one", "links": [], "channel": "scrapy"},
{"date": "2015-01-09T08:12:11.213460+00:00", "nick": "deltaskelta", "message": "its just finishing its first link and quitting", "links": [], "channel": "scrapy"},
{"date": "2015-01-09T13:10:52.916421+00:00", "nick": "barraponto", "message": "is it possible to change settings from within a spider?", "links": [], "channel": "scrapy"},
{"date": "2015-01-09T13:11:06.290101+00:00", "nick": "barraponto", "message": "i wanted to change the IMAGES_STORE path according to the spider run.", "links": [], "channel": "scrapy"},
{"date": "2015-01-09T13:32:44.817901+00:00", "nick": "barraponto", "message": "should i use settings.override?", "links": [], "channel": "scrapy"},
{"date": "2015-01-09T23:54:37.557755+00:00", "nick": "GaryOak_", "message": "Hello", "links": [], "channel": "scrapy"},
{"date": "2015-01-09T23:55:44.995942+00:00", "nick": "GaryOak_", "message": "I'm using an ItemLoader and scrapy keeps returning arrays instead of strings to the JSON output file, if I just use selector and extract it returns a single string", "links": [], "channel": "scrapy"},
{"date": "2015-01-09T23:56:04.988896+00:00", "nick": "GaryOak_", "message": "is there any way around this, to get strings instead of arrays with a single item?", "links": [], "channel": "scrapy"},
{"date": "2015-01-10T19:29:30.142536+00:00", "nick": "ndb", "message": "is there some way to run the spider outside scrapyd or cmdline", "links": [], "channel": "scrapy"},
{"date": "2015-01-10T19:29:58.866275+00:00", "nick": "ndb", "message": "I have a django form where I wanna launch the spider with some user settings", "links": [], "channel": "scrapy"},
{"date": "2015-01-10T19:35:25.612237+00:00", "nick": "brati", "message": "ndb: yeah, but you have to watch out for some things.", "links": [], "channel": "scrapy"},
{"date": "2015-01-10T19:36:04.677456+00:00", "nick": "brati", "message": "http://doc.scrapy.org/en/latest/topics/practice...", "links": ["http://doc.scrapy.org/en/latest/topics/practices.html"], "channel": "scrapy"},
{"date": "2015-01-10T19:36:30.741257+00:00", "nick": "ndb", "message": "maybe I should update the architecture using scrapyd and jsonrpc", "links": [], "channel": "scrapy"},
{"date": "2015-01-10T19:36:32.912768+00:00", "nick": "ndb", "message": "later :P", "links": [], "channel": "scrapy"},
{"date": "2015-01-10T19:36:45.877554+00:00", "nick": "brati", "message": "and connecting the handler to spider_closed only, as given in the best practices, is not perfect... Had some problems with it, but don't remember which", "links": [], "channel": "scrapy"},
{"date": "2015-01-10T19:36:47.300656+00:00", "nick": "ndb", "message": "brati: nice, tkz", "links": [], "channel": "scrapy"},
{"date": "2015-01-12T05:13:43.408096+00:00", "nick": "is_null", "message": "hi all, i don't understand why there's one url which scrapy claims to have \"Crawled\" but without executing my callback", "links": [], "channel": "scrapy"},
{"date": "2015-01-12T05:13:59.516461+00:00", "nick": "is_null", "message": "i've been trying to debug that for more than an hour, jeeez", "links": [], "channel": "scrapy"},
{"date": "2015-01-12T05:21:45.662187+00:00", "nick": "is_null", "message": "and i mean without, because it doesn't break there for that url", "links": [], "channel": "scrapy"},
{"date": "2015-01-12T05:22:07.601621+00:00", "nick": "is_null", "message": "it's not ignoring nor anything, the only thing logged for this url is \"Crawled ...\"", "links": [], "channel": "scrapy"},
{"date": "2015-01-12T05:26:10.927106+00:00", "nick": "is_null", "message": "when i force my callback with scrapy parse -c it works", "links": [], "channel": "scrapy"},
{"date": "2015-01-12T05:26:18.346303+00:00", "nick": "is_null", "message": "but not with scrapy crawl", "links": [], "channel": "scrapy"},
{"date": "2015-01-12T05:29:08.305069+00:00", "nick": "is_null", "message": "oh well it's time to debug this from line 1 then xD", "links": [], "channel": "scrapy"},
{"date": "2015-01-12T06:14:17.361678+00:00", "nick": "is_null", "message": "nevermind, i refactored like crazy and it works now, pretty awesome B)", "links": [], "channel": "scrapy"},
{"date": "2015-01-12T08:38:03.944159+00:00", "nick": "haggi", "message": "Hi, i am new to scrapy and have a problem with my first CrawlSpider. Getting the content i want and write this into csv file works perfect. But this Webseite uses Pagination with javascript:__doPostBack. I cant get this solved. Can someone help ? thx", "links": [], "channel": "scrapy"},
{"date": "2015-01-12T13:21:04.288102+00:00", "nick": "Sujay", "message": "Hi guys. Anyone here know how to control when exactly scrapyd changes to the directory specified with the --rundir option?", "links": [], "channel": "scrapy"},
{"date": "2015-01-12T13:21:08.352612+00:00", "nick": "Sujay", "message": "(https://github.com/scrapy/scrapyd/issues/70)", "links": ["https://github.com/scrapy/scrapyd/issues/70"], "channel": "scrapy"},
{"date": "2015-01-12T13:21:30.223514+00:00", "nick": "Sujay", "message": "Is it controlled by the twisted server?", "links": [], "channel": "scrapy"},
{"date": "2015-01-12T15:52:31.007209+00:00", "nick": "haggi", "message": "Hi, i am new to scrapy and have a problem with my first CrawlSpider. Getting the content i want and write this into csv file works perfect. But this Webseite uses Pagination with javascript:__doPostBack. I cant get this solved. Can someone help ? thx", "links": [], "channel": "scrapy"},
{"date": "2015-01-12T16:15:32.761255+00:00", "nick": "nramirezuy", "message": "haggi: http://doc.scrapy.org/en/latest/faq.html#what-s...", "links": ["http://doc.scrapy.org/en/latest/faq.html#what-s-this-huge-cryptic-viewstate-parameter-used-in-some-forms"], "channel": "scrapy"},
{"date": "2015-01-12T16:19:16.007761+00:00", "nick": "haggi", "message": "thank you 4 the link. i just dont get it :(", "links": [], "channel": "scrapy"},
{"date": "2015-01-12T16:19:31.985202+00:00", "nick": "haggi", "message": "the Pagination Links on the site are: <a id=\"MainContent_ProductsOverview1_rptPagesTop_btnPage_1\" class=\"btnPage\" href=\"javascript:__doPostBack('ctl00$MainContent$ProductsOverview1$rptPagesTop$ctl02$btnPage','')\">2</a>", "links": [], "channel": "scrapy"},
{"date": "2015-01-12T16:19:35.410787+00:00", "nick": "Sujay", "message": "Hi guys, has anyone ever run into issues with using --rundir? Namely that when certain objects are initialised (like Scheduler), the directory hasn't been changed to rundir", "links": [], "channel": "scrapy"},
{"date": "2015-01-12T16:19:39.989314+00:00", "nick": "Sujay", "message": "but later it is.", "links": [], "channel": "scrapy"},
{"date": "2015-01-12T16:29:45.848211+00:00", "nick": "nramirezuy", "message": "haggi: just check the site you will see that viewstate; _doPostBack is something common on .NET sites that also use _VIEWSTATE", "links": [], "channel": "scrapy"},
{"date": "2015-01-12T16:30:12.885584+00:00", "nick": "nramirezuy", "message": "you just have to send the Form with the hidden values", "links": [], "channel": "scrapy"},
{"date": "2015-01-12T16:32:00.550587+00:00", "nick": "haggi", "message": "i see that after the bode-tag: input type=\"hidden\" name=\"__VIEWSTATE\" id=\"__VIEWSTATE\" value=\"XXX\"", "links": [], "channel": "scrapy"},
{"date": "2015-01-12T16:32:17.335334+00:00", "nick": "haggi", "message": "do i need that XXX value ? sorry. i am noob :D", "links": [], "channel": "scrapy"},
{"date": "2015-01-12T16:48:09.642075+00:00", "nick": "nramirezuy", "message": "haggi: You need to send the Form back to the server with that value", "links": [], "channel": "scrapy"},
{"date": "2015-01-12T16:50:46.638396+00:00", "nick": "haggi", "message": "like Here? http://stackoverflow.com/questions/15560746/tro...", "links": ["http://stackoverflow.com/questions/15560746/troubles-using-scrapy-with-javascript-dopostback-method"], "channel": "scrapy"},
{"date": "2015-01-12T16:51:26.563152+00:00", "nick": "haggi", "message": "every example uses the url-parameter like &sort=Sorting&page=2", "links": [], "channel": "scrapy"},
{"date": "2015-01-12T16:51:44.640478+00:00", "nick": "haggi", "message": "but i dont know them on the site i crawl or cant find them out", "links": [], "channel": "scrapy"},
{"date": "2015-01-12T16:59:57.790884+00:00", "nick": "haggi", "message": "thanks, cu !", "links": [], "channel": "scrapy"},
{"date": "2015-01-12T22:46:28.735924+00:00", "nick": "cipher__", "message": "Can scrapy be used in normal python scripts, without calling the scrapy utility to start it and such? Something easy to distribute?", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T09:15:02.927902+00:00", "nick": "h4k1m", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T09:15:16.781162+00:00", "nick": "h4k1m", "message": "is it possible to reduce the memory used by a crawler?", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T09:15:30.995809+00:00", "nick": "h4k1m", "message": "I know the I can reduce the number of concurrent requests but it's not helping", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T09:15:41.248629+00:00", "nick": "h4k1m", "message": "I need to limit memory usage of my crawlers to 1Go", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T10:33:56.183354+00:00", "nick": "Sujay", "message": "Hi guys", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T10:34:01.436680+00:00", "nick": "Sujay", "message": "is scrapyd still maintained?", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T14:09:38.858056+00:00", "nick": "ev___", "message": "good morning everyone", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T14:10:35.084224+00:00", "nick": "ev___", "message": "is there a way to have multiple feed URIS? I want to save the CSV to S3 as well as a local directory", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T14:38:21.020411+00:00", "nick": "ev___", "message": "anyone around?", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T14:47:17.573986+00:00", "nick": "vak", "message": "hi all", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T14:49:25.277995+00:00", "nick": "vak", "message": "in my case start URLs are added in real-time. I have no idea yet with what spider to start with...", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T14:49:30.845217+00:00", "nick": "vak", "message": "any hints?", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T14:50:10.192372+00:00", "nick": "vak", "message": "are there any examples for such a case?", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T14:51:10.755204+00:00", "nick": "vak", "message": "that is, not all starting URLs are known when crawling started.", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:12:44.458458+00:00", "nick": "ev___", "message": "vak:  you can yield Request(url, callback) in the script to have scrapy request non-start_urls", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:13:17.954670+00:00", "nick": "ev___", "message": "http://doc.scrapy.org/en/latest/topics/request-...", "links": ["http://doc.scrapy.org/en/latest/topics/request-response.html?highlight=request"], "channel": "scrapy"},
{"date": "2015-01-13T15:19:20.457274+00:00", "nick": "BitMonster", "message": "anyone really good at coding scrapy spiders?", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:21:42.908679+00:00", "nick": "ev___", "message": "I've written about 100 of them", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:23:17.692659+00:00", "nick": "barraponto", "message": "BitMonster: i've written a lot too.", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:23:22.018963+00:00", "nick": "BitMonster", "message": "so yeah i would say that qualifies lol", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:23:31.496637+00:00", "nick": "ev___", "message": "what's your issue?", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:23:46.514731+00:00", "nick": "barraponto", "message": "used to be my job, back when i contributed to scrapy", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:23:48.962901+00:00", "nick": "barraponto", "message": "oh the days...", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:23:57.452007+00:00", "nick": "ev___", "message": "what happened?", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:24:27.534661+00:00", "nick": "BitMonster", "message": "basically trying to learn how to increase the complexity of my scrapy spiders and my computer science teacher isnt helping cause she hates me lol", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:24:42.522578+00:00", "nick": "ev___", "message": "why increase the complexity?", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:25:02.086882+00:00", "nick": "BitMonster", "message": "i meant i want more than a basic spider", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:25:40.222876+00:00", "nick": "barraponto", "message": "i usually think more in terms of data", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:25:48.600400+00:00", "nick": "barraponto", "message": "like i want to get *more* data.", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:25:52.292134+00:00", "nick": "barraponto", "message": "goals over means", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:26:10.505959+00:00", "nick": "ev___", "message": "bitmonster, do you have a particularly challenging site you are looking at?", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:27:18.696226+00:00", "nick": "BitMonster", "message": "yes exactly, increase the amount and types of data that are collected", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:27:35.174573+00:00", "nick": "BitMonster", "message": "i need to learn more of the modules for scrapy", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:27:42.074972+00:00", "nick": "BitMonster", "message": "anyone still use beautiful soup?", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:28:02.676419+00:00", "nick": "nramirezuy", "message": "vak: there is a project that will allow to crawl urls in real time, but is not open yet. Project name is: scrapyrt", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:28:04.835825+00:00", "nick": "barraponto", "message": "lxml is faster, better, stronger", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:28:24.225166+00:00", "nick": "barraponto", "message": "nramirezuy: what do you mean in real time?", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:28:44.538122+00:00", "nick": "vak", "message": "ev___: thank you for the hint! reading :)", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:28:59.652053+00:00", "nick": "nramirezuy", "message": "you ask for one url when you need to crawl it", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:29:22.903379+00:00", "nick": "nramirezuy", "message": "it gets crawled and the results back", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:29:30.544660+00:00", "nick": "ev___", "message": "BitMonster: I use the native xpath/lxml parsing in Scrapy, I don't use beautiful soup", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:29:41.974387+00:00", "nick": "barraponto", "message": "nramirezuy: isn't that what scrapy does right now?", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:30:16.356911+00:00", "nick": "vak", "message": "nramirezuy: thank you! are you the author of scrapyrt, by the way?", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:30:32.662099+00:00", "nick": "nramirezuy", "message": "I'm not working on it", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:30:53.692275+00:00", "nick": "vak", "message": "ok", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:31:11.568738+00:00", "nick": "ev___", "message": "https://github.com/scrapinghub/scrapyrt", "links": ["https://github.com/scrapinghub/scrapyrt"], "channel": "scrapy"},
{"date": "2015-01-13T15:31:21.429981+00:00", "nick": "ev___", "message": "have not seen this yet, it is very new", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:31:36.458186+00:00", "nick": "nramirezuy", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:31:38.652372+00:00", "nick": "nramirezuy", "message": "it is new", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:31:50.272831+00:00", "nick": "nramirezuy", "message": "I think they release it", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:32:10.361417+00:00", "nick": "nramirezuy", "message": "it used to be on bitbucket", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:33:05.846317+00:00", "nick": "BitMonster", "message": "so is my teacher right when she says Beautiful Soup is outdated?", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:33:45.449005+00:00", "nick": "barraponto", "message": "BitMonster: i think so", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:34:04.003927+00:00", "nick": "barraponto", "message": "nramirezuy: what's the difference from scrapyd?", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:34:26.401113+00:00", "nick": "nramirezuy", "message": "scrapyd is a service to run full crawls", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:34:32.068917+00:00", "nick": "nramirezuy", "message": "this allow you to do partial ones", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:34:50.582672+00:00", "nick": "barraponto", "message": "oh.", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:35:05.095144+00:00", "nick": "barraponto", "message": "kinda like scrapy shell but with an api.", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:35:12.025866+00:00", "nick": "nramirezuy", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:35:13.774005+00:00", "nick": "ev___", "message": "BitMonster: I think it is useful for small scale scripts, it's easy but slow.  Scrapy's built in html/xpath/css parsing is faster and integrated.", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:35:33.125720+00:00", "nick": "barraponto", "message": "cool. is it used for that web ui from scrapinghub?", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:35:42.950162+00:00", "nick": "barraponto", "message": "not the official service, the opensource project", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:35:55.648440+00:00", "nick": "barraponto", "message": "portia.", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:36:04.406752+00:00", "nick": "ev___", "message": "I think the scrapinghub team abandoned dev on scrapyd because they push for peopel to use their scrapycloud hosted product", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:36:22.712751+00:00", "nick": "BitMonster", "message": "yeah ive been using HtmlXPathSelector :P", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:36:40.938994+00:00", "nick": "nramirezuy", "message": "scrapyd was something from scrapy", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:36:48.275154+00:00", "nick": "barraponto", "message": "ev___: afaik, it does use scrapyd.", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:37:16.123594+00:00", "nick": "nramirezuy", "message": "we are barely using scrapyd", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:38:14.716269+00:00", "nick": "barraponto", "message": "nramirezuy: is it getting replaced with something else?", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:38:21.381107+00:00", "nick": "ev___", "message": "I use scrapy to deploy my scrapes all the time, much easier to make a curl request I find", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:38:29.169644+00:00", "nick": "ev___", "message": "scrapyd tha tis", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:39:03.923441+00:00", "nick": "ev___", "message": "i have a master scrapy server I push all my projects too and then clone the AMI on EC2 and execute the curl to schedule.json", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:39:13.682916+00:00", "nick": "ev___", "message": "haven't found a better way to manage lots of scrapy jobs at a large scale", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:39:20.511723+00:00", "nick": "vak", "message": "scrapyrt doesn't seem to match my case, because a callback there doesn't seem to support arguments. In my case i have slightly individual callbacks for different URLs and it is not what I could implement adding a new Spider for each URL...", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:40:01.181075+00:00", "nick": "nramirezuy", "message": "we have our own backend; we use VMs for runs; storage for logs, requests, items", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:40:24.926907+00:00", "nick": "ev___", "message": "I use an RDS instance on EC2 and mysql pipeline for each job, and then S3 feed_uri", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:40:40.847245+00:00", "nick": "nramirezuy", "message": "vak: we don't use arguments on scrapy we use meta", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:40:51.931559+00:00", "nick": "nramirezuy", "message": "I think you can build Requests with meta", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:42:37.463870+00:00", "nick": "BitMonster", "message": "lololol our teacher is allowing us to crawl an isolated netowkr on campus hehe", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:42:40.355271+00:00", "nick": "vak", "message": "nramirezuy: oh, i see", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:43:17.509316+00:00", "nick": "nramirezuy", "message": "let me see if I can get a dev of scrapyrt in here", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:47:21.657233+00:00", "nick": "vak", "message": "ev___: so i do smth like \"spider = MySpider();settings = get_project_settings(); crawler = Crawler(settings);crawler.configure();crawler.crawl(spider);crawler.start()\" -- that is, start my spider with empty set of start URLs and then yield Request(url, callback) when needed?", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:47:32.742057+00:00", "nick": "BitMonster", "message": "ill link my spiders for proof feeding and help when i get home", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:47:55.719574+00:00", "nick": "ev___", "message": "do any of you guys know how to have two FEED_URIs? I have a default S3 bucket I output everything to, but I want a 2nd csv file created on a local directory", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:48:04.776078+00:00", "nick": "vak", "message": "ev___: I'd expect that spider will exit if the start URLs set is empty...", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:48:08.766971+00:00", "nick": "ev___", "message": "if i do scrapy crawl spider -o file.csv, it will not dump on S3", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:48:29.993474+00:00", "nick": "ev___", "message": "I guess I could create an new Item pipeline and write row to CSV on every yield item, but think that might be clumsy?", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:49:37.400240+00:00", "nick": "ev___", "message": "vak: I only run scrapy jobs via 'scrapy crawl spider' or 'http://scrapyd/schedule.json', not via a script", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:49:43.019265+00:00", "nick": "nramirezuy", "message": "ev__: extend the middleware", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:49:43.133725+00:00", "nick": "ev___", "message": "so I am not too familiar with how that works", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:52:29.588463+00:00", "nick": "nramirezuy", "message": "vak: I asked a dev of scrapyrt to join", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:52:54.076245+00:00", "nick": "nramirezuy", "message": "if you have questions", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:53:14.783809+00:00", "nick": "nramirezuy", "message": "Pawel can answer", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:53:36.255186+00:00", "nick": "vak", "message": "thank you!", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:53:47.066790+00:00", "nick": "vak", "message": "PawelMiech: ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:53:52.058390+00:00", "nick": "nramirezuy", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:53:53.971493+00:00", "nick": "PawelMiech", "message": "hello guys", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:54:21.762039+00:00", "nick": "PawelMiech", "message": "what kind of questions are you interested in?", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:54:29.256818+00:00", "nick": "PawelMiech", "message": "we just made github repo open", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:54:50.064148+00:00", "nick": "PawelMiech", "message": "it's here: https://github.com/scrapinghub/scrapyrt", "links": ["https://github.com/scrapinghub/scrapyrt"], "channel": "scrapy"},
{"date": "2015-01-13T15:55:40.968547+00:00", "nick": "PawelMiech", "message": "there are some docs in github repo", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:55:41.081236+00:00", "nick": "barraponto", "message": "s3 feed uri?", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:56:33.907978+00:00", "nick": "nramirezuy", "message": "barraponto: ItemExporters", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:56:53.916236+00:00", "nick": "vak", "message": "PawelMiech: i've read through the README", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:57:10.060706+00:00", "nick": "vak", "message": "PawelMiech: btw, did you get the private message?", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:58:00.016033+00:00", "nick": "barraponto", "message": "oh", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T15:59:21.179482+00:00", "nick": "barraponto", "message": "PawelMiech: does scrapyrt get arguments? (like scrapy crawl spidername -a somearg=abc123)", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:00:02.855442+00:00", "nick": "PawelMiech", "message": "yes it does take arguments", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:00:05.234838+00:00", "nick": "PawelMiech", "message": "as params to api", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:00:14.813251+00:00", "nick": "barraponto", "message": "cool.", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:00:57.180974+00:00", "nick": "PawelMiech", "message": "there is one endpoint crawl.json", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:01:04.379104+00:00", "nick": "PawelMiech", "message": "which can be requested with GET and POST", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:01:16.993715+00:00", "nick": "PawelMiech", "message": "and in POST you can add more complex things in json string", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:01:53.476605+00:00", "nick": "PawelMiech", "message": "for example you can pass request meta to spider request in POST handler", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:01:54.539577+00:00", "nick": "vak", "message": "PawelMiech: so, there were two questions for me after looking at scrapyrt:", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:02:38.528321+00:00", "nick": "vak", "message": "PawelMiech: how to pass an argument to the callback (something internal request id) ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:03:15.263787+00:00", "nick": "PawelMiech", "message": "how to pass argument to callback? callback should accept self and response as argument", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:03:46.774365+00:00", "nick": "PawelMiech", "message": "if you want to have some piece of data in callback you can pass it in request meta", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:03:57.634850+00:00", "nick": "vak", "message": "PawelMiech: 2. how can I get know that there are no requests in processing to kill the server? (when there are no more URLs to fetch)", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:04:59.892297+00:00", "nick": "vak", "message": "PawelMiech: ok, thank you, with META it is now fully clear)", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:05:09.614670+00:00", "nick": "PawelMiech", "message": "for question two", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:05:14.528237+00:00", "nick": "vak", "message": "then only the 2nd question)", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:06:11.006041+00:00", "nick": "PawelMiech", "message": "when there are no more requests to parse spider is closed normally", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:06:33.368008+00:00", "nick": "PawelMiech", "message": "this works just like in normal crawl", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:06:50.549532+00:00", "nick": "PawelMiech", "message": "spider crawls, downloads requests, when nothing more is scheduled it goes into idle state", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:06:59.475282+00:00", "nick": "PawelMiech", "message": "after being idle it is closed", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:07:10.513251+00:00", "nick": "PawelMiech", "message": "spiders are opened in closed in scrapyrt just like they are in normal crawls", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:07:29.552342+00:00", "nick": "PawelMiech", "message": "* should be \"spider are opened **and** closed\"", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:07:44.989745+00:00", "nick": "PawelMiech", "message": "we are not keeping them around between requests", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:08:00.369150+00:00", "nick": "Sujay", "message": "Does anyone here work on scrapyd?", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:08:10.494628+00:00", "nick": "Sujay", "message": "If not, anyone know if there is a scrapyd community?", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:08:12.814840+00:00", "nick": "PawelMiech", "message": "I mean we are not keeping them in memory between user requests to API", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:08:35.791606+00:00", "nick": "vak", "message": "PawelMiech: you say that *spider* is closed. Do you mean the scrapyrt server?", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:08:49.054701+00:00", "nick": "PawelMiech", "message": "no just spider", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:09:03.191852+00:00", "nick": "vak", "message": "so, you mean the instance of spider is deleted?", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:09:07.188919+00:00", "nick": "PawelMiech", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:09:30.135751+00:00", "nick": "PawelMiech", "message": "server gets request - creates spider - makes request - cloeses spider - returns response", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:10:23.001363+00:00", "nick": "PawelMiech", "message": "so spiders are opened and closed while server is running", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:10:24.781663+00:00", "nick": "vak", "message": "that's clear, ok. And how could one shutdown the scrapyrt safely and be sure that no downloading is aborted?", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:11:56.360946+00:00", "nick": "PawelMiech", "message": "well there is no guarantee that downloading will not be aborted", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:12:07.270732+00:00", "nick": "vak", "message": "some healthy delay after the last request?", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:13:15.232392+00:00", "nick": "vak", "message": "or... ok. i am doing requests, so i could count the responses somehow...", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:13:26.643137+00:00", "nick": "PawelMiech", "message": "hmm so what is your use-case?", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:13:35.165198+00:00", "nick": "PawelMiech", "message": "you want to schedule request and close server after getting response?", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:13:45.670799+00:00", "nick": "vak", "message": "ok.", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:16:13.673158+00:00", "nick": "nramirezuy", "message": "sujay: there is no scrapyd community. It was a project maintained by scrapy community. But we aren't doing it anymore.", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:17:19.808201+00:00", "nick": "barraponto", "message": "Sujay: do you want to further develop scrapyd?", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:17:19.920702+00:00", "nick": "Sujay", "message": "Ah", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:17:35.021410+00:00", "nick": "Sujay", "message": "Well, I wanted to use it", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:17:35.455436+00:00", "nick": "vak", "message": "in my current case URLs are produced by external module in real-time. I need only download the page and process it with a callback *without* any new deeper crawling from those URLs.", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:17:42.766684+00:00", "nick": "Sujay", "message": "but I found a couple of issues", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:18:12.413000+00:00", "nick": "nramirezuy", "message": "You can try reporting them on the repository", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:18:14.755818+00:00", "nick": "Sujay", "message": "namely https://github.com/scrapy/scrapyd/issues/70 and I submitted a PR https://github.com/scrapy/scrapyd/pull/71", "links": ["https://github.com/scrapy/scrapyd/issues/70", "https://github.com/scrapy/scrapyd/pull/71"], "channel": "scrapy"},
{"date": "2015-01-13T16:18:29.546966+00:00", "nick": "Sujay", "message": "but there hadn't been any activity on them", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:18:37.781219+00:00", "nick": "Sujay", "message": "Ah, so scrapyd is deprecated? That's a shame.", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:18:38.600773+00:00", "nick": "vak", "message": "when external module says \"there are no more URLs\" i have to wait untill all downloads are done and stop scrapyrt.", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:18:49.721224+00:00", "nick": "vak", "message": "PawelMiech: ^^", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:19:03.494178+00:00", "nick": "nramirezuy", "message": "kinda deprecated", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:19:15.061239+00:00", "nick": "PawelMiech", "message": "vak: I think what you may find useful is max_requests param to API", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:19:16.959775+00:00", "nick": "Sujay", "message": "Ah :(", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:19:29.092750+00:00", "nick": "Sujay", "message": "Well, I've running off a forked version for now", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:19:34.279861+00:00", "nick": "Sujay", "message": "but thanks for letting me know", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:19:39.507792+00:00", "nick": "Sujay", "message": "I'll try to find an alternative.", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:19:42.361811+00:00", "nick": "PawelMiech", "message": "max_requests are for limiting amount of requests - so this will ensure you won't get too deep", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:19:55.820212+00:00", "nick": "Sujay", "message": "Was scrapyd simply not that popular? (Or not popular enough to maintain?)", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:19:59.541163+00:00", "nick": "vak", "message": "and also 3rd question: will autothrottling work with scrapyrt? I do not want to overload/abuse sites with my requests...", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:20:04.415559+00:00", "nick": "PawelMiech", "message": "for stopping scrapyrt - you need to manually stop it", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:20:37.758885+00:00", "nick": "vak", "message": "i saw max_requests already in README :)", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:20:53.409521+00:00", "nick": "PawelMiech", "message": "for autothrottling: yes it works", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:21:09.141228+00:00", "nick": "vak", "message": "wow", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:21:12.024827+00:00", "nick": "PawelMiech", "message": "all middleware and pipelines from your proejct and build in scrapy middleware should work", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:21:28.897330+00:00", "nick": "vak", "message": "cool!", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:21:56.253188+00:00", "nick": "vak", "message": "have to try it then!", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:22:01.091339+00:00", "nick": "PawelMiech", "message": "basically what you do is just add sever to your project", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:22:23.970539+00:00", "nick": "nramirezuy", "message": "It had not too much activity; and the main devs of scrapy work at scrapinghub and we don't really use scrapyd.", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:24:01.863206+00:00", "nick": "Sujay", "message": "Ah.", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:25:51.111626+00:00", "nick": "nramirezuy", "message": "Autothrottle isn't a good idea because it will use the 5sec delay everytime", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:26:32.853080+00:00", "nick": "nramirezuy", "message": "I also wonder what happens when you schedule several requests for the same spider", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:26:38.520975+00:00", "nick": "nramirezuy", "message": "they use the same instance?", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:27:49.018611+00:00", "nick": "PawelMiech", "message": "yes they do", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:28:20.650566+00:00", "nick": "Sujay", "message": "nramirezuy, barraponto: do you have any recommendations for alternatives to scrapyd?", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:28:41.232016+00:00", "nick": "Sujay", "message": "The other option I have is to roll my own \"spider starter\"", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:28:48.427187+00:00", "nick": "Sujay", "message": "or continue work on my fork", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:29:11.823570+00:00", "nick": "PawelMiech", "message": "if you have several requests for each one you'll get one spider", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:29:20.326760+00:00", "nick": "nramirezuy", "message": "I guess you can continue working on your fork. we been looking for a maintainer", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:29:32.264048+00:00", "nick": "PawelMiech", "message": "I mean one instance of spider", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:30:00.699504+00:00", "nick": "vak", "message": "PawelMiech: ok, i got it", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:30:05.303733+00:00", "nick": "nramirezuy", "message": "so I can schedule several requests to one spider instance?", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:30:47.435441+00:00", "nick": "PawelMiech", "message": "well if by \"schedule several request\" you mean make several requests to API", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:31:11.549649+00:00", "nick": "PawelMiech", "message": "then every of requests to API will generate one request for spider and create one spider", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:31:31.154404+00:00", "nick": "PawelMiech", "message": "you can't schedule a list of requests to API", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:31:50.431788+00:00", "nick": "PawelMiech", "message": "for example currenty you cannot give API list of urls to parse in GET params", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:32:40.303293+00:00", "nick": "nramirezuy", "message": "but the API could recognize if there is a spider for that request already instantiated and add it", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:32:59.218899+00:00", "nick": "PawelMiech", "message": "something like url=http://one,http://two,http://three is not supported", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:33:09.205324+00:00", "nick": "PawelMiech", "message": "in principle it could yes", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:33:19.055861+00:00", "nick": "PawelMiech", "message": "but we would have to handle all complications that arise from this", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:33:39.980105+00:00", "nick": "vak", "message": "to use my own callback one has to create the own spider class with the callback. Currently I don't see how to provide it then to scrapyrt... how my Spider could be found and imported from scrapyrt. PYTHONPATH is not enough if the name of my spider module is not known to scrapyrt...", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:34:09.810687+00:00", "nick": "PawelMiech", "message": "vak: after installing scrapyrt you need to run it in your project directory", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:34:19.183466+00:00", "nick": "PawelMiech", "message": "cd to project directory", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:34:20.962013+00:00", "nick": "PawelMiech", "message": "scrapyrt there", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:35:34.058457+00:00", "nick": "PawelMiech", "message": "nramirezruy: it could be more efficient your scenario of keeping spider, so this could be possible improvement", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:36:03.059042+00:00", "nick": "PawelMiech", "message": "altough by \"complications\" here i mean something like this", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:36:11.383612+00:00", "nick": "PawelMiech", "message": "one user schedules spider and waits for response", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:36:27.189562+00:00", "nick": "PawelMiech", "message": "other user schedules same spider with different url and also waits for response", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:36:52.025224+00:00", "nick": "PawelMiech", "message": "currently each one will get one spider and response from this spider", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:37:12.213354+00:00", "nick": "vak", "message": "PawelMiech: so, and what is spider_name  for?", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:37:42.319569+00:00", "nick": "vak", "message": "target.com_products", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:37:43.057240+00:00", "nick": "PawelMiech", "message": "spider name?", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:37:51.916963+00:00", "nick": "nramirezuy", "message": "I just feel weird the idea of instantiating a Spider instance per request.", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:37:54.369717+00:00", "nick": "PawelMiech", "message": "spider name should be name of your spider", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:37:56.109485+00:00", "nick": "vak", "message": "target is a module?", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:37:57.927897+00:00", "nick": "barraponto", "message": "i'm having issues with scrapyd deploy :(", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:38:00.644150+00:00", "nick": "nramirezuy", "message": "is like scrapy parse command", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:38:07.019360+00:00", "nick": "barraponto", "message": "it's always timing out!", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:38:40.821177+00:00", "nick": "PawelMiech", "message": "nramirezuy keep in mind that spiders can generate requests as well", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:38:41.718616+00:00", "nick": "vak", "message": "sorry, target is package?", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:39:01.575808+00:00", "nick": "PawelMiech", "message": "spider name should be name of your spider", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:39:19.267482+00:00", "nick": "PawelMiech", "message": "name attribute of spider class", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:39:30.295588+00:00", "nick": "PawelMiech", "message": "normal name you use in scrapy project", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:40:05.835053+00:00", "nick": "PawelMiech", "message": "returning to spider instances: so spiders can schedule requests as well from callbacks", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:40:30.707714+00:00", "nick": "vak", "message": "well, but how the module (the file) with my spider will be found by scrapyrt? in directory where the scrapyrt is started could be many files...", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:40:50.959119+00:00", "nick": "PawelMiech", "message": "it looks for scrapy.cfg", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:40:59.101204+00:00", "nick": "vak", "message": "auchhh....", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:42:53.669330+00:00", "nick": "PawelMiech", "message": "do you have scrapy.cfg?", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:45:27.911286+00:00", "nick": "vak", "message": "not yet)", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:46:00.457342+00:00", "nick": "vak", "message": "this is the same as in scrapy projects I guess", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:49:38.648177+00:00", "nick": "vak", "message": "PawelMiech: i see an example here: https://github.com/scrapinghub/scrapyrt/blob/ma...", "links": ["https://github.com/scrapinghub/scrapyrt/blob/master/tests/sample_data/testproject/scrapy.cfg"], "channel": "scrapy"},
{"date": "2015-01-13T16:50:45.542391+00:00", "nick": "PawelMiech", "message": "yeah", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:50:57.442988+00:00", "nick": "PawelMiech", "message": "basically you just need to give path to project settings", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:51:19.317983+00:00", "nick": "vak", "message": "PawelMiech: so it looks like i have to put the import in myproject/settings.py", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:51:44.156678+00:00", "nick": "vak", "message": "ooookkkeyy)", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:51:44.269154+00:00", "nick": "PawelMiech", "message": "no you don't need to import anything :)", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:51:50.342927+00:00", "nick": "vak", "message": "auch...", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:52:04.923834+00:00", "nick": "vak", "message": "but how? :)", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:52:06.347569+00:00", "nick": "PawelMiech", "message": "you just need to have a scrapy project", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:52:16.979915+00:00", "nick": "PawelMiech", "message": "and within this project start server", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:52:22.064025+00:00", "nick": "vak", "message": "damn. i got it )", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T16:52:35.126057+00:00", "nick": "vak", "message": "thank you!", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T17:29:25.341862+00:00", "nick": "PawelMiech", "message": "hey guys alex____ is scrapyrt dev so any questions about scrapyrt can be asked to Alex as well", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T21:25:27.018575+00:00", "nick": "sudshekhar", "message": "Hi, I am new to scrapy and to open source development in general so forgive me if this is a very silly question but I wanted to know in what cases does a travis CI build fail?", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T21:26:15.223221+00:00", "nick": "sudshekhar", "message": "I just created a pretty simple pull request (changing a documentation bug) yet the travis build seems to have failed.", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T21:27:02.584360+00:00", "nick": "sudshekhar", "message": "I am basically asking this because the error I see upon clicking the \"details\" link shows that the travis build failed with the same errors I get while trying to build scrapy from scratch and running the test cases", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T23:04:27.789834+00:00", "nick": "GaryOak_", "message": "Does anyone know about some sort of callback functionality for scrapy?", "links": [], "channel": "scrapy"},
{"date": "2015-01-13T23:05:01.509192+00:00", "nick": "GaryOak_", "message": "So that you can perform some action when your spider is completed?", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T10:12:15.685908+00:00", "nick": "deltaskelta", "message": "Hi! I've got a problem that is driving me mad, memory on one spider expands to the max, while another almost identical spider stay stable", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T10:12:38.005085+00:00", "nick": "deltaskelta", "message": "I have tried to isolate every section of code to mimic them both ways, but I can't find what is going on", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T10:13:41.394255+00:00", "nick": "deltaskelta", "message": "I suspect it is related to the outside website of one spider returning a huge amount of request objects from the crawl, but I can't find an easy way to limit these via the docs", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T10:57:46.590381+00:00", "nick": "deltaskelta", "message": "Hi! anyone alive in here?", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T11:15:27.591552+00:00", "nick": "barraponto", "message": "deltaskelta: yes", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T11:15:31.472196+00:00", "nick": "barraponto", "message": "is your code public?", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T11:20:06.008270+00:00", "nick": "deltaskelta", "message": "its not on github or anything like that, but I can put it in a pastebin or something if you want to have a look", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T11:20:25.684947+00:00", "nick": "deltaskelta", "message": "barraponto: ^^", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T11:28:27.052184+00:00", "nick": "barraponto", "message": "deltaskelta: ok", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T11:55:56.234488+00:00", "nick": "pawelmhm", "message": "deltaskelta what's the difference between spiders? are they parsing different urls?", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T12:12:42.599034+00:00", "nick": "deltaskelta", "message": "pawelmhm, yes, they are made for different urls. ill pastebin both spiders here", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T12:13:45.444406+00:00", "nick": "deltaskelta", "message": "pawelmhm, on second thought I am almost 100% sure it is the links being extracted by the different URLs, one is much more link rich and leaves 100,000+ URLs in the request memcache", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T12:14:20.931188+00:00", "nick": "deltaskelta", "message": "I can't remember the exact name of the memory cache at the moment, but it is the setting when you turn memory debug on", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T12:14:47.794236+00:00", "nick": "deltaskelta", "message": "I've been searching high and low for a way to limit the amount of URLs it takes in, but I can't come up with anything", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T15:10:36.496009+00:00", "nick": "haggi", "message": "hi, can someone help me with my spider. i want to change a dropdown on every crawled site, bevor crawling. it is \"items per Page\", i want to change from 6 to 96. it is onchange=javascript:setTimeout('__doPostBack(\\'ctl00$MainContent$ProductsOverview1$ddlArtikelProSeite\\',\\'\\')', 0)", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T15:11:52.554408+00:00", "nick": "haggi", "message": "maybe this is possible with selenium ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T16:49:23.040766+00:00", "nick": "barraponto", "message": "haggi: maybe. there's also splash", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T16:55:52.916497+00:00", "nick": "haggi", "message": "splash ? ok i wil check this out .thx", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T17:05:50.565762+00:00", "nick": "samtc", "message": "barraponto: first time I hear about splash, thats nice", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T18:59:47.075254+00:00", "nick": "sudshekhar", "message": "Hi, can someone please help me understand in what way will CODE be different from CODE 2 : https://gist.github.com/SudShekhar/41253a3f19e6...", "links": ["https://gist.github.com/SudShekhar/41253a3f19e609a9eb2e"], "channel": "scrapy"},
{"date": "2015-01-14T19:00:12.747288+00:00", "nick": "sudshekhar", "message": "I am trying to understand how ItemLoaders work and their advantages above the manual method", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T19:00:43.175920+00:00", "nick": "sudshekhar", "message": "in the gist above, the output of the two parts is different and scrapy check (spider contracts) fail in case of the item loader", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T19:20:45.995433+00:00", "nick": "barraponto", "message": "if spiders change the crawler settings concurrently, they overwrite each other :(", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T19:26:03.085158+00:00", "nick": "nramirezuy", "message": "sudshekhar: loaders manager an internal list which allows you to call the add_xpath for the same field several times.", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T19:26:39.719420+00:00", "nick": "nramirezuy", "message": "to do input and the final output loaders use processors", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T19:27:13.513582+00:00", "nick": "nramirezuy", "message": "the default one is called Identity and returns the value without touching anything", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T19:27:18.407292+00:00", "nick": "nramirezuy", "message": "even that internal list", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T19:27:56.100219+00:00", "nick": "nramirezuy", "message": "so basically you have a list inside a list on CODE 2 and CODE 1 are simple lists", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T19:28:10.567570+00:00", "nick": "nramirezuy", "message": "barraponto: what do you mean?", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T19:32:12.180148+00:00", "nick": "barraponto", "message": "nramirezuy: like, i wanted to control the path where imagespipeline sends the files", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T19:32:55.019102+00:00", "nick": "barraponto", "message": "nramirezuy: so inside the spider, i used to set the crawler.settings['store'] to something with the spider parameter in it.", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T19:33:18.277150+00:00", "nick": "barraponto", "message": "however, it fails because i run several spiders at once, seemingly in the same crawler.", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T19:33:20.227929+00:00", "nick": "barraponto", "message": "scrapyd stuff.", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T19:33:22.388505+00:00", "nick": "barraponto", "message": ":(", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T19:33:54.176419+00:00", "nick": "nramirezuy", "message": "you should use a crawler per spider", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T19:34:01.887611+00:00", "nick": "barraponto", "message": "nramirezuy: i think it's scrapyd.", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T19:34:42.260688+00:00", "nick": "barraponto", "message": "nramirezuy: because it's the same spider class..", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T19:34:49.326994+00:00", "nick": "barraponto", "message": "so maybe it reuses the crawler", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T19:34:50.926264+00:00", "nick": "barraponto", "message": "whatever.", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T19:34:51.380279+00:00", "nick": "nramirezuy", "message": "I can't help you there; never touched scrapyd :(", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T19:35:04.540537+00:00", "nick": "barraponto", "message": "i'm going into the pipeline and changing file_path", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T19:38:21.498375+00:00", "nick": "barraponto", "message": "nramirezuy: that \"info\" parameter is not documented anywhere :,(", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T19:38:38.057599+00:00", "nick": "barraponto", "message": "i know it has the spider in it (from seeing the source) but what else?", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T19:42:16.756094+00:00", "nick": "sudshekhar", "message": "nramirezuy: so if I use Join() in the output processor of the loader, will the two outputs be same?", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T19:44:16.628890+00:00", "nick": "nramirezuy", "message": "I think you should use TakeFirst as output processor", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T19:44:29.714002+00:00", "nick": "nramirezuy", "message": "Join will return a string", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T19:44:38.157093+00:00", "nick": "sudshekhar", "message": "ohh okay.", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T19:44:53.557898+00:00", "nick": "sudshekhar", "message": "Yeah got that. Let me try that once!", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T19:44:55.163796+00:00", "nick": "nramirezuy", "message": "play with it on scrapy shell", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T19:46:14.798649+00:00", "nick": "sudshekhar", "message": "okay!", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T19:50:05.216507+00:00", "nick": "barraponto", "message": "join joins. takefirst takes the first item in the list.", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T19:50:15.319460+00:00", "nick": "barraponto", "message": "kinda obvious but...", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T22:26:47.725773+00:00", "nick": "ast0", "message": "hi!", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T22:27:24.998054+00:00", "nick": "ast0", "message": "i'm looking for advice to write a Link Extractor for json, anyone willing to help me out ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T22:34:27.934455+00:00", "nick": "nramirezuy", "message": "ast0: the regex one should fit", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T22:41:21.397245+00:00", "nick": "ast0", "message": "nramirezuy: that's probably not the best option if i only need to extract links from a specific sub-object for instance", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T22:41:46.273951+00:00", "nick": "ast0", "message": "i'll give it a try however", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T22:41:59.469780+00:00", "nick": "nramirezuy", "message": "feed it with the specific", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T22:42:42.231209+00:00", "nick": "nramirezuy", "message": "whats the reason of having a linkextractor for json?", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T22:43:01.271565+00:00", "nick": "nramirezuy", "message": "do you wnat to use the canonicalize, unique, etc?", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T22:44:46.177921+00:00", "nick": "ast0", "message": "the urls i fetch send json response", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T22:45:31.803306+00:00", "nick": "ast0", "message": "i'd like to avoid parsing and browsing the json in every spider, i'd rather have a link extractor doing this job", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T22:46:33.350099+00:00", "nick": "ast0", "message": "i probably won't be using canonicalize or unique for now", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T22:48:28.605282+00:00", "nick": "sudshekhar", "message": "Hi, when declaring our own item loader, what should be the manner in which we are supposed to call the ItemLoader's init function?", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T22:49:50.213517+00:00", "nick": "sudshekhar", "message": "so that we can do, itemloader = DmozLoader(selector=sel) and then populate stuff here.", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T22:50:12.679431+00:00", "nick": "sudshekhar", "message": "assuming we have built the dmozloader class with prior knowledge of the DmozItem", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T22:51:30.081597+00:00", "nick": "nramirezuy", "message": "ast0: maybe you can extend linkextractor and build something custom. If right now is kind of rough because we dont have jsonpath in place, yet", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T22:52:22.871266+00:00", "nick": "nramirezuy", "message": "sudshekhar: are you asking if there is a different initiating with selector or with the response?", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T22:52:58.069020+00:00", "nick": "ast0", "message": "nramirezuy: yep, i anticipated that and was planning to do something rough to browse the json, that's not really a problem for me", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T22:53:32.537906+00:00", "nick": "sudshekhar", "message": "sort of. I am trying to create a custome ItemLoader. The documentation specifies that you add details about processor of different columns (name, title etc) here", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T22:54:00.799246+00:00", "nick": "nramirezuy", "message": "ast0: here is something you can use, https://gist.github.com/nramirezuy/e61e3e0a5ead...", "links": ["https://gist.github.com/nramirezuy/e61e3e0a5eadbd37eebc"], "channel": "scrapy"},
{"date": "2015-01-14T22:54:12.908255+00:00", "nick": "sudshekhar", "message": "but there's no mention of the init and other functions this inherited loader must have so that it can accept selector /response as arguments", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T22:56:08.534528+00:00", "nick": "nramirezuy", "message": "about selector/response... doing ItemLoader(sel=Selector(response)) and ItemLoader(response=response) is the same", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T22:56:43.357994+00:00", "nick": "nramirezuy", "message": "what do you mean by functions inherited ? method or are you talking about processors ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T22:57:24.127134+00:00", "nick": "sudshekhar", "message": "http://doc.scrapy.org/en/master/topics/loaders.... : Here if I want to pass a selector/response to the ProductLoader class", "links": ["http://doc.scrapy.org/en/master/topics/loaders.html#declaring-item-loaders"], "channel": "scrapy"},
{"date": "2015-01-14T22:57:27.838159+00:00", "nick": "sudshekhar", "message": "what should I do?", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T22:57:43.636031+00:00", "nick": "ast0", "message": "nramirezuy: that's great thanks", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T22:59:50.329525+00:00", "nick": "nramirezuy", "message": "http://doc.scrapy.org/en/master/topics/loaders....", "links": ["http://doc.scrapy.org/en/master/topics/loaders.html#itemloader-objects"], "channel": "scrapy"},
{"date": "2015-01-14T23:02:50.993742+00:00", "nick": "ast0", "message": "I'm also struggling to understand what the LinkExtractor interface really is. The doc says \"The only public method that every LinkExtractor has is extract_links [...]\" so I figured this example should work https://gist.github.com/as0n/520260d528bec0272dc1 but I never get the \"JLE extracting\" message, what am I doing wrong ?", "links": ["https://gist.github.com/as0n/520260d528bec0272dc1"], "channel": "scrapy"},
{"date": "2015-01-14T23:09:01.529922+00:00", "nick": "sudshekhar", "message": "nramirezuy: Thanks got it!", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T23:13:02.160471+00:00", "nick": "sudshekhar", "message": "One more thing, each processor works on a different field; is there any function or processor which gets the output of all the fields together before the load_item() returns the item?", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T23:14:26.914035+00:00", "nick": "sudshekhar", "message": "for example, if I want to drop the current item being formed if my title field comes out to be empty for any particular item, can this be done without creating the whole item first?", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T23:29:00.461543+00:00", "nick": "sudshekhar", "message": "ast0: your class works for me", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T23:29:34.310112+00:00", "nick": "sudshekhar", "message": "I just did ext = Jsonlink(\"sample route\")", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T23:29:55.190084+00:00", "nick": "sudshekhar", "message": "and called ext.extract_links(response)", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T23:30:02.865402+00:00", "nick": "sudshekhar", "message": "was this what you were asking or is it something else?", "links": [], "channel": "scrapy"},
{"date": "2015-01-14T23:31:18.207428+00:00", "nick": "ast0", "message": "yes it does, my bad, i realised my problem is with the way CrawlSpider (doesn't) process responses other than HtmlResponse", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T07:09:10.302541+00:00", "nick": "aaearon", "message": "im using a crawlspider and in parse_start_url i am returning a formrequest (i need to post some data), with a callback to a method that returns another formrequest with a callback to a method that checks ot see if i got the correct page finally", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T07:09:31.942101+00:00", "nick": "aaearon", "message": "i have my rule to try to open links on the final page but it doesnt seem to be triggering", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T07:09:35.216787+00:00", "nick": "aaearon", "message": "i havent overriden parse", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T08:23:13.871424+00:00", "nick": "mike_cmc", "message": "hi all ! noob here.. looking for some help. anyone available ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T08:32:18.469502+00:00", "nick": "mike_cmc", "message": "Ok.. I'll ask and maybe someone will answer :) I want to crawl products from a category but i don't want to follow the links found on the product page itself. The url for the category is site.com/category_name/ and the product url is site.com/brand_name/product-name-here.html . How should I define the crawling rules ?", "links": ["http://site.com/category_name/", "http://site.com/brand_name/product-name-here.html"], "channel": "scrapy"},
{"date": "2015-01-16T08:44:01.160014+00:00", "nick": "mike_cmc", "message": "anyone ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T08:44:05.756625+00:00", "nick": "mike_cmc", "message": "anyone at all ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T08:45:13.990016+00:00", "nick": "mike_cmc", "message": "i posted the question on stackoverflow also : http://stackoverflow.com/questions/27980083/how...", "links": ["http://stackoverflow.com/questions/27980083/how-to-crawl-links-with-scrapy-only-from-specific-category-and-ignore-links-on"], "channel": "scrapy"},
{"date": "2015-01-16T09:56:16.461473+00:00", "nick": "Solopher", "message": "Hi all :)", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T09:58:24.775229+00:00", "nick": "Solopher", "message": "I have a small question about the LinkExtractor", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T09:59:02.329692+00:00", "nick": "Solopher", "message": "Atm I have a start_requests function which loads the first page as a new Request so the linkextractor will pick it up", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T10:00:27.824542+00:00", "nick": "Solopher", "message": "My problem: If there is no pagination on the page, the items I want to scrape are not picked up", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T10:05:00.800085+00:00", "nick": "Solopher", "message": "Oke I already found a solution, checking if the page has pagination :)", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T10:10:42.502155+00:00", "nick": "branduren", "message": "is it possible to manually add scraped links to the \"crawl list\" ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T10:12:33.421871+00:00", "nick": "branduren", "message": "I'm not interested to crawl all links on the page", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T10:16:07.245477+00:00", "nick": "Solopher", "message": "I guess with the LinkExtractor branduren ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T10:23:16.026571+00:00", "nick": "branduren", "message": "I'm scraping a site that has hundreds of links that are not unique in any way, but I want to follow some of them.", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T10:25:53.066214+00:00", "nick": "branduren", "message": "so I was wondering if I could set follow = false. But add some specific scraped links to a list that should be followed", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T10:34:16.547375+00:00", "nick": "branduren", "message": "Solopher: did you solve your problem ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T10:41:31.406662+00:00", "nick": "branduren", "message": "can't you just add: rules = [Rule(LinkExtractor(allow=[r'site.com/([a-zA-Z0-9_-]+)/([a-zA-Z0-9_-]+)/$']), follow=False)] ... and add the category url to the start_urls ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T10:48:08.840396+00:00", "nick": "Solopher", "message": "I solved my problem yes", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T10:48:38.526457+00:00", "nick": "Solopher", "message": "I can't add it to the start_urls because I'm using a FormRequest to search", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T12:10:52.188078+00:00", "nick": "branduren", "message": "Think I solved my problem... Needed this: yield Request(my_scraped_link, callback=self.parse)", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T12:26:10.916527+00:00", "nick": "Solopher", "message": "Ahh", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T12:26:11.673721+00:00", "nick": "Solopher", "message": "great", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T12:40:19.353769+00:00", "nick": "Solopher", "message": "btw is that a dutch name? \"branduren\" ? branduren", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T12:47:36.074897+00:00", "nick": "branduren", "message": "just a nick :) but yes, I believe it's a dutch word", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T12:48:39.273365+00:00", "nick": "mike_cmc", "message": "hello", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T12:49:02.385670+00:00", "nick": "mike_cmc", "message": "anybody got a few spare minutes to help me get started using scrapy ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T12:49:24.458434+00:00", "nick": "mike_cmc", "message": "i posted my issue on stackoverflow also", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T12:49:46.730906+00:00", "nick": "mike_cmc", "message": "http://stackoverflow.com/questions/27980083/how...", "links": ["http://stackoverflow.com/questions/27980083/how-to-crawl-links-with-scrapy-only-from-specific-category-and-ignore-links-on"], "channel": "scrapy"},
{"date": "2015-01-16T12:50:40.657633+00:00", "nick": "mike_cmc", "message": "i just need some guidance so that i don't go a wrong path because i need this done fairly qick", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T12:51:37.586102+00:00", "nick": "mike_cmc", "message": "I know it's super basic what i'm trying to do but i`m new to both scrapy and python and i need a few tips to follow that will lead me to the desired outcome", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T12:52:40.517876+00:00", "nick": "mike_cmc", "message": "Anybody seeing what i'm writing ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T12:52:54.643150+00:00", "nick": "mike_cmc", "message": "i have a feeling i`m talking to myself :)", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:37:52.248976+00:00", "nick": "barraponto", "message": "mike_cmc: hi", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:38:06.630653+00:00", "nick": "mike_cmc", "message": "hello", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:38:19.509766+00:00", "nick": "Solopher", "message": "Hello", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:38:26.943053+00:00", "nick": "barraponto", "message": "mike_cmc: did you get it working?", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:38:31.609181+00:00", "nick": "mike_cmc", "message": "no", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:38:35.887345+00:00", "nick": "mike_cmc", "message": "i'm getting an error", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:38:42.318030+00:00", "nick": "mike_cmc", "message": "ImportError: No module named items", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:39:01.479906+00:00", "nick": "mike_cmc", "message": "the spider folder is called aorobot", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:39:10.186988+00:00", "nick": "mike_cmc", "message": "and the spider's name", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:39:19.581016+00:00", "nick": "mike_cmc", "message": "is also aorobot", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:39:30.771016+00:00", "nick": "barraponto", "message": "can you post your code to github or pastebin or whatever?", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:39:36.163147+00:00", "nick": "mike_cmc", "message": "ok", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:39:42.444183+00:00", "nick": "mike_cmc", "message": "just a sec", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:41:36.153925+00:00", "nick": "mike_cmc", "message": "http://pastebin.com/GXRE51Sr", "links": ["http://pastebin.com/GXRE51Sr"], "channel": "scrapy"},
{"date": "2015-01-16T13:42:11.824724+00:00", "nick": "mike_cmc", "message": "i was in the process of changing the class name", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:42:23.266270+00:00", "nick": "mike_cmc", "message": "it was class Aorobot", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:42:41.080973+00:00", "nick": "mike_cmc", "message": "and i think it's self referencing", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:42:51.062319+00:00", "nick": "mike_cmc", "message": "instead of looking in the folder name", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:42:56.027361+00:00", "nick": "mike_cmc", "message": "*folder", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:43:31.420152+00:00", "nick": "barraponto", "message": "and what's the folder name?", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:44:11.439643+00:00", "nick": "mike_cmc", "message": "aorobot", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:44:44.068160+00:00", "nick": "Solopher", "message": "from aorobot.items import Item, Field", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:44:45.847636+00:00", "nick": "Solopher", "message": "I guess?", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:44:52.549813+00:00", "nick": "barraponto", "message": "Solopher: nope, it's python", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:45:09.977513+00:00", "nick": "barraponto", "message": "mike_cmc: python will import from a local python file if it finds it there", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:45:30.934492+00:00", "nick": "Solopher", "message": "Is the filename item.py or items.py?", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:45:31.089637+00:00", "nick": "barraponto", "message": "so either your project or your file will have to change name", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:45:35.079950+00:00", "nick": "barraponto", "message": "it sucks, yeah, i know", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:45:38.561397+00:00", "nick": "mike_cmc", "message": "items", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:45:40.351064+00:00", "nick": "mike_cmc", "message": ".py", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:45:48.543601+00:00", "nick": "Solopher", "message": "from scrapy.items import Item, Field", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:46:02.431619+00:00", "nick": "mike_cmc", "message": "i have from aorobot.items import Product", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:46:13.966667+00:00", "nick": "mike_cmc", "message": "and items.py has", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:46:14.794376+00:00", "nick": "mike_cmc", "message": "from scrapy.item import Item, Field", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:46:15.138424+00:00", "nick": "mike_cmc", "message": "class Product(Item):", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:46:15.224892+00:00", "nick": "mike_cmc", "message": "product_h1 = Field()", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:46:20.915200+00:00", "nick": "barraponto", "message": "mike_cmc: yeah, python will see aorobot.py and try to import from it", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:46:30.076299+00:00", "nick": "barraponto", "message": "instead of trying to import from folder aorobot.", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:46:37.682679+00:00", "nick": "mike_cmc", "message": "what should i do", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:46:39.168436+00:00", "nick": "barraponto", "message": "just change the spider file name", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:46:47.516671+00:00", "nick": "barraponto", "message": "the spider can keep its name, the class can keep its name", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:46:50.517514+00:00", "nick": "barraponto", "message": "just change the file name.", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:47:13.637250+00:00", "nick": "mike_cmc", "message": "if i change the filename, do i need to change anything else ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:47:47.961791+00:00", "nick": "mike_cmc", "message": "i've seen in videos that the spider filename should be the same as the spider class", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:47:54.848281+00:00", "nick": "mike_cmc", "message": "is that correct ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:48:04.917627+00:00", "nick": "mike_cmc", "message": "(mandatory ?)", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:48:50.387457+00:00", "nick": "Solopher", "message": "just try? :P", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:49:04.841542+00:00", "nick": "mike_cmc", "message": "tha's what i'm doing :)", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:49:12.483516+00:00", "nick": "mike_cmc", "message": "let's see..", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:49:38.112528+00:00", "nick": "mike_cmc", "message": "is there a cache i need to clear ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:49:44.694615+00:00", "nick": "mike_cmc", "message": "it's still looking for aorobot.py", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:49:54.005627+00:00", "nick": "mike_cmc", "message": "and the filename si now aorocrawlbot.py", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:50:29.586287+00:00", "nick": "mike_cmc", "message": "BTW, also a python noob", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:50:36.734882+00:00", "nick": "mike_cmc", "message": "sorry for the idiotic questions", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:50:45.022494+00:00", "nick": "barraponto", "message": "well", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:50:47.165424+00:00", "nick": "mike_cmc", "message": "i see there are pyc files", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:50:55.929812+00:00", "nick": "mike_cmc", "message": "i thnk those are some sort of cache", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:50:56.330575+00:00", "nick": "barraponto", "message": "yeah, but python is smart in updating those", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:51:06.736378+00:00", "nick": "barraponto", "message": "are you seeing the same error?", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:51:23.172841+00:00", "nick": "barraponto", "message": "btw... what scrapy version are you using?", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:51:43.160350+00:00", "nick": "mike_cmc", "message": "0.24.4", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:52:08.954307+00:00", "nick": "Solopher", "message": "I'm creating a virtualenv now to test", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:52:17.172397+00:00", "nick": "mike_cmc", "message": "i'm running  from a virtual environment that i've previously created for installing and using portia", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:52:46.836933+00:00", "nick": "mike_cmc", "message": "the error message persists File \"/aorobot/aorobot/spiders/aorobot.py\", line 4, in <module>", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:52:46.925906+00:00", "nick": "mike_cmc", "message": "ImportError: No module named items", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:54:38.437690+00:00", "nick": "mike_cmc", "message": "yeah.. apparently python is not smart like that", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:54:45.541451+00:00", "nick": "mike_cmc", "message": "it doesn't update those pyc files", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:55:02.166253+00:00", "nick": "mike_cmc", "message": "just deleted them and it works.. i have a new error !! :))", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:55:13.685820+00:00", "nick": "Solopher", "message": "Ahh", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:55:16.498495+00:00", "nick": "Solopher", "message": "whats the new error?", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:56:00.656652+00:00", "nick": "mike_cmc", "message": "branduren was kind enough to give me a way to crawl pages of a category", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:56:02.568220+00:00", "nick": "mike_cmc", "message": "start_urls = ['http://www.aoro.ro/parfumuri/?f=%d-1-1' % (base_url, x) for x in range(1, 521)]", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:56:16.959970+00:00", "nick": "mike_cmc", "message": "but it doesn't know who base_url is", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:56:25.812700+00:00", "nick": "mike_cmc", "message": "and i have no clue", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:56:52.874683+00:00", "nick": "mike_cmc", "message": "how to fix it", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:57:13.050327+00:00", "nick": "mike_cmc", "message": "i understand what it's doing but i don't know py sintax", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:57:22.458905+00:00", "nick": "mike_cmc", "message": "and how i should fix this", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:57:47.479140+00:00", "nick": "Solopher", "message": "It fills a list with start_urls", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:57:59.875521+00:00", "nick": "mike_cmc", "message": "yeah i figured", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:58:10.433338+00:00", "nick": "mike_cmc", "message": "but name 'base_url' is not defined", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:58:22.838694+00:00", "nick": "Solopher", "message": "ahh", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:58:33.233428+00:00", "nick": "mike_cmc", "message": "should i do somethink like base_url = 'http://www.aoro.ro/'", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:58:34.337970+00:00", "nick": "mike_cmc", "message": "?", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:58:53.113861+00:00", "nick": "mike_cmc", "message": "before that call", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T13:58:53.380360+00:00", "nick": "mike_cmc", "message": "?", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:00:53.393904+00:00", "nick": "Solopher", "message": "start_urls = ['http://www.aoro.ro/parfumuri/?f=%d-1-1' % (x) for x in range(1, 521)]", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:00:58.653603+00:00", "nick": "Solopher", "message": "I guess you need something like that?", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:01:53.218280+00:00", "nick": "mike_cmc", "message": "awesome !! :D", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:02:15.658459+00:00", "nick": "mike_cmc", "message": "THANK YOU ! :D", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:02:26.018051+00:00", "nick": "Solopher", "message": ":)", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:02:53.849231+00:00", "nick": "mike_cmc", "message": "one last thing", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:02:54.597753+00:00", "nick": "mike_cmc", "message": ":)", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:03:06.012648+00:00", "nick": "Solopher", "message": "oke", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:03:06.325904+00:00", "nick": "mike_cmc", "message": "how should i output to a json file", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:03:25.918365+00:00", "nick": "mike_cmc", "message": "my parse_item function has a return item", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:04:24.958399+00:00", "nick": "Solopher", "message": "Oke", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:04:36.859581+00:00", "nick": "Solopher", "message": "You need to add", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:04:39.299791+00:00", "nick": "Solopher", "message": ".extract()", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:04:46.733470+00:00", "nick": "Solopher", "message": "item['product_h1'] = response.xpath('//h1/text()').extract()", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:04:49.248943+00:00", "nick": "Solopher", "message": "and then", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:04:54.718940+00:00", "nick": "Solopher", "message": "scrapy crawl aoro.ro -o some.json -t json", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:05:11.182962+00:00", "nick": "mike_cmc", "message": "i'll give it a try", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:07:21.633708+00:00", "nick": "mike_cmc", "message": "do i have to let the spider finish ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:07:43.822432+00:00", "nick": "mike_cmc", "message": "cause i did a cmd-x to stop it and the file is empty", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:08:12.534299+00:00", "nick": "mike_cmc", "message": "the debug log show's it's extractign ok", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:08:12.813876+00:00", "nick": "Solopher", "message": "hmm", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:08:13.625934+00:00", "nick": "mike_cmc", "message": "{'product_h1': [u'Bvlgari Man Eau\\xa0de\\xa0Toilette\\xa0pentru\\xa0barbati']}", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:08:26.458470+00:00", "nick": "mike_cmc", "message": "but it didn't write to file...", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:08:27.452518+00:00", "nick": "Solopher", "message": "You're using the scrapy crawl command?", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:08:39.884751+00:00", "nick": "mike_cmc", "message": "scrapy crawl aoro.ro -o 15_01_2014.jl -t json", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:09:08.932272+00:00", "nick": "Solopher", "message": "its working here", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:09:13.736067+00:00", "nick": "Solopher", "message": "[{\"product_h1\": [\"Promotii - Cosmetice\"]},", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:09:13.823512+00:00", "nick": "Solopher", "message": "{\"product_h1\": [\"Versace pour Homme Eau\\u00a0de\\u00a0Toilette\\u00a0pentru\\u00a0barbati\"]},", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:09:13.823553+00:00", "nick": "Solopher", "message": "{\"product_h1\": [\"Lancome Tresor Midnight Rose Eau\\u00a0De\\u00a0Parfum\\u00a0pentru\\u00a0femei\"]},", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:09:13.824062+00:00", "nick": "Solopher", "message": "{\"product_h1\": [\"Versace Versace Woman Eau\\u00a0De\\u00a0Parfum\\u00a0pentru\\u00a0femei\"]},", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:09:13.824539+00:00", "nick": "Solopher", "message": "{\"product_h1\": [\"Calvin Klein Obsession Eau\\u00a0De\\u00a0Parfum\\u00a0pentru\\u00a0femei\"]},", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:09:19.274592+00:00", "nick": "Solopher", "message": "etc.", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:09:33.589528+00:00", "nick": "mike_cmc", "message": "also i see it working", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:09:37.252164+00:00", "nick": "mike_cmc", "message": "but not writing to file", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:09:54.655237+00:00", "nick": "Solopher", "message": "what platform are you on?", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:10:20.363147+00:00", "nick": "mike_cmc", "message": "should i change from  return item to yeld item ? i have no clue what that does, but i've seen others using it", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:10:32.740204+00:00", "nick": "mike_cmc", "message": "mac os maveriks", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:13:15.760024+00:00", "nick": "Solopher", "message": "nah", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:13:23.786838+00:00", "nick": "Solopher", "message": "Is simplejson installed?", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:13:28.792476+00:00", "nick": "Solopher", "message": "In your python installation?", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:13:58.658269+00:00", "nick": "mike_cmc", "message": "i'm running scrapy crawl in a virtual environment that is used by portia", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:14:07.917323+00:00", "nick": "Solopher", "message": "Can you try: scrapy crawl aoro.ro -o 15_01_2014.xml -t xml", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:14:13.539893+00:00", "nick": "Solopher", "message": "Is that XML file filled with data?", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:14:54.747799+00:00", "nick": "mike_cmc", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:15:19.920381+00:00", "nick": "Solopher", "message": "Then there is a problem with your json :P", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:15:27.203345+00:00", "nick": "Solopher", "message": "in your virtualenv can you try: pip install simplejson", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:15:35.777141+00:00", "nick": "Solopher", "message": "brb going outside for a smoke.", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:25:41.362567+00:00", "nick": "mike_cmc", "message": "ok. .installed simplejson", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:25:50.448689+00:00", "nick": "mike_cmc", "message": "and it's outputing to file now", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:26:06.742138+00:00", "nick": "mike_cmc", "message": "but i need to set an encoding or something", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:26:47.124039+00:00", "nick": "mike_cmc", "message": "because i get wierd \\u00a0 in the output", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:30:14.940612+00:00", "nick": "Solopher", "message": "see this post: http://stackoverflow.com/questions/9181214/scra...", "links": ["http://stackoverflow.com/questions/9181214/scrapy-text-encoding"], "channel": "scrapy"},
{"date": "2015-01-16T14:30:26.005959+00:00", "nick": "Solopher", "message": "and then in the \"Update\" part", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:31:31.373302+00:00", "nick": "mike_cmc", "message": "nice", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:32:36.023526+00:00", "nick": "mike_cmc", "message": "any tips on how i should require a field to be present in the result of the request to pass on the callback function that outputs the items ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:37:20.304700+00:00", "nick": "sudshekhar", "message": "mike_cmc : you might use a try catch block for it. if the try item['required field'] is invoked, you should yield the item", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:37:32.820845+00:00", "nick": "sudshekhar", "message": "else just do : \"yield None\"", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:37:59.838378+00:00", "nick": "sudshekhar", "message": "there must be a better way though, I just used this as it worked :P", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:38:42.611400+00:00", "nick": "mike_cmc", "message": "i'll try to lookup the implementation the guys at scrapinghub used in portia", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:39:03.168247+00:00", "nick": "mike_cmc", "message": "i thought it was a pretty cool feature", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:39:20.340875+00:00", "nick": "mike_cmc", "message": "from what i understood portia is also built with scrapely", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:39:35.359173+00:00", "nick": "mike_cmc", "message": "and i thnk that's scrapely functionality", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:40:53.969282+00:00", "nick": "aaearon", "message": "how do i kick off a spider within a scrapy shell", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:41:22.828594+00:00", "nick": "mike_cmc", "message": "scrapy crawl the_spider_name", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:41:37.085089+00:00", "nick": "mike_cmc", "message": "to get the spidername do : scrapy list", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:42:20.300183+00:00", "nick": "aaearon", "message": "can i crawl a local file? i guess it wouldnt really make sense in some instances", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:45:30.730053+00:00", "nick": "nramirezuy", "message": "aaearon: you can; use file:// scheme", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T14:45:45.015934+00:00", "nick": "aaearon", "message": "oh thats great", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T15:05:21.700932+00:00", "nick": "aaearon", "message": "so i have a single url with about a thousand different combonations of post parameters. whats the best way to go about this?", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T15:36:32.580610+00:00", "nick": "mike_cmc", "message": "how can i extract al the text from a table's td's and pass it to an item ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T15:38:24.762393+00:00", "nick": "nramirezuy", "message": "using xpath", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T15:38:40.597019+00:00", "nick": "mike_cmc", "message": "i tried this response.xpath('//*[@id=\"variants\"]...", "links": ["mailto:response.xpath('//*[@id=\"variants\"]/tbody//tr//td/h2/a/text()').extract()"], "channel": "scrapy"},
{"date": "2015-01-16T15:38:50.021161+00:00", "nick": "mike_cmc", "message": "and when i do print", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T15:38:52.733199+00:00", "nick": "mike_cmc", "message": "i get []", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T15:39:08.382345+00:00", "nick": "mike_cmc", "message": "i'm trying this in scrapy shell", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T15:39:28.263034+00:00", "nick": "mike_cmc", "message": "it's a table with the id variants", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T15:39:37.666302+00:00", "nick": "nramirezuy", "message": "you are extracting the text inside the links that are inside a h2 element", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T15:40:03.171786+00:00", "nick": "mike_cmc", "message": "that's what i'm trying", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T15:40:32.602056+00:00", "nick": "mike_cmc", "message": "but like i said.. print item returns []", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T15:40:34.659485+00:00", "nick": "nramirezuy", "message": "then try removing /tbody", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T15:40:49.710363+00:00", "nick": "nramirezuy", "message": "test it against scrapy shell", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T15:40:50.982259+00:00", "nick": "mike_cmc", "message": "aaa.. that might be it", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T15:40:59.179667+00:00", "nick": "mike_cmc", "message": "google chrome copy paste..", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T15:41:03.071274+00:00", "nick": "mike_cmc", "message": "just a sec", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T15:42:14.732042+00:00", "nick": "mike_cmc", "message": "it worked for a single row", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T15:42:24.422733+00:00", "nick": "mike_cmc", "message": "/*[@id=\"variants\"]/tr[1]/td[2]/h2/a", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T15:42:36.321932+00:00", "nick": "mike_cmc", "message": "but when i deleted [1] and added another /", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T15:42:50.216940+00:00", "nick": "mike_cmc", "message": "/*[@id=\"variants\"]//tr/td[2]/h2/a", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T15:42:59.786134+00:00", "nick": "mike_cmc", "message": "it returned empty", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T15:43:11.355218+00:00", "nick": "mike_cmc", "message": "did i do something wrong ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T15:43:21.639260+00:00", "nick": "mike_cmc", "message": "isn't that the way to get all tr's ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T15:43:30.220464+00:00", "nick": "mike_cmc", "message": "by adding //tr", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T15:43:32.968735+00:00", "nick": "mike_cmc", "message": "?", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T15:43:54.132720+00:00", "nick": "nramirezuy", "message": "the xpath is fine; verify the html", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T15:45:11.469675+00:00", "nick": "aaearon", "message": "yeah tbody really screwed me for a couple hours", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T15:46:04.462674+00:00", "nick": "nramirezuy", "message": "when you have doubts the best is to check on scrapy shell", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T15:49:40.409082+00:00", "nick": "mike_cmc", "message": "hm..", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T15:49:45.460287+00:00", "nick": "mike_cmc", "message": "can't get it to work", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T15:49:59.979295+00:00", "nick": "mike_cmc", "message": "i stripped everyting down to the id", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T15:50:18.476542+00:00", "nick": "mike_cmc", "message": ">>> title2 = response.xpath('//*[@id=\"variants\"]...", "links": ["mailto:response.xpath('//*[@id=\"variants\"]/text()').extract()"], "channel": "scrapy"},
{"date": "2015-01-16T15:50:18.750024+00:00", "nick": "mike_cmc", "message": ">>> print title2", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T15:50:18.836558+00:00", "nick": "mike_cmc", "message": "[]", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T15:50:36.073798+00:00", "nick": "barraponto", "message": "mike_cmc: what's the url?", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T15:50:53.607773+00:00", "nick": "mike_cmc", "message": "http://www.aoro.ro/gloria-vanderbilt/vanderbilt...", "links": ["http://www.aoro.ro/gloria-vanderbilt/vanderbilt-eau-de-toilette-pentru-femei/"], "channel": "scrapy"},
{"date": "2015-01-16T15:51:30.908669+00:00", "nick": "nramirezuy", "message": "mike_cmc: /text() is the direct text inside the element try //text()", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T15:52:27.254656+00:00", "nick": "mike_cmc", "message": "aha !", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T15:52:28.022117+00:00", "nick": "mike_cmc", "message": ":D", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T15:52:30.076482+00:00", "nick": "mike_cmc", "message": "thanks", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T15:52:33.421784+00:00", "nick": "mike_cmc", "message": "didn't know that", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T15:53:12.999877+00:00", "nick": "mike_cmc", "message": "how can i get the values from the second and third columns ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T15:53:26.484070+00:00", "nick": "nramirezuy", "message": "[N]", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T15:53:30.350346+00:00", "nick": "nramirezuy", "message": "on the td", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T15:53:44.201187+00:00", "nick": "mike_cmc", "message": "response.xpath('//*[@id=\"variants\"]...", "links": ["mailto:response.xpath('//*[@id=\"variants\"]//td[2]//text()').extract()"], "channel": "scrapy"},
{"date": "2015-01-16T15:54:20.900333+00:00", "nick": "mike_cmc", "message": "i just get the first td", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T15:54:38.101206+00:00", "nick": "mike_cmc", "message": "correcton * second td from the first tr", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T15:55:34.715934+00:00", "nick": "nramirezuy", "message": "//tr[1]//td[2]", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T15:58:41.263812+00:00", "nick": "mike_cmc", "message": "i was aiming for //tr/tr[2]", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T15:58:42.724441+00:00", "nick": "mike_cmc", "message": ":)", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T16:00:44.862754+00:00", "nick": "mike_cmc", "message": "can i add something before/after .extract() to replace wierd utf-8 characters like \\xa0", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T16:00:50.854679+00:00", "nick": "mike_cmc", "message": "?", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T16:02:24.981320+00:00", "nick": "mike_cmc", "message": "just tried extract().replace(u'\\xa0', u' ')", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T16:02:25.090402+00:00", "nick": "mike_cmc", "message": "and got", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T16:02:25.479239+00:00", "nick": "mike_cmc", "message": "Traceback (most recent call last):", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T16:02:25.479284+00:00", "nick": "mike_cmc", "message": "File \"<console>\", line 1, in <module>", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T16:02:25.479760+00:00", "nick": "mike_cmc", "message": "AttributeError: 'list' object has no attribute 'replace'", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T16:12:09.853338+00:00", "nick": "sudshekhar", "message": "mike_cmc: extract() has returned a list. you need to iterate over it and replace all the string", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T16:13:11.742423+00:00", "nick": "mike_cmc", "message": "i won't bother.. i got a pipeline that fixes that in the final output. it was just bugging me but i'll survive :)", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T16:22:02.528241+00:00", "nick": "mike_cmc", "message": "sudshekhar how can i remove something from a string i just extracted", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T16:22:14.042810+00:00", "nick": "mike_cmc", "message": "i mean .. in the same line with (...).extract()", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T16:22:35.169051+00:00", "nick": "mike_cmc", "message": "do something like re.sub('str to remove','',str)", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T16:23:02.767986+00:00", "nick": "mike_cmc", "message": "(sorry for the noob question.. just starting out with python)", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T16:24:24.581715+00:00", "nick": "sudshekhar", "message": "str.replace(\"string to replace\", \"\")", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T16:24:39.864152+00:00", "nick": "sudshekhar", "message": "this will give you a new string which you can now store in some var and use", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T16:26:15.697554+00:00", "nick": "mike_cmc", "message": "how can i iterate a list ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T16:26:28.936438+00:00", "nick": "mike_cmc", "message": "to do that to all the results", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T16:27:01.619230+00:00", "nick": "mike_cmc", "message": "i get more than one result after the extract and that doesn't work as you said, because it's a list", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T16:27:05.627733+00:00", "nick": "mike_cmc", "message": "not a simple string", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T16:27:20.109948+00:00", "nick": "sudshekhar", "message": "for x in lst:", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T16:27:31.497174+00:00", "nick": "sudshekhar", "message": "^ this will let you loop through the whole list one by one", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T16:28:19.446945+00:00", "nick": "mike_cmc", "message": "can i do that in one row ? i mean after the .extract()", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T16:28:21.311676+00:00", "nick": "sudshekhar", "message": "if you're trying to do what I say , do this : Youritem['key'] = [x.replace(\"part to replace\",\"\") for x in Youritem['key'] ]", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T16:28:35.254787+00:00", "nick": "mike_cmc", "message": "aha", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T16:28:36.711742+00:00", "nick": "sudshekhar", "message": "*said", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T16:28:49.901995+00:00", "nick": "mike_cmc", "message": "so first i extract then iterate", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T16:28:52.709340+00:00", "nick": "mike_cmc", "message": "right ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T16:28:54.484651+00:00", "nick": "sudshekhar", "message": "yeah", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T16:28:56.541932+00:00", "nick": "mike_cmc", "message": "aha", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T16:29:17.666964+00:00", "nick": "mike_cmc", "message": "thanks... thought there was a shorthand way to do it in one line", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T16:29:23.267456+00:00", "nick": "sudshekhar", "message": "there are better ways to do it, like if you're using a item pipeline, but this should work as a basic solution", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T17:00:38.708488+00:00", "nick": "sudshekhar", "message": "nramirezuy: Anything you believe a newbie to the scrapy code can contribute to?", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T17:01:00.275462+00:00", "nick": "sudshekhar", "message": "I have been looking at source code for quite sometime now, but am not sure where to start contributing", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T17:01:04.002823+00:00", "nick": "sudshekhar", "message": "*contributing", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T17:06:00.611811+00:00", "nick": "barraponto", "message": "sudshekhar: take a look at the issue queue", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T17:06:49.170156+00:00", "nick": "sudshekhar", "message": "barraponto: there seem to very few issues without a PR already and most of these haven't been merged and/or commented upon. So I am not sure what to make of it", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T17:15:10.208564+00:00", "nick": "sudshekhar", "message": "for example: can I try to work on this issue: https://github.com/scrapy/scrapy/issues/1005 . seems interesting and might also be useful for people in general", "links": ["https://github.com/scrapy/scrapy/issues/1005"], "channel": "scrapy"},
{"date": "2015-01-16T17:16:05.055634+00:00", "nick": "sudshekhar", "message": "there are jsonpath libraries that work similar to xpath (https://github.com/kennknowles/python-jsonpath-rw) which can be used to provide a robust and useful solution", "links": ["https://github.com/kennknowles/python-jsonpath-rw"], "channel": "scrapy"},
{"date": "2015-01-16T17:18:42.766150+00:00", "nick": "barraponto", "message": "sudshekhar: take a look at jmespath too", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T17:18:44.960534+00:00", "nick": "barraponto", "message": "http://jmespath.org/tutorial.html", "links": ["http://jmespath.org/tutorial.html"], "channel": "scrapy"},
{"date": "2015-01-16T17:18:52.546142+00:00", "nick": "barraponto", "message": "it is used by aws/boto", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T17:19:00.354045+00:00", "nick": "barraponto", "message": "and also by the awesome jq command line", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T17:20:39.740379+00:00", "nick": "sudshekhar", "message": "yeah, this looks really good too", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T17:21:09.376399+00:00", "nick": "sudshekhar", "message": "I will look at it properly once", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T17:21:40.675384+00:00", "nick": "sudshekhar", "message": "however, I was thinking that the json handler should typically be a separate class by itself (as in a subclass of Itemloader)", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T17:22:03.426768+00:00", "nick": "sudshekhar", "message": "because as far as I know, you won't be using xpath and jsonpath in the same place", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T17:22:12.604850+00:00", "nick": "sudshekhar", "message": "does this make sense or am i missing something?", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T17:22:54.483722+00:00", "nick": "sudshekhar", "message": "* this is in reference to the suggestion (made in the PR) of adding new methods to the item loader class", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T17:31:38.376781+00:00", "nick": "barraponto", "message": "yes, it should be a separate ItemLoader class", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T17:33:16.276339+00:00", "nick": "sudshekhar", "message": "okay.. I will try to create something along these lines then.", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T17:33:29.549898+00:00", "nick": "sudshekhar", "message": "thanks for your help :)", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T17:37:37.680598+00:00", "nick": "nramirezuy", "message": "I think it should be the same class. Last changes on ItemLoader and Selector were unification", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T17:38:57.622375+00:00", "nick": "nramirezuy", "message": "for example what if you want to parse json inside a xml/html element", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T17:39:59.735768+00:00", "nick": "nramirezuy", "message": "I would expect to do something like selector.xpath('/element/text()').jpath('./data/whatIwant')", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T17:42:11.082608+00:00", "nick": "sudshekhar", "message": "hmm. then what about the path being given as a selector?", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T17:42:35.094822+00:00", "nick": "sudshekhar", "message": "l.add_path('name', '//div[@class=\"product_name\"]',...)", "links": ["mailto:'//div[@class=\"product_name\"]',\"./jpathData\""], "channel": "scrapy"},
{"date": "2015-01-16T17:42:53.258738+00:00", "nick": "sudshekhar", "message": "where the last optional part specifies the json request (if any)", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T17:44:01.042830+00:00", "nick": "nramirezuy", "message": "You need to implement a Selector first", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T17:44:33.214710+00:00", "nick": "nramirezuy", "message": "otherwise you could try to do a PR with a processor", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T17:44:54.709237+00:00", "nick": "nramirezuy", "message": "that get text, parse the json and try to get the path", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T17:48:07.795300+00:00", "nick": "sudshekhar", "message": "hmm. Okay! I will try and see which of these two options can work and will get back to you.", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T17:49:40.646006+00:00", "nick": "sudshekhar", "message": "the processor shouldn't be very hard to create specially if external libararies can be utilized. So another question: using external libraries like jmespath", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T17:49:42.149767+00:00", "nick": "sudshekhar", "message": "is okay?", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T17:53:12.083892+00:00", "nick": "nramirezuy", "message": "yes it is; but we wont them required", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T17:55:55.690489+00:00", "nick": "sudshekhar", "message": "okay. I will try and write some code and get back to you.", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T19:24:05.182861+00:00", "nick": "mike_cmc", "message": "hell", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T19:24:06.893472+00:00", "nick": "mike_cmc", "message": "hello", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T19:24:07.745035+00:00", "nick": "mike_cmc", "message": ":)", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T19:24:09.073664+00:00", "nick": "mike_cmc", "message": "sorry", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T19:24:14.989741+00:00", "nick": "mike_cmc", "message": "anybody here ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T19:24:44.965819+00:00", "nick": "mike_cmc", "message": "i have more of a python question than a scrapy one", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T19:24:55.427688+00:00", "nick": "mike_cmc", "message": "i have this item", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T19:24:56.236323+00:00", "nick": "mike_cmc", "message": "item['product_variant_description'] = response.xpath('//*[@id=\"variants\"]...", "links": ["mailto:response.xpath('//*[@id=\"variants\"]//tr/td[2]/p/text()').extract()"], "channel": "scrapy"},
{"date": "2015-01-16T19:26:15.694833+00:00", "nick": "mike_cmc", "message": "which gets me", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T19:26:15.996671+00:00", "nick": "mike_cmc", "message": "'product_variant_description': [u':MEX0009',", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T19:26:15.996719+00:00", "nick": "mike_cmc", "message": "u'Cod:MEX0194',", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T19:26:15.997241+00:00", "nick": "mike_cmc", "message": "u'Cod:MEX0042',", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T19:26:15.997609+00:00", "nick": "mike_cmc", "message": "u'some string i want to keep',", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T19:26:16.550198+00:00", "nick": "mike_cmc", "message": "u'Cod:MEX0248'],", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T19:26:25.445964+00:00", "nick": "mike_cmc", "message": "and i want to iterate over it", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T19:26:54.646379+00:00", "nick": "mike_cmc", "message": "and have all the strings beginning with Cod: removed", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T19:27:01.717812+00:00", "nick": "mike_cmc", "message": "so it would look something like", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T19:27:35.808533+00:00", "nick": "mike_cmc", "message": "'product_variant_description': [u'',", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T19:27:36.046310+00:00", "nick": "mike_cmc", "message": "u'',", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T19:27:36.046366+00:00", "nick": "mike_cmc", "message": "u'',", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T19:27:36.047284+00:00", "nick": "mike_cmc", "message": "u'some string i want to keep',", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T19:27:36.047798+00:00", "nick": "mike_cmc", "message": "u''],", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T19:27:43.394720+00:00", "nick": "mike_cmc", "message": "how would i go about doing that ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T19:27:57.676466+00:00", "nick": "mike_cmc", "message": "i did item['product_variant_description'] = [x.re.sub('(\\bCod:\\b.*$)','')for x in item['product_variant_description']]", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T19:28:06.235609+00:00", "nick": "mike_cmc", "message": "but that doesn't seem to do the trick", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T19:28:12.787343+00:00", "nick": "mike_cmc", "message": "it just returns empty", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T19:28:37.037232+00:00", "nick": "mike_cmc", "message": "i mean it breaks the code : exceptions.AttributeError: 'unicode' object has no attribute 're'", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T19:28:51.603749+00:00", "nick": "mike_cmc", "message": "ideeas ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T19:28:53.827242+00:00", "nick": "mike_cmc", "message": "suggestions ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T19:39:36.445287+00:00", "nick": "barraponto", "message": "mike_cmc: there are many ways", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T19:39:41.863151+00:00", "nick": "barraponto", "message": "my favorite is list comprehension", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T19:39:47.491410+00:00", "nick": "barraponto", "message": "should be something like", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T19:40:11.883665+00:00", "nick": "barraponto", "message": "[value for value in list_of_values if not value.startswith('Cod:')]", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T20:01:22.111188+00:00", "nick": "mike_cmc", "message": "barraponto still here ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T20:02:15.478790+00:00", "nick": "mike_cmc", "message": "i just realised .. i need to have the string inside one of the other values", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T20:02:20.984174+00:00", "nick": "mike_cmc", "message": "to be more specific", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T20:02:32.237806+00:00", "nick": "barraponto", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T20:02:51.929784+00:00", "nick": "mike_cmc", "message": "i have this page", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T20:02:52.550158+00:00", "nick": "mike_cmc", "message": "http://www.aoro.ro/mexx/ice-touch-man-eau-de-to...", "links": ["http://www.aoro.ro/mexx/ice-touch-man-eau-de-toilette-pentru-barbati/"], "channel": "scrapy"},
{"date": "2015-01-16T20:03:10.301351+00:00", "nick": "mike_cmc", "message": "and for the 3rd product variant", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T20:03:30.939991+00:00", "nick": "mike_cmc", "message": "i want to store the text \"Parfumul este ambalat ...\"", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T20:04:32.996232+00:00", "nick": "mike_cmc", "message": "for the others that don't have that description i will just save the sku (Cod:XXXXX)", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T20:07:30.819198+00:00", "nick": "barraponto", "message": "mike_cmc: response.css('#variants .info p::text') works for me", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T20:09:02.582413+00:00", "nick": "mike_cmc", "message": "i just ran that in scrapy shell", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T20:09:20.112634+00:00", "nick": "mike_cmc", "message": "and i have no clue how to use the data it spitted out", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T20:12:18.032910+00:00", "nick": "mike_cmc", "message": "barraponto: here's how i would like the data to look", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T20:12:19.149948+00:00", "nick": "mike_cmc", "message": "http://pastebin.com/ZxLdVP4u", "links": ["http://pastebin.com/ZxLdVP4u"], "channel": "scrapy"},
{"date": "2015-01-16T20:17:08.334022+00:00", "nick": "barraponto", "message": "mike_cmc: ['' if description.startsswith('Cod') else description for description in descriptions]", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T20:18:20.147362+00:00", "nick": "mike_cmc", "message": "but that would give me 5 items in the list 4 of wich will be empty and the third will be the string", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T20:19:01.420200+00:00", "nick": "mike_cmc", "message": "i think i need to add the string as a sub item of the third item", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T20:35:28.630536+00:00", "nick": "barraponto", "message": "your example was a list with 4 empty strings and one actual content", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T21:02:16.833651+00:00", "nick": "sudshekhar", "message": "Hi, on running tox -- tests/test_contrib_loader.py , I am getting : Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T21:02:24.794161+00:00", "nick": "sudshekhar", "message": "ERROR:   py27: could not install deps [-rrequirements.txt, boto, Pillow, django, leveldb, -rtests/requirements.txt]", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T21:02:43.018994+00:00", "nick": "sudshekhar", "message": "any idea on how to handle this error?", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T21:05:19.016664+00:00", "nick": "sudshekhar", "message": "I have installed the dependencies using pip but not sure what else to do", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T23:57:03.325975+00:00", "nick": "rxxxxxxxxx", "message": "Hi - new user of Scrapy here. I'm really just getting started...", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T23:57:30.174920+00:00", "nick": "rxxxxxxxxx", "message": "After advice if anyone can offer it. Not looking for tech support type help, just whether or not I'm going down the right track", "links": [], "channel": "scrapy"},
{"date": "2015-01-16T23:58:10.721566+00:00", "nick": "rxxxxxxxxx", "message": "Is it possible to use Scrapy to retrieve data from properly documented APIs, and then return that data to a CSV file for later graphing?", "links": [], "channel": "scrapy"},
{"date": "2015-01-17T08:43:22.097226+00:00", "nick": "branduren", "message": "if I have: request = scrapy.Request(\"http://fx.com/page.html\";, callback=self.parse_page2) ... and parse_page2 returns an item ... how can I get the item object from request ?", "links": ["http://scrapy.Request(\"http://fx.com/page.html\""], "channel": "scrapy"},
{"date": "2015-01-17T09:59:09.720201+00:00", "nick": "aaearon", "message": "item = MyItem()", "links": [], "channel": "scrapy"},
{"date": "2015-01-17T09:59:12.090636+00:00", "nick": "aaearon", "message": "yield Item", "links": [], "channel": "scrapy"},
{"date": "2015-01-17T09:59:15.548244+00:00", "nick": "aaearon", "message": "err yield item", "links": [], "channel": "scrapy"},
{"date": "2015-01-17T09:59:21.370156+00:00", "nick": "aaearon", "message": "is how my 'parse_page2' looks", "links": [], "channel": "scrapy"},
{"date": "2015-01-17T09:59:30.564294+00:00", "nick": "aaearon", "message": "if i understand", "links": [], "channel": "scrapy"},
{"date": "2015-01-17T10:11:51.374743+00:00", "nick": "aaearon", "message": "is this redirect harmless? http://pastebin.com/ceTeT0Ux I'm using a CrawlSpider to parse a list of results to get the page in the first place", "links": ["http://pastebin.com/ceTeT0Ux"], "channel": "scrapy"},
{"date": "2015-01-17T10:32:59.350346+00:00", "nick": "yogicxl", "message": "Hi, i am new to scrapy and I wish to explore the codebase. How do i get started?", "links": [], "channel": "scrapy"},
{"date": "2015-01-17T12:37:18.675896+00:00", "nick": "aaearon", "message": "whats the trick with chaining two post requests together? for example, first post is a search with the required form data and the second is another post request that changes the amount of returned results", "links": [], "channel": "scrapy"},
{"date": "2015-01-17T12:37:51.344884+00:00", "nick": "aaearon", "message": "at a point, the crawlspider calls the first post request and runs the rules on that", "links": [], "channel": "scrapy"},
{"date": "2015-01-17T19:45:29.353228+00:00", "nick": "aaearon", "message": "can i emulate post requests in the shell?", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T02:23:12.984741+00:00", "nick": "scrappy", "message": "Hi there, Is there a way to start from a URL and go 3 levels but NOT follow any URLS that are its parents?", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T02:36:30.208010+00:00", "nick": "barraponto", "message": "scrappy: well. what is a parent?", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T02:37:07.623000+00:00", "nick": "scrappy", "message": "I guess because of dynamic links today, its difficult to figure that out isnt it?", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T02:37:39.013614+00:00", "nick": "scrappy", "message": "Not all sites follow a directory path :/", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T02:38:15.052372+00:00", "nick": "scrappy", "message": "barraponto: what about depth limit?", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T02:39:10.535519+00:00", "nick": "barraponto", "message": "scrappy: well, using CrawlSpider it might get tricky", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T02:39:47.163197+00:00", "nick": "barraponto", "message": "scrappy: but a regular spider can have its own link selection logic", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T02:40:13.974267+00:00", "nick": "barraponto", "message": "scrappy: since it has the response.url context, it can just check the links and see if they're parents of current url", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T02:41:27.696464+00:00", "nick": "scrappy", "message": "you mean parents of start url?", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T02:55:21.239917+00:00", "nick": "barraponto", "message": "scrappy: no, of the currently parsed url", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T10:43:40.696730+00:00", "nick": "soul-d", "message": "bug report in the tutorial  you are defining a function albeit as string  before the function", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T10:44:05.126845+00:00", "nick": "soul-d", "message": "i soend 30 min to find  that 'parse_torrent'", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T10:44:27.626312+00:00", "nick": "soul-d", "message": "because my laptop screen is in front other screen lol", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T11:45:51.758760+00:00", "nick": "soul-d", "message": "how do i debug  xpaths in a shell", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T12:18:16.635822+00:00", "nick": "mike_cmc", "message": "hi!", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T12:18:41.144449+00:00", "nick": "mike_cmc", "message": "how can i select an item that has two classes ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T12:19:11.011916+00:00", "nick": "mike_cmc", "message": "i have a div that has a class \"item\" and the last div has \"item nextpage\"", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T12:19:33.870117+00:00", "nick": "mike_cmc", "message": "i tryed /div[@class=\"nextpage\"]/", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T12:19:41.038285+00:00", "nick": "mike_cmc", "message": "but it doesn't work", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T12:20:35.911440+00:00", "nick": "mike_cmc", "message": "rubber ducky .. this works :)) /div[@class=\"item nextpage\"]/", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T12:22:12.211392+00:00", "nick": "mike_cmc", "message": "but now i'm stuck again.. i'm trying to yield a request using", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T12:22:15.911200+00:00", "nick": "mike_cmc", "message": "def parse(self,response):", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T12:22:15.911237+00:00", "nick": "mike_cmc", "message": "sel = Selector(response)", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T12:22:15.911680+00:00", "nick": "mike_cmc", "message": "link = sel.xpath('//*[@id=\"products\"]/div[@class=\"item nextpage\"]/a/@href').extract()", "links": ["mailto:nextpage\"]/a/@href').extract("], "channel": "scrapy"},
{"date": "2015-01-18T12:22:15.912028+00:00", "nick": "mike_cmc", "message": "yield Request(link, callback=self.parse_linkpage)", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T12:22:35.927030+00:00", "nick": "mike_cmc", "message": "and i get  raise TypeError('Request url must be str or unicode, got %s:' % type(url).__name__)", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T12:22:36.013746+00:00", "nick": "mike_cmc", "message": "exceptions.TypeError: Request url must be str or unicode, got list:", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T12:49:28.750054+00:00", "nick": "mike_cmc", "message": "anybody available ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T13:38:14.279254+00:00", "nick": "aaearon", "message": "mike_cmc", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T13:38:23.361083+00:00", "nick": "mike_cmc", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T13:38:26.919154+00:00", "nick": "aaearon", "message": "in my experience, when dealing with xpaths, everything is returned as a list", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T13:38:36.409404+00:00", "nick": "aaearon", "message": "so sel.xpath('//*[@id=\"products\"]/div[@class=\"item nextpage\"]/a/@href')[0].extract()", "links": ["mailto:nextpage\"]/a/@href')[0].extract("], "channel": "scrapy"},
{"date": "2015-01-18T13:38:43.509122+00:00", "nick": "aaearon", "message": "or sel.xpath('//*[@id=\"products\"]/div[@class=\"item nextpage\"]/a/@href').extract()[0] would work too i believe", "links": ["mailto:nextpage\"]/a/@href').extract()[0"], "channel": "scrapy"},
{"date": "2015-01-18T13:38:53.155581+00:00", "nick": "mike_cmc", "message": "yeah.. moved on since that issue", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T13:38:59.409418+00:00", "nick": "mike_cmc", "message": "now something else is buggin me", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T13:39:56.091615+00:00", "nick": "mike_cmc", "message": "i'm trying to build a crawler that will take all the products from a page, visit them ,extract some data, and then visit the \"next page\" link on the product listing", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T13:40:37.619814+00:00", "nick": "mike_cmc", "message": "i've tried a simple CrawlSpider but for some reason it misses a couple products", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T13:40:45.009156+00:00", "nick": "mike_cmc", "message": "and i can't figure out why", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T13:40:50.759105+00:00", "nick": "aaearon", "message": "hmm", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T13:41:11.580590+00:00", "nick": "mike_cmc", "message": "and i figured i'd build a crawlspider of my onw", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T13:41:28.456375+00:00", "nick": "mike_cmc", "message": "PM ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T13:41:39.406164+00:00", "nick": "aaearon", "message": "is the product details on two pages or the product listenings?", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T13:41:58.685757+00:00", "nick": "aaearon", "message": "i was in a similiar situation and what i was able to do was get all the results on a single page", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T13:42:35.899130+00:00", "nick": "mike_cmc", "message": "the product listing is this (also my start_urls) http://www.aoro.ro/parfumuri/", "links": ["http://www.aoro.ro/parfumuri/"], "channel": "scrapy"},
{"date": "2015-01-18T13:42:39.084216+00:00", "nick": "aaearon", "message": "in my case i made a post request to get a result, then made a second post request to show all results (the page gave me options of 10,15,20,25 but i was able to do 1000)", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T13:42:53.770576+00:00", "nick": "aaearon", "message": "ah", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T13:42:58.387930+00:00", "nick": "mike_cmc", "message": "i'm trying to get all the links from the 15 pages", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T13:43:02.304439+00:00", "nick": "mike_cmc", "message": "products sorry", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T13:43:24.109523+00:00", "nick": "mike_cmc", "message": "follow them, get the data and then go on to the next page", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T13:43:46.232339+00:00", "nick": "mike_cmc", "message": "which i want to access by following the last link (the one after the 15'th product)", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T13:44:09.522754+00:00", "nick": "mike_cmc", "message": "i've managed to build a spider that gets all the links from all the 557 pages", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T13:44:15.160857+00:00", "nick": "mike_cmc", "message": "as items", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T13:44:35.573559+00:00", "nick": "mike_cmc", "message": "and i'm trying to figure out how to issue a Request for those URL's", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T13:46:10.945191+00:00", "nick": "aaearon", "message": "im no expert but i think you'd use a crawlspider with at least two rules. one for extracting the product url and one for extracting the 'next page' url", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T13:46:38.711554+00:00", "nick": "mike_cmc", "message": "that't what i think i should do", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T13:46:46.947464+00:00", "nick": "mike_cmc", "message": "but i don't realy know how", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T13:47:04.492366+00:00", "nick": "aaearon", "message": "do you have any rule right now?", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T13:47:08.252472+00:00", "nick": "mike_cmc", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T13:47:14.475982+00:00", "nick": "mike_cmc", "message": "Rule(LinkExtractor(restrict_xpaths=('//*[@id=\"products\"]/div[@class=\"item nextpage\"]/a')),follow=True,callback='parse_item')", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T13:47:51.354033+00:00", "nick": "mike_cmc", "message": "and i was thinking that my callback function should yield a Request", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T13:48:07.214084+00:00", "nick": "mike_cmc", "message": "but i'm not sure if that's the way to go", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T13:49:57.752103+00:00", "nick": "mike_cmc", "message": "any ideeas ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T13:51:29.572583+00:00", "nick": "aaearon", "message": "so id try this for my rules: http://pastebin.com/Yd803zPc", "links": ["http://pastebin.com/Yd803zPc"], "channel": "scrapy"},
{"date": "2015-01-18T13:52:11.802452+00:00", "nick": "aaearon", "message": "with self.get_next_page(self, response) that would do some sort of checking of the response.body to make sure you get the page you intend on getting", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T13:52:59.823398+00:00", "nick": "aaearon", "message": "and get_next_page yielding a request with something like yield Request(url=response.url, callback=self.parse)", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T13:53:22.758991+00:00", "nick": "aaearon", "message": "in crawlspider you dont want to override the parse function as that is the function where rules are ran", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T13:53:38.839939+00:00", "nick": "mike_cmc", "message": "yeah.. i read that", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T13:54:13.180061+00:00", "nick": "aaearon", "message": "but from what ive read you definitely need two rules. one to extract the product link(s), and one to extract the next page url", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T13:55:59.440474+00:00", "nick": "aaearon", "message": "if you are confident you are going to get what you want each time when you hit the 'next' button, you could probably skip the get_next_page function and callback directly to parse in the rule", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T13:56:54.597970+00:00", "nick": "mike_cmc", "message": "the last page doesn't have the \"nextpage\" link", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T13:57:01.014227+00:00", "nick": "mike_cmc", "message": "so i think it's usefull", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T13:57:21.136128+00:00", "nick": "mike_cmc", "message": "also in an earlyer test i saw that i wasn't gettng the producs on the first page", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T13:57:44.527344+00:00", "nick": "mike_cmc", "message": "it was crawling page 2 and then returning the product links", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T13:58:19.788933+00:00", "nick": "mike_cmc", "message": "how can i first crawl the first 15 products and then go to the next page ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T13:58:59.323461+00:00", "nick": "aaearon", "message": "well it will be done through your rules", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T13:59:52.021358+00:00", "nick": "aaearon", "message": "so if you look at my pastebin, first line is a rule that is to find the url of the product page, and send the page for parsing to your `parse_production` function with your xpath stuff to get name, stock info, etc", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:00:49.556796+00:00", "nick": "aaearon", "message": "second rule is to find the url of the 'next page' link and send the page/Response to the get_next_page function that will yield a Response with a callback to parse", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:01:10.927946+00:00", "nick": "aaearon", "message": "parse will run the rules on the second page of results", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:01:18.783463+00:00", "nick": "aaearon", "message": "find the products on that page, the next page, etc", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:02:02.128227+00:00", "nick": "mike_cmc", "message": "but when the crawler enters the product page will it just extract the data or will it try to follow other links found on that page ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:02:52.001733+00:00", "nick": "aaearon", "message": "depends on what you return. im guessing your parse_perfume or whatever its called that extracts the data will yield and item", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:03:07.973690+00:00", "nick": "aaearon", "message": "and since you are yielding an item and not a Response with a callback to another function, it will just stop there", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:03:09.732268+00:00", "nick": "mike_cmc", "message": "yield or return ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:03:36.718025+00:00", "nick": "aaearon", "message": "yield", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:03:39.429660+00:00", "nick": "mike_cmc", "message": "(i'm new to python and this gave me a headache :) )", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:04:23.477496+00:00", "nick": "aaearon", "message": "yes me too so no worries. im still struggling with difference between yield and return", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:04:58.925042+00:00", "nick": "mike_cmc", "message": "will you be available in 5-10 mins ? i'll try to change my code and see where i end up", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:05:02.672124+00:00", "nick": "aaearon", "message": "sure", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:05:09.014221+00:00", "nick": "mike_cmc", "message": "thx", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:05:17.332733+00:00", "nick": "aaearon", "message": "im using yield for item only because thats what ive seen in examples..", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:12:16.427352+00:00", "nick": "soul-d", "message": "how do you call the spider class ather adding the login thing", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:13:05.462015+00:00", "nick": "aaearon", "message": "what do you mean?", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:13:21.124888+00:00", "nick": "soul-d", "message": "i added the login as a class ?\\", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:13:38.285214+00:00", "nick": "aaearon", "message": "you mean the login post as a method?", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:14:03.049712+00:00", "nick": "soul-d", "message": "now that is finished i need to go back to origonal spider that fetched data", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:14:29.012366+00:00", "nick": "soul-d", "message": "there was data missing cause you need to be loged in for that", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:15:30.680339+00:00", "nick": "soul-d", "message": "so spider worked already or started to  but now i added just the class the example and trying  to call  myspider() in the login check", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:15:33.936931+00:00", "nick": "aaearon", "message": "well you need a function that returns a FormRequest", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:16:20.719312+00:00", "nick": "aaearon", "message": "you dont want two seperate spiders", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:16:29.914364+00:00", "nick": "aaearon", "message": "http://doc.scrapy.org/en/latest/topics/request-...", "links": ["http://doc.scrapy.org/en/latest/topics/request-response.html#using-formrequest-from-response-to-simulate-a-user-login"], "channel": "scrapy"},
{"date": "2015-01-18T14:16:38.006116+00:00", "nick": "aaearon", "message": "the class is LoginSpider but i think that may be a bit confusing", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:17:07.720354+00:00", "nick": "aaearon", "message": "you want your login in the same spider as your parsing", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:17:37.096898+00:00", "nick": "soul-d", "message": "?", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:18:05.811890+00:00", "nick": "aaearon", "message": "what code you have so far", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:18:23.435074+00:00", "nick": "aaearon", "message": "it sounds like you have two seperate spiders for the same scrape job: one that logs in and one that does the scraping", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:18:36.456161+00:00", "nick": "aaearon", "message": "it sounds like you need to combine the two", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:18:40.127453+00:00", "nick": "soul-d", "message": "yes i still have the original class of spiders", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:19:14.809297+00:00", "nick": "aaearon", "message": "by adding a function that posts login data that returns a Response with a callback to your parsing function", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:20:02.877218+00:00", "nick": "soul-d", "message": "so only need to parsing function and the rules etc", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:20:20.637893+00:00", "nick": "aaearon", "message": "you only want a single spider yes", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:28:43.275315+00:00", "nick": "soul-d", "message": "now it doesnt start crawling the site :/", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:29:55.959892+00:00", "nick": "aaearon", "message": "you will have to show some code man", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:30:37.657194+00:00", "nick": "mike_cmc", "message": "@aaearon a little help ? http://pastebin.com/q7jr16Xw", "links": ["http://pastebin.com/q7jr16Xw"], "channel": "scrapy"},
{"date": "2015-01-18T14:31:43.967072+00:00", "nick": "mike_cmc", "message": "i'm not seeing anything in the console.. just scrapy's logs", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:31:52.462329+00:00", "nick": "aaearon", "message": "line 39", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:31:57.870930+00:00", "nick": "mike_cmc", "message": "aka.. it not returning items..", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:31:59.660490+00:00", "nick": "aaearon", "message": "where it has a return, but isnt returning anything", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:32:04.204792+00:00", "nick": "aaearon", "message": "id change to         return Response(url=response.url)", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:32:17.441231+00:00", "nick": "soul-d", "message": "http://pastebin.com/Yb9vE4f2", "links": ["http://pastebin.com/Yb9vE4f2"], "channel": "scrapy"},
{"date": "2015-01-18T14:32:23.132323+00:00", "nick": "soul-d", "message": "top was the old spider", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:32:26.312715+00:00", "nick": "aaearon", "message": "that will send the url of the next page back to parse function, where the spider will then crawl that url using the rules you defined", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:32:39.014573+00:00", "nick": "aaearon", "message": "also mike_cmc", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:33:17.373612+00:00", "nick": "soul-d", "message": "gonna grab a coffee and break", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:33:34.223858+00:00", "nick": "soul-d", "message": "staring to long doesnt help either lol", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:34:11.407134+00:00", "nick": "aaearon", "message": "if nothing is getting crawled id check the xpaths", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:34:16.214122+00:00", "nick": "aaearon", "message": "that you've defined in the rules", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:34:26.759454+00:00", "nick": "mike_cmc", "message": "it wasn't going into the functions..", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:34:34.475490+00:00", "nick": "mike_cmc", "message": "that's why it wasn't returning", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:34:58.571824+00:00", "nick": "mike_cmc", "message": "i removed self. from callback=", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:35:05.157376+00:00", "nick": "mike_cmc", "message": "and it seems to work", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:35:13.510824+00:00", "nick": "aaearon", "message": "ah ok", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:35:27.795350+00:00", "nick": "aaearon", "message": "yeah thats my bad", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:37:47.580744+00:00", "nick": "aaearon", "message": "soul-d i think you are on the right path w/ your second spider but i believe it needs to be a CrawlSpider to use rules. and since it needs to be a crawlspider, you do not want to override parse because otherwise the rules wont fire.", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:38:41.547921+00:00", "nick": "aaearon", "message": "so just change the name of your parse func to 'login' or something", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:41:39.691206+00:00", "nick": "aaearon", "message": "and i think your rule needs to look soemthing more like 'rules =(Rule(LinkExtractor(allow=['/PC/*/*']), callback='parse_world', follow=True),)", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:45:59.845179+00:00", "nick": "soul-d", "message": "still nothing :(   ill upload the modded code and then ill take a break", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:47:43.268795+00:00", "nick": "soul-d", "message": "http://pastebin.com/SappXxwW", "links": ["http://pastebin.com/SappXxwW"], "channel": "scrapy"},
{"date": "2015-01-18T14:48:32.624897+00:00", "nick": "aaearon", "message": "http://pastebin.com/mfXG6syg", "links": ["http://pastebin.com/mfXG6syg"], "channel": "scrapy"},
{"date": "2015-01-18T14:49:15.718226+00:00", "nick": "aaearon", "message": "with after_login", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:49:27.050249+00:00", "nick": "aaearon", "message": "a successful login needs to return a response", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:55:54.924864+00:00", "nick": "soul-d", "message": "that script exits with  some exception", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:58:27.272442+00:00", "nick": "soul-d", "message": "on the start url link so afther  the login", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:59:00.617149+00:00", "nick": "aaearon", "message": "just assume you will need to upload all exceptions to pastebin :(", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:59:02.857184+00:00", "nick": "aaearon", "message": ":)", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T14:59:05.913209+00:00", "nick": "aaearon", "message": "its easier not to guess", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T15:01:20.715334+00:00", "nick": "sd-gpu", "message": "clone of soul-d here http://pastebin.com/Za1v6RTe", "links": ["http://pastebin.com/Za1v6RTe"], "channel": "scrapy"},
{"date": "2015-01-18T15:01:43.993250+00:00", "nick": "soul-d", "message": "since i am programming on other pc ;)", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T15:02:24.195370+00:00", "nick": "aaearon", "message": "no idea about that exception.. id see if stackoverflow/google has something", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T15:04:00.863935+00:00", "nick": "soul-d", "message": "break first need food", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T15:09:29.326331+00:00", "nick": "soul-d", "message": "ah CrawlSpider in class", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T15:09:57.851242+00:00", "nick": "soul-d", "message": "but seems im not realy logged in log proably wrong type of form i am trying to login to", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T15:10:30.779177+00:00", "nick": "aaearon", "message": "you can use chrome and developer tools to capture post requests", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T15:12:07.963293+00:00", "nick": "soul-d", "message": "ah ok thanks for the help gonna grab food for real ;)", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T15:12:24.487895+00:00", "nick": "aaearon", "message": "no problemo", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T15:42:37.679301+00:00", "nick": "soul-d", "message": "back ok ofcourse it wont login since i just copied the visable names from the page but how does the capture work  don't think you can use inspect element on this", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T15:44:27.119077+00:00", "nick": "soul-d", "message": "its some input = $logincontrol$username type ofthings", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T15:57:08.624901+00:00", "nick": "soul-d", "message": "ok captured a post packed", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:02:08.385664+00:00", "nick": "soul-d", "message": "any idea how to use it ? what to look for isn obvious \\", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:02:22.628882+00:00", "nick": "aaearon", "message": "do you have an actual packet", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:02:40.692585+00:00", "nick": "aaearon", "message": "http://stackoverflow.com/questions/15603561/how...", "links": ["http://stackoverflow.com/questions/15603561/how-can-i-debug-a-http-post-in-chrome"], "channel": "scrapy"},
{"date": "2015-01-18T16:02:42.057338+00:00", "nick": "soul-d", "message": "yes post  lookin in header", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:03:38.505599+00:00", "nick": "aaearon", "message": "you have to find the form data that is sent", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:04:50.736302+00:00", "nick": "sd-gpu", "message": "i only find something like pagecontent_0$contentrow2_0$columncontent_0$twoblocks_0$blockitemleft_0$LoginControl$UserName", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:05:16.445083+00:00", "nick": "aaearon", "message": "so thats the key for the username", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:05:42.427932+00:00", "nick": "sd-gpu", "message": "oh ok lol well maybe i should try stupid lonng names i jsut assumed cant be that one probably some javascript", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:05:59.808035+00:00", "nick": "aaearon", "message": "yeah looks funky but maybe works :)", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:06:58.592365+00:00", "nick": "sd-gpu", "message": "pagecontent_0$contentrow2_0$columncontent_0$twoblocks_0$blockitemleft_0$LoginControl$LoginButton", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:07:01.151530+00:00", "nick": "sd-gpu", "message": "important ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:07:27.799244+00:00", "nick": "aaearon", "message": "i would use alllllll the ones you find", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:07:42.844650+00:00", "nick": "aaearon", "message": "then once you can replicate the login behavior", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:07:49.153818+00:00", "nick": "aaearon", "message": "slowly take away ones that you think may not be important", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:10:18.977699+00:00", "nick": "soul-d", "message": "yeah i mean do i need to tell scrapy there is a button to click like adding that to the dict with Buttonor somthing \\", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:10:32.379606+00:00", "nick": "soul-d", "message": "so scrapy can emulate the click ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:16:29.213675+00:00", "nick": "aaearon", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:16:42.302042+00:00", "nick": "aaearon", "message": "but its not a click", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:16:44.964409+00:00", "nick": "aaearon", "message": "its a request", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:17:37.608461+00:00", "nick": "aaearon", "message": "when you login, you are sending a POST request with the form data (in your case, pagecontent_0$contentrow2_0$columncontent_0$twoblocks_0$blockitemleft_0$LoginControl$UserName, pagecontent_0$contentrow2_0$columncontent_0$twoblocks_0$blockitemleft_0$LoginControl$Password, etc)", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:18:25.998406+00:00", "nick": "aaearon", "message": "to the url http://shop.gameworld.nl/Utility/Login", "links": ["http://shop.gameworld.nl/Utility/Login"], "channel": "scrapy"},
{"date": "2015-01-18T16:18:38.504667+00:00", "nick": "aaearon", "message": "you are already doing this BUT you are not using the correct keys", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:18:52.949060+00:00", "nick": "soul-d", "message": "i am using those keys now", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:26:41.308501+00:00", "nick": "mike_cmc", "message": "@aaearon any ideea on how to get a list of the duplicates found ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:27:14.161942+00:00", "nick": "brati", "message": "Do I have to do something special to catch exceptions in a middleware? It does not come directly from the spider, but from the item loader. I see in the log \"Debug: Crawling ...\" then a long stack trace for the exception, but then not my log.warning and not my mail :(", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:29:29.902140+00:00", "nick": "brati", "message": "I guess when I set \"return []\" in the middleware, I should NOT see the exception on the log anymore?", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:31:40.245824+00:00", "nick": "mike_cmc", "message": "and after that, how can i disable duplicate checking", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:32:57.768238+00:00", "nick": "soul-d", "message": "aaearon, shoulnd i call the get_login ? it seems to be skipping the login part ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:33:35.852316+00:00", "nick": "mike_cmc", "message": "anybody knwo how to get a list of the duplicates found ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:34:23.714790+00:00", "nick": "aaearon", "message": "whats getting filtered?", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:34:26.368943+00:00", "nick": "aaearon", "message": "results or next pages?", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:34:31.842509+00:00", "nick": "aaearon", "message": "items* or next pages?", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:34:38.014944+00:00", "nick": "mike_cmc", "message": "items i think", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:35:02.239971+00:00", "nick": "aaearon", "message": "there is a dont_filter=True/False arg that you can put somewhere", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:35:02.488477+00:00", "nick": "mike_cmc", "message": "dupefilter/filtered : 1482", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:35:37.371738+00:00", "nick": "mike_cmc", "message": "but first i want to know the urls' or something to know if it's ok to leave it on or not", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:35:44.672204+00:00", "nick": "soul-d", "message": "how can i start the crawler interactivly and debug it ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:35:59.123548+00:00", "nick": "aaearon", "message": "idk how to get the urls", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:36:21.098543+00:00", "nick": "mike_cmc", "message": "anything abut the item", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:37:33.873505+00:00", "nick": "mike_cmc", "message": "how can i save the log that scrapy outputs ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:37:41.930607+00:00", "nick": "mike_cmc", "message": "i see i can activate dupefilter_debug", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:37:43.327824+00:00", "nick": "aaearon", "message": "what do your product's urls look like", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:37:51.470498+00:00", "nick": "aaearon", "message": "scrapy crawl -h", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:37:55.091689+00:00", "nick": "brati", "message": "mike_cmc: I usually save it by command line piping", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:38:03.706217+00:00", "nick": "brati", "message": "mike_cmc: ehm I mean redirection", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:38:10.048304+00:00", "nick": "soul-d", "message": "cant i hitch scrapy on a session or something easyer this is getting to complex for something simple", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:38:45.193434+00:00", "nick": "mike_cmc", "message": "thx", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:38:53.309075+00:00", "nick": "aaearon", "message": "soul-d: http://doc.scrapy.org/en/latest/topics/shell.ht...", "links": ["http://doc.scrapy.org/en/latest/topics/shell.html#invoking-the-shell-from-spiders-to-inspect-responses"], "channel": "scrapy"},
{"date": "2015-01-18T16:45:10.956732+00:00", "nick": "soul-d", "message": "how do i make it start with the login it seemingly doesnt even try ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:46:24.739500+00:00", "nick": "aaearon", "message": "code", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:47:20.560864+00:00", "nick": "soul-d", "message": "where how what like def init  a call ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:53:03.739774+00:00", "nick": "aaearon", "message": "show your code", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:53:08.105620+00:00", "nick": "aaearon", "message": "what you have so far", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:55:35.214893+00:00", "nick": "sd-gpu", "message": "http://pastebin.com/3R2JFUCg", "links": ["http://pastebin.com/3R2JFUCg"], "channel": "scrapy"},
{"date": "2015-01-18T16:56:02.840951+00:00", "nick": "aaearon", "message": "you need to override start_requests", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:56:10.474290+00:00", "nick": "aaearon", "message": "rename get_login to start_requests", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:59:13.144626+00:00", "nick": "soul-d", "message": "type error takes 2 arguments one given", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T16:59:25.471772+00:00", "nick": "soul-d", "message": "maybe like example only self", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T17:00:17.343771+00:00", "nick": "soul-d", "message": "global name Request not defined", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T17:11:34.176745+00:00", "nick": "soul-d", "message": "ok now got that working its not crawing the other one anymore lol but i did get 302 back", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T17:14:29.454560+00:00", "nick": "sd-gpu", "message": "http://pastebin.com/iZni3xHS", "links": ["http://pastebin.com/iZni3xHS"], "channel": "scrapy"},
{"date": "2015-01-18T17:16:45.696661+00:00", "nick": "aaearon", "message": "look at your after_login", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T17:16:55.753485+00:00", "nick": "aaearon", "message": "its not returning anytihng", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T17:16:58.417492+00:00", "nick": "aaearon", "message": "so it stops there", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T17:17:35.265197+00:00", "nick": "aaearon", "message": "http://doc.scrapy.org/en/latest/topics/spiders.... read the cycle", "links": ["http://doc.scrapy.org/en/latest/topics/spiders.html"], "channel": "scrapy"},
{"date": "2015-01-18T17:18:21.427601+00:00", "nick": "soul-d", "message": "its not returning cause its suposed to call the callback ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T17:18:56.354277+00:00", "nick": "aaearon", "message": "you arent returning anything", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T17:19:06.885711+00:00", "nick": "aaearon", "message": "look at line 42 and 43", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T17:19:17.983892+00:00", "nick": "aaearon", "message": "you are printing then assigning a variable", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T17:19:39.300918+00:00", "nick": "aaearon", "message": "look what you're doing in start_requests and login", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T17:19:51.291903+00:00", "nick": "aaearon", "message": "what objects are they returning?", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T17:22:46.251747+00:00", "nick": "soul-d", "message": "no clue", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T17:26:23.271573+00:00", "nick": "soul-d", "message": "it craweled  when login wasnt working now it isnt and you expect me  to understand how scrapy handles this", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T17:28:47.346814+00:00", "nick": "aaearon", "message": "you arent returning anything in after_login", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T17:30:17.725397+00:00", "nick": "mike_cmc", "message": "any clues : Spider must return Request, BaseItem or None, got 'Response' ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T17:36:34.581381+00:00", "nick": "soul-d", "message": "ok yeah you meant call with", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T17:38:49.183531+00:00", "nick": "soul-d", "message": "but now it ignoges start url and gets a redirect to the base link", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T17:47:01.049500+00:00", "nick": "soul-d", "message": "any hint at all to get that last thing to work  and i finaly can get to work and get the data from the webpage ? how can i make it start craw  from the start url afther redirect", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T17:48:29.803335+00:00", "nick": "aaearon", "message": "return [Request(url=start_urls[0], callback=self.myspider)", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T17:54:28.674505+00:00", "nick": "aaearon", "message": "is there a way for items to have a field listed in the output even if it has no value?", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T18:05:01.871151+00:00", "nick": "soul-d", "message": "do i need to search the page for links to crawl myself now ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T18:05:19.668249+00:00", "nick": "soul-d", "message": "cause it isnt crawling the start url", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T18:06:41.173877+00:00", "nick": "soul-d", "message": "should there be something with the rules and that last return", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T18:07:16.880503+00:00", "nick": "aaearon", "message": "what is after_login returning", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T18:08:56.380647+00:00", "nick": "sd-gpu", "message": "return [scrapy.Request(url=self.start_urls[0], callback= self.myspider)]", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T18:09:26.796158+00:00", "nick": "aaearon", "message": "callback=self.parse", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T18:09:41.554320+00:00", "nick": "aaearon", "message": "parse is the built in function of crawlspider where the rules get ran", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T18:11:25.301798+00:00", "nick": "sd-gpu", "message": "and should myspider fuction to extract   data  be passed with  that  function as callback it might also be that myspider stops cause it only returns the  scrapy item ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T18:13:01.326052+00:00", "nick": "aaearon", "message": "the rule should callback to myspider", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T18:13:38.123472+00:00", "nick": "soul-d", "message": "ah yes forgot that lets try", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T18:14:45.849246+00:00", "nick": "soul-d", "message": "selfparse response ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T18:14:51.364517+00:00", "nick": "soul-d", "message": "it misses an argument", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T18:15:00.360915+00:00", "nick": "aaearon", "message": "you know the deal.. paste bin what you have", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T18:16:44.116427+00:00", "nick": "sd-gpu", "message": "http://pastebin.com/sWPHW0W8", "links": ["http://pastebin.com/sWPHW0W8"], "channel": "scrapy"},
{"date": "2015-01-18T18:18:28.432834+00:00", "nick": "soul-d", "message": "i think its the () prehaps", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T18:18:34.306616+00:00", "nick": "aaearon", "message": "correct", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T18:19:05.232363+00:00", "nick": "soul-d", "message": ":D works", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T18:23:25.285082+00:00", "nick": "soul-d", "message": "thanks for the help  now i can finaly program the other half", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T18:25:17.313390+00:00", "nick": "aaearon", "message": "good deal", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T18:29:59.793772+00:00", "nick": "soul-d", "message": "if you use the debubber to inspect can you continue the script until it comes back again ? so i can inetractivly craw few pages and check everything ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T18:30:54.352025+00:00", "nick": "soul-d", "message": "ah well no matter can use fetch i gues see if that willshow me as loged in", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T19:57:34.159844+00:00", "nick": "aaearon", "message": "when overwriting process_item in a pipeline", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T19:57:41.015149+00:00", "nick": "aaearon", "message": "is there an easy way to set a default value for each key?", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T19:57:45.201878+00:00", "nick": "aaearon", "message": "field*", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T19:57:51.424498+00:00", "nick": "nkuttler", "message": "erm.. i've implemented a WebDriverWait() for selenium, but how do i get the updated DOM tree from the driver into a Selector?", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T19:58:01.242265+00:00", "nick": "aaearon", "message": "i know you can do item.setdefault('key', 'value') but i dont want to do that for every field", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T20:00:59.381649+00:00", "nick": "brati", "message": "aaearon: is a loop better? for key in self.fields: self.setdefault(key, 'value')? (should work, but I am not sure)", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T20:01:17.382609+00:00", "nick": "aaearon", "message": "yeah exactly what im trying now", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T20:01:47.108189+00:00", "nick": "aaearon", "message": "it makes the most sense", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T20:17:49.795016+00:00", "nick": "soul-d", "message": "mmm how do you make the crawler find a next page it currently quits afther 25", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T20:24:19.884946+00:00", "nick": "soul-d", "message": "nm had to scroll back to see an crawl error", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T20:27:14.490145+00:00", "nick": "soul-d", "message": "had to return the out of  functon if new page is the crawl :) so far so good", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T20:33:27.381709+00:00", "nick": "soul-d", "message": "but i guess it does pages out of order lol", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T21:37:29.867105+00:00", "nick": "aaearon", "message": "any clues as to why my done function isnt being called? https://gist.github.com/anonymous/d53253849784e...", "links": ["https://gist.github.com/anonymous/d53253849784e86568ca"], "channel": "scrapy"},
{"date": "2015-01-18T21:37:42.706756+00:00", "nick": "aaearon", "message": "i see '[physician] INFO: Spider closed (finished)", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T21:37:42.981638+00:00", "nick": "aaearon", "message": "'", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T21:37:47.334545+00:00", "nick": "aaearon", "message": "but reactor does not stop", "links": [], "channel": "scrapy"},
{"date": "2015-01-18T21:41:16.616003+00:00", "nick": "aaearon", "message": "blah was issue w/ pycharms deployment", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T08:05:33.879866+00:00", "nick": "aaearon", "message": "how are people setting default field values ? in my item pipeline i have 'for key in item.keys(): item.setdefault(key, default_value)'", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T08:05:53.963208+00:00", "nick": "aaearon", "message": "but default_value isnt showing up for fields that were never scraped and added to my itemloader", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T10:23:45.650736+00:00", "nick": "mike_cmc", "message": "ola !", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T10:23:59.295143+00:00", "nick": "mike_cmc", "message": "aaearon are you around ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T10:24:11.438318+00:00", "nick": "aaearon", "message": "in and out whats up", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T10:24:29.765249+00:00", "nick": "mike_cmc", "message": "did you use scrapy's web service ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T10:24:47.058860+00:00", "nick": "aaearon", "message": "eh not really. ive used it a few times", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T10:24:47.177713+00:00", "nick": "mike_cmc", "message": "i see in the docs it has something for viewing spider activity", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T10:24:49.267549+00:00", "nick": "aaearon", "message": "scrapyd yea?", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T10:24:55.685342+00:00", "nick": "mike_cmc", "message": "on port 6080 i think", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T10:25:09.810509+00:00", "nick": "mike_cmc", "message": "http://doc.scrapy.org/en/latest/topics/webservi...", "links": ["http://doc.scrapy.org/en/latest/topics/webservice.html"], "channel": "scrapy"},
{"date": "2015-01-19T10:25:12.069293+00:00", "nick": "aaearon", "message": "yea", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T10:25:18.186997+00:00", "nick": "mike_cmc", "message": "but it's not scrapyd", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T10:25:27.187407+00:00", "nick": "aaearon", "message": "ahhh no idea then", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T10:26:04.085685+00:00", "nick": "mike_cmc", "message": "ok then... so i should see scrapyd ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T10:26:24.181473+00:00", "nick": "mike_cmc", "message": "i see something about pausing and resuming a spider", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T10:26:36.837437+00:00", "nick": "mike_cmc", "message": "is that something i need to do in scrapyd or scrapy ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T11:35:52.425578+00:00", "nick": "aaearon", "message": "how can i run multiple spiders, one at a time, with a single reactor?", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T11:35:59.047340+00:00", "nick": "aaearon", "message": "kind of like http://doc.scrapy.org/en/latest/topics/practice... but i dont want them running all at the same time", "links": ["http://doc.scrapy.org/en/latest/topics/practices.html#running-multiple-spiders-in-the-same-process"], "channel": "scrapy"},
{"date": "2015-01-19T14:51:18.668404+00:00", "nick": "London3D", "message": "Hello.", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T14:52:16.209167+00:00", "nick": "London3D", "message": "Is anyone using Scrapy to collect server status from cloud service control pannels?", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T16:09:56.688921+00:00", "nick": "barraponto", "message": "London3D: i'm not. i wonder if it wouldn't be easier with APIs.", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T19:14:20.642856+00:00", "nick": "barraponto", "message": "the code in documentation does not work: http://doc.scrapy.org/en/latest/topics/practice...", "links": ["http://doc.scrapy.org/en/latest/topics/practices.html#dynamic-creation-of-item-classes"], "channel": "scrapy"},
{"date": "2015-01-19T19:14:42.480226+00:00", "nick": "barraponto", "message": "and it is supposed to be a good practice. maybe i'm using it wrong...", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T19:51:49.178692+00:00", "nick": "barraponto", "message": "there, i fixed it: https://github.com/scrapy/scrapy/pull/1018", "links": ["https://github.com/scrapy/scrapy/pull/1018"], "channel": "scrapy"},
{"date": "2015-01-19T19:52:55.407897+00:00", "nick": "barraponto", "message": "hmm why didn't my PR show up here?", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T19:54:18.478361+00:00", "nick": "barraponto", "message": "nramirezuy: can you ask dangra or pablo to take a look at my PR? it fixes a doc regression documented in #398", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T19:57:21.336376+00:00", "nick": "nramirezuy", "message": "I commented it", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T20:09:25.518766+00:00", "nick": "barraponto", "message": "nramirezuy: i answered it.", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T20:09:39.161276+00:00", "nick": "barraponto", "message": "nramirezuy: basically, *that* is the whole fix.", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T20:12:12.606878+00:00", "nick": "nramirezuy", "message": "merged", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T20:13:30.871883+00:00", "nick": "barraponto", "message": "nramirezuy: thanks!", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T20:13:36.217124+00:00", "nick": "nramirezuy", "message": "np", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T20:13:52.703447+00:00", "nick": "barraponto", "message": "nramirezuy: it should be backported to 0.24 too, since those docs are misleading", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T20:14:12.303684+00:00", "nick": "barraponto", "message": "(probably 0.22 too, if it's still supported)", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T20:22:11.303107+00:00", "nick": "sudshekhar", "message": "nramirezuy: regarding the json processor, do you wish to make jmespath a pre-req for scrapy or just the json processor?", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T20:22:30.815634+00:00", "nick": "nramirezuy", "message": "just for the processor", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T20:22:41.995116+00:00", "nick": "sudshekhar", "message": "I am asking this as I am not sure whether to import jmespath,ast and json at the top of the processor or the class", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T20:22:49.497671+00:00", "nick": "sudshekhar", "message": "ohh okay", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T20:23:17.054159+00:00", "nick": "nramirezuy", "message": "but it should raise an exception at import or at instantiation", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T20:23:48.969550+00:00", "nick": "sudshekhar", "message": "also, which of these is a better approach : import json and ast both in __init__ and __call__ (first time to check deps and second time to run)", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T20:24:06.313805+00:00", "nick": "sudshekhar", "message": "or import once in __init__ and store it as a class variable for further use", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T20:25:07.370277+00:00", "nick": "sudshekhar", "message": "this is the second approach: https://gist.github.com/SudShekhar/829a78356e12...", "links": ["https://gist.github.com/SudShekhar/829a78356e122803122d"], "channel": "scrapy"},
{"date": "2015-01-19T20:26:40.475793+00:00", "nick": "sudshekhar", "message": "incase you know of a better way, please do let me know about it :)", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T20:28:37.890241+00:00", "nick": "barraponto", "message": "sudshekhar: are you familiar with pep8?", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T20:30:46.327589+00:00", "nick": "nramirezuy", "message": "barraponto: pep8 has something to say for these cases?", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T20:30:57.653170+00:00", "nick": "sudshekhar", "message": "barraponto: not too much.", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T20:30:58.438208+00:00", "nick": "barraponto", "message": "nramirezuy: no no, i'm reviewing the commits.", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T20:31:05.681365+00:00", "nick": "nramirezuy", "message": "ah ok", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T20:31:36.291169+00:00", "nick": "sudshekhar", "message": "I read them, but admittedly there would be quite some mistakes there . I was planning to improve the quality once I get some reviews about the content", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T20:32:03.093013+00:00", "nick": "barraponto", "message": "sudshekhar: there is a standard coding style in python called PEP8. it is easy to check your code against using automated tools.", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T20:32:18.804749+00:00", "nick": "barraponto", "message": "sudshekhar: those tools also make it easier to spot issues such as forgetting a parens.", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T20:32:32.401490+00:00", "nick": "nramirezuy", "message": "json is not being used on the class", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T20:32:50.240700+00:00", "nick": "nramirezuy", "message": "json.dumps(jsonValue))", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T20:32:52.542634+00:00", "nick": "barraponto", "message": "i'm a vim user, but i know there are pep8 plugins for most of the editors. try it, it will help you avoid those mistakes.", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T20:33:02.172651+00:00", "nick": "barraponto", "message": "sudshekhar: also, why do you import a global ast?", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T20:33:10.824615+00:00", "nick": "nramirezuy", "message": "why do you dump it ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T20:33:19.455366+00:00", "nick": "sudshekhar", "message": "barraponto: sure, will do :)", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T20:33:56.010471+00:00", "nick": "sudshekhar", "message": "nramirezuy: the dump is to get the dictionary back to string form. (incase the query returns a dict)", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T20:34:17.876147+00:00", "nick": "nramirezuy", "message": "doesnt search always returns a dict ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T20:34:20.453135+00:00", "nick": "sudshekhar", "message": "barraponto: I didn't global ast as in?", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T20:34:34.155866+00:00", "nick": "barraponto", "message": "https://github.com/SudShekhar/scrapy/commit/77f...", "links": ["https://github.com/SudShekhar/scrapy/commit/77f1cd5249beabc38a62e9f37c8a0700a18e1aaa#commitcomment-9345747"], "channel": "scrapy"},
{"date": "2015-01-19T20:35:13.641696+00:00", "nick": "barraponto", "message": "the whole \"one import per line\" is not a silly rule, it makes it easier to spot which line failed (and therefore, what the issue might be)", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T20:35:14.201986+00:00", "nick": "sudshekhar", "message": "barraponto: ast is another python library", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T20:35:44.324866+00:00", "nick": "sudshekhar", "message": "nramirezuy: no, the result is a dict or a string (depending on the query)", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T20:36:19.259896+00:00", "nick": "sudshekhar", "message": "barraponto: I get your point. Will keep it in mind (and install the plugins to ensure such mistakes don't happen in the future)", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T20:37:07.312341+00:00", "nick": "nramirezuy", "message": "I mean it will always return a python object decoded", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T20:37:17.058408+00:00", "nick": "nramirezuy", "message": "if the query points to a str it should return a str", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T20:37:24.123300+00:00", "nick": "nramirezuy", "message": "if it is a dict it should return a dict", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T20:38:52.062906+00:00", "nick": "sudshekhar", "message": "but isn't the processor supposed to return a list of strings? That was my thinking when I did this", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T20:39:49.509568+00:00", "nick": "sudshekhar", "message": "if we follow that approach, we will have to return the first dict we get (since, appending/processing other scraped values won't make sense)", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T20:42:41.754541+00:00", "nick": "barraponto", "message": "nramirezuy: btw, you can close https://github.com/scrapy/scrapy/issues/398#iss... too", "links": ["https://github.com/scrapy/scrapy/issues/398#issuecomment-40894736"], "channel": "scrapy"},
{"date": "2015-01-19T20:42:50.087142+00:00", "nick": "barraponto", "message": "i meant the issue, not the comment.", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T20:43:18.819904+00:00", "nick": "nramirezuy", "message": "that Issue is closed", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T20:43:31.676060+00:00", "nick": "nramirezuy", "message": "pablohoffman     closed this   on 29 Sep 2013", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T20:44:21.921541+00:00", "nick": "barraponto", "message": "oh well. now it is closed and re-fixed.", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T20:50:18.826635+00:00", "nick": "sudshekhar", "message": "barraponto: might seem like a very silly question but can you tell me how to build and run scrapy directly from source?", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T20:50:31.907659+00:00", "nick": "sudshekhar", "message": "till now I used the following command: pip install -e git+git://github.com/scrapy/scrapy.git#egg=Scrapy to get the source", "links": ["http://git+git://github.com/scrapy/scrapy.git#egg=Scrapy"], "channel": "scrapy"},
{"date": "2015-01-19T20:51:46.718935+00:00", "nick": "sudshekhar", "message": "and (not sure how) but pip set this up to be the source directory for scrapy. It lets me test everything and edit code but to make github commits I need to redo all the changes into the github local repo and then commit", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T20:59:54.830942+00:00", "nick": "nramirezuy", "message": "is there a  way to slice a list using jmespath ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T21:02:19.562339+00:00", "nick": "sudshekhar", "message": "yeah", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T21:03:06.898773+00:00", "nick": "sudshekhar", "message": "incase you want a quick overview : http://jmespath.org/tutorial.html", "links": ["http://jmespath.org/tutorial.html"], "channel": "scrapy"},
{"date": "2015-01-19T21:06:32.711137+00:00", "nick": "nramirezuy", "message": "it shows indexing", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T21:06:37.683394+00:00", "nick": "nramirezuy", "message": "and [*]", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T21:07:17.114156+00:00", "nick": "nramirezuy", "message": "but i mean slicing like range(10)[0:2] == [0, 1]", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T21:07:46.263709+00:00", "nick": "sudshekhar", "message": "there seems a  pull request regarding this, but it hasn't been merged : https://github.com/boto/jmespath/pull/30", "links": ["https://github.com/boto/jmespath/pull/30"], "channel": "scrapy"},
{"date": "2015-01-19T21:08:34.846040+00:00", "nick": "sudshekhar", "message": "currently we can search for single elements : jmespath.search(\"foo[1]\",l).", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T21:17:31.878765+00:00", "nick": "nramirezuy", "message": "It's a pain for working with lists :P", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T21:19:52.099038+00:00", "nick": "nramirezuy", "message": "anyways I think it should take a value at a time, if you want to process multiple values with the processor you should combine it with MapCompose", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T21:21:38.626873+00:00", "nick": "nramirezuy", "message": "and just return the search result", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T21:22:03.794185+00:00", "nick": "sudshekhar", "message": "but if we really want to expose dictionaries to the user, it would almost necessitate writing python code each time you use this", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T21:22:24.000866+00:00", "nick": "nramirezuy", "message": "what do you mean?", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T21:22:31.513716+00:00", "nick": "sudshekhar", "message": "as in Json processor just becomes an interface to this search library instead of being a complete processor by itself", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T21:24:28.274899+00:00", "nick": "sudshekhar", "message": "but yeah, returning a dictionary will give the user much more control over the program", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T21:24:48.327398+00:00", "nick": "nramirezuy", "message": "since it doesn't support slicing and it is kinda hard to build complex expressions", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T21:25:20.409837+00:00", "nick": "nramirezuy", "message": "I think that returning the actual value of the selection is the best", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T21:25:30.179686+00:00", "nick": "nramirezuy", "message": "it will allow you to select a list", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T21:25:37.329215+00:00", "nick": "nramirezuy", "message": "and apply a slicing right away", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T21:26:42.604359+00:00", "nick": "sudshekhar", "message": "yeah, this seems to be better", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T21:27:30.814135+00:00", "nick": "sudshekhar", "message": "so the Jsonprocessor will take only one value at a time, return whatever the query returns directly to the user, right?", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T21:28:05.485912+00:00", "nick": "nramirezuy", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T21:28:32.851391+00:00", "nick": "nramirezuy", "message": "it will allow you to put lists directly", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T21:28:53.679489+00:00", "nick": "nramirezuy", "message": "most of the time the entire response is json", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T21:29:13.831915+00:00", "nick": "sudshekhar", "message": "yeah! Okay, so I will update the code and the PR accordingly. Thanks for the help :)", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T21:29:21.811718+00:00", "nick": "nramirezuy", "message": "so you will be doing add_value('key', response.body, JsonProcessor(query))", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T21:30:13.597711+00:00", "nick": "nramirezuy", "message": "on those weird cases where you have add_xpath('key', xpath, MapCompose(JsonProcessor('query')))", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T21:31:01.011722+00:00", "nick": "nramirezuy", "message": "Also I noticed that you can use it directly  over python objects like dicts and lists", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T21:33:15.243886+00:00", "nick": "sudshekhar", "message": "^ I didn't get the significance of this", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T21:33:30.911256+00:00", "nick": "nramirezuy", "message": ">>> jmespath.compile('prices[]').search(dict(prices=range(10)))", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T21:33:31.707272+00:00", "nick": "nramirezuy", "message": "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T21:35:48.677121+00:00", "nick": "sudshekhar", "message": "yeah, I got this. Are you trying to say that users can thus, utilize this processor in other unrelated contexts too, if they wish?", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T21:37:16.060665+00:00", "nick": "nramirezuy", "message": "Yes. It  may be useful for nested dicts", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T21:38:28.380260+00:00", "nick": "nramirezuy", "message": "for example I have some complex items on a Storage. And I'm using this to select values with a command line tool", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T21:41:09.801117+00:00", "nick": "sudshekhar", "message": "in such a situation, giving this feature as a part of utils might make more sense (then the json processor, just uses the utils function directly)", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T21:42:13.799313+00:00", "nick": "sudshekhar", "message": "but it would lead to some more dependencies (which may or may not be a good thing)", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T21:43:59.922380+00:00", "nick": "nramirezuy", "message": "for now it is ok as a processor only", "links": [], "channel": "scrapy"},
{"date": "2015-01-19T21:44:55.399289+00:00", "nick": "sudshekhar", "message": "okay!", "links": [], "channel": "scrapy"},
{"date": "2015-01-20T14:15:06.085423+00:00", "nick": "aaearon", "message": "im scraping search results in a way where there will be many duplicates. in order to make less requests against the web server, i want to query the database im inserting the items for to see if i already have it in my database", "links": [], "channel": "scrapy"},
{"date": "2015-01-20T14:15:27.855455+00:00", "nick": "aaearon", "message": "is an idea like that appropriate for a custom middleware or just another method in my spider where it queries the database?", "links": [], "channel": "scrapy"},
{"date": "2015-01-20T14:48:55.357906+00:00", "nick": "aaearon", "message": "went with a downloader_middleware", "links": [], "channel": "scrapy"},
{"date": "2015-01-20T16:32:58.660954+00:00", "nick": "Google", "message": "Hi", "links": [], "channel": "scrapy"},
{"date": "2015-01-20T16:49:03.181749+00:00", "nick": "timini", "message": "Hey how can i get all the rows **except the first** table with xpath? /table/tr[2:] ??", "links": [], "channel": "scrapy"},
{"date": "2015-01-20T16:49:30.201282+00:00", "nick": "timini", "message": "something like that?", "links": [], "channel": "scrapy"},
{"date": "2015-01-20T17:08:30.843249+00:00", "nick": "aaearon", "message": "i dontk now if you can timini", "links": [], "channel": "scrapy"},
{"date": "2015-01-20T17:13:17.121780+00:00", "nick": "nramirezuy", "message": "//tr[position() > 1]", "links": [], "channel": "scrapy"},
{"date": "2015-01-21T09:55:55.962138+00:00", "nick": "paul---", "message": "hi", "links": [], "channel": "scrapy"},
{"date": "2015-01-21T09:56:52.144266+00:00", "nick": "paul---", "message": "if anyone here is interested in paid work to get a scrapinghub/scrapy environment setup for me, with the right workflows /data pipelines and also include capability for scraping websockets please get in touch via pm", "links": [], "channel": "scrapy"},
{"date": "2015-01-21T11:19:40.342026+00:00", "nick": "barraponto", "message": "paul---: yes.", "links": [], "channel": "scrapy"},
{"date": "2015-01-21T11:50:55.447624+00:00", "nick": "AndyRez", "message": "hi all", "links": [], "channel": "scrapy"},
{"date": "2015-01-21T12:11:44.854507+00:00", "nick": "AndyRez", "message": "Can anyone assist, I am having a hard time fixing python path with my scrapy + django project..", "links": [], "channel": "scrapy"},
{"date": "2015-01-21T12:12:18.932039+00:00", "nick": "AndyRez", "message": "I think I have imported the right directories and paths but i still get an error", "links": [], "channel": "scrapy"},
{"date": "2015-01-21T12:12:49.977545+00:00", "nick": "AndyRez", "message": "and checking my sys.path my django directory shows up in my scrapy project folder...", "links": [], "channel": "scrapy"},
{"date": "2015-01-21T12:32:34.155989+00:00", "nick": "AndyRez", "message": "ok fixed it ...", "links": [], "channel": "scrapy"},
{"date": "2015-01-21T12:32:36.276637+00:00", "nick": "AndyRez", "message": "nvm", "links": [], "channel": "scrapy"},
{"date": "2015-01-21T14:17:32.201628+00:00", "nick": "skraapaap", "message": "Hi, is DEPTH_LIMIT and DOWNLOAD_DELAY set in scrapy.cfg?", "links": [], "channel": "scrapy"},
{"date": "2015-01-21T14:31:21.496161+00:00", "nick": "aaearon", "message": "skraapaap settings.py", "links": [], "channel": "scrapy"},
{"date": "2015-01-21T14:31:31.618331+00:00", "nick": "aaearon", "message": "but can also be set per spider as well", "links": [], "channel": "scrapy"},
{"date": "2015-01-21T14:31:36.974454+00:00", "nick": "aaearon", "message": "download delay at least", "links": [], "channel": "scrapy"},
{"date": "2015-01-21T14:32:16.632166+00:00", "nick": "skraapaap", "message": "thank you aaearon", "links": [], "channel": "scrapy"},
{"date": "2015-01-21T14:32:19.119731+00:00", "nick": "skraapaap", "message": "let me see", "links": [], "channel": "scrapy"},
{"date": "2015-01-21T14:33:39.626890+00:00", "nick": "skraapaap", "message": "Ah, I was mucking about in scrapy.cfg. No wonder. Is there a randomised delay option?", "links": [], "channel": "scrapy"},
{"date": "2015-01-21T14:36:01.043784+00:00", "nick": "skraapaap", "message": "Perfect thanks @aaearon, exiting spiders are easier to debug :)", "links": [], "channel": "scrapy"},
{"date": "2015-01-21T14:36:27.986288+00:00", "nick": "aaearon", "message": "no problemo", "links": [], "channel": "scrapy"},
{"date": "2015-01-21T15:12:15.215312+00:00", "nick": "skraapaap", "message": "Let's say I have a broad site crawl that uses a \"yield scrapy.Request(url, callback=self.parse)\" for URL grabbing and parsing, but I also want to combine this with a parse_form (for parsing forms fields and killing the spider on an exit variable), can I just do two callbacks?", "links": [], "channel": "scrapy"},
{"date": "2015-01-21T15:32:35.904259+00:00", "nick": "timini", "message": "hey how can I make another request in the parse method?", "links": [], "channel": "scrapy"},
{"date": "2015-01-21T15:37:25.165762+00:00", "nick": "nramirezuy", "message": "timini: yield it", "links": [], "channel": "scrapy"},
{"date": "2015-01-21T15:37:41.132024+00:00", "nick": "nramirezuy", "message": "skraapaap: just call it as a normal method", "links": [], "channel": "scrapy"},
{"date": "2015-01-21T15:41:30.698908+00:00", "nick": "timini", "message": "Sorry I hope you dont mind if I ask an conceptual question,", "links": [], "channel": "scrapy"},
{"date": "2015-01-21T15:42:27.018283+00:00", "nick": "timini", "message": "a ScrapyItem represents some data in the way it is represented on a website,", "links": [], "channel": "scrapy"},
{"date": "2015-01-21T15:43:40.613494+00:00", "nick": "timini", "message": "a ScrapyItem represents some data in the way it is represented on a website, is there somewhere I can convert this into my own data model?", "links": [], "channel": "scrapy"},
{"date": "2015-01-21T16:23:33.881954+00:00", "nick": "aaearon", "message": "can i disable a downloader middleware when starting a scrapy shell?", "links": [], "channel": "scrapy"},
{"date": "2015-01-21T16:32:38.993724+00:00", "nick": "nramirezuy", "message": "timini: And which is your own data model?", "links": [], "channel": "scrapy"},
{"date": "2015-01-21T16:33:07.717734+00:00", "nick": "nramirezuy", "message": "aaearon: -s param for settings", "links": [], "channel": "scrapy"},
{"date": "2015-01-21T16:34:41.351912+00:00", "nick": "aaearon", "message": "thank you nramirezuy", "links": [], "channel": "scrapy"},
{"date": "2015-01-21T16:35:11.761373+00:00", "nick": "aaearon", "message": "im trying to get strings \u00b4seperated\u00b4 by a <br> tag. how do i get the first string? http://pastebin.com/0LnkE6FG", "links": ["http://pastebin.com/0LnkE6FG"], "channel": "scrapy"},
{"date": "2015-01-21T16:37:28.944790+00:00", "nick": "aaearon", "message": "ok this is how i did it: http://pastebin.com/UwDxKpHY", "links": ["http://pastebin.com/UwDxKpHY"], "channel": "scrapy"},
{"date": "2015-01-21T16:37:33.976085+00:00", "nick": "aaearon", "message": "i think my xpaths are getting hella ugly", "links": [], "channel": "scrapy"},
{"date": "2015-01-21T16:38:13.217395+00:00", "nick": "nramirezuy", "message": "yea :P", "links": [], "channel": "scrapy"},
{"date": "2015-01-21T17:09:06.441566+00:00", "nick": "barraponto", "message": "why can't the [0] be insinde the same xpath() call?", "links": [], "channel": "scrapy"},
{"date": "2015-01-21T18:14:21.264098+00:00", "nick": "aaearon", "message": "barraponto because i need to loop through the list it creates", "links": [], "channel": "scrapy"},
{"date": "2015-01-21T20:48:03.670828+00:00", "nick": "nugnug", "message": "hey, i want to scrape another protocol, I've been looking at scrapy and it looks like a nice collection of tools, can i trivially enough write a spider for another protocol that fits in nicely with the ecosystem?", "links": [], "channel": "scrapy"},
{"date": "2015-01-21T20:48:16.683023+00:00", "nick": "nugnug", "message": "also has anyone used something like spiped with scrapy, seems like a natural fit", "links": [], "channel": "scrapy"},
{"date": "2015-01-21T20:48:52.427880+00:00", "nick": "nugnug", "message": "i've programmed python and twisted off and on for years", "links": [], "channel": "scrapy"},
{"date": "2015-01-21T21:14:45.058794+00:00", "nick": "barraponto", "message": "nugnug: spiped looks nice, but does scrapy need to support spiped?", "links": [], "channel": "scrapy"},
{"date": "2015-01-21T21:14:55.318790+00:00", "nick": "barraponto", "message": "i mean, would an http proxy using spiped be enough?", "links": [], "channel": "scrapy"},
{"date": "2015-01-21T21:15:16.302919+00:00", "nick": "nugnug", "message": "right", "links": [], "channel": "scrapy"},
{"date": "2015-01-21T21:15:21.844467+00:00", "nick": "nugnug", "message": "it's a transparent swap in", "links": [], "channel": "scrapy"},
{"date": "2015-01-22T03:08:51.508357+00:00", "nick": "scasca", "message": "I am looking to create a scraper that will just save the full response (its in json) and the craweler will just need to do sequencial id for the page example www.domain.com/index.php?id=1 id=2 , Would scrapy be over kill I can do this with curl but I would like to ability to have multiple connections at the tim", "links": ["http://www.domain.com/index.php?id=1"], "channel": "scrapy"},
{"date": "2015-01-22T03:10:04.528312+00:00", "nick": "dpn`", "message": "scasca, scrapy is built on twisted, using something like treq and twisted might be more suitable", "links": [], "channel": "scrapy"},
{"date": "2015-01-22T03:11:05.436861+00:00", "nick": "scasca", "message": "dpn~: would that be complicated", "links": [], "channel": "scrapy"},
{"date": "2015-01-22T03:31:20.094507+00:00", "nick": "scasca", "message": "If i do not want to parse the response i want to stop the full json response what would i do", "links": [], "channel": "scrapy"},
{"date": "2015-01-22T03:32:47.802833+00:00", "nick": "dpn`", "message": "scasca, in terms of lines of code and things you have to learn, treq would be simpler", "links": [], "channel": "scrapy"},
{"date": "2015-01-22T03:32:56.183842+00:00", "nick": "dpn`", "message": "but I don't have a full understanding of your problem", "links": [], "channel": "scrapy"},
{"date": "2015-01-22T03:33:06.650355+00:00", "nick": "dpn`", "message": "if you already know scrapy then there may be less to learn there", "links": [], "channel": "scrapy"},
{"date": "2015-01-22T06:24:50.849094+00:00", "nick": "aaearon", "message": "how can i dynamically create fields in my item?", "links": [], "channel": "scrapy"},
{"date": "2015-01-22T07:08:50.119325+00:00", "nick": "paul---", "message": "if anyone here is interested in paid work to get a scrapinghub/scrapy environment setup for me, with the right workflows /data pipelines and also include capability for scraping websockets please get in touch via pm </endshamelessplug>", "links": [], "channel": "scrapy"},
{"date": "2015-01-22T10:19:00.807080+00:00", "nick": "timini", "message": "Hey How can I add new URLs to be crawled from my spider?", "links": [], "channel": "scrapy"},
{"date": "2015-01-22T10:20:03.358370+00:00", "nick": "nkuttler", "message": "timini: add them to start_urls?", "links": [], "channel": "scrapy"},
{"date": "2015-01-22T10:21:50.077681+00:00", "nick": "timini", "message": "OK but this is a url that i extract from one of the start urls", "links": [], "channel": "scrapy"},
{"date": "2015-01-22T10:24:23.219515+00:00", "nick": "timini", "message": "Someone mentioned I should yeild a URL but from where?", "links": [], "channel": "scrapy"},
{"date": "2015-01-22T10:27:33.015075+00:00", "nick": "timini", "message": "ok i got it", "links": [], "channel": "scrapy"},
{"date": "2015-01-22T10:32:27.657041+00:00", "nick": "timini", "message": "Say I have different types of pages I want to parse in different ways, do I just make one monolithic parse method that checks the url of the page?", "links": [], "channel": "scrapy"},
{"date": "2015-01-22T10:32:50.558285+00:00", "nick": "timini", "message": "and applies a different parsing method depending on the url..", "links": [], "channel": "scrapy"},
{"date": "2015-01-22T10:47:53.850872+00:00", "nick": "aaearon", "message": "if i understand your scenario, thats what i would do", "links": [], "channel": "scrapy"},
{"date": "2015-01-22T10:48:13.547604+00:00", "nick": "aaearon", "message": "if 'UNIQUE_IDENTIFIER' in response.body: yield whatever with callback whatever", "links": [], "channel": "scrapy"},
{"date": "2015-01-22T10:52:41.364804+00:00", "nick": "timini", "message": "cool thanks, that makes sense", "links": [], "channel": "scrapy"},
{"date": "2015-01-22T16:49:06.608759+00:00", "nick": "timini", "message": "Hey is it possible to download an ftp file?", "links": [], "channel": "scrapy"},
{"date": "2015-01-22T16:52:51.578307+00:00", "nick": "timini", "message": "I try using Request to download an FTP file but I get an error", "links": [], "channel": "scrapy"},
{"date": "2015-01-22T17:52:00.422062+00:00", "nick": "aaearon", "message": "anyone have experience with proxymesh and other services like it?", "links": [], "channel": "scrapy"},
{"date": "2015-01-23T13:54:02.653325+00:00", "nick": "bru`", "message": "Hi! I'm trying to scrape a website (say http://www.web.net) whose hyperlinks are only /foo/bar (which will fetch http://www.web.net/foo/bar). However that won't work in a scrapy spider when I yield a request", "links": ["http://www.web.net", "http://www.web.net/foo/bar"], "channel": "scrapy"},
{"date": "2015-01-23T13:54:29.224036+00:00", "nick": "bru`", "message": "What it the prefered way to prefix the string with the protocole + the website? Simple python string manipulation? Something surer & cleaner?", "links": [], "channel": "scrapy"},
{"date": "2015-01-23T13:54:53.823209+00:00", "nick": "barraponto", "message": "bru`: crawl spider is smart enough.", "links": [], "channel": "scrapy"},
{"date": "2015-01-23T13:55:01.230202+00:00", "nick": "barraponto", "message": "bru`: give it a try.", "links": [], "channel": "scrapy"},
{"date": "2015-01-23T13:55:34.868497+00:00", "nick": "bru`", "message": "ok cool (I'm currently using the base one)", "links": [], "channel": "scrapy"},
{"date": "2015-01-23T13:56:25.210860+00:00", "nick": "barraponto", "message": "bru`: well, if you want to do it during parse, there urlparse.urljoin to join the pieces.", "links": [], "channel": "scrapy"},
{"date": "2015-01-23T13:56:46.523106+00:00", "nick": "barraponto", "message": "however, keep in mind that the base url might not be the current url (there's a freaking <base> html tag, for instance)", "links": [], "channel": "scrapy"},
{"date": "2015-01-23T13:56:51.097093+00:00", "nick": "barraponto", "message": "from scrapy.utils.response import get_base_url", "links": [], "channel": "scrapy"},
{"date": "2015-01-23T13:57:10.528292+00:00", "nick": "barraponto", "message": "get_base_url(response) should give you the base url", "links": [], "channel": "scrapy"},
{"date": "2015-01-23T13:57:12.770240+00:00", "nick": "bru`", "message": "barraponto:  I'm not especially comfortable with the rules; I did not see how to manage a 2-level websites: several pages listing results, which I have to navigate, and each result page yields a signle Item", "links": [], "channel": "scrapy"},
{"date": "2015-01-23T13:57:16.786370+00:00", "nick": "barraponto", "message": "urljoin it with the part you found.", "links": [], "channel": "scrapy"},
{"date": "2015-01-23T13:57:19.464875+00:00", "nick": "bru`", "message": "ok I'll have a look into that! cheers", "links": [], "channel": "scrapy"},
{"date": "2015-01-23T13:57:19.715696+00:00", "nick": "barraponto", "message": "and go :)", "links": [], "channel": "scrapy"},
{"date": "2015-01-23T13:57:47.166856+00:00", "nick": "barraponto", "message": "bru`: use separate parse functions for each of the rules", "links": [], "channel": "scrapy"},
{"date": "2015-01-23T13:58:13.024969+00:00", "nick": "barraponto", "message": "callback=my_item_parser and callback=my_list_parser", "links": [], "channel": "scrapy"},
{"date": "2015-01-23T13:58:16.304378+00:00", "nick": "barraponto", "message": "for instance.", "links": [], "channel": "scrapy"},
{"date": "2015-01-23T14:00:20.626165+00:00", "nick": "bru`", "message": "That's what I'm currently doing with the base parser. Well I'l read the CrawlSpider doc more thoroughly", "links": [], "channel": "scrapy"},
{"date": "2015-01-23T14:07:36.943795+00:00", "nick": "barraponto", "message": "if you want to paste your spider on gist or pastebin i could help you out with crawlspider", "links": [], "channel": "scrapy"},
{"date": "2015-01-23T15:31:25.708350+00:00", "nick": "timini", "message": "Hey is it possible to have realtions between scrapy items?", "links": [], "channel": "scrapy"},
{"date": "2015-01-23T15:39:58.640170+00:00", "nick": "nramirezuy", "message": "relations ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-23T15:40:19.773516+00:00", "nick": "nramirezuy", "message": "you can generate a hash of the item and store it somewhere as an id", "links": [], "channel": "scrapy"},
{"date": "2015-01-23T15:43:35.987280+00:00", "nick": "timini", "message": "oh cool", "links": [], "channel": "scrapy"},
{"date": "2015-01-23T15:45:23.105306+00:00", "nick": "barraponto", "message": "timini: or urls, i usually use urls.", "links": [], "channel": "scrapy"},
{"date": "2015-01-23T15:46:50.858082+00:00", "nick": "barraponto", "message": "nramirezuy: i want to normalize those relations after the job finishes. is there someway to trigger a second step after the items are all scraped?", "links": [], "channel": "scrapy"},
{"date": "2015-01-23T15:47:16.945722+00:00", "nick": "nramirezuy", "message": "spider_closed ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-23T15:47:18.601354+00:00", "nick": "barraponto", "message": "like pipelines, but getting all the items at once.", "links": [], "channel": "scrapy"},
{"date": "2015-01-23T15:47:32.419133+00:00", "nick": "nramirezuy", "message": "the items are already in their current storage", "links": [], "channel": "scrapy"},
{"date": "2015-01-23T15:47:43.307432+00:00", "nick": "nramirezuy", "message": "why would you do something inside scrapy", "links": [], "channel": "scrapy"},
{"date": "2015-01-23T15:48:03.003605+00:00", "nick": "barraponto", "message": "dunno. of course i could just call a second command.", "links": [], "channel": "scrapy"},
{"date": "2015-01-23T15:48:21.161232+00:00", "nick": "nramirezuy", "message": "you can read the file inside spider_closed signal", "links": [], "channel": "scrapy"},
{"date": "2015-01-23T15:48:27.676883+00:00", "nick": "nramirezuy", "message": "and do something", "links": [], "channel": "scrapy"},
{"date": "2015-01-23T15:49:09.067971+00:00", "nick": "nramirezuy", "message": "timini: better use request fingerprint, url doesn't support POST", "links": [], "channel": "scrapy"},
{"date": "2015-01-23T15:49:39.553505+00:00", "nick": "nramirezuy", "message": "and request fingerprint doesn't support items comming from the same request", "links": [], "channel": "scrapy"},
{"date": "2015-01-23T15:51:01.601419+00:00", "nick": "nramirezuy", "message": "barraponto: does that wok? reading the file on spider_closed signal? Otherwise you have to have the items in memory.", "links": [], "channel": "scrapy"},
{"date": "2015-01-23T15:51:23.197390+00:00", "nick": "barraponto", "message": "nramirezuy: i'm kinda new to using signals", "links": [], "channel": "scrapy"},
{"date": "2015-01-23T15:57:33.989862+00:00", "nick": "barraponto", "message": "nramirezuy: how would i use signals without running scrapy from a script?", "links": [], "channel": "scrapy"},
{"date": "2015-01-23T15:58:33.249791+00:00", "nick": "nramirezuy", "message": "you connect the signal. In the from_crawler method of the spider", "links": [], "channel": "scrapy"},
{"date": "2015-01-23T15:58:53.723588+00:00", "nick": "nramirezuy", "message": "crawler.signals gives you the SignalManager instance you need to use", "links": [], "channel": "scrapy"},
{"date": "2015-01-23T16:01:06.373368+00:00", "nick": "barraponto", "message": "nramirezuy: like this http://doc.scrapy.org/en/latest/topics/extensio...", "links": ["http://doc.scrapy.org/en/latest/topics/extensions.html?highlight=from_crawler"], "channel": "scrapy"},
{"date": "2015-01-23T16:01:08.319276+00:00", "nick": "barraponto", "message": "?", "links": [], "channel": "scrapy"},
{"date": "2015-01-23T16:01:45.992381+00:00", "nick": "nramirezuy", "message": "yea", "links": [], "channel": "scrapy"},
{"date": "2015-01-23T16:01:58.152793+00:00", "nick": "nramirezuy", "message": "crawler.signals.connect(ext.spider_closed, signal=signals.spider_closed)", "links": [], "channel": "scrapy"},
{"date": "2015-01-23T16:04:06.724907+00:00", "nick": "barraponto", "message": "but that example is an extension. should it be possible to set it up from the spider?", "links": [], "channel": "scrapy"},
{"date": "2015-01-23T16:04:30.532034+00:00", "nick": "nramirezuy", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2015-01-23T16:04:39.854093+00:00", "nick": "nramirezuy", "message": "spider has a from_crawler method", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T10:02:03.064510+00:00", "nick": "Mercury_", "message": "anyone here ??", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T10:04:02.042861+00:00", "nick": "Mercury_", "message": "anyone ??", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T19:39:12.228864+00:00", "nick": "kiran", "message": "Hello everyone !", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T19:43:12.989830+00:00", "nick": "kiran", "message": "I am working on running multiple spiders from a scrapy program. And it works when I run a list of individual spiders in a for loop. I wanted to know if there are any methods I can use to run a distributed crawler architecture to scrape these sites more quickly ? I am running these spiders programmatically  so for some reason I am not able to follow how to integrate scrapy-redis into my project.", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T19:45:36.607333+00:00", "nick": "aaearon", "message": "so you can use scrapyd to run the spiders on multiple machines", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T19:46:51.329307+00:00", "nick": "kiran_", "message": "sorry was disconnected.", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T19:47:11.294367+00:00", "nick": "aaearon", "message": "so you can use scrapyd to run the spiders on multiple machines", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T19:47:39.030534+00:00", "nick": "aaearon", "message": "or just use a script to continously run the spider", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T19:48:24.175076+00:00", "nick": "kiran_", "message": "by continuously run it you mean pause/start the spider?", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T19:49:21.183294+00:00", "nick": "aaearon", "message": "programatically loop the running of the spider using scrapy api http://doc.scrapy.org/en/latest/topics/practice...", "links": ["http://doc.scrapy.org/en/latest/topics/practices.html?highlight=script"], "channel": "scrapy"},
{"date": "2015-01-24T19:51:49.474250+00:00", "nick": "kiran_", "message": "I am running them programmatically but it sometime takes 19 hours to scrape 1 site.", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T19:52:38.617154+00:00", "nick": "kiran_", "message": "looping would be better if I create a cron to run them once a day.", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T19:54:00.810813+00:00", "nick": "kiran_", "message": "Is that what you were saying. I don't think I got what you tried to say there", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T19:54:08.634883+00:00", "nick": "aaearon", "message": "i think i may have confused myself :P", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T19:54:24.482842+00:00", "nick": "aaearon", "message": "so scrapy-redis is what you want", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T19:54:37.007501+00:00", "nick": "aaearon", "message": "if you want to run multiple instances of the same spider on the same 'workload'", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T19:55:08.428852+00:00", "nick": "aaearon", "message": "there is also https://github.com/sebdah/scrapy-mongodb", "links": ["https://github.com/sebdah/scrapy-mongodb"], "channel": "scrapy"},
{"date": "2015-01-24T19:56:34.426885+00:00", "nick": "kiran_", "message": "Ya I wanted to run the same spider on multiple servers", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T19:57:13.517670+00:00", "nick": "kiran_", "message": "scrapy-mongodb looks like something I would use to insert data", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T19:58:02.494466+00:00", "nick": "kiran_", "message": "I am okay on that part about insertion", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T19:58:12.202626+00:00", "nick": "kiran_", "message": "which is handled by the pipelines.", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T19:59:30.306106+00:00", "nick": "aaearon", "message": "so with scrapy-redis where are you getting hung up", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:00:14.962831+00:00", "nick": "kiran_", "message": "It needs a python egg for the project to run", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:00:25.358654+00:00", "nick": "kiran_", "message": "I dont have that information", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:00:32.049925+00:00", "nick": "kiran_", "message": "for my spiders", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:01:03.376529+00:00", "nick": "aaearon", "message": "where does it say that", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:08:55.383948+00:00", "nick": "kiran", "message": "disconnected again.", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:09:04.169163+00:00", "nick": "kiran", "message": "it doesn't say that but from the example project I thought I can only launch them through the scrapy command line", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:10:20.503889+00:00", "nick": "kiran", "message": "So I basically have to start the spiders on each server and connect to the redis server. And that will be all ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:10:32.350753+00:00", "nick": "aaearon", "message": "im confused to where you are seeing 'egg'", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:10:35.400897+00:00", "nick": "aaearon", "message": "yes", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:11:10.050708+00:00", "nick": "kiran", "message": "Awesome", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:11:40.432485+00:00", "nick": "kiran", "message": "Now I just need to figure how to start them all at the same time. I am guessing cronjobs will be useful for this.", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:12:55.375231+00:00", "nick": "aaearon", "message": "maybe scrapyd is something that will work for you", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:13:09.086980+00:00", "nick": "aaearon", "message": "a daemon that you deploy a spider to and can use json api calls to start them, get status, etc", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:15:35.058960+00:00", "nick": "kiran", "message": "Thats true but I am not able to figure how to launch a spider into scrapy deamon which isn't an egg", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:16:08.439762+00:00", "nick": "aaearon", "message": "what tutorial are you following", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:16:44.158790+00:00", "nick": "aaearon", "message": "id try something like this http://bgrva.github.io/blog/2014/04/13/deploy-c...", "links": ["http://bgrva.github.io/blog/2014/04/13/deploy-crawler-to-ec2-with-scrapyd/"], "channel": "scrapy"},
{"date": "2015-01-24T20:16:50.974532+00:00", "nick": "aaearon", "message": "maybe you are missing the initial deploy part", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:17:14.476606+00:00", "nick": "kiran", "message": "http://scrapyd.readthedocs.org/en/latest/deploy...", "links": ["http://scrapyd.readthedocs.org/en/latest/deploy.html"], "channel": "scrapy"},
{"date": "2015-01-24T20:17:57.327860+00:00", "nick": "aaearon", "message": "i think the act of deploying creates an egg?", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:18:10.306918+00:00", "nick": "aaearon", "message": "i followed the link i pasted and was able to use scrapyd successfully", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:18:17.321358+00:00", "nick": "aaearon", "message": "and i hadnt heard of an egg until tonight :)", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:18:30.699738+00:00", "nick": "aaearon", "message": "The simplest way to deploy your project is by using the deploy command, which automates the process of building the egg uploading it using the Scrapyd HTTP JSON API.", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:19:12.413671+00:00", "nick": "kiran", "message": "Alright", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:20:03.978888+00:00", "nick": "kiran", "message": "I am going through the link you sent now", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:25:31.317547+00:00", "nick": "kiran", "message": "Am I supposed to write the scrapy.cfg for each spider that I want to run ?", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:26:56.637508+00:00", "nick": "aaearon", "message": "no as .cfg is part of your scrapy project, which the spiders are a part of", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:27:18.975839+00:00", "nick": "aaearon", "message": "you are defining deployment targets for your project", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:28:14.549140+00:00", "nick": "kiran", "message": "This is how my directory structure looks like", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:28:15.278565+00:00", "nick": "kiran", "message": "http://pastebin.com/YdnEriBJ", "links": ["http://pastebin.com/YdnEriBJ"], "channel": "scrapy"},
{"date": "2015-01-24T20:28:30.111423+00:00", "nick": "kiran", "message": "I din't create a scrapy project at all", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:28:50.343832+00:00", "nick": "kiran", "message": "Just wrote individual spiders and ran them programmatically", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:29:29.873030+00:00", "nick": "aaearon", "message": "if your spiders are all 'related' i might stick them into one project", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:30:04.135675+00:00", "nick": "aaearon", "message": "the tutorial provides the command to create a project", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:30:11.826084+00:00", "nick": "aaearon", "message": "including structure", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:30:28.410074+00:00", "nick": "kiran", "message": "okay", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:31:51.539825+00:00", "nick": "kiran", "message": "individual spiders have their own pipelines that were not related so I had to run them seperately with the settings.set(settingsname, settingsvalue)", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:32:23.266805+00:00", "nick": "kiran", "message": "I am beginning to think there needs to be major changes that will need to be implemented now", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:32:49.493302+00:00", "nick": "aaearon", "message": "yeah you may have done it the hard way", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:33:09.751798+00:00", "nick": "kiran", "message": "ya", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:33:17.606915+00:00", "nick": "kiran", "message": "sucks", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:33:49.536126+00:00", "nick": "aaearon", "message": "if you want a pipeline to apply to a spider, just do something like 'if spider.name == 'AZCC':", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:34:24.342416+00:00", "nick": "kiran", "message": "ya that would be something I could do.", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:34:54.012555+00:00", "nick": "kiran", "message": "there are like 50 states for whom I would have to do that", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:35:16.189210+00:00", "nick": "aaearon", "message": "each with their own pipeline?", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:35:28.600807+00:00", "nick": "kiran", "message": "ya", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:35:42.572624+00:00", "nick": "kiran", "message": "Some are common pipelines", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:39:19.236836+00:00", "nick": "kiran", "message": "Heres how it actually looks like", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:39:22.033477+00:00", "nick": "kiran", "message": "http://pastebin.com/jBTK2xsd", "links": ["http://pastebin.com/jBTK2xsd"], "channel": "scrapy"},
{"date": "2015-01-24T20:39:43.514949+00:00", "nick": "kiran", "message": "I used core.py to run each spider from 1 script", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:40:36.406248+00:00", "nick": "kiran", "message": "settings.py contains some of the common spiders settings. and also which pipelines apply to each spider.", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:41:48.755826+00:00", "nick": "aaearon", "message": "you can pass settings when you run 'scrapy crawl'", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:43:08.195505+00:00", "nick": "kiran_", "message": "disconnted again.", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:43:35.755813+00:00", "nick": "aaearon", "message": "you can pass settings when you run 'scrapy crawl'", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:46:07.271134+00:00", "nick": "aaearon", "message": "but yes i would organize everything into a scrapy project", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:46:17.723241+00:00", "nick": "aaearon", "message": "you already have the hard stuff down. the scraping and custom pipelines", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:46:24.243271+00:00", "nick": "aaearon", "message": "just may have to do a little reoganizing/refactoring", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:46:36.955702+00:00", "nick": "kiran_", "message": "theres a lot of work actually", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:47:36.282317+00:00", "nick": "kiran_", "message": "But ya I dont think I will be able to escape the scrapy project", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:47:46.889371+00:00", "nick": "aaearon", "message": "i dont think you'll want to in the end", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:48:08.190414+00:00", "nick": "kiran_", "message": "true", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:48:10.832801+00:00", "nick": "kiran_", "message": "Thanks for the help", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:49:55.714378+00:00", "nick": "kiran_", "message": "I will start right away", "links": [], "channel": "scrapy"},
{"date": "2015-01-24T20:50:44.617490+00:00", "nick": "aaearon", "message": "no problemo", "links": [], "channel": "scrapy"},
{"date": "2015-01-25T19:24:54.587796+00:00", "nick": "lavalamp", "message": "hello folks", "links": [], "channel": "scrapy"},
{"date": "2015-01-25T19:25:21.323547+00:00", "nick": "lavalamp", "message": "i think ive found a bug in scrapy but wanted to check with you all first", "links": [], "channel": "scrapy"},
{"date": "2015-01-25T19:25:33.706677+00:00", "nick": "lavalamp", "message": "this an appropriate place to do that?", "links": [], "channel": "scrapy"},
{"date": "2015-01-25T19:28:48.231285+00:00", "nick": "lavalamp", "message": "welp - here goes", "links": [], "channel": "scrapy"},
{"date": "2015-01-25T19:28:53.504201+00:00", "nick": "lavalamp", "message": "Scrapy Response objects", "links": [], "channel": "scrapy"},
{"date": "2015-01-25T19:29:07.049464+00:00", "nick": "lavalamp", "message": "if i was to want to get cookie information from them, i have to go to response.headers", "links": [], "channel": "scrapy"},
{"date": "2015-01-25T19:29:10.695207+00:00", "nick": "lavalamp", "message": "which is a dict", "links": [], "channel": "scrapy"},
{"date": "2015-01-25T19:29:26.780758+00:00", "nick": "lavalamp", "message": "from there i would have to retrieve any headers whose keys match \"Set-Cookie\"", "links": [], "channel": "scrapy"},
{"date": "2015-01-25T19:29:37.247989+00:00", "nick": "lavalamp", "message": "only thing is, its a dict", "links": [], "channel": "scrapy"},
{"date": "2015-01-25T19:29:42.872405+00:00", "nick": "lavalamp", "message": "meaning there can only be one \"Set-Cookie\" value", "links": [], "channel": "scrapy"},
{"date": "2015-01-25T19:29:58.879405+00:00", "nick": "lavalamp", "message": "ive confirmed that even on sites that return multiple Set-Cookie headers, only one of them ever makes it to the Scrapy Response object", "links": [], "channel": "scrapy"},
{"date": "2015-01-25T20:20:14.193566+00:00", "nick": "sudshekhar", "message": "lavalamp: you might want to look at the cookiejar :http://doc.scrapy.org/en/master/topics/downloader-middleware.html#cookies-mw", "links": [], "channel": "scrapy"},
{"date": "2015-01-25T20:20:57.234206+00:00", "nick": "sudshekhar", "message": "I think it's used to handles cases like these (I haven't used it myself though)", "links": [], "channel": "scrapy"},
{"date": "2015-01-25T20:22:24.454820+00:00", "nick": "sudshekhar", "message": "I think response.meta['cookiejar'] should have the all the cookies.", "links": [], "channel": "scrapy"},
{"date": "2015-01-25T20:30:57.521723+00:00", "nick": "lavalamp", "message": "thanks sudshekhar - that's good to know. for my current endeavour i only want to know what cookies were set by the crawled page, but im sure i could use cookiejar to figure that out", "links": [], "channel": "scrapy"},
{"date": "2015-01-25T20:31:43.461628+00:00", "nick": "sudshekhar", "message": "glad to assist :)", "links": [], "channel": "scrapy"},
{"date": "2015-01-25T21:07:54.002740+00:00", "nick": "sudshekhar", "message": "lavalamp: are you able to check the cookies using the scrapy shell?", "links": [], "channel": "scrapy"},
{"date": "2015-01-25T21:08:43.894818+00:00", "nick": "sudshekhar", "message": "I was just trying out cookie jar but for some reason, I don't see any cookie data in either the response or request object while using 'scrapy shell url' command", "links": [], "channel": "scrapy"},
{"date": "2015-01-25T21:13:55.078854+00:00", "nick": "sudshekhar", "message": "okay never mind. It worked.", "links": [], "channel": "scrapy"},
{"date": "2015-01-26T11:17:28.204863+00:00", "nick": "aaearon", "message": "someone has a proxymesh middleware stashed away somewhere so i dont have to reinvent the wheel?", "links": [], "channel": "scrapy"},
{"date": "2015-01-26T11:37:40.850666+00:00", "nick": "barraponto", "message": "aaearon: http://proxymesh.com/http-client-examples/", "links": ["http://proxymesh.com/http-client-examples/"], "channel": "scrapy"},
{"date": "2015-01-26T11:37:49.120095+00:00", "nick": "barraponto", "message": "it specifically mentions how to setup scrapy", "links": [], "channel": "scrapy"},
{"date": "2015-01-26T11:38:17.048617+00:00", "nick": "aaearon", "message": "i was more referring to a middleware that uses proxymesh's custom headers to not use a particular proxy ip, etc", "links": [], "channel": "scrapy"},
{"date": "2015-01-26T13:36:53.635844+00:00", "nick": "AndyRez", "message": "Hi all, from a project, how do I save to a directory", "links": [], "channel": "scrapy"},
{"date": "2015-01-26T13:37:49.188889+00:00", "nick": "AndyRez", "message": "scrapy crawl \"spider-name\" -o file.csv -t csv /directory gives a crawl error..", "links": [], "channel": "scrapy"},
{"date": "2015-01-26T13:38:27.959655+00:00", "nick": "AndyRez", "message": "crawl: error: running 'scrapy crawl' with more than one spider is no longer supported", "links": [], "channel": "scrapy"},
{"date": "2015-01-26T13:39:16.159494+00:00", "nick": "aaearon", "message": "i dont think -t is a valid flag", "links": [], "channel": "scrapy"},
{"date": "2015-01-26T13:39:29.475740+00:00", "nick": "aaearon", "message": "oh it my apologies", "links": [], "channel": "scrapy"},
{"date": "2015-01-26T13:39:43.996401+00:00", "nick": "aaearon", "message": "i think you have to run scrapy crawl from the directory you want it to be saved to", "links": [], "channel": "scrapy"},
{"date": "2015-01-26T13:39:56.797548+00:00", "nick": "aaearon", "message": "or maybe try a full path when doing -o", "links": [], "channel": "scrapy"},
{"date": "2015-01-26T13:41:02.778760+00:00", "nick": "AndyRez", "message": "aaearon: not working..", "links": [], "channel": "scrapy"},
{"date": "2015-01-26T13:42:27.366169+00:00", "nick": "AndyRez", "message": "or doesn't that option works with a project?", "links": [], "channel": "scrapy"},
{"date": "2015-01-26T13:49:37.909826+00:00", "nick": "AndyRez", "message": "anyone?", "links": [], "channel": "scrapy"},
{"date": "2015-01-26T13:54:13.888732+00:00", "nick": "AndyRez", "message": ":/", "links": [], "channel": "scrapy"},
{"date": "2015-01-26T15:23:01.183670+00:00", "nick": "barraponto", "message": "AndyRez: aaearon: -t is a valid flag", "links": [], "channel": "scrapy"},
{"date": "2015-01-26T15:23:18.361672+00:00", "nick": "barraponto", "message": "AndyRez: -t is the \"template\". actually the file format", "links": [], "channel": "scrapy"},
{"date": "2015-01-26T15:23:23.209973+00:00", "nick": "AndyRez", "message": "barraponto: yeah I know its a valid tag", "links": [], "channel": "scrapy"},
{"date": "2015-01-26T15:23:24.039013+00:00", "nick": "barraponto", "message": "AndyRez: -o is the output path", "links": [], "channel": "scrapy"},
{"date": "2015-01-26T15:23:35.044293+00:00", "nick": "barraponto", "message": "AndyRez: so just pass it like -o /directory/file.csv", "links": [], "channel": "scrapy"},
{"date": "2015-01-26T15:23:41.299523+00:00", "nick": "AndyRez", "message": "but I am saying writing to a directory doesn't seem to work..", "links": [], "channel": "scrapy"},
{"date": "2015-01-26T15:23:44.812673+00:00", "nick": "AndyRez", "message": "barraponto: oh..", "links": [], "channel": "scrapy"},
{"date": "2015-01-26T15:23:48.300180+00:00", "nick": "AndyRez", "message": "let me try that...", "links": [], "channel": "scrapy"},
{"date": "2015-01-26T15:23:48.409364+00:00", "nick": "barraponto", "message": "actually", "links": [], "channel": "scrapy"},
{"date": "2015-01-26T15:23:55.502696+00:00", "nick": "barraponto", "message": "-o somepath/somefile.csv", "links": [], "channel": "scrapy"},
{"date": "2015-01-26T15:24:26.205915+00:00", "nick": "barraponto", "message": "don't put an initial slash or else you will try to write to the / folder (which is root-only in linux and shouldn't be messed with at all)", "links": [], "channel": "scrapy"},
{"date": "2015-01-26T15:25:02.809743+00:00", "nick": "AndyRez", "message": "barraponto: oh yeah.. I did", "links": [], "channel": "scrapy"},
{"date": "2015-01-26T15:25:04.955937+00:00", "nick": "AndyRez", "message": "thanks...", "links": [], "channel": "scrapy"},
{"date": "2015-01-26T15:25:11.780244+00:00", "nick": "AndyRez", "message": "It works now... Appreciate much..", "links": [], "channel": "scrapy"},
{"date": "2015-01-26T15:59:45.971620+00:00", "nick": "AndyRez", "message": "barraponto:", "links": [], "channel": "scrapy"},
{"date": "2015-01-26T16:05:49.367665+00:00", "nick": "barraponto", "message": "?", "links": [], "channel": "scrapy"},
{"date": "2015-01-26T18:54:02.481625+00:00", "nick": "careerharbour", "message": "Hello", "links": [], "channel": "scrapy"},
{"date": "2015-01-26T18:54:05.880753+00:00", "nick": "careerharbour", "message": "Any one here?", "links": [], "channel": "scrapy"},
{"date": "2015-01-26T18:56:58.383623+00:00", "nick": "careerharbour", "message": "I am new to programming but really wish to be able to use scrapy", "links": [], "channel": "scrapy"},
{"date": "2015-01-26T18:57:07.858388+00:00", "nick": "careerharbour", "message": "I noticed that it is heavily object oriented", "links": [], "channel": "scrapy"},
{"date": "2015-01-26T18:57:19.134328+00:00", "nick": "careerharbour", "message": "I dont really understand OOP", "links": [], "channel": "scrapy"},
{"date": "2015-01-26T18:57:23.660729+00:00", "nick": "careerharbour", "message": "anyone who can help?", "links": [], "channel": "scrapy"},
{"date": "2015-01-26T19:22:49.125236+00:00", "nick": "barraponto", "message": "careerharbour: OOP is easy, you don't have to worry about it.", "links": [], "channel": "scrapy"},
{"date": "2015-01-26T19:23:15.236713+00:00", "nick": "barraponto", "message": "careerharbour: also, scrapy is kind of callback oriented, more of a functional paradigm. but that's ok too.", "links": [], "channel": "scrapy"},
{"date": "2015-01-26T19:23:29.146036+00:00", "nick": "barraponto", "message": "careerharbour: what's your use case? i mean, what are you trying to scrape?", "links": [], "channel": "scrapy"},
{"date": "2015-01-26T19:28:29.034969+00:00", "nick": "careerharbour", "message": "barraponto: I am trying to scrape: www.pwc.co.uk", "links": ["http://www.pwc.co.uk"], "channel": "scrapy"},
{"date": "2015-01-26T19:28:33.660557+00:00", "nick": "careerharbour", "message": "Like top employers in London", "links": [], "channel": "scrapy"},
{"date": "2015-01-26T19:28:52.584280+00:00", "nick": "careerharbour", "message": "to automatically autofill the information of job seekers, my customers, onto their online application forms", "links": [], "channel": "scrapy"},
{"date": "2015-01-26T19:29:03.800836+00:00", "nick": "careerharbour", "message": "I already have an MVP: www.careerharbour.co.uk", "links": ["http://www.careerharbour.co.uk"], "channel": "scrapy"},
{"date": "2015-01-26T19:30:34.099187+00:00", "nick": "barraponto", "message": "careerharbour: that's interesting. you will sometimes face issues with javascript and captchas.", "links": [], "channel": "scrapy"},
{"date": "2015-01-26T19:30:41.354398+00:00", "nick": "careerharbour", "message": "Yes", "links": [], "channel": "scrapy"},
{"date": "2015-01-26T19:30:45.005191+00:00", "nick": "careerharbour", "message": "But not catchas", "links": [], "channel": "scrapy"},
{"date": "2015-01-26T19:30:48.406180+00:00", "nick": "careerharbour", "message": "captchas", "links": [], "channel": "scrapy"},
{"date": "2015-01-26T19:30:53.570965+00:00", "nick": "careerharbour", "message": "due to experience", "links": [], "channel": "scrapy"},
{"date": "2015-01-26T19:30:56.256412+00:00", "nick": "careerharbour", "message": "they dont have that", "links": [], "channel": "scrapy"},
{"date": "2015-01-26T19:30:59.736504+00:00", "nick": "careerharbour", "message": "javascript yes", "links": [], "channel": "scrapy"},
{"date": "2015-01-26T19:31:04.118096+00:00", "nick": "barraponto", "message": "there are ways to workaround it with splash.", "links": [], "channel": "scrapy"},
{"date": "2015-01-26T19:31:16.929878+00:00", "nick": "barraponto", "message": "i'm inclined to use other technologies too, like phantomjs/casperjs.", "links": [], "channel": "scrapy"},
{"date": "2015-01-26T19:32:26.533237+00:00", "nick": "careerharbour", "message": "Yes but I know Python and the other programmers are also more comfortable with this language", "links": [], "channel": "scrapy"},
{"date": "2015-01-26T19:33:20.002435+00:00", "nick": "barraponto", "message": "python rocks :)", "links": [], "channel": "scrapy"},
{"date": "2015-01-26T19:33:55.399903+00:00", "nick": "barraponto", "message": "i have not tried splash, actually. but i believe it will work just fine.", "links": [], "channel": "scrapy"},
{"date": "2015-01-27T00:52:40.633900+00:00", "nick": "sflw", "message": "Anyone have experience crawling a page like this where pagination is triggered by javascript rather than loading a new page: https://www.kickstarter.com/discover/advanced?s...", "links": ["https://www.kickstarter.com/discover/advanced?state=successful&amp=&category_id=12&amp=&woe_id=0&amp=&sort=magic%3F"], "channel": "scrapy"},
{"date": "2015-01-27T03:08:25.381830+00:00", "nick": "barraponto", "message": "sflw: yes", "links": [], "channel": "scrapy"},
{"date": "2015-01-27T13:26:29.236031+00:00", "nick": "bluedevil1995", "message": "hi! I just started using Scrapy to build my own crawler. I also want to contribute to it. How can I start?", "links": [], "channel": "scrapy"},
{"date": "2015-01-27T13:37:05.929453+00:00", "nick": "nikolaosk", "message": "subscribe to the scrapy-users mailing list", "links": [], "channel": "scrapy"},
{"date": "2015-01-27T13:37:12.385586+00:00", "nick": "nikolaosk", "message": "to the github notification", "links": [], "channel": "scrapy"},
{"date": "2015-01-27T13:37:14.422097+00:00", "nick": "nikolaosk", "message": "the blog", "links": [], "channel": "scrapy"},
{"date": "2015-01-27T13:37:25.608585+00:00", "nick": "nikolaosk", "message": "find issues", "links": [], "channel": "scrapy"},
{"date": "2015-01-27T13:58:01.889555+00:00", "nick": "barraponto", "message": "too bad bluedevil is gone :(", "links": [], "channel": "scrapy"},
{"date": "2015-01-28T17:29:04.811907+00:00", "nick": "e66", "message": "I have executed product_loader.add_xpath('price', '//div[@id=\"center_column\"]/script[2]/text()', re=\"var productPrice='([0-9.]+)'\")", "links": [], "channel": "scrapy"},
{"date": "2015-01-28T17:29:58.737232+00:00", "nick": "e66", "message": "now if price is < 500 shipping_cost would be 25 else 0. How can I use such expression on product_loader without query the price's xpath query again?", "links": [], "channel": "scrapy"},
{"date": "2015-01-28T18:18:28.689969+00:00", "nick": "Vooloo", "message": "I need a mini API inside a bigger projec that I can query locally. That mini API will scrape data from 3 different websites and consolidate this data and return it in JSON form. Also need to support proxies. Is scrapy ideal for this?", "links": [], "channel": "scrapy"}]